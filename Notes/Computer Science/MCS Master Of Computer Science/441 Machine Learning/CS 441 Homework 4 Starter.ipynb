{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7scc-8QJvE"
      },
      "source": [
        "# CS441: Applied ML - HW 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QagOldZDQJvG"
      },
      "source": [
        "## Part I: Clustering and Fast Retrieval\n",
        "\n",
        "Include all the code for Part 1 in this section"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install libomp-dev\n",
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "yzYp9WGPKPhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vO_W4UH7NNBo"
      },
      "outputs": [],
      "source": [
        "# initialization code\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "from scipy import stats\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "import faiss\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "# loads MNIST data and reformat to 768-d vectors with values in range 0 to 1\n",
        "# splits into train/val/test sets and provides indices for subsets of train\n",
        "def load_mnist():\n",
        "  (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "  x_train = np.reshape(x_train, (len(x_train), 28*28))\n",
        "  x_test = np.reshape(x_test, (len(x_test), 28*28))\n",
        "  maxval = x_train.max()\n",
        "  x_train = x_train/maxval\n",
        "  x_test = x_test/maxval\n",
        "  x_val = x_train[:10000]\n",
        "  y_val = y_train[:10000]\n",
        "  x_train = x_train[10000:]\n",
        "  y_train = y_train[10000:]\n",
        "  train_indices = dict()\n",
        "  train_indices['xs'] = np.arange(50)\n",
        "  train_indices['s'] = np.arange(500)\n",
        "  train_indices['m'] = np.arange(5000)\n",
        "  train_indices['all'] = np.arange(50000)\n",
        "  return (x_train, y_train), (x_val, y_val), (x_test, y_test), train_indices\n",
        "\n",
        "# displays a set of mnist images (can print 1 row OR 1 column)\n",
        "def display_mnist(x, subplot_rows=1, subplot_cols=1):\n",
        "  s = np.ceil(max(subplot_rows, subplot_cols)/10)\n",
        "  if subplot_rows>1 or subplot_cols>1:\n",
        "      fig, ax = plt.subplots(subplot_rows, subplot_cols, figsize=(15,15))\n",
        "      for i in range(len(x)):\n",
        "        ax[i].imshow(np.reshape(x[i], (28,28)), cmap='gray')\n",
        "        ax[i].axis('off')\n",
        "  else:\n",
        "      plt.imshow(np.reshape(x, (28,28)), cmap='gray')\n",
        "      plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "# counts the number of examples per class\n",
        "def class_count_mnist(y):\n",
        "  count = np.zeros((10,),dtype='uint32')\n",
        "  for i in np.arange(10):\n",
        "    count[i] = sum(y==i)\n",
        "  return count"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# example of using MNIST load, display, indices, and count functions\n",
        "(x_train, y_train), (x_val, y_val), (x_test, y_test), train_indices = load_mnist()\n",
        "display_mnist(x_train[:10],1,10)\n",
        "print('Total size: train={}, val={}, test ={}'.format(len(x_train), len(x_val), len(x_test)))\n",
        "print('Train subset size: xs={}, s={}, m={}, all={}'.format(len(train_indices['xs']),len(train_indices['s']),len(train_indices['m']),len(train_indices['all'])))\n",
        "print('Class count for s: {}'.format(class_count_mnist(y_train[train_indices['s']])))"
      ],
      "metadata": {
        "id": "-PwqmwHdhpox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Helper functions\n",
        "\n",
        "# Returns the purity of the clustering according to labels,\n",
        "# and the label counts of each cluster\n",
        "def get_purity(labels, cluster_idx):\n",
        "  nclasses = int(max(labels)+1)\n",
        "  nidx = int(max(cluster_idx)+1)\n",
        "  counts = np.zeros((nidx, nclasses), np.int32)\n",
        "  for i in range(len(labels)):\n",
        "    c = labels[i]\n",
        "    idx = cluster_idx[i]\n",
        "    counts[idx, c]+=1\n",
        "  purity = np.sum(np.max(counts, axis=1))/len(cluster_idx)\n",
        "  return purity, counts\n",
        "\n",
        "# Returns an index list that can be used to re-order the counts according to\n",
        "# which label is most common\n",
        "def get_cluster_order(counts):\n",
        "  idx = np.argmax(counts, axis=1)\n",
        "  idx = np.argsort(idx)\n",
        "  return idx"
      ],
      "metadata": {
        "id": "1kRnIo4EYCKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###*Problem* 1.1 Clustering"
      ],
      "metadata": {
        "id": "cOWXdvq5xGQ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSYmcXqYX8c-"
      },
      "outputs": [],
      "source": [
        "# set dimensions\n",
        "dim = x_train.shape[1]\n",
        "\n",
        "# Set the number of clusters\n",
        "clusters =  [10, 20, 30,40,50,60,70,80,90, 100]\n",
        "\n",
        "purity_k = []\n",
        "mean_k = []\n",
        "\n",
        "for K in clusters:\n",
        "  # Initialize the k-means clustering object\n",
        "\n",
        "\n",
        "  # Train the k-means model\n",
        "\n",
        "\n",
        "  # Assign each data point to a cluster\n",
        "\n",
        "\n",
        "  # Record and print Purity\n",
        "\n",
        "\n",
        "  #Get Cluster indexes & Display the centroids\n",
        "\n",
        "\n",
        "  # Record and print Average Distance\n",
        "\n",
        "\n",
        "#Plot K vs Purity and K vs Mean_Distances"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem 1.2 Fast Retrieval"
      ],
      "metadata": {
        "id": "RBAik2DFxYUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Brute Force (IndexFlatL2)\n",
        "\n",
        "# set dimensions\n",
        "dim = x_train.shape[1]\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Initialize the index for Brute Force\n",
        "\n",
        "# Add Data & Print time to add\n",
        "\n",
        "\n",
        "\n",
        "# Perform Search and Compute Time required\n",
        "\n",
        "\n",
        "# Compute Test Error\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DImlhRb9eZz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## LSH\n",
        "\n",
        "# set dimensions\n",
        "dim = x_train.shape[1]\n",
        "\n",
        "start_time = time.time()\n",
        "# Initialize the index for LSH\n",
        "\n",
        "# Add Data & Print time to add\n",
        "\n",
        "\n",
        "\n",
        "# Perform Search and Compute Time required\n",
        "\n",
        "\n",
        "# Compute Test Error\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0LhbG3lP62-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Estimating PDFs"
      ],
      "metadata": {
        "id": "tT6lkJ7kxgu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import faiss\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/drive/My Drive/CS441/hw4/\" # revise as needed\n",
        "im = cv2.imread(datadir + '3985783648_4702b45d13_c.jpg')\n",
        "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)/255  # converts to RGB ordering and values between 0 and 1\n",
        "crop = cv2.imread(datadir + '3985783648_4702b45d13_c_crop.jpg')\n",
        "crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)/255"
      ],
      "metadata": {
        "id": "4X46lGv_VFNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_score_maps(im, score_map, thresh):\n",
        "  '''\n",
        "    Input: im (h, w, 3): RGB image\n",
        "           score_map (h, w, 1) or (h*w, 1)\n",
        "           thresh: floating point threshold, typically between -2 and 2\n",
        "  '''\n",
        "  plt.imshow(im)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  plt.imshow(np.reshape(score_map, (im.shape[:2])))\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  plt.imshow(np.reshape(score_map>thresh, (im.shape[0], im.shape[1])), cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "  plt.imshow(np.tile(np.reshape(score_map>thresh, (im.shape[0], im.shape[1], 1)), (1,1,3))*im)\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Q_HJz3v4m-UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method 1 (Per-dimension discrete pdf)"
      ],
      "metadata": {
        "id": "syl2JjWXzm8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estimate discrete pdf\n",
        "def estimate_discrete_pdf(x, K, prior=1):\n",
        "  '''\n",
        "    Input: x(nx, ): an array of discrete values from 0 to K-1\n",
        "           K: number of possible discrete values\n",
        "           prior: initial count for each discrete value\n",
        "    Output: p(nvalues, ): p[i] is the estimated probability of discrete value i\n",
        "\n",
        "    This function can be used for Method 1 and Method 2\n",
        "  '''\n",
        "\n",
        "  p = np.ones(K,)*prior # initialize p\n",
        "  # TO DO:\n",
        "  # (1) loop through values\n",
        "  # (2) increment element of p corresponding to each value\n",
        "  # (3) divide by sum of p\n",
        "  return p"
      ],
      "metadata": {
        "id": "bAOp3rpuXvns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set number of bins K\n",
        "\n",
        "# reshape pixels to (h*w, 3)\n",
        "\n",
        "# convert continuous values to discrete values ranging from 0 to K-1\n",
        "# e.g. x-->min(int(x*K), K-1) if x ranges from 0 to 1\n",
        "\n",
        "# get pdf for each dimension using estimate_discrete_pdf\n",
        "\n",
        "# estimate score for each pixel in full image according to log pdfs\n",
        "\n",
        "# display"
      ],
      "metadata": {
        "id": "XgQxvRsmM7X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method 2 (K-means)"
      ],
      "metadata": {
        "id": "OLVhjPQPzreH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# set K\n",
        "\n",
        "# reshape pixels to (h*w, 3)\n",
        "\n",
        "# discretize all three color channels together using KMeans\n",
        "\n",
        "# get pdf over discrete values\n",
        "\n",
        "# estimate score for each pixel in full image according to log pdfs\n",
        "\n",
        "# display"
      ],
      "metadata": {
        "id": "1_5j_iarzeh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Method 3 (GMM)"
      ],
      "metadata": {
        "id": "JfiV39Mjzttg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "# reshape pixels to (h*w, 3)\n",
        "\n",
        "# get joint pdf using GMMs (choose number of components and other parameters)\n",
        "\n",
        "# estimate score for each pixel in full image according to log pdfs\n",
        "\n",
        "# display"
      ],
      "metadata": {
        "id": "zk9jeBXTzepC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: PCA and Data Compression"
      ],
      "metadata": {
        "id": "qFkxQPfIyAoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import faiss\n",
        "\n",
        "(x_train, y_train), (x_val, y_val), (x_test, y_test), train_indices = load_mnist()\n",
        "\n",
        "# Compute the principal components using x_train\n",
        "\n",
        "# Display First 10 Components\n"
      ],
      "metadata": {
        "id": "kdtJ3jroeFwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot of first two PCA dimensions\n",
        "import seaborn as sns  #sns.scatterplot(x, y, hue=labels)\n"
      ],
      "metadata": {
        "id": "6b2Au3iTbIZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot cumulative explained variance\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "i2zjIB2dbJCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select number of dimensions that explains 90% of variance, according to your plot above\n",
        "\n",
        "\n",
        "# Display the first 10 samples before and after PCA compression\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the index for Brute Force\n",
        "\n",
        "\n",
        "# Add Data & Print time to add\n",
        "\n",
        "\n",
        "# Perform Search and Compute Time required\n",
        "\n",
        "\n",
        "# Compute Test Error\n",
        "\n"
      ],
      "metadata": {
        "id": "K3rpOFiMbM1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Stretch Goals\n",
        "Include all your code used for part 4 in this section. You can copy-paste code from parts 1, 2 or 3 if it is re-usable."
      ],
      "metadata": {
        "id": "3X3j_efPhh6e"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5SnDgjKVdLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}