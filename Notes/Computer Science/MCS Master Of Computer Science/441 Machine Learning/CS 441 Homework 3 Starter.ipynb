{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd7scc-8QJvE"
      },
      "source": [
        "## CS441: Applied ML - HW 3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: CLIP: Contrastive Language-Image Pretraining\n",
        "Include all the code for Part 1 in this section"
      ],
      "metadata": {
        "id": "B3Bmk_x8P4uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Prepare data"
      ],
      "metadata": {
        "id": "b6JyJbuCVBSA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Here](https://drive.google.com/file/d/1zJ1KfymSfsbmD6QS-F0eUC8T1PkqW0_j/view?usp=sharing) is the json file you need for labels of flowers 102"
      ],
      "metadata": {
        "id": "85mSljTL-Q8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "import torch\n",
        "from torchvision.datasets import Flowers102\n",
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "C4nnOrdbVE_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "# datadir = \"/content/drive/My Drive/CS441/hw3/\"\n",
        "datadir = \".\"\n"
      ],
      "metadata": {
        "id": "UVFkDZ6eRNWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "758ObWpDeKvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_flower_data(img_transform=None):\n",
        "    if os.path.isdir(datadir+ \"flowers-102\"):\n",
        "      do_download = False\n",
        "    else:\n",
        "      do_download = True\n",
        "    train_set = Flowers102(root=datadir, split='train', transform=img_transform, download=do_download)\n",
        "    test_set = Flowers102(root=datadir, split='val', transform=img_transform, download=do_download)\n",
        "    classes = json.load(open(osp.join(datadir, \"flowers102_classes.json\")))\n",
        "\n",
        "    return train_set, test_set, classes"
      ],
      "metadata": {
        "id": "CpIrWhS8VM4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# READ ME!  This takes some time (a few minutes), so if you are using Colabs,\n",
        "#           first set to use GPU: Edit->Notebook Settings->Hardware Accelerator=GPU, and restart instance\n",
        "\n",
        "# Data structure details\n",
        "#   flower_train[n][0] is the nth train image\n",
        "#   flower_train[n][1] is the nth train label\n",
        "#   flower_test[n][0] is the nth test image\n",
        "#   flower_test[n][1] is the nth test label\n",
        "#   flower_classes[k] is the name of the kth class\n",
        "flower_train, flower_test, flower_classes = load_flower_data()"
      ],
      "metadata": {
        "id": "rceL8iFHVPWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(flower_train), len(flower_test)"
      ],
      "metadata": {
        "id": "sEgaoLcxVS8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display a sample in Flowers 102 dataset\n",
        "sample_idx = 0 # Choose an image index that you want to display\n",
        "print(\"Label:\", flower_classes[flower_train[sample_idx][1]])\n",
        "flower_train[sample_idx][0]"
      ],
      "metadata": {
        "id": "HMQXeEFzVXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2 Prepare CLIP model"
      ],
      "metadata": {
        "id": "eiNItn6aVb4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "XS2djYMFVgAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip"
      ],
      "metadata": {
        "id": "-GYOR8A8VhPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sets device to \"cuda\" if a GPU is available\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "# If this takes a really long time, stop and then restart the download\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)"
      ],
      "metadata": {
        "id": "nCREo-mfVk7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 CLIP zero-shot prediction"
      ],
      "metadata": {
        "id": "nqopdlSoVn-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"The following is an example of using CLIP pre-trained model for zero-shot prediction task\"\"\"\n",
        "# Prepare the inputs\n",
        "n = 200\n",
        "image, class_id = flower_train[n]\n",
        "image_input = clip_preprocess(image).unsqueeze(0).to(device) # extract image and put in device memory\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}, a type of flower.\") for c in flower_classes]).to(device) # put text to match to image in device memory\n",
        "\n",
        "# Calculate features\n",
        "with torch.no_grad():\n",
        "    image_features = clip_model.encode_image(image_input) # compute image features with CLIP model\n",
        "    text_features = clip_model.encode_text(text_inputs) # compute text features with CLIP model\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True) # unit-normalize image features\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True) # unit-normalize text features\n",
        "\n",
        "# Pick the top 5 most similar labels for the image\n",
        "similarity = (100.0 * image_features @ text_features.T) # score is cosine similarity times 100\n",
        "p_class_given_image= similarity.softmax(dim=-1)  # P(y|x) is score through softmax\n",
        "values, indices = p_class_given_image[0].topk(5) # gets the top 5 labels\n",
        "\n",
        "# Print the probability of the top five labels\n",
        "print(\"Ground truth:\", flower_classes[class_id])\n",
        "print(\"\\nTop predictions:\\n\")\n",
        "for value, index in zip(values, indices):\n",
        "    print(f\"{flower_classes[index]:>16s}: {100 * value.item():.2f}%\")\n",
        "image"
      ],
      "metadata": {
        "id": "JoroMsvhVqWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 YOUR TASK: Test CLIP zero-shot performance on Flowers 102"
      ],
      "metadata": {
        "id": "-xsgj_yIWA7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "P-1MRojmV_KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load flowers dataset again. This time, with clip_preprocess as transform\n",
        "flower_train_trans, flower_test_trans, flower_classes = load_flower_data(img_transform=clip_preprocess)"
      ],
      "metadata": {
        "id": "QVsfdPPzWKPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clip_zero_shot(data_set, classes):\n",
        "    data_loader = DataLoader(data_set, batch_size=64, shuffle=False)  # dataloader lets you process in batch which is way faster\n",
        "    # Needs code here\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "4C2Ut3sZWMZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = clip_zero_shot(data_set=flower_test_trans, classes=flower_classes)\n",
        "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "5_L3ntLcWazE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.5 YOUR TASK: Test CLIP linear probe performance on Flowers 102"
      ],
      "metadata": {
        "id": "hABMuuN6WwSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "pT4IF_4SWvj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In this part, train a linear classifier on CLIP features\n",
        "return: image features, labels in numpy format.\n",
        "\"\"\"\n",
        "def get_features(data_set):\n",
        "    # Needs code here\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "t9s_yFNEW0bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the image features\n",
        "train_features, train_labels = get_features(flower_train_trans)\n",
        "test_features, test_labels = get_features(flower_test_trans)\n",
        "\n",
        "# Perform logistic regression\n",
        "# Needs code here\n",
        "\n",
        "# Evaluate using the logistic regression classifier\n",
        "# Needs code here\n",
        "accuracy = ...\n",
        "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "yRFUbLtwXN05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.6 YOUR TASK: Evaluate a nearest-neighbor classifier on CLIP features"
      ],
      "metadata": {
        "id": "NWI6aAs2Xypt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "djgfYEvNX0bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(x_train, y_train, x_test, y_test, K=1):\n",
        "    # Needs code here\n",
        "\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "hqvEVvP8YVzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = knn(train_features, train_labels, test_features, test_labels, K=1)\n",
        "print(f\"\\nAccuracy = {100*accuracy:.3f}%\")"
      ],
      "metadata": {
        "id": "EqAESETdYaW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Fine-Tune for Pets Image Classification\n",
        "Include all the code for Part 2 in this section"
      ],
      "metadata": {
        "id": "aEqhjXvX2SEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Prepare Data"
      ],
      "metadata": {
        "id": "8VnGuQjt2yKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim.lr_scheduler as lrs\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "cdImCd6foYCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount and define data dir\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "datadir = \"/content/\"\n",
        "save_dir = \"/content/drive/My Drive/CS441/hw3\""
      ],
      "metadata": {
        "id": "qHqtzjWwpAG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pet_dataset(train_transform = None, test_transform = None):\n",
        "    OxfordIIITPet = datasets.OxfordIIITPet\n",
        "    if os.path.isdir(datadir+ \"oxford-iiit-pet\"):\n",
        "      do_download = False\n",
        "    else:\n",
        "      do_download = True\n",
        "    training_set = OxfordIIITPet(root = datadir,\n",
        "                             split = 'trainval',\n",
        "                             transform = train_transform,\n",
        "                             download = do_download)\n",
        "\n",
        "    test_set = OxfordIIITPet(root = datadir,\n",
        "                           split = 'test',\n",
        "                           transform = test_transform,\n",
        "                           download = do_download)\n",
        "    return training_set, test_set\n",
        ""
      ],
      "metadata": {
        "id": "4M4q5zIHoYKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = load_pet_dataset()\n",
        "\n",
        "# Display a sample in OxfordIIIPet dataset\n",
        "sample_idx = 0 # Choose an image index that you want to display\n",
        "print(\"Label:\", train_set.classes[train_set[sample_idx][1]])\n",
        "train_set[sample_idx][0]"
      ],
      "metadata": {
        "id": "C4TLZkEnLW1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Data Preprocess"
      ],
      "metadata": {
        "id": "8WxSQnR3oBI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "pVS7ycpYMXai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to add augmentation choices\n",
        "\n",
        "# Apply data augmentation\n",
        "train_transform = transforms.Compose([\n",
        "            transforms.Resize(224),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std= [0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "            transforms.Resize(224),  # resize to 224x224 because that's the size of ImageNet images\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std= [0.229, 0.224, 0.225]),\n",
        "        ])"
      ],
      "metadata": {
        "id": "2ycl4Q1uMBFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feel free to change\n",
        "train_set, test_set = load_pet_dataset(train_transform, test_transform)\n",
        "train_loader = DataLoader(dataset=train_set,\n",
        "                          batch_size=64,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_set,\n",
        "                          batch_size=64,\n",
        "                          shuffle=False,\n",
        "                          num_workers=2)\n"
      ],
      "metadata": {
        "id": "6fEFI_CVMA9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Helper Functions\n",
        "\n"
      ],
      "metadata": {
        "id": "i7wFK09AoEiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the number of parameters and model structure\n",
        "def display_model(model):\n",
        "  # Check number of parameters\n",
        "  summary_dict = {}\n",
        "  num_params = 0\n",
        "  summary_str = ['='*80]\n",
        "\n",
        "  for module_name, module in model.named_children():\n",
        "      summary_count = 0\n",
        "      for name, param in module.named_parameters():\n",
        "          if(param.requires_grad):\n",
        "              summary_count += param.numel()\n",
        "              num_params += param.numel()\n",
        "      summary_dict[module_name] = [summary_count]\n",
        "      summary_str+= [f'- {module_name: <40} : {str(summary_count):^34s}']\n",
        "\n",
        "  summary_dict['total'] = [num_params]\n",
        "\n",
        "  # print summary string\n",
        "  summary_str += ['='*80]\n",
        "  summary_str += ['--' +  f'{\"Total\":<40} : {str(num_params) + \" params\":^34s}' +'--']\n",
        "  print('\\n'.join(summary_str))\n",
        "\n",
        "  # print model structure\n",
        "  print(model)"
      ],
      "metadata": {
        "id": "kHgfXgw1HaZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot loss or accuracy\n",
        "def plot_losses(train, val, test_frequency, num_epochs):\n",
        "    plt.plot(train, label=\"train\")\n",
        "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0 or i == 1)]\n",
        "    plt.plot(indices, val, label=\"val\")\n",
        "    plt.title(\"Loss Plot\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_accuracy(train, val, test_frequency, num_epochs):\n",
        "    indices = [i for i in range(num_epochs) if ((i+1)%test_frequency == 0 or i ==0 or i == 1)]\n",
        "    plt.plot(indices, train, label=\"train\")\n",
        "    plt.plot(indices, val, label=\"val\")\n",
        "    plt.title(\"Training Plot\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def save_checkpoint(save_dir, model, save_name = 'best_model.pth'):\n",
        "    save_path = os.path.join(save_dir, save_name)\n",
        "    torch.save(model.state_dict(), save_path)\n",
        "\n",
        "def load_model(model, save_dir, save_name = 'best_model.pth'):\n",
        "    save_path = os.path.join(save_dir, save_name)\n",
        "    model.load_state_dict(torch.load(save_path))\n",
        "    return model"
      ],
      "metadata": {
        "id": "-poihz_hoN0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 YOUR TASK: Fine-Tune Pre-trained Network on Pets\n",
        "Read and understand the code and then uncomment it.  Then, set up your learning rate, learning scheduler, and train/evaluate. Adjust as necessary to reach target performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "mDkQzodjoIBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer):\n",
        "    \"\"\"\n",
        "    Train network\n",
        "    :param train_loader: training dataloader\n",
        "    :param model: model to be trained\n",
        "    :param criterion: criterion used to calculate loss (should be CrossEntropyLoss from torch.nn)\n",
        "    :param optimizer: optimizer for model's params (Adams or SGD)\n",
        "    :return: mean training loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    loss_ = 0.0\n",
        "    losses = []\n",
        "\n",
        "    # TO DO: read this documentation and then uncomment the line below; https://pypi.org/project/tqdm/\n",
        "    # it_train = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training ...\", position = 0) # progress bar\n",
        "    for i, (images, labels) in it_train:\n",
        "\n",
        "        # TO DO: read/understand and then uncomment these lines\n",
        "        #images, labels = images.to(device), labels.to(device)\n",
        "        #optimizer.zero_grad()\n",
        "        #prediction = model(images)\n",
        "        #loss = criterion(prediction, labels)\n",
        "        #it_train.set_description(f'loss: {loss:.3f}')\n",
        "        #loss.backward()\n",
        "        #optimizer.step()\n",
        "        #losses.append(loss)\n",
        "\n",
        "    return torch.stack(losses).mean().item()\n",
        "\n",
        "def test(test_loader, model, criterion):\n",
        "    \"\"\"\n",
        "    Test network.\n",
        "    :param test_loader: testing dataloader\n",
        "    :param model: model to be tested\n",
        "    :param criterion: criterion used to calculate loss (should be CrossEntropyLoss from torch.nn)\n",
        "    :return: mean_accuracy: mean accuracy of predicted labels\n",
        "             test_loss: mean test loss during testing\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # TO DO: read this documentation and then uncomment the line below; https://pypi.org/project/tqdm/\n",
        "    #it_test = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Validating ...\", position = 0)\n",
        "    for i, (images, labels) in it_test:\n",
        "\n",
        "      # TO DO: read/understand and then uncomment these lines\n",
        "      #images, labels = images.to(device), labels.to(device)\n",
        "      #with torch.no_grad():  # https://pytorch.org/docs/stable/generated/torch.no_grad.html\n",
        "      #  output = model(images)\n",
        "      #preds = torch.argmax(output, dim=-1)\n",
        "      #loss = criterion(output, labels)\n",
        "      #losses.append(loss.item())\n",
        "      #correct += (preds == labels).sum().item()\n",
        "      #total += len(labels)\n",
        "\n",
        "    mean_accuracy = correct / total\n",
        "    test_loss = np.mean(losses)\n",
        "    print('Mean Accuracy: {0:.4f}'.format(mean_accuracy))\n",
        "    print('Avg loss: {}'.format(test_loss))\n",
        "\n",
        "    return mean_accuracy, test_loss"
      ],
      "metadata": {
        "id": "1KxcnMexI22W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "# loads a pre-trained ResNet-34 model\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True)\n",
        "target_class = 37\n",
        "# TO DO: replace the last layer with a new linear layer for Pets classification\n",
        "\n",
        "model = model.to(device)\n",
        "display_model(model) # displays the model structure and parameter count\n"
      ],
      "metadata": {
        "id": "YvjJaCVMe47r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Setting. Feel free to change.\n",
        "num_epochs = 20\n",
        "test_interval = 5\n",
        "\n",
        "# TO DO: set initial learning rate\n",
        "learn_rate = []\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "# TO DO: define your learning rate scheduler, e.g. StepLR\n",
        "# https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.StepLR.html#torch.optim.lr_scheduler.StepLR\n",
        "lr_scheduler = []\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "train_losses = []\n",
        "train_accuracy_list = []\n",
        "test_losses = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "\n",
        "# Iterate over the DataLoader for training data\n",
        "for epoch in tqdm(range(num_epochs), total=num_epochs, desc=\"Training ...\", position=1):\n",
        "    train_loss = train(train_loader, model, criterion, optimizer) # Train the Network for one epoch\n",
        "    # TO DO: uncomment the line below. It should be called each epoch to apply the lr_scheduler\n",
        "    # lr_scheduler.step()\n",
        "    train_losses.append(train_loss)\n",
        "    print(f'Loss for Training on epoch {str(epoch)} is {str(train_loss)} \\n')\n",
        "\n",
        "    if(epoch%test_interval==0 or epoch==1 or epoch==num_epochs-1):\n",
        "        print('Evaluating Network')\n",
        "\n",
        "        train_accuracy, _ = test(train_loader, model, criterion) # Get training accuracy\n",
        "        train_accuracy_list.append(train_accuracy)\n",
        "\n",
        "        print(f'Training accuracy on epoch {str(epoch)} is {str(train_accuracy)} \\n')\n",
        "\n",
        "        test_accuracy, test_loss = test(test_loader, model, criterion) # Get testing accuracy and error\n",
        "        test_losses.append(test_loss)\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "\n",
        "        print(f'Testing accuracy on epoch {str(epoch)} is {str(test_accuracy)} \\n')\n",
        "\n",
        "        # Checkpoints are used to save the model with best validation accuracy\n",
        "        if test_accuracy >= max(test_accuracy_list):\n",
        "          print(\"Saving Model\")\n",
        "          save_checkpoint(save_dir, model, save_name = 'best_model.pth') # Save model with best performance\n"
      ],
      "metadata": {
        "id": "raaHUabTckg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Plotting of losses and accuracy"
      ],
      "metadata": {
        "id": "hMKf6AfaJ4jq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_losses(train_losses, test_losses, test_interval, num_epochs)\n",
        "plot_accuracy(train_accuracy_list, test_accuracy_list, test_interval, num_epochs)"
      ],
      "metadata": {
        "id": "eTz1EeJncj-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Evaluating trained model"
      ],
      "metadata": {
        "id": "pazjkAtaoKHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: initialize your trained model as you did before so that you can load the parameters into it\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet34', pretrained=True).to(device)\n",
        "# replace last layer\n",
        "\n",
        "load_model(model, save_dir) # Load the trained weight\n",
        "\n",
        "test_accuracy, test_loss= test(test_loader, model, criterion)\n",
        "print(f\"Testing accuracy is {str(test_accuracy)} \\n\")"
      ],
      "metadata": {
        "id": "kzIdvHhTJ1uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: No coding for this part"
      ],
      "metadata": {
        "id": "aLfzu_r7m4H5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4: Stretch Goals\n",
        "Include any new code needed for Part 3 here"
      ],
      "metadata": {
        "id": "3X3j_efPhh6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# example network definition that needs to be modified for custom network stretch goal\n",
        "\n",
        "class Network(nn.Module):\n",
        "    def __init__(self, num_classes=10, dropout = 0.5):\n",
        "        super(Network, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(64, 256, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(256 * 6 * 6, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, c, H, W = x.shape\n",
        "        features = self.features(x)\n",
        "        pooled_features = self.avgpool(features)\n",
        "        output = self.classifier(torch.flatten(pooled_features, 1))\n",
        "        return output"
      ],
      "metadata": {
        "id": "gudTxvWmbMlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5SnDgjKVdLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}