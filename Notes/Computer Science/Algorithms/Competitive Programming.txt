	~9999 Tasks In Competitive Programming.txt
	Lazar Ilic

The target of this document is to record many little tricks and insights from the broader competitive programming corpus in a highly legible terse little review document to assist in preparation for big important rounds. Notes and key ideas, as well as tasks and editorials. Perhaps I will aspire to trawling through ~9999 tasks and editorials, and write up relevant new-to-me cruxes. Many thanks to fellow web loggers.

Perhaps I will get around to programmatically scraping out AtCoder tasks with editorials in .tex so that a reader may simply scroll through sorted by reverse difficulty. Hardest first. Even consider doing this with C++ code as well demonstrating various uses of the AtCoder template library. Maybe it is sharper to suggest that competitors practise implementing rather than reading codes after editorials.

"
[AtCoder] Re: 1 Large Tasks And Editorials .pdf File

Ayano Tobaru (AtCoder)
 
From:
support@atcoder.zendesk.com
To:
Lazar Ilic

Tue, Nov 5 at 9:55 PM

##- 返信の際は、この行より上にご記入ください -##
Hi,
 
We sincerely apologize, but we are unable to do that.
 
本メールはAtCoderから送信されています。 配信元: Zendesk
"

	Number Theory
	Trees
	Tree Heaps
	More Esoteric Data Structures
	Web Logs
		Maxwell Zhang
		Oleksandr Kulkov
	Tasks

	Number Theory

AtCoder. Project Leonhard Euler.

Maxwell Zhang:
https://github.com/mzhang2021/cp-blog/blob/master/_posts/2021-09-04-burnside.md

"
Yet Another Competitive Programming Blog
Studies say your rating will increase exponentially after reading.

Burnside's Lemma
Disclaimer: I typed this up a few months prior to making this website as personal notes for myself, so it's possible that the article may be unclear in certain sections. If you have any tips on making certain sections clearer, feel free to comment below.

Burnside's lemma is a topic that's often explained in a group theory context and in a very abstract manner. Here I will attempt to document a more intuitive explanation of the lemma.

Main Idea
Consider the following simple problem: Given a cube, if we color each face one of three colors, how many rotationally distinct colorings are there?

The condition of being rotationally distinct means we have to account for an intractable amount of cases. For example, what if the opposite sides are the same color? Or what if this subset of sides are the same color? The casework gets tedious very quickly, so Burnside's Lemma helps us simplify that casework.

The statement of Burnside's Lemma:

|Classes|=1|G|∑π∈GI(π)

That's a very dense definition! In simple terms, we say the number of rotationally distinct colorings is equal to the average over all ways of transforming the object.

Wait what?
We use the example of a cube.

First, let's say our cube is locked in place, we cannot rotate it in any way. The number of colorings is then 36: we have 3 choices of colors for each of the 6 faces.

Now, let's consider the next method of transformation: the cube rotates 90∘ counterclockwise along the axis through the top and bottom face:

(It's difficult to draw 3D on a 2D plane, so I've shaded the two faces the axis goes through in green)

Essentially, we want to count the number of colorings of this cube, where each coloring maps to itself via a single 90∘ counterclockwise rotation about the red axis. I think these are called “fixed points.” Let's say we color the front face blue. Note that this means in all colorings of this group, the right face must also be blue. Why? Because all colorings must be able to “reach” one another with the power of a single 90∘ counterclockwise rotation. Applying this same argument repeatedly, we note that therefore all four lateral faces must be blue. So in total, there are 33 distinct colorings for this group: one color for the top face, one color for the bottom face, and one color for all four lateral faces.

Let's consider another transformation: a 90∘ clockwise rotation about the red axis. This, as you might guess, is the exact same as the previous case, so we get 33 colorings from this group.

Here's a more interesting transformation: a 180∘ rotation about the red axis (note I didn't specify clockwise or counterclockwise since they're the same). In this case, the front face must match the back face, but the front face need not match the right face, because there is no way to apply a 180∘ rotation to map the front face onto where the right face would be. Thus, we have 34 colorings for this group: top, bottom, left/right, and front/back.

We will continue these calculations with all transformative methods, with others such as rotating about the axis through the left and right face, or rotating about the axis through opposite corners. You can see all the calculations enumerated in this video. In total, there are 24 different groups. The number of rotationally distinct colorings ends up being 57.

Atcoder ABC 198F: Cube
Same problem as our cube coloring problem, but instead of colorings, we write numbers on each face such that they sum to a constant S≤1018.

Applying Burnside's Lemma will give us equations for which we have to count the number of solutions to. For example, a+b+4c=S. Many methods of counting solutions to such an equation become intractable when S is large or when we have many variables. A simple way is using matrix exponentiation: let dpS be the number of solutions to a+b+4c=S. Using the inclusion-exclusion principle, we get the recurrence dpS=dpS−1+dpS−1+dpS−4−dpS−2−dpS−5−dpS−5+dpS−6. Essentially, we add all ways based on choosing a,b, or c as our next variable to increase by 1. However, increasing a by 1, then b, versus increasing b by 1, then a, are equivalent, so we subtract out one occurrence of counting both a and b increasing by 1. Then, we over-subtracted out increasing a,b,c all by 1, so we add that back. The complexity of our solution is thus (logS), ignoring constant factors.

Colored Beads on Necklace
Say we have a necklace with N pearls, each which can be colored one of M different colors. How many rotationally distinct colorings are there? This problem is the example used in the Competitive Programmer's Handbook.

Our transformation possibilities are rotate 0, rotate 1, rotate 2, ..., rotate n−1 beads clockwise. With no rotation, the number of ways is MN. With one rotation, we reduce it down to just M ways, since each bead must share the same color as the next. With rotating by 2, we have M2 ways because all beads at even positions must share a color, and all beads at odd positions must share a color, but there is no relation between beads of different parity positions. In general, if we rotate by k
, we have Mgcd(k,N) ways.

To prove this, we essentially want to count the number of unique values of (i+kp)mod N for all positions 0≤i<N and p≥0. Let's find the minimum p until the position maps back to itself (i.e. kp = 0mod N ), which happens when p=N/gcd(k,N). One way of thinking about this is that k already contains some part of the prime factorization of N, namely gcd(k,N), and we need the other part of the factorization present in p. So if each component is of size N/gcd(k,N), then we have N/(N/gcd(k,N))=gcd(k,N) components.

Thus, our answer is

1N∑k=0N−1Mgcd(k,N)

Colored Beads on Necklace with Reflection
Same problem as previous, but now two necklaces are also identical if we allow reflections alongside rotations.

To make sure we're not accidentally counting the same transformation twice, we always reflect over the horizontal axis first, then rotate. So we have 2N different transformations. And when I reflect, that means positions i and N−i map onto each other, assuming zero-based indexing. There isn't an easy closed-form way of expressing this, but we don't have to use a closed form. If we use DSU to merge together indices that map onto each other after the transformation, we can just count the number of components at the end. So this actually gives an algorithmic way of counting the fixed points, which is easily generalized to other problems!

By the way, there is a closed form of expressing this. First, we add up ∑N−1k=0Mgcd(k,N) to account for rotation without reflection. Now for reflection, instead of reflecting and then rotating, we can just rotate the reflection axis itself. For odd N, we have N choices of a reflection axis, where each goes through exactly one bead and the center of the circle, giving us MN/2+1 ways to color (N/2 pairs formed over the reflection axis, plus 1 for the single bead that we initially chose). For even N, we have N/2 reflection axes that go through two beads, and N/2 reflection axes that go through no beads but are instead in between beads, giving us N/2⋅MN/2+1+N/2⋅MN/2 ways. Finally, we divide our total by 2N to get our answer.

References
https://www.youtube.com/watch?v=6kfbotHL0fs

https://cp-algorithms.com/combinatorics/burnside.html
"

	Sorting And Searching

Dropping out log[n] multiplicative factors. Taking prefix sums often. 2 pointers. Ternary search.

	Trees

Breadth First Search


Depth First Search


Meta Hacker Cup 2023 Round 2 Task C
Interesting. Firstly, it ought to be noted that one may compress up any path so that afterwards without loss of generality each node has >1 child. Now it follows from the usual natural bottoms up Breadth First Search Tree Dynamic Program strategy of generating hitting paths upwards towards the root that each relevant string must have multiplicity at least L, the number of leafs on the tree. So we may go through and filter down to those strings and then execute quite simply upon them.

Bonus Toy Contrived Variant[s]: querying 100000 nodes on a tree with 1000000 underlying couleurs appearing on each node in a vector of integer input under sum condition such that there are at most 2n nodes on each Breadth First Search Level n and then a really artificial kind of geometric exponential weight function looking for couleurs with overall total summation > y such that some insight relating to the summation of an arithmetico-geometric series leads to... a good strategy being querying the root node top node down in top down order to later deducing out other couleurs we do not query... perhaps contriving fudging the numbers around so that at most like ~10 or ~100 couleurs could be valid and we can rule out many early on in the search to decrease runtime but at the very least force some kind of a quantitative analysis of essentially the Pigeonhole Principle up front.

	Tree Heaps
	Treaps

Meta Hacker Cup 2024 Round 0 Task D2

"
This problem shares some similarities with problem D1, with key differences in bold.

Candice is playing a solitaire game of curling on a 1-dimensional sheet of ice, using stones that are 1 unit wide. She will throw N stones (numbered 1 to N) from position 0, targeting a position G units to the right. In curling, though we say a stone is “thrown”, it’s actually slid along the ice.

The ith stone will be thrown with energy E_i, and will travel E_i units to the right unless it collides with another stone, in which case it will transfer its remaining energy to the stone it hits. Formally, we repeat this process until all stones are stationary:

If the moving stone is at position p and there is a stationary stone at position p+1, the moving stone stops at position p, and the stone at position p+1 is now the moving stone with the remaining energy of the previous moving stone.

Otherwise, the moving stone moves 1 unit to the right and its energy is reduced by 1. If the moving stone now has energy 0, it becomes stationary.

After all of the stones are thrown, which stone is closest to the goal position G, and how far away from the goal is it?

Constraints
1≤T≤80
1≤N≤300,000
1≤G≤1,000,000
N≤E_i≤1,000,000

All stones are thrown with energy E_i≥N, so that stones do not pile up near Candice, but the energies are not necessarily unique.

The sum of N across all test cases is at most 2,000,000.

Input Format
Input begins with an integer T, the number of test cases. Each case starts with a line that contains the integers N and G. Then N lines follow, the ith of which contains E_i.

Output Format
For the ith test case, print "Case #i: " followed by the index of the stone that ends up closest to the goal, G, and how far away it is from G. If there’s a tie, output the stone with the lowest index.

Sample Explanation
The third sample case is depicted below. The first stone stops at position 8, and the second stone stops at position 7. The third stone starts with an energy of 9, but stops at position 6, transferring 3 energy to the second stone. The second stone is already touching the first stone, so it transfers 3 energy to the first stone, which then moves to position 11. The fourth stone starts with energy 6, and stops at position 5, transferring 1 energy to the next stone, which again transfers 1 energy to the next stone, which then moves to position 8. so the final positions of the stones are [11,8,6,5] with stone 2 being at position 8, the goal.
"

This task is extremely easy to simulate by binary searching each time for the relevant prefix to shift down 1 with an add-to-range operation followed by the relevant insertion. At the end update l and r if we are in the edge case where the target is outside of either bound then binary search for the left and right bounding indices around the target distance. And compute to output the index as each dude from right to left in the configuration must have been index 1, 2, 3, ... shot out from the initial origin. And the key here is easy if one has the code lying around for:

SuperMemo
POJ3580 Peking University Online Judge

Many such codes are lying around on the internet with different performances and legibility. I have a couple and could potentially hack around a few more to fit the usual [0,...] indexing versus the contrived [1,...] used in that task. It supports ADD adding to ranges, REVERSE reversing ranges, REVOLVE cyclically shifting a range by a desired quantity, INSERT insertion in to a desired index, DELETE deletion of a dude at a specified index, MIN find the minimum value on a range, and I think some extensions also implemented maybe supported computing indexof and orderof. So that is interesting. Are there non trivial treap tasks? Time to scour the literature on Competitive Programming and see...

NOI '05 P2 - Maintaining A Sequence
National Olympiad In Informatics, China

The USACO United States Of America Computing Olynpiad Guide solution from Finn Rudolph is OK. Somewhat legible. Seems like this is a canonical trick which appears often enough in data structures tasks so I might want to save that note over in the other files for very frequent chronic review... if this one ever does become size like 9999 or whatever.

USACO United States Of America Computing Olympiad Airplane Boarding

"
Analysis: Airplane Boarding by Steven Hao

We will compute the number of steps each cow takes before sitting down, starting with cow N and going down sequentially.

We will store a set of pairs of two integers. Each pair represents a seat and a time: a pair (3, 4) means that a cow wishing to move to seat infinity must pass by seat 3 at time 4.

For cow N, the set contains only (0, 0); the only restriction we have on cow N is that he must be at position 0 at time 0. For cow N - i, the set will contain at least (-i, 0) as cow N - i must be at position -i at time 0.

We can find the first time cow i can reach seat S[i] by searching the set for the pair (a, b) such that a < S[i] and b - a is maximized. If cow i is restricted by the pair (a, b), then cow i will reach seat S[i] at time b - a + S[i] and will sit down at time b - a + S[i] + T[i].

To maintain the set when transitioning from one cow to the next, we simply look at all the pairs the most recently inserted cow passed by and subtract 1 from the position. In other words: if cow i is in seat 5 at time 10, then cow i - 1 can not be past seat 4 at time 10.

This immediately yields a O(N^2) solution. We will store the set as an array of pairs.

For cow i, search for the pair (a, b) with the maximum value of b - a satisfying a < S[i]. Compute the time cow i sits down (V[i] = b - a + S[i] + T[i]). Then, insert the pair (S[i], V[i]) into the list (cow i must be at seat S[i] at time V[i]). To transition to cow i - 1, we perform a range update by replacing all (a, b) such that a <= S[i] with (a - 1, b). We then print out the maximum value of V taken over all cows.

The O(N log N) solution stores the set as a monotonic queue. We observe that we do not need to store two pairs (a, b) and (c, d) if a < c and b - a > d - c; the pair (c, d) will never be the maximum of any prefix. Furthermore, in any subset, the pair (a, b) with the maximum value of b - a is simply the pair with maximum a.

To maintain the monotonic queue, after inserting a pair (a, b), delete all pairs (c, d) with c >= a and d - c <= b - a. Note that the range update does not affect the order of the pairs within the monotonic queue.

We will need a data structure that can support the searching step, insertion step, and the range update step. A balanced binary search tree with lazy propagation can handle these each in O(log N) time, for a total runtime of O(N log N), which is fast enough for N = 200000. Below is my implementation using a skip-list, which is more or less equivalent to a BBST.
"

IOI International Olympiad In Informatics 2013 - Game

"
Author: Andi Qu

Table of Contents
Approach 1 - Sparse segment tree of BBSTs
Approach 2 - Memory-optimized 2D sparse segment tree
Time Complexity: $\mathcal O(Q \log R \cdot \log C)$

We're asked to handle point updates and range GCD queries on a 2D grid. This
implies that we should use a 2D range-query data structure like a 2D segment
tree (N.B. not a Fenwick tree, as the GCD function has no inverse).

In 1D ($C = 1$), this can be solved by a fairly straightforward use of a segment
tree: each node stores the GCD of its two children. Since $R$ can be quite big,
this needs to be a sparse segment tree; another alternative would be a balanced
binary tree like a treap.

However, a sparse 2D segment tree uses just a bit too much memory, and only
scores 80 points. Fortunately for us, there are two ways to get around this!

Approach 1 - Sparse segment tree of BBSTs

Although BBSTs use 4 times less memory than segment trees, a BBST of BBSTs (e.g. a range tree) is rather unpleasant to implement. However, a segment tree of BBSTs is much nicer to implement, and is good enough to score 100 points!

In my implementation below, I use an implicit treap because they support point updates and range queries. Each segment tree node stores a treap, and updating a node involves changing $\mathcal O(\log C)$ values in its treap (similar to updating a 2D segment tree node).

Approach 2 - Memory-Optimised 2D Sparse Segment Tree

Although the previous approach is somewhat simpler, this approach was intended,
and involves optimizing the memory usage of a sparse segment tree from
$\mathcal O(N \log(\text{size of range}))$ to $\mathcal O(N)$.

Essentially, we don't instantiate nodes in the segment tree if they are not leaf
nodes and only contain a single leaf node in their subtree, as those nodes are
redundant. What we end up with is a segment tree with $N$ leaves and $2N - 1$
nodes. See the sparse segment tree module
for more details.

Note that we can only apply this trick to the segment trees of the columns.
This means that the memory complexity is $\mathcal O(Q \log R)$, which is good
enough to score 100 points.
"

	More Esoteric Data Structures

	Web Logs

	Maxwell Zhang
	https://github.com/mzhang2021/cp-blog/tree/master/_posts

"
---
layout: post
title: "First Post"
tags: [intro]
featured: true
---

Welcome to my blog! I go by [smax](https://codeforces.com/profile/smax) on Codeforces and most other competitive programming sites, and this is a fun personal project for me that I've always wanted to do. On this blog, I'll post articles about techniques, tricks, opinions, or pretty much anything remotely related to CP. Do check it out if you're interested. This site is powered by <a href="https://jekyllrb.com/">Jekyll</a> and the <a href="https://github.com/wowthemesnet/mediumish-theme-jekyll">"Mediumish"</a> theme, so if you like how the site looks be sure to check those out.

<!-- For starters, here's a link to my first tutorial article on [Burnside's Lemma]({{ site.baseurl }}/burnside/). I typed this up a few months prior to making this website as personal notes for myself, so it's possible that the article may be unclear in certain sections. If you have any tips on making certain sections clearer, feel free to comment below.

If you're instead interested in something more opinionated, you can check out [this article]({{ site.baseurl }}/difficulty/) on what makes a CP problem hard. If you find you like the stuff I post, feel free to sign up for the mailing list by scrolling down and clicking on the alert, so that you get instantly notified whenever I post. -->

## FAQ
### I found your blog but have no idea what competitive programming is. What is it?
Competitive programming (or CP for short) refers to programming competitions where contestants try to solve challenging algorithm problems as quickly as possible. The problems often utilize ingenious applications of traditional algorithms and data structures or deploy slick math/logic tricks, making them very enjoyable to try to come up with solutions for. If you're thinking of trying it out, just make an account on [codeforces.com](https://codeforces.com) and try your hand at some problems at this [link](https://codeforces.com/problemset?order=BY_RATING_ASC). The column with numbers (such as 800) refers to the difficulty of the problem, so you can skip forward a few pages any time you're ready to move to the next difficulty level.

### Why not just post blogs on Codeforces? Why make a separate blog?
1. I'm trying to learn web development anyways, so I figured I'd make my first personal web dev project this blog.
2. Not everything I post here will be useful for the general public, and I don't want to spam on Codeforces forums. Occasionally, if I write some really useful tutorial or something, I might consider making a redirect post on Codeforces.

### How often will you post?
Whenever I have a good idea, I'll try to make a post. In practice it'll probably be once a month. You can subscribe to the mailing list by scrolling down and filling in the form in the alert so that you get notified whenever I post.

### How long will this blog run for?
I plan on keeping this blog alive for as long as I feel motivated, minimum one year after release so that this doesn't become yet another CP initiative advertised on Codeforces that dies after a few weeks.

### What's your setup?
I use [VS Code](https://code.visualstudio.com/) with some [scripts](https://github.com/mzhang2021/cp-library/tree/master/scripts/parse) for scraping test cases from online judges and running them.

### Do you have an implementation library?
[Yes.](https://github.com/mzhang2021/cp-library)

### I have a question that's not on here.
Ask in the comments, and I might update this post with it.

---
layout: post
title: "Burnside's Lemma"
tags: [tutorial, math]
usemathjax: true
---

**Disclaimer:** I typed this up a few months prior to making this website as personal notes for myself, so it's possible that the article may be unclear in certain sections. If you have any tips on making certain sections clearer, feel free to comment below.

---

Burnside's lemma is a topic that's often explained in a group theory context and in a very abstract manner. Here I will attempt to document a more intuitive explanation of the lemma.

## Main Idea

Consider the following simple problem: Given a cube, if we color each face one of three colors, how many **rotationally distinct** colorings are there?

The condition of being rotationally distinct means we have to account for an intractable amount of cases. For example, what if the opposite sides are the same color? Or what if this subset of sides are the same color? The casework gets tedious very quickly, so Burnside's Lemma helps us simplify that casework.

The statement of Burnside's Lemma:

$$
|\text{Classes}| = \frac{1}{|G|} \sum_{\pi \in G} I(\pi)
$$

That's a very dense definition! In simple terms, we say the number of rotationally distinct colorings is equal to the average over all ways of transforming the object.

## Wait what?

We use the example of a cube.

First, let's say our cube is locked in place, we cannot rotate it in any way. The number of colorings is then $3^6$: we have $3$ choices of colors for each of the $6$ faces.

Now, let's consider the next method of transformation: the cube rotates $90^\circ$ counterclockwise along the axis through the top and bottom face:

![image 1]({{ site.baseurl }}/assets/images/burnside-1.png)

(It's difficult to draw 3D on a 2D plane, so I've shaded the two faces the axis goes through in green)

Essentially, we want to count the number of colorings of this cube, where **each coloring maps to itself via a single $90^\circ$ counterclockwise rotation about the red axis.** I think these are called "fixed points." Let's say we color the front face blue. Note that this means in all colorings of this group, the right face must also be blue. Why? Because all colorings must be able to "reach" one another with the power of a single $90^\circ$ counterclockwise rotation. Applying this same argument repeatedly, we note that therefore all four lateral faces must be blue. So in total, there are $3^3$ distinct colorings for this group: one color for the top face, one color for the bottom face, and one color for all four lateral faces.

Let's consider another transformation: a $90^\circ$ clockwise rotation about the red axis. This, as you might guess, is the exact same as the previous case, so we get $3^3$ colorings from this group.

Here's a more interesting transformation: a $180^\circ$ rotation about the red axis (note I didn't specify clockwise or counterclockwise since they're the same). In this case, the front face must match the back face, but the front face need not match the right face, because there is no way to apply a $180^\circ$ rotation to map the front face onto where the right face would be. Thus, we have $3^4$ colorings for this group: top, bottom, left/right, and front/back.

We will continue these calculations with all transformative methods, with others such as rotating about the axis through the left and right face, or rotating about the axis through opposite corners. You can see all the calculations enumerated in [this video](https://youtu.be/6kfbotHL0fs?t=744). In total, there are $24$ different groups. The number of rotationally distinct colorings ends up being $57$.

## [Atcoder ABC 198F: Cube](https://atcoder.jp/contests/abc198/tasks/abc198_f)

Same problem as our cube coloring problem, but instead of colorings, we write numbers on each face such that they sum to a constant $S \leq 10^{18}$.

Applying Burnside's Lemma will give us equations for which we have to count the number of solutions to. For example, $a + b + 4c = S$. Many methods of counting solutions to such an equation become intractable when $S$ is large or when we have many variables. A simple way is using matrix exponentiation: let $dp_S$ be the number of solutions to $a + b + 4c = S$. Using the inclusion-exclusion principle, we get the recurrence $dp_S = dp_{S-1} + dp_{S-1} + dp_{S-4} - dp_{S-2} - dp_{S-5} - dp_{S-5} + dp_{S-6}$. Essentially, we add all ways based on choosing $a, b, $ or $c$ as our next variable to increase by $1$. However, increasing $a$ by $1$, then $b$, versus increasing $b$ by $1$, then $a$, are equivalent, so we subtract out one occurrence of counting both $a$ and $b$ increasing by $1$. Then, we over-subtracted out increasing $a, b, c$ all by $1$, so we add that back. The complexity of our solution is thus $\mathcal O(\log S)$, ignoring constant factors.

## Colored Beads on Necklace

Say we have a necklace with $N$ pearls, each which can be colored one of $M$ different colors. How many rotationally distinct colorings are there? This problem is the example used in the Competitive Programmer's Handbook.

Our transformation possibilities are rotate $0$, rotate $1$, rotate $2$, $\dots$, rotate $n - 1$ beads clockwise. With no rotation, the number of ways is $M^N$. With one rotation, we reduce it down to just $M$ ways, since each bead must share the same color as the next. With rotating by $2$, we have $M^2$ ways because all beads at even positions must share a color, and all beads at odd positions must share a color, but there is no relation between beads of different parity positions. In general, if we rotate by $k$, we have $M^{\gcd(k, N)}$ ways.

To prove this, we essentially want to count the number of unique values of $(i + kp) \mod N$ for all positions $0 \leq i < N$ and $p \geq 0$. Let's find the minimum $p$ until the position maps back to itself (i.e. $kp = 0 \mod N$), which happens when $p = N / \gcd(k, N)$. One way of thinking about this is that $k$ already contains some part of the prime factorization of $N$, namely $\gcd(k, N)$, and we need the other part of the factorization present in $p$. So if each component is of size $N / \gcd(k, N)$, then we have $N / (N / \gcd(k, N)) = \gcd(k, N)$ components.

Thus, our answer is

$$
\frac{1}{N} \sum_{k=0}^{N-1} M^{\gcd(k, N)}
$$

## Colored Beads on Necklace with Reflection

Same problem as previous, but now two necklaces are also identical if we allow reflections alongside rotations.

To make sure we're not accidentally counting the same transformation twice, we always reflect over the horizontal axis first, then rotate. So we have $2N$ different transformations. And when I reflect, that means positions $i$ and $N - i$ map onto each other, assuming zero-based indexing. There isn't an easy closed-form way of expressing this, but we don't have to use a closed form. If we use DSU to merge together indices that map onto each other after the transformation, we can just count the number of components at the end. So this actually gives an algorithmic way of counting the fixed points, which is easily generalized to other problems!

By the way, there is a closed form of expressing this. First, we add up $\sum_{k=0}^{N-1} M^{\gcd(k, N)}$ to account for rotation without reflection. Now for reflection, instead of reflecting and then rotating, we can just rotate the reflection axis itself. For odd $N$, we have $N$ choices of a reflection axis, where each goes through exactly one bead and the center of the circle, giving us $M^{N/2 + 1}$ ways to color ($N / 2$ pairs formed over the reflection axis, plus $1$ for the single bead that we initially chose). For even $N$, we have $N / 2$ reflection axes that go through two beads, and $N / 2$ reflection axes that go through no beads but are instead in between beads, giving us $N / 2 \cdot M^{N / 2 + 1} + N / 2 \cdot M^{N / 2}$ ways. Finally, we divide our total by $2N$ to get our answer.

## References

[https://www.youtube.com/watch?v=6kfbotHL0fs](https://www.youtube.com/watch?v=6kfbotHL0fs)

[https://cp-algorithms.com/combinatorics/burnside.html](https://cp-algorithms.com/combinatorics/burnside.html)

[Competitive Programmer's Handbook](https://github.com/pllk/cphb/)

---
layout: post
title: "What Makes a Problem Hard?"
tags: [opinion]
usemathjax: true
---

Just go to the comment section of any announcement blog for a contest, and you'll likely see a comment like "A < C < B" or "E was much easier than D." Or look at any leaderboard, and there's always at least a few contestants who solve the problems out of order or skip some problem. Problem difficulty is a very subjective thing, and it's always near impossible to create a set of problems such that A is strictly easier than B, which is strictly easier than C, and so on. Why is that the case, and what makes a problem difficult?

## Knowledge
The difficulty of the algorithms and concepts used for a problem definitely contribute to problem difficulty. For instance, greedy is a very fundamental concept, and applying the "greedy" tag in Codeforces problemset yields problems of all difficulties from 800 to 3500. On the other hand, applying the "flows" tag quickly brings problems up to the 1800-2000+ range, and applying the "fft" tag almost guarantees the problem is at least 2000+ in rating, because of the complexity of these algorithms (at the time of writing this, there is an 800 problem marked with "flows", but I am almost certain that is a troll).

## Search Space
This aspect is one that I think gets overlooked the most when people claim a problem is too easy for its spot in a contest. When you approach a problem, there are a lot of ways you can go about it, and a lot of potential approaches to try. Not all approaches will lead to the solution. Take [this problem](https://codeforces.com/contest/1491/problem/F) for example. It's a 2700 rated interactive problem. And if you've read the editorial or know the solution, you might conclude that the problem should be rated way lower because the solution is so elementary. But the difficulty of this problem doesn't come the complexity of the concepts used, but the sheer size of its search space. I did this contest live, and I remember trying all sorts of different ways to take advantage of the $\lfloor \log_2{n} \rfloor$ bound in queries, some very close to the editorial idea.

## Number of Observations/Steps
Partly related to search space is the number of layers of observation you have to get through to solve the problem. Div 2 As and Bs typically only have one key observation that cracks the entire problem, which is why they can be solved in mere minutes by high rated contestants and have low ratings (as an aside, an unfortunate consequence of these problems is that sometimes solving them can be dumb luck because of how unmotivated the observations can be, and even a high rated contestant could get "stuck" on one if they just happen to miss the observation). On the other hand, a more difficult problem often contains multiple layers of reduction to get to the final solution.

The example I'll use is [Falling Sand](https://codeforces.com/contest/1534/problem/F2), which is a problem with several steps. In my opinion, each individual step is not difficult, but it can be difficult to assemble all of the necessary steps during a live contest. The solution breaks down as follows:

<div class="spoiler">
<ol>
<li>Recognize that the relation between the blocks of sand can be modeled as a directed graph, where block A can reach block B if perturbing block A will cause it to perturb block B when falling.</li>
<li>The directed graph actually only contains $\mathcal O(nm)$ edges, as it is sufficient to draw an edge from a block to the closest block below it in its adjacent columns.</li>
<ol>
<li>The above two observations are sufficient to solve F1. In F1, we just need to make all the sand blocks fall, so after compressing the graph into a DAG of SCCs, we can count the number of nodes with an indegree of 0 as our answer.</li>
</ol>
<li>In F2, we don't need to make all the sand blocks fall, just $a_i$ in each column. It is always optimal to prioritize making the bottommost $a_i$ blocks fall in each column $i$, as those blocks will always fall anyways in any optimal solution.</li>
<li>Let's consider the set of nodes with indegree 0 in the DAG after condensing into SCCs, denoted as $S$. Obviously it is optimal to only perturb nodes in $S$ directly, because if we perturb some lower node, we could have perturbed one of its ancestors instead and achieved the same effect (yeah I know I'm using the words "node" and "block" interchangeably). Now let's consider the nodes that need to be perturbed. Notice that the subset of $S$ which can reach that node, directly or indirectly, are always contiguous if $S$ is sorted by column. If a node in column $i$ can reach our target node, and a node in column $j > i$ can also reach our target node, some node in $S$ in column $k$ such that $i < k < j$ would also be able to reach our target node.</li>
<ol>
<li>To understand why that is true, notice that the structure of how blocks get perturbed is always "hit all blocks in this and adjacent columns," so there's a "path" of contiguous nodes from column $i$ to column $j$ for some node in column $i$ to be able to reach column $j$. Since the nodes in $S$ are always the highest nodes in their column, there's no way they would not be able to reach the "path" if they were in some column $k$ such that $i < k < j$. To understand what I'm talking about, refer to the diagram below (red blocks are in $S$, green is our target block):</li>
</ol>
<img src="{{ site.baseurl }}/assets/images/difficulty-1.png" alt="image 1">
<li>Therefore, the problem reduces down to picking the minimum number of nodes in $S$ to cover all intervals. This is a well known problem with a greedy solution: sort the intervals by right endpoint, and whenever an interval is not covered, increase the answer by 1 and pick the node on the interval's right endpoint.</li>
</ol>
</div>

## Combining Search Space and Number of Observations
Let's say the search space of a problem is a graph. You start at the beginning, and you want to reach the solution by traversing through a path of nodes by making a series of observations. Then a larger search space refers to larger degree nodes, and more observations means the path to the solution is longer. So an easy problem might look something like this:

![image 2]({{ site.baseurl }}/assets/images/difficulty-2.png)

or this:

![image 3]({{ site.baseurl }}/assets/images/difficulty-3.png)

The first diagram is what a Div 2 A might look like: there's one observation, and if you find it you solve the problem. The second diagram is what an easy DS problem might look like: there might be multiple steps, but each step is very obvious to get to. Naturally, a hard problem both has high degree nodes and a long path:

![image 4]({{ site.baseurl }}/assets/images/difficulty-4.png)

If you're a strong competitive programmer with lots of problem solving expertise, the problem search space might look more like this to you:

![image 5]({{ site.baseurl }}/assets/images/difficulty-5.png)

The red Xs are a result of experience: you know which ideas tend to work better, and you know how to approach certain types of problems.

And to complete this analogy, if you're missing knowledge or experience, the problem might look like this to you:

![image 6]({{ site.baseurl }}/assets/images/difficulty-6.png)

You're missing edges altogether, so you might not even be able to solve the problem, but those edges will appear with practice and time.

## Implementation
There's one aspect of problem solving I have not mentioned yet: after coming up with the idea, you still need to write the code! So what makes implementation difficult?
1. Lines of Code - When I say lines of code, I exclude templated code. Take [Good Graph](https://codeforces.com/contest/1555/problem/F) as an example. Most submissions have lines of code in 3 digits, but a large portion of this problem is just copy pasting heavy light decomposition or segment tree, so the actual difficulty of implementation is low. On the flip side, [Binary Table](https://codeforces.com/contest/1439/problem/A2) was a problem with a notoriously long and difficult implementation for its spot in the contest. It's not too bad for experienced contestants, but it's definitely way more implementation heavy than a Div 1 A should be.
2. Finnicky Details - I've encountered plenty of problems where working out the indexing was a pain in the ass, or the problem had many cases to consider. In recent memory, [Minimax](https://codeforces.com/contest/1530/problem/E) had an obnoxious amount of cases, and my [implementation](https://codeforces.com/contest/1530/submission/122824719) in contest ended up being extremely messy. It's honestly a miracle that I could even get it to work at all. [Funny Substrings](https://codeforces.com/contest/1538/problem/E) was another problem where I had a disgusting [implementation](https://codeforces.com/contest/1538/submission/119011583) in contest that demanded a high attention to detail, although this one was more my fault for overlooking a simpler solution.
3. Constant Factor Optimization - It's always frustrating when you have the correct complexity, but your solution is too slow because of unnecessarily tight limits set by the authors. There are ways to minimize this happening by adopting certain strategies. For example, I always pass non-primitives by const reference to functions, and I avoid STL data structures when not needed because C++ STL data structures like `std::set` and `std::map` consume a non-trivial amount of overhead.

## So back to the original question...
Different people have different opinions on problem difficulty because they've practiced and become more experienced in different skills. Each person's graph for the search space will look different. And the problemsetter's opinion of difficulty often differs drastically from competitors because they sometimes come up with the solution before the problem, or craft the problem with a specific solution in mind already.

## The Takeaway
Is there a takeaway from this article? I don't know. I'm moreso just expressing an opinion on a subject that I've given some thought about. That being said, I do think the way I think about problem solving provides some insight into how I think improving at CP works: as you practice, you expand the number of edges you can access, and you also get better at pruning bad edges. Maybe in the future I'll write an article on how to practice. Thanks for reading, and feel free to leave your thoughts on this subject in the comments below.

---
layout: post
title: "Applications of Divide and Conquer"
tags: [tutorial, algo]
usemathjax: true
---

Divide and conquer (D&Q for short) is a common and powerful problem solving paradigm in the world of algorithms. There are numerous well-known examples such as merge sort and fast Fourier transform, and in this article I will cover some (maybe less common) applications of D&Q that are more specific to competitive programming. The sections progress from simple to complex, so more experienced competitors can skip to a later section of the article.

## Core Concept
The core concept of D&Q is to divide our problem into subproblems, then combine them together in some way. Consider the classic example of merge sort, a D&Q algorithm to sort an array. Let's write a recursive function `sort(a)` which will sort the array $a$. First, we can split our array into two halves and sort each of those halves recursively with `sort(left)` and `sort(right)`:

```c++
void sort(vector<int> &a) {
    int n = (int) a.size();
    if (n == 1)
        return;
    int m = n / 2;
    // two halves denoted "left" and "right"
    vector<int> left, right;
    for (int i=0; i<m; i++)
        left.push_back(a[i]);
    for (int i=m; i<n; i++)
        right.push_back(a[i]);
    sort(left);
    sort(right);
    // now we need to combine the result of the two halves in some way
}
```

Then, after we've sorted the two individual halves, we can merge them together in linear time with the following algorithm:

```c++
// merges two sorted arrays into one sorted array
int n = (int) left.size(), m = (int) right.size(), i = 0, j = 0;
vector<int> comb;
while (i < n || j < m) {
    if (i < n && (j == m || left[i] < right[j]))
        comb.push_back(left[i++]);
    else
        comb.push_back(right[j++]);
}
```

<details markdown="1" style="margin-bottom: 5%"><summary>Complete Mergesort Code</summary>

```c++
vector<int> merge(const vector<int> &left, const vector<int> &right) {
    int n = (int) left.size(), m = (int) right.size(), i = 0, j = 0;
    vector<int> comb;
    while (i < n || j < m) {
        if (i < n && (j == m || left[i] < right[j]))
            comb.push_back(left[i++]);
        else
            comb.push_back(right[j++]);
    }
    return comb;
}

void sort(vector<int> &a) {
    int n = (int) a.size();
    if (n == 1)
        return;
    int m = n / 2;
    vector<int> left, right;
    for (int i=0; i<m; i++)
        left.push_back(a[i]);
    for (int i=m; i<n; i++)
        right.push_back(a[i]);
    sort(left);
    sort(right);
    a = merge(left, right);
}
```
</details>

In general, the code for any divide and conquer algorithm might look something like this:

```
solve(whole part) {
    solve(left part)
    solve(right part)
    combine(result of solve(left part), result of solve(right part))
}
```

Analyzing the time complexity of a D&Q algorithm isn't always obvious, and the most common way it's done in literature is using the [master theorem](https://en.wikipedia.org/wiki/Master_theorem_(analysis_of_algorithms)). However, I personally don't find the master theorem very intuitive, so I prefer to just analyze the complexity by looking at the recursive tree of states. To explain what I mean, take a look at the following diagram for merge sort:

![image 1]({{ site.baseurl }}/assets/images/divide-and-conquer-1.png)
<div style="text-align: center; margin-bottom: 5%"><em>Image courtesy of <a href="https://www.interviewbit.com/tutorial/merge-sort-algorithm/">InterviewBit</a></em></div>

To understand the complexity of merge sort, we note that the height of the recursive tree is at most $\mathcal O(\log n)$, because on each layer the size of the subarrays we are working with get halved, and we can only half an array of size $n$ at most $\log_2 n$ times. On each layer, we are merging together two subarrays from the next layer, and the sum of all subarray sizes is the size of the original array, or $n$. Since we do $\mathcal O(n)$ work on each of $\mathcal O(\log n)$ layers, the complexity of merge sort is $\mathcal O(n \log n)$.

This was a traditional simple example, so now let's jump into some harder problems!

## [Codeforces Edu 94E: Clear the Multiset](https://codeforces.com/problemset/problem/1400/E)

The problem essentially translates to the following: given an array, we can perform two types of operations:
1. Subtract 1 from some subarray of non-zero values.
2. Subtract any positive $x$ from any one value.

What is the minimum number of operations to reduce the array to all 0s?

There is an approach to this problem using dynamic programming, but let's consider a D&Q approach. Let's denote `solve(l, r)` as the minimum number of operations to reduce the subarray $a[l, r]$ to 0. For now, assume $a_i > 0$. An upper bound on our answer is `r - l + 1` by applying the type 2 operation to reduce each array element to 0. Now what if we want to perform type 1 operations? Note that it's always optimal to apply type 1 operations to the entire subarray, because that gets us closer to reducing all of them to 0. It is never optimal to use type 1 operations on some subset of the subarray, then type 2 operations in between, because we can achieve the same effect by applying type 1 operations to the entire array when possible and use less operations. We can apply type 1 operations until some element gets reduced to 0, which will first be the minimum element. After some index $a_m$ gets reduced to 0, we can compute and add `solve(l, m - 1)` and `solve(m + 1, r)` as subproblems. The code looks as follows:

```c++
int solve(int l, int r) {
    if (l > r)
        return 0;
    // we will apply type 1 operation (min a_i) times
    int mn = *min_element(a.begin() + l, a.begin() + r + 1), idx = -1;
    for (int i=l; i<=r; i++) {
        a[i] -= mn;
        if (a[i] == 0)
            idx = i;    // split at the index that drops to 0 (notice that ties don't matter)
    }
    assert(idx != -1);
    // return minimum of our two options: solve recursively, or just apply type 2 operations to each element
    return min(solve(l, idx - 1) + solve(idx + 1, r) + mn, r - l + 1);
}
```
<details markdown="1" style="margin-bottom: 5%"><summary>An Aside</summary>
Notice that this code allows applying type 2 operations on some $a_i = 0$, which is technically not allowed. Luckily, we will never do that in the optimal solution anyways, so it's ok that our code permits this possibility. This is actually a very common trick in CP: relaxing the conditions of the problem to ease the implementation because we know the optimal solution will follow a stricter set of conditions. [This problem](https://codeforces.com/contest/1473/problem/E) is another example of that trick (though it's not a D&Q problem).
</details>

One question left: what's the time complexity? One way of bounding the time complexity is doing the same thing we did in our merge sort example: bound the height of the recursive tree and the amount of work we perform on each height. The worst case would be doing uneven splits every time (i.e. always having the minimum element be $l$): $[1, n] \implies [2, n] \implies [3, n] \implies \dots$, giving us $\mathcal O(n)$ height. On each layer, we iterate over all the elements in the array at most once in one of the states on that layer, giving us $\mathcal O(n)$ work per layer, so our complexity is $\mathcal O(n^2)$, which is fast enough for the constraints of this problem.

Another approach is simply counting the number of total states. A naive bound would be $\mathcal O(n^2)$ states, since there are $\frac{n(n+1)}{2}$ possible subarrays, but we can prove a better bound. Consider this crudely drawn recursive tree:

![image 2]({{ site.baseurl }}/assets/images/divide-and-conquer-2.png)
<div markdown="1" style="text-align: center; margin-bottom: 5%">A circle with some $[l, r]$ written on it represents the recursive state `solve(l, r)`.
</div>

In this diagram, I actually include the minimum index still, so instead of splitting $[l, r]$ into $[l, m - 1]$ and $[m + 1, r]$, I split it into $[l, m]$ and $[m + 1, r]$. I do this because this is a general proof that extends to other problems, and including $m$ in one of the intervals can only make the complexity worse, so the proof still applies to this problem.

Now here's the intuition: let's pretend indices $1, 2, \dots, n$ actually refer to nodes in a graph. The state $[l, r]$ refers to a connected component of vertices $l, l + 1, \dots, r$. And initially, we have $[1, n]$ representing a line graph with edges connecting $(i, i + 1)$ for all $1 \leq i < n$, or $n - 1$ total edges. What does a split of a state into two other states represent in this analogy? It represents deleting some edge $(m, m + 1)$ and splitting the component into two separate ones. And since we can only delete at most $n - 1$ edges, we only have at most $2(n - 1) + 1$ states, or $\mathcal O(n)$ states! A naive bound on the amount of work we do at each state is $\mathcal O(n)$, so the complexity is $\mathcal O(n^2)$.

Notice that our second way of analyzing the complexity more easily lends itself to a subquadratic solution. Instead of naively iterating to find the minimum index, we can use a [RMQ sparse table](https://cp-algorithms.com/data_structures/sparse-table.html#toc-tgt-3) to instantly find it in $\mathcal O(1)$. And instead of subtracting `mn` from each index like in the code above, we notice that we're simply subtracting `mn` from every array element, so we can instead maintain some extra parameter `delta` in our recursive method to keep track of how much we've subtracted from all elements in our current range. So we've now reduced the amount of work done at each state to $\mathcal O(1)$, giving us an $\mathcal O(n)$ or $\mathcal O(n \log n)$, depending on how we precompute our sparse table.

<details markdown="1"><summary>$\mathcal O(n)$ Code</summary>

```c++
int solve(int l, int r, int delta) {
    if (l > r)
        return 0;
    // we will apply type 1 operation (min a_i) times
    int idx = sparse_table.query(l, r), mn = a[idx] - delta;
    // return minimum of our two options: solve recursively, or just apply type 2 operations to each element
    return min(solve(l, idx - 1, delta + mn) + solve(idx + 1, r, delta + mn) + mn, r - l + 1);
}
```
</details>

<details style="margin-bottom: 5%"><summary>An Aside</summary>
If you're familiar with top-down dynamic programming, you may recognize the similarities in code. You could think of this D&Q strategy as a special case of dynamic programming, except we don't need to memoize because our recursive structure is a tree, so each state is visited exactly once. If we were to draw a "recursive tree of states" for a typical dynamic programming problem, it would likely not be a tree, but be a DAG instead, so we would need memoization.
</details>

## [binarysearch.com: Every Sublist Containing Unique Element](https://binarysearch.com/problems/Every-Sublist-Containing-Unique-Element)

*I find this problem unusually hard for a coding interview question, especially one of "medium" difficulty, but I digress.*

We'll deploy a similar approach to the previous problem. Let `solve(l, r)` return `true` if every subarray contains a unique element, and `false` otherwise. Right off the bat, if $a[l, r]$ contains no unique element, then we are done and can return `false`. Otherwise, let's say $a_m$ is a unique element. Any subarray of $a[l, r]$ containing $a_m$ will for sure contain a unique element, which would be $a_m$, so a subarray of $a[l, r]$ containing no unique elements must not contain $a_m$. Thus, to search for the existence of such a subarray, we split into two subproblems `solve(l, m - 1)` and `solve(m + 1, r)`, and if any of those return `false`, we also return `false` from `solve(l, r)`. To check if there is a unique element, we can iterate over all elements and store them in a hashmap. The worst case complexity of this algorithm is $\mathcal O(n^2)$, since we visit $\mathcal O(n)$ states and perform $\mathcal O(n)$ work at each state.

We can apply a simple optimization: instead of splitting just at one index, we can split at every unique index. That is, if $a_{m_1}, a_{m_2}, \dots, a_{m_k}$ are all unique elements, then we can split `solve(l, r)` into `solve(l, a[m_1] - 1)`, `solve(a[m_1] + 1, a[m_2] - 1)`, ..., `solve(a[m_k] + 1, r)`. While the worst case complexity is still $\mathcal O(n^2)$, this optimization is sufficient to get AC on this problem. Weak test cases I guess.

<details markdown="1" style="margin-bottom: 5%"><summary>Optimized $\mathcal O(n^2)$</summary>

```c++
bool recur(int l, int r, const vector<int> &nums) {
    if (l >= r)
        return true;
    unordered_map<int, int> mp;
    for (int i=l; i<=r; i++)
        mp[nums[i]]++;
    int last = l;
    bool uniq = false;
    for (int i=l; i<=r; i++)
        if (mp[nums[i]] == 1) {
            uniq = true;
            if (!recur(last, i - 1, nums))
                return false;
            last = i + 1;
        }
    return uniq && recur(last, r, nums);
}

bool solve(vector<int>& nums) {
    return recur(0, (int) nums.size() - 1, nums);
}
```
The above code ACs in 235 ms.
</details>

But we can do better. We can achieve worst case $\mathcal O(n \log n)$. To do that, we apply the following optimization: instead of always iterating our for loop from the left to right, we iterate from both sides. So for the subarray $a[l, r]$, we first check $a_l$, then $a_r$, then $a_{l+1}$, then $a_{r-1}$, then $a_{l+2}$, and so on. We can check if some $a_m$ is unique in $a[l, r]$ in $\mathcal O(1)$ by precomputing `left[m]` and `right[m]`, arrays storing the index of the next occurrence of $a_m$ to the left or right of $a_m$. So if a state splits into two states of size $a$ and $b$, the amount of work done at that state is $\mathcal O(\min(a, b))$.

It turns out this single optimization is sufficient to achieve $\mathcal O(n \log n)$. To understand why, let's once again consult the recursive tree of states.

![image 2]({{ site.baseurl }}/assets/images/divide-and-conquer-2.png)

Once again, we use the analogy of nodes and components that we used in the previous problem (treating each state representing the range $[l, r]$ as a component of nodes $l, l + 1, \dots, r$). Instead of thinking of it as splitting a graph into individual nodes, let's think in reverse: we have $n$ nodes, and want to merge them into one component. Two child states merge to become one state. If we merge two components of size $a$ and $b$, we do $\min(a, b)$ work. Does this sound familiar? That's right, this is just [small-to-large](https://usaco.guide/plat/merging?lang=cpp) merging! In small-to-large merging, when we merge two components of size $a$ and $b$, we only iterate over the nodes in the smaller of the two components, so we do $\mathcal O(\min(a, b))$ work. That's essentially the same thing we're doing here: we only do $\mathcal O(\min(a, b))$ work since we only iterate as much as twice the size of the smaller subproblem in our divide and conquer (twice since we iterate on both ends). And so by the same analysis as small-to-large merging, our runtime is $\mathcal O(n \log n)$.

<details markdown="1" style="margin-bottom: 5%"><summary>Small-to-Large Complexity Proof</summary>
Consider merging two components $A$ and $B$, with sizes $|A|$ and $|B|$. WLOG, assume $|A| \leq |B|$. Small-to-large merging will iterate over each node in $A$ and add it to $B$, doing $\mathcal O(|A|)$ work. Let's track how many times we iterate over a node. If a node gets iterated over, it means it was in the smaller of two components being merged together. So after the merging, it will be in a component that is at least twice the size of its original component. The size of a component can only be doubled $\mathcal O(\log n)$ times before it reachs size $n$. So a node can only be iterated over at most $\mathcal O(\log n)$ times. Thus, when we sum up the work done on each of $n$ nodes, we get $\mathcal O(n \log n)$ complexity.
</details>

## [HackerRank: Find the Path](https://www.hackerrank.com/challenges/shortest-path/problem)

A naive solution is to run Dijkstra for each query, giving us a $\mathcal O(nmq \log (nm))$ approach. It's difficult to improve upon this online algorithm with some sort of precomputation, because the optimal path could go "backwards" or wrap around in any arbitrary snake-like path. So as you might have guessed, we'll solve this problem offline with D&Q on the queries.

<!-- Let's consider some recursive function `recur(l, r)` that will process queries with both endpoints lying in columns $[l, r]$. Consider column $m = \lfloor \frac{l + r}{2} \rfloor$. Any query with one endpoint to the left of this column and one endpoint to the right of this column must pass through this column at some point in its path. Another way of phrasing this is that any path between the two endpoints is a concatenation of two paths originating from column $m$. Since $n$ is small, we can brute force each cell in column $m$ and run Dijkstra from that cell, and update our answer for all queries accordingly. Afterwards, we call `recur(l, m)` and `recur(m + 1, r)`. We have a $\mathcal O(\log m)$ height recursive tree with $\mathcal O(n(nm \log (nm) + q))$ work on each layer, giving us $\mathcal O(n \log m (nm \log (nm) + q))$ as our final complexity. -->

Let's consider some recursive function `solve(l, r)` that will process queries for which the path between its endpoints lies in columns $[l, r]$. Consider column $m = \lfloor \frac{l + r}{2} \rfloor$. Any path in columns $[l, r]$ falls into one of three cases:
1. The path lies solely in columns $[l, m]$.
2. The path lies solely in columns $[m + 1, r]$.
3. The path exists in both halves and crosses over column $m$ (note that both endpoints can still be in the same half for this case).

Cases 1 and 2 will be handled in our recursive subproblems, `solve(l, m)` and `solve(m + 1, r)`. For case 3, notice that since our path crosses column $m$, it can be expressed as the concatenation of two paths originating from a single cell in column $m$. Since $n$ is small, we can brute force each cell in column $m$ and run Dijkstra from that cell, and update our answer for all queries with endpoints in columns $[l, r]$. We have a $\mathcal O(\log m)$ height recursive tree with $\mathcal O(n(nm \log (nm) + q))$ work on each layer, giving us $\mathcal O(n \log m (nm \log (nm) + q))$ as our final complexity.

<details markdown="1" style="margin-bottom: 5%"><summary>Old Messy Code Written 4 Months Ago</summary>

```c++
#include <bits/stdc++.h>
using namespace std;

const int INF = 1e9;

const int dx[] = {-1, 0, 1, 0};
const int dy[] = {0, 1, 0, -1};

typedef tuple<int, int, int> node;

int main() {
    ios_base::sync_with_stdio(false);
    cin.tie(NULL);

    int n, m;
    cin >> n >> m;
    vector<vector<int>> a(n, vector<int>(m));
    for (int i=0; i<n; i++)
        for (int j=0; j<m; j++)
            cin >> a[i][j];
    int q;
    cin >> q;
    vector<array<int, 5>> queries;
    for (int i=0; i<q; i++) {
        int r1, c1, r2, c2;
        cin >> r1 >> c1 >> r2 >> c2;
        queries.push_back({r1, c1, r2, c2, i});
    }

    vector<vector<vector<int>>> dist(n, vector<vector<int>>(n, vector<int>(m)));

    auto dijkstra = [&] (int s, int t, int l, int r) -> void {
        for (int i=0; i<n; i++)
            for (int j=l; j<=r; j++)
                dist[s][i][j] = INF;
        priority_queue<node, vector<node>, greater<node>> pq;
        pq.emplace(dist[s][s][t] = a[s][t], s, t);
        while (!pq.empty()) {
            auto [d, x, y] = pq.top();
            pq.pop();
            if (d > dist[s][x][y])
                continue;
            for (int i=0; i<4; i++) {
                int nx = x + dx[i], ny = y + dy[i];
                if (0 <= nx && nx < n && l <= ny && ny <= r && d + a[nx][ny] < dist[s][nx][ny])
                    pq.emplace(dist[s][nx][ny] = d + a[nx][ny], nx, ny);
            }
        }
    };

    vector<int> ret(q, INF);

    auto solve = [&] (auto &self, int l, int r, const vector<array<int, 5>> &queries) -> void {
        if (l > r)
            return;
        int x = (l + r) / 2;
        for (int i=0; i<n; i++)
            dijkstra(i, x, l, r);
        vector<array<int, 5>> left, right;
        for (auto [r1, c1, r2, c2, i] : queries) {
            for (int j=0; j<n; j++)
                ret[i] = min(ret[i], dist[j][r1][c1] + dist[j][r2][c2] - a[j][x]);
            if (c1 < x && c2 < x)
                left.push_back({r1, c1, r2, c2, i});
            else if (c1 > x && c2 > x)
                right.push_back({r1, c1, r2, c2, i});
        }
        self(self, l, x - 1, left);
        self(self, x + 1, r, right);
    };

    solve(solve, 0, m - 1, queries);
    for (int x : ret)
        cout << x << "\n";

    return 0;
}
```
</details>

## [CodeChef SEGPROD: Product on the Segment by Modulo](https://www.codechef.com/problems/SEGPROD)

This problem is very peculiar because of its abnormally large bounds and weird input method to facilitate that. Let's use the same approach as the previous problem, which is to D&Q on the queries. If our recursive method is currently processing the range $[l, r]$, then we can process all queries with the left endpoint $\leq m = \lfloor \frac{l + r}{2} \rfloor$ and right endpoint $> m$. To do this, we precompute $product[i, m]$ for all $l \leq i \leq m$ and $product[m + 1, i]$ for all $m < i \leq r$, and then each query is just the product of two precomputed values. Unfortunately, there are two issues with this approach:
1. The problem forces us to solve it online, and this is an offline algorithm.
2. The complexity is $\mathcal O((n + q) \log n)$, which is actually too much because $q \leq 2 \cdot 10^7$.

First, we can easily make our above algorithm online by storing all the prefix and suffix products precomputed at each layer, consuming $\mathcal O(n \log n)$ memory. Then, for each query, we can traverse recursively until we reach the correct layer, giving us $\mathcal O(\log n)$ per query.

Next, to improve our query complexity, we'll use the following trick. Let's round $n$ up to the nearest power of 2. Now take a look at the ranges we get in our recursive tree:

![image 3]({{ site.baseurl }}/assets/images/divide-and-conquer-3.png)

Notice anything peculiar? That's right, each state splits into two deeper states based on whether a bit is on or off. So to find the correct layer where the left and right endpoint of our query lie on different sides of $m$, we simply need to find the most significant bit where they differ. Or in other words, we need to find the most significant bit in the xor of the endpoints, which can be computed in $\mathcal O(1)$ using builtin functions like `__lg` or precomputing a lookup table. So we've reduced our complexity to $\mathcal O(n \log n + q)$, which is fast enough to get AC.

<details markdown="1" style="margin-bottom: 5%"><summary>Code</summary>

```c++
#include <bits/stdc++.h>
using namespace std;

int main() {
    ios_base::sync_with_stdio(false);
    cin.tie(NULL);

    int t;
    cin >> t;
    while (t--) {
        int n, p, q;
        cin >> n >> p >> q;
        int k = n == 1 ? 0 : __lg(n - 1) + 1;
        vector<int> a(1 << k);
        for (int i=0; i<n; i++)
            cin >> a[i];
        vector<int> b(q / 64 + 2);
        for (int &x : b)
            cin >> x;

        vector<vector<long long>> prod(k, vector<long long>(1 << k));

        auto recur = [&] (auto &self, int lg, int l, int r) -> void {
            if (lg < 0)
                return;
            int m = (l + r) / 2;
            for (int i=m-1; i>=l; i--)
                prod[lg][i] = a[i] * (i == m - 1 ? 1 : prod[lg][i+1]) % p;
            for (int i=m; i<r; i++)
                prod[lg][i] = a[i] * (i == m ? 1 : prod[lg][i-1]) % p;
            self(self, lg - 1, l, m);
            self(self, lg - 1, m, r);
        };

        auto query = [&] (int l, int r) -> int {
            if (l == r)
                return a[l];
            int lg = __lg(l ^ r);
            return prod[lg][l] * prod[lg][r] % p;
        };

        recur(recur, k - 1, 0, 1 << k);

        int x = 0, l, r;
        for (int i=0; i<q; i++) {
            if (i % 64 == 0) {
                l = (b[i / 64] + x) % n;
                r = (b[i / 64 + 1] + x) % n;
            } else {
                l = (l + x) % n;
                r = (r + x) % n;
            }
            if (l > r)
                swap(l, r);
            x = (query(l, r) + 1) % p;
        }
        cout << x << "\n";
    }

    return 0;
}
```
</details>

## CDQ Divide and Conquer
This final section refers to a specific application of D&Q: CDQ D&Q, which I first read about in [this Codeforces comment](https://codeforces.com/blog/entry/68263#comment-525816) and in the [linked PDF](https://assets.hkoi.org/training2018/dc.pdf). The general gist of the technique is as follows: say we have a data structure and two types of operations:
1. Update the data structure.
2. Query for some aggregate in the data structure.

Additionally, say this problem is much easier to solve offline than online. That is, if all the update operations came before all the query operations, then there exists some offline algorithm to solve it. CDQ D&Q allows us to solve the online version with an extra $\mathcal O(\log n)$ factor.

For our example problem, we'll use the one mentioned in [this blog](https://codeforces.com/blog/entry/68263). There is an online solution using 2D segment tree that solves this in $\mathcal O(q \log^2 q)$, but 2D segment tree has a non-trivial time and memory constant factor and is often difficult to squeeze into the problem bounds if not intended.

How do we solve this problem offline? This is a well-known problem: we perform a linesweep. Every time we encounter the right endpoint of an interval, we insert the length of that interval at the left endpoint of that interval in a segment tree. Every time we encounter the right endpoint of some query, we query for the maximum value in $[l, r]$ in the segment tree.

Now for the CDQ D&Q step: we will perform divide and conquer on the time of each operation (first operation is at time 1, second operation at time 2, etc.). When we are considering all operations with time in the range $[l, r]$, we will do the following: consider $m = \lfloor \frac{l + r}{2} \rfloor$. For all update operations with time $\leq m$, we insert them. For all query operations with time $> m$, we apply the offline algorithm to update their answers. In this case, we are considering the contribution of all update operations in $[l, m]$ to the query operations in $[m + 1, r]$. Any update contributions in $[1, l - 1]$ would have been considered on an earlier layer, and any update contributions in $[m + 1, r]$ will be considered in a later layer, when we recurse to $[m + 1, r]$. The code would look something like the following:

```c++
void solve(int l, int r) {
    if (l == r)
        return;
    int m = (l + r) / 2;
    solve(l, m);
    vector<Op> updates, queries;
    for (int i=l; i<=m; i++)
        updates.push_back(op[i]);
    for (int i=m+1; i<=r; i++)
        queries.push_back(op[i]);
    offlineAlgorithm(updates, queries);
    solve(m + 1, r);
}
```

You might be wondering, why am I calling `solve(l, m)` before computing the contribution of $[l, m]$ on $[m + 1, r]$, and not after? In this problem, it doesn't matter, but CDQ D&Q can also be applied to some dynamic programming problems where the order does matter. We would first call `solve(l, m)` to fully compute the final values of $dp[l, m]$. Once those DP values are final, we can rely on them to update the values $dp[m + 1, r]$. An example of this is in [this Atcoder problem](https://atcoder.jp/contests/abc213/tasks/abc213_h).

## Finale
Hopefully you gleaned something from this tutorial! I tried to order the article from simplest to most complicated techniques, and tried to explain stuff unambiguously but also concisely. If you have any feedback or questions, don't hesitate to drop a comment.

## Problems (Roughly Ordered Based on Techniques Used in Article)
[Codeforces Round 689D: Divide and Summarize](https://codeforces.com/contest/1461/problem/D)

[Codeforces Edu 105D: Dogeforces](https://codeforces.com/problemset/problem/1494/D)

[Codeforces Round 429D: Destiny](https://codeforces.com/problemset/problem/840/D)

[SPOJ ADACABAA: Ada and Species](https://www.spoj.com/problems/ADACABAA/)

[Atcoder ABC 213H: Stroll](https://atcoder.jp/contests/abc213/tasks/abc213_h)

This is a very short list, so comment any other problems you know.

## References (For CDQ D&Q)
[https://codeforces.com/blog/entry/68263?#comment-525816](https://codeforces.com/blog/entry/68263?#comment-525816)

[https://assets.hkoi.org/training2018/dc.pdf](https://assets.hkoi.org/training2018/dc.pdf)

---
layout: post
title: "Long Challenges are Awesome"
tags: [opinion]
usemathjax: true
---

Short contests are fun, although not without its fair share of frustrating moments. Have you ever come so close to solving a problem, but ran out of time? If only you had 10 extra minutes! Or have you ever "bricked" on a problem? That is, you come across a problem that should be within your rating range and easily solved, but you just can't think of the trick needed to solve it? This has happened to me many times, with the most recent example being [Global Round 15](https://codeforces.com/contest/1552/standings/participant/117714327#p117714327). I could not think of how to get past the naive $\mathcal O(n^2)$ solution for problem B for the longest time, and I had to skip that problem to do the other ones first in order to save my performance.

Now don't get me wrong. Codeforces contests are great, and the adrenaline rush they provide, from the last minute clutch solves to the anticipation of system tests, is unmatched. But sometimes, I just want to take it chill and casually solve problems. That's where the long challenge comes in.

The long challenge format is where contestants are given a long period of time, usually a week or several days, to work on a set of problems. Some notable examples include CodeChef Long Challenges, HackerEarth Circuits, and the Qualification Round of Google Code Jam. In this article, I'll talk about why I have a great appreciation for this contest format, and why this can be the most gratifying format to compete in.

## First, the elephant in the room.

Unfortunately, we can't talk about long challenges without acknowledging cheating. People cheat on programming contests. Programming contests are usually online and participated in from the comfort of your home, so no one can monitor your behavior. Couple that with the strong emphasis that certain software engineer employers place on competitive programming for hiring, and there's a tremendous incentive to cheat. The long challenge format is by far the easiest to cheat on, because solve times don't matter and there's a longer time frame to obtain answers from other people.

If I'm being honest, there's really not much one can do about cheating in programming contests. The fundamental nature of it being virtual means you can't spy on other contestants to prevent this type of behavior. There are countermeasures like MOSS (which doesn't really work and can't stop sharing of just ideas) and reporting by contestants, but the amount of cheating that happens each contest is likely intractable for contest admins to effectively shut down. What I can say is that cheating becomes far less prevalent as you move up in rating, so you could cheating as motivation to get out of lower divisions as quickly as possible. Also, since I usually don't do long contests for any form of rating (CodeChef long challenges are unrated for Div 1, and I don't really care about HackerEarth rating), I focus more on just trying my best to solve all the problems in the problemset. Cheating sucks, but it doesn't significantly impact my ability to enjoy long challenges.

Now with that out of the way, allow me to list some of the aspects I enjoy about this format.

## 1. Schedule Convenience
Where I live, the most popular contest platforms like Codeforces, Atcoder, and Codechef always host their short contests in the morning and end at noon. Especially now that school classes are returning to in-person, it's difficult to compete in these contests during the weekdays without them cutting into a significant part of my day, and it's annoying to wake up early on weekends to do them. Long challenges grant more flexibility in the timing, so I can work on them in the afternoon or nighttime instead.

## 2. Focus on Problem Solving
With long challenges, I feel like it's just me and the problems. I don't necessarily have to be sitting at my desk to think about problems. I can read them first, then keep them in the back of my head as I'm going through the rest of my day. If I'm tired or am getting repeatedly stuck on the same ideas, I can wait until tomorrow to try and get a fresh perspective. I remember in my first ever CodeChef long challenge, I was stuck on the problem [Blackjack](https://www.codechef.com/JAN21C/problems/BLKJK) after thinking about it for a few hours. For the next few days, I just kept it in the back of my head, working on the problem every once in a while and resetting my ideas constantly. Then one night, while I was brushing my teeth, I had an epiphany. A different way of thinking about the problem. And so I immediately rushed to my computer, coded up the solution, and claimed my AC.

I've also had plenty of moments where I've come up with a solution while on a walk. And because I'm walking outside instead of being at my computer, I can take the time to think about the implementation details and achieve mental clarity before I begin implementing. Long challenges give me the time to truly appreciate problem solving as an art.

## 3. Approximation Problems
Most long challenges will also have an approximation problem as a tiebreaker. In these problems, it is impossible to compute the optimal answer in the given limits, and contestants compete to use heuristics and approximation algorithms to get as close to the optimal answer as possible. I'm really bad at these because I'm used to approaching problems by thinking about the worst case and the optimal solution instead of thinking about heuristics, but these types of problems can be interesting to think about nonetheless.

Now, I'll talk more specifically about two platforms that host long challenges regularly: CodeChef and HackerEarth.

## CodeChef Long Challenges
![image 1]({{ site.baseurl }}/assets/images/long-challenge-1.png)
<div style="text-align: center; margin-bottom: 5%"><em>Despite finishing earlier than some of the contestants above me, I still appear below them on the leaderboard, so I have no clue what algorithm CodeChef uses for tiebreakers.</em></div>

CodeChef is where I first encountered the long challenge format. Long challenges usually happen once a month and have 8-10 problems. Scores are determined solely based on points from problems (time is disregarded), so multiple contestants can get the same place. They used to be rated for all divisions, but recently they were downgraded to just being rated for Div 3, presumably to prevent cheaters from reaching 7 stars with only long challenges. They also no longer have approximation problems, which is a bit of a shame, but it also means it's now possible for me to get a perfect score :D.

In terms of problem quality, CodeChef problems do not shy away from heavy implementation, although that is often not the bottleneck in the difficulty of their problems. I'd say the early problems are usually random fluff, where you just do one print statement, while the later problems contain some interesting ideas. For example, the August long challenge had a problem called [Alternating LG Queries](https://www.codechef.com/AUG21A/problems/ALTLG), which I thought required a neat observation.

<details markdown="1" style="margin-bottom: 5%"><summary>The Observation</summary>
GCD and LCM behave like a clamp function, so it is sufficient to compress a series of composed GCD and LCM functions into one GCD and one LCM function. The rough proof of this is to consider each prime factor separately, then notice that the exponent of the prime factor is either min'd or max'd, which is exactly what a clamp function does.
</details>

In terms of difficulty, I would say that older long challenges were harder since they were rated for Div 1 and often contained problems requiring research of esoteric algorithms to solve. Nowadays, the long challenges rated for Div 3 can usually be AK'd (means to solve all the problems) by a competitor that is Master+ on Codeforces (at least, that's the rating I was when I did them). All in all, I'd say the quality of these contests are pretty good.

## HackerEarth Circuits
![image 2]({{ site.baseurl }}/assets/images/long-challenge-2.png)

HackerEarth Circuits usually happen once a month as well, and tend to have 7 algorithm problems and 1 approximation problem. Scores are determined first based on total points, then sum of solve times as tiebreaker. Personally, I don't understand why they even incorporate solve times into the score calculation, since the whole point of long challenges is to be friendly to different time zones and disregard the time aspect of programming competitions. Luckily, the times don't really matter in practice since the approximation problem provides a tiebreaker anyways.

Problem quality for HackerEarth contests is definitely sketchier than the popular platforms. The problems tend to gather their difficulty from the algorithms and concepts used instead of the level of logical thinking required to make the observations. Also, some of the problems definitely push the boundary of what it means to be a **programming** problem. Take for example the problem [A Trigonometry Function](https://www.hackerearth.com/problem/algorithm/trignometry-function-4c614376/) (and people say Codeforces is Mathforces!).

<details markdown="1" style="margin-bottom: 5%"><summary>How I solved this in contest</summary>
Let $S(n) = \sum_{k=0}^n f(k)$. We want to compute $S(q) - S(p - 1)$.

We can split $f(n)$ into the sum of two functions, $f_1(n) = 2 \cdot 7^{n/2} \cdot \cos(n \theta)$ and $f_2(n) = n \cdot 4^n$.

Let's first tackle $f_1(n)$. It's not even immediately obvious that for integer inputs this function outputs an integer, let alone how this function behaves. Let's use WolframAlpha to compute the first few terms of the sequence.

$$
f_1(0) = 2 \\
f_1(1) = 4 \\
f_1(2) = 2 \\
f_1(3) = -20 \\
f_1(4) = -94 \\
f_1(5) = -236
$$

Ok, I'm not seeing any pattern at all. But maybe OEIS does. While there isn't an entry on OEIS for this exact sequence, there is an entry on OEIS for $a(n) = f_1(n) / 2$: [A213421](https://oeis.org/search?q=1%2C+2%2C+1%2C+-10%2C+-47%2C+-118&sort=&language=english&go=Search). Most of the formulas aren't too useful, except for the last one: $a(n) = 4 a(n - 1) - 7 a(n - 2)$.

Now remember, we don't need $a(n)$, but rather $S(n)$. Let's redefine $S(n) = \sum_{k=0}^n a(k)$ and just multiply our final answer by 2 at the end. A quick substitution will help us derive a recurrence relation for $S(n)$:

$$
\begin{align*}
S(n) &= a(0) + a(1) + \dots + a(n - 1) + a(n) \\
&= S(n - 1) + a(n) \\
&= S(n - 1) + 4 a(n - 1) - 7 a(n - 2)
\end{align*}
$$

Now we can calculate $S(n)$ in $\mathcal O(\log n)$ time using [matrix exponentiation](https://usaco.guide/plat/matrix-expo?lang=cpp). In our matrix, we maintain the values $a(n), a(n - 1), S(n)$.

For $f_2(n)$, there's a simple closed form from [WolframAlpha](https://www.wolframalpha.com/input/?i=sum+n*4%5En) (WolframAlpha is OP): $\sum_{k=0}^n f_2(k) = \frac{4}{9}(3 \cdot 4^n n - 4^n + 1)$.

</details>

<details markdown="1" style="margin-bottom: 5%"><summary>Rant about HackerEarth Contest Preparation</summary>
Aside from problem quality itself, the quality of the preparation and contest design is questionable. I questioned earlier why solve times were included in the scoring, but I think the real reason is simply that the devs recycled the contest system used for their short contests and couldn't be bothered to adapt it to sensible rules for a long contest. Furthermore, there's almost always some issue with at least one of the problems in each contest. [One problem](https://www.hackerearth.com/practice/algorithms/dynamic-programming/introduction-to-dynamic-programming-1/practice-problems/algorithm/matrix-and-xor-operation-a2e19185/) from December Circuits incorrectly handled certain inputs. The checker for the [approximation problem](https://www.hackerearth.com/practice/basic-programming/bit-manipulation/basics-of-bit-manipulation/practice-problems/approximate/longest-grid-path-a28ff86f/) in July Circuits was wrong and allowed paths going through blocked cells. The checker for the [approximation problem](https://www.hackerearth.com/practice/algorithms/string-algorithm/basics-of-string-manipulation/practice-problems/approximate/kolmogorov-78780f09/) in January Circuits contradicted the output format in the problem statement, and participants literally had to figure out that information from repeated submissions during the contest. Finally, most recently HackerEarth contests banned participants from copy pasting into the submission box for no reason, meaning they somehow expect you to retype complex algorithms from scratch into their submission box instead of importing it from your library. It's possible that they've fixed that issue, but I haven't competed on HackerEarth recently, because why would I?

The worst offense is that these mistakes are never corrected because nobody on the contest team ever replies. I even personally emailed the contest team for one of their contests once and only got a reply several days after the contest ended. Needless to say, I've stopped competing on this platform for the time being. It's really a shame that one of the only sources for long challenge contests was butchered so badly by poor preparation.
</details>

So that concludes my thoughts about long challenges. They're an underappreciated format that I wish there were more of, and they can be a lot of fun to compete in. Let me know your thoughts in the comments below.

---
layout: post
title: "Solutions to SPOJ GSS Series"
tags: [tutorial, algo]
usemathjax: true
---

SPOJ has a series of problems with problem codes [GSS1](https://www.spoj.com/problems/GSS1/), [GSS2](https://www.spoj.com/problems/GSS2/), ..., [GSS8](https://www.spoj.com/problems/GSS8). The problems are intended as educational range query problems, and while they are a bit outdated, they can still be interesting to work through and think about. I highly recommend thinking about each problem on your own before looking at the editorial. Also, don't feel the need to go in order, because they are definitely not ordered by difficulty. Finally, while I might include snippets of code for clarity of explanation, writing the full code for each problem will be left as an exercise to the reader :)

**Prerequisite: Segment Tree, General Understanding of Combinative Data Structures** \\
Note that the way I explain these problems will be framed by how I implement segment trees, which you can find [here (without lazy propagation)](https://github.com/mzhang2021/cp-library/blob/master/implementations/data-structures/SegmentTreeNode.h) and [here (with lazy propagation)](https://github.com/mzhang2021/cp-library/blob/master/implementations/data-structures/SegmentTreeNodeLazy.h). Essentially, I store aggregate information in segment tree nodes, and each query range can be decomposed into a combination of multiple segment tree nodes.

## [GSS1](https://www.spoj.com/problems/GSS1/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given an array $\|a_i\| \leq 15007$ of size $n \leq 5 \cdot 10^4$, answer $m$ queries of the following form: given $(x, y)$, find

$$
\max_{x \leq i \leq j \leq y} (a_i + \dots + a_j)
$$

In other words, find the maximum subarray sum of the subarray $a[x, y]$.

*For some reason the problem provides no bounds on $m$.*

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
The trick for any segment tree problem is the following:
1. Figure out what information we need to store in the nodes.
2. Figure out how to combine two nodes.

In this case, it might help to start with thinking about how combining two nodes works, as that will guide us to know what info we need to maintain. Let's say we're considering the node representing range $[l, r]$ which is merged from two child nodes $[l, m]$ and $[m + 1, r]$, where $m = \lfloor \frac{l + r}{2} \rfloor$. The maximum subarray sum in $a[l, r]$ could fall into one of three cases:
1. It lies solely in the range $[l, m]$.
2. It lies solely in the range $[m + 1, r]$
3. It exists in both halves.

In the first and second case, we just take the maximum subarray sum computed in the child nodes. In the third case, the optimal subarray would consist of some suffix of the range $[l, m]$ and some prefix of the range $[m + 1, r]$. We can also compute the maximum prefix and suffix sum for each node, so that case 3 is simply reduced to the maximum suffix in the left child plus the maximum prefix in the right child. So this tells us what info we need to store in each node: the maximum subarray sum, the maximum prefix sum, the maximum suffix sum, and the total sum. We get the following method for updating:
```c++
// in my template the function signature is pull(a, b), but I'm using merge(left, right) here instead for clarity
void merge(Node left, Node right) {
    maxSum = max({left.maxSum, right.maxSum, left.maxSuffix + right.maxPrefix});
    maxPrefix = max(left.maxPrefix, left.sum + right.maxPrefix);
    maxSuffix = max(right.maxSuffix, right.sum + left.maxSuffix);
    sum = left.sum + right.sum;
}
```

Be careful that you handle ranges of all negative numbers correctly, because the problem does not allow for non-empty subarrays.
</details>
</details>

## [GSS2](https://www.spoj.com/problems/GSS2/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

*The original problem statement is very obviously poorly translated, so I literally had to read the comments to understand the problem.*

Given an array $\|a_i\| \leq 10^5$ of size $n \leq 10^5$, answer $q \leq 10^5$ queries of the following form: given $(x, y)$, find

$$
\max_{x \leq i \leq j \leq y} (a_i + \dots + a_j)
$$

but with the extra condition that duplicate elements are ignored. So for example, if some subarray contains three copies of $2$, we only add $2$ once.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
The extra condition of ignoring duplicates makes a direct online approach with segment tree difficult, because we don't have a concise way of maintaining the set of all elements that have at least one occurrence in each node. So let's consider an offline approach.

There's a general sweepline approach for solving range query problems offline: sweep from left to right while updating some sort of data structure. When you encounter the right endpoint of some query, use the data structure to find the answer for that query.

In this case, our data structure will be an array $p$, where $p_i$ represents the maximum subarray sum excluding duplicates with left endpoint beginning at index $i$. We will also maintain an array $last_x$ storing the rightmost index with value $x$ in the array that we've encountered in our sweepline so far.

Let's see what happens when we encounter $a_i$ in our sweepline. $a_i$ could contribute to every subarray beginning at $last_{a_i} + 1, last_{a_i} + 2, \dots, i$. It cannot contribute to a subarray beginning at $last_{a_i}$ or earlier, since such a subarray would already contain a copy of $a_i$ at index $last_{a_i}$ if it were to also include $a_i$, and duplicate elements only get added once. So we perform the following update:
```c++
for (int index=last[a[i]]+1; index<=i; index++) {
    sum[index] += a[i];
    p[index] = max(p[index], sum[index]);
}
```
Then, when we encounter some query $(l, r)$, its answer is simply

$$
\max_{i=l}^r p_i
$$

This offline algorithm runs in $\mathcal O(n(n + q))$, so let's now boost the efficiency to $\mathcal O((n + q) \log n)$ by upgrading $p$ to a segment tree. We need a segment tree that can handle the following operations:
1. Add $x$ to all $a_i$ in some range.
2. Query for the maximum $p_i$ in some range.

After every operation, $p_i := \max(p_i, a_i)$, so $p_i$ is the maximum of all values $a_i$ ever changed into.

Believe it or not, this is possible with a segment tree, and is described in the "Historic Information" section of this [segment tree beats](https://codeforces.com/blog/entry/57319) tutorial.

<details markdown="1" style="margin-bottom: 5%"><summary>The Idea</summary>
We maintain four values in each segment tree node: `a, p, lazy, historic_lazy`. Whenever we propagate lazy values, we do `historic_lazy = max(historic_lazy, lazy + propagated_historic_lazy)` and `lazy += propagated_lazy`. Whenever we apply the pending lazy update, we do `p = max(p, a + historic_lazy)` and `a += lazy`. And finally, when we merge, we do `p = max(left.p, right.p)` and `a = max(left.a, right.a)`. Take a moment to think about why this works, and why an additional `historic_lazy` value is necessary (i.e. why can't we just use one lazy value?).
</details>

That's it! We've successfully solved this problem in $\mathcal O((n + q) \log n)$. Personally, I think this is the hardest GSS problem out of the entire series.

</details>
</details>

## [GSS3](https://www.spoj.com/problems/GSS3/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given an array $\|a_i\| \leq 10^4$ of size $n \leq 5 \cdot 10^4$, process $m \leq 5 \cdot 10^4$ queries of two types:
1. Set $a_x := y$.
2. Given $(x, y)$, find

$$
\max_{x \leq i \leq j \leq y} (a_i + \dots + a_j)
$$

In other words, find the maximum subarray sum of the subarray $a[x, y]$.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
This is just GSS1 with updates. The update method doesn't change any of the above logic.
</details>
</details>

## [GSS4](https://www.spoj.com/problems/GSS4/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given an array of $a_i$ of size $n \leq 10^5$, process $m \leq 10^5$ queries of two types:
1. Given $(x, y)$, perform $a_i := \lfloor \sqrt a_i \rfloor$ for $x \leq i \leq y$.
2. Given $(x, y)$, find $\sum_{i=x}^y a_i$.

Constraints on $a_i$: $a_i > 0, \sum_{i=1}^n a_i \leq 10^{18}$

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
The first operation is difficult to handle with traditional lazy propagation, because it's not clear how applying the square root operation to a subarray affects the sum of that subarray. Let's consider the naive way of updating the segment tree: we traverse all the way down to each leaf node in the range and update them directly, giving us $\mathcal O(n)$ per query. While that's obviously too slow, it's actually not far off from the right idea.

We will use the same idea as [segment tree beats](https://codeforces.com/blog/entry/57319) (don't worry, this is easier than the proof for segment tree beats). Observe that for $a_i \leq 10^{18}$, we can only apply $a_i := \lfloor \sqrt a_i \rfloor$ 6 times before $a_i$ converges to 1. So let's say you're currently at some node, and you want to apply the square root operation to all elements in the range covered by that node. If all the elements are 1, applying the square root operation on them again is useless, so we can just stop traversing further.

Let's prove a bound of $\mathcal O((n + m) \log n)$ from that optimization. Consider the following diagram:

![image 1]({{ site.baseurl }}/assets/images/gss-1.png)

We will refer to the blue nodes as *ordinary nodes* and the green nodes as *extra nodes* (the same terminology used in the segment tree beats blog). Ordinary nodes are nodes we would visit in standard segment tree anyways. In the naive algorithm, we would also visit all the extra nodes, which could be $\mathcal O(n)$ per query. We will keep some counter on each leaf node representing how many more times we can apply the square root operation to that element before it reduces to 1 (the red numbers in the diagram). The counter can be at most 6 for each leaf node. Finally, we keep some counter on each non-leaf node that equals the sum of the counters of its children. Let $\Phi$ equal the sum of all counters written on all nodes in the tree. What is the initial value of $\Phi$? It is $\mathcal O(n \log n)$.

<details markdown="1" style="margin-bottom: 5%"><summary>Why?</summary>
Each leaf node contributes at most +6 to the counter of every ancestor. There are $\mathcal O(\log n)$ ancestors per leaf node, so each leaf node contributes $+6 \log n$ to $\Phi$. Thus, summing up the total contribution from all leaf nodes gives $\Phi = 6n \log n$, or $\mathcal O(n \log n)$.
</details>

The number of ordinary nodes we visit is $\mathcal O(m \log n)$, since that's just standard segment tree. Under what cases will we visit an extra node? We visit an extra node whenever there exists a non-one element in that subtree, aka if the counter on the extra node is positive. So after the operation, that counter and thus $\Phi$ will decrease by one, because we will have square rooted some leaf node in that subtree. Thus, every movement to an extra node is accounted for by 1 unit of $\Phi$, so the total amount of times we visit some extra node is bounded by the initial value of $\Phi$, or $\mathcal O(n \log n)$. The total complexity is therefore $\mathcal O((n + m) \log n)$.

</details>
</details>

## [GSS5](https://www.spoj.com/problems/GSS5/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given an array of $\|a_i\| \leq 10^4$ of size $n \leq 10^4$, process $m \leq 10^4$ queries of the following form: given $(x_1, y_1, x_2, y_2)$, find

$$
\max_{x_1 \leq i \leq y_1, x_2 \leq j \leq y_2} (a_i + \dots + a_j)
$$

where $x_1 \leq x_2$ and $y_1 \leq y_2$.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
First, read the solution for GSS1, as this problem will compute the same information in each segment tree node.

We will split the problem into two separate cases:

1. $[x_1, y_1]$ and $[x_2, y_2]$ form two disjoint intervals.
2. $[x_1, y_1]$ and $[x_2, y_2]$ overlap.

In the first case, the optimal subarray consists of some suffix of $a[x_1, y_1]$, the total sum of $a[y_1 + 1, x_2 - 1]$, and some prefix of $a[x_2, y_2]$. We can query for all three of those parts with the same segment tree used in GSS1.

In the second case, we have a few more possible cases, all handled with our segment tree:
1. The subarray starts in $[x_1, x_2]$ and ends in $[x_2, y_1]$.
2. The subarray starts in $[x_1, x_2]$ and ends in $[y_1, y_2]$.
3. The subarray starts in $[x_2, y_1]$ and ends in $[x_2, y_1]$.
4. The subarray starts in $[x_2, y_1]$ and ends in $[y_1, y_2]$.

Putting both cases together, the logic of each query looks something like this:
```c++
if (y1 <= x2) {
    Node a = st.query(x1, y1), b = st.query(y1, x2), c = st.query(x2, y2);
    cout << a.maxSuffix + b.sum + c.maxPrefix - arr[y1] - arr[x2] << "\n";
} else {
    Node a = st.query(x1, x2), b = st.query(x2, y1), c = st.query(y1, y2);
    cout << max({a.maxSuffix + b.maxPrefix - arr[x2], a.maxSuffix + b.sum + c.maxPrefix - arr[x2] - arr[y1],
                b.maxSum, b.maxSuffix + c.maxPrefix - arr[y1]}) << "\n";
}
```

</details>
</details>

## [GSS6](https://www.spoj.com/problems/GSS6/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given an array of $\|a_i\| \leq 10^4$ of size $n \leq 10^5$, process $q \leq 10^5$ queries of four types:
1. Insert $y$ before index $x$ in the array (so the array size increases).
2. Delete the element at index $x$ (so the array size decreases).
3. Set $a_x := y$.
4. Given $(x, y)$, find

$$
\max_{x \leq i \leq j \leq y} (a_i + \dots + a_j)
$$

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
This is GSS3, but you can now insert or delete elements from the array and change its size. Segment tree struggles with inserts and deletes, but luckily there's another combinative data structure that handles this with ease: [tre](https://cp-algorithms.com/data_structures/treap.html)[aps](https://codeforces.com/blog/entry/84017)!

Once you know what a treap's capabilities are, it should be apparent how this problem is solved. We store the same info from GSS1 in each treap node with the same merge function. Insert and delete can both be implemented as a series of split and merge functions. So this problem is solved in $\mathcal O(q \log n)$.

</details>
</details>

## [GSS7](https://www.spoj.com/problems/GSS7/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given a tree of $n \leq 10^5$ nodes, each with some associated value $\|x_i\| \leq 10^4$, process $q \leq 10^5$ queries of two types:
1. Find the maximum subarray sum of the array formed from values on the path from node $a$ to $b$.
2. Set all values on the path from node $a$ to $b$ to $c$.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
This is GSS3 but on a tree. Luckily, there's a well-known way of adapting a range query solution to a tree with only an extra $\mathcal O(\log n)$ factor: [heavy light](https://cp-algorithms.com/graph/hld.html) [decomposition](https://codeforces.com/blog/entry/81317)!

Once you know what HLD's capabilities are, it should be apparent how this problem is solved. Just be wary that depending on how you implement HLD, the orientation may make a difference. To explain what I mean, take a look at the following code snippet below:
```c++
template<class B>
void process(int u, int v, B op) {
    bool s = false;
    for (; root[u]!=root[v]; u=par[root[u]]) {
        if (depth[root[u]] < depth[root[v]]) {
            swap(u, v);
            s ^= true;
        }
        op(pos[root[u]], pos[u], s);
    }
    if (depth[u] > depth[v]) {
        swap(u, v);
        s ^= true;
    }
    op(pos[u], pos[v], !s);
}

int query(int u, int v) {
    Node ls, rs;
    process(u, v, [this, &ls, &rs] (int l, int r, bool s) {
        Node cur = st.query(l, r);
        if (s) rs.merge(cur, rs);
        else ls.merge(cur, ls);
    });
    swap(ls.maxPrefix, ls.maxSuffix);
    Node ret;
    ret.merge(ls, rs);
    return ret.maxSum;
}
```
My full HLD implementation can be found [here](https://github.com/mzhang2021/cp-library/blob/master/implementations/graphs/HLD.h) and uses the same style as [this implementation](https://codeforces.com/blog/entry/22072). Essentially, the orientation of the segment tree is from smaller DFS time to larger DFS time (top to bottom). Therefore, I maintain the merged results of the query in two separate halves for two separate branches of the path connected by the LCA, and I swap the prefix and suffix of the left branch at the end to make its orientation consistent with the right half. If this doesn't make sense to you, try drawing out the diagram or tracing the code to see what happens if you don't account for orientation when merging.

</details>
</details>

## [GSS8](https://www.spoj.com/problems/GSS8/)

<details markdown="1" style="margin-bottom: 5%"><summary>Problem</summary>

Given an array of $a_i < 2^{32}$ of size $n \leq 10^5$, process $q \leq 10^5$ queries of four types:
1. Insert $val$ before index $pos$ in the array (so the array size increases).
2. Delete the element at index $pos$ (so the array size decreases).
3. Set $a_{pos} := val$.
4. Given $(l, r, k)$, find

$$
\sum_{i=l}^r a_i \cdot (i - l + 1)^k \mod 2^{32}
$$

where $k \leq 10$.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>
Finally a problem that isn't a variation of GSS1! First off, we can handle the insert and delete operations with a [tre](https://cp-algorithms.com/data_structures/treap.html)[aps](https://codeforces.com/blog/entry/84017). The tricky part is going to be figuring out how to calculate the fourth operation.

Since $k$ is small, we can compute $ans[0], ans[1], \dots, ans[10]$ in each treap node. This way, we just print $ans[k]$ from the correct node to answer each query.

Consider the following small example: we are merging two nodes. The left node represents the array $[a_1, a_2]$ and the right node represents the array $[a_3, a_4]$. Say we've already computed $ans[k]$ in both nodes, and we want to compute $ans[k]$ for the merging of both nodes. In other words, we want:

$$
ans_l[k] = a_1 \cdot 1^k + a_2 \cdot 2^k \\
ans_r[k] = a_3 \cdot 1^k + a_4 \cdot 2^k \\
merge(ans_l[k], ans_r[k]) = ans[k] = a_1 \cdot 1^k + a_2 \cdot 2^k + a_3 \cdot 3^k + a_4 \cdot 4^k
$$

The first half of what we want is just $ans_l[k]$. The second half is almost $ans_r[k]$, except the base of each power term gets shifted up ($1^k$ becomes $3^k$, $2^k$ becomes $4^k$). Notice that each base gets shifted up by `size_of_left_node` (which I will denote as $size_l$), so we can rewrite the terms as $a_3 \cdot (1 + size_l)^k + a_4 \cdot (2 + size_l)^k$. There's a well-known theorem for expanding terms of the form $(a + b)^k$: the [Binomial Theorem](https://en.wikipedia.org/wiki/Binomial_theorem). So expanding yields:

$$
\begin{align*}
a_i \cdot ((i - size_l) + size_l)^k &= a_i \cdot \sum_{j=0}^k \left(\binom{k}{j} \cdot (i - size_l)^j \cdot (size_l)^{k-j}\right) \\
&= \sum_{j=0}^k \left(\binom{k}{j} \cdot (a_i (i - size_l)^j) \cdot (size_l)^{k-j}\right)
\end{align*}
$$

And when we sum up all of these for all $a_i$ in the right node:

$$
\sum_{\forall i} \sum_{j=0}^k \left(\binom{k}{j} \cdot (a_i (i - size_l)^j) \cdot (size_l)^{k-j}\right) \\
= \sum_{j=0}^k \left(\binom{k}{j} \cdot ans_r[j] \cdot (size_l)^{k-j}\right)
$$

This can be shown with some rearrangement of the terms.

So now, we've successfully shown how to compute the contribution of the right node to $ans[k]$ from $ans_r[0], ans_r[1], \dots, ans_r[k]$. After precomputing $(size_l)^j$ and $\binom{k}{j}$ for $0 \leq j, k \leq 10$, we can compute each of $ans[j]$ in $\mathcal O(k^2)$. The code for the merge function looks as follows:

```c++
treap->size = getSize(treap->l) + getSize(treap->r) + 1;
power[0] = 1;
for (int i=1; i<=MAXK; i++)
    power[i] = power[i-1] * (getSize(treap->l) + 1);
for (int i=0; i<=MAXK; i++) {
    treap->ans[i] = (treap->l ? treap->l->ans[i] : 0) + power[i] * treap->val;
    if (treap->r)
        for (int j=0; j<=i; j++)
            treap->ans[i] += choose[i][j] * power[i-j] * treap->r->ans[j];
}
```
*The code above differs slightly from the explanation because the treap merge actually consists of three parts: the left child, the right child, and the middle value stored in the current node.*

Thus, our final complexity is $\mathcal O(k^2 \log n)$ per query.

</details>
</details>

Let me know if you have any ideas for what my next article topic should be. I might cover the SPOJ QTREE series, or write about something else.

---
layout: post
title: "How Fair are Facebook Hacker Cup's Rules?"
tags: [opinion]
usemathjax: true
---

Facebook Hacker Cup is different from many other competitive programming competitions due to its submission format: instead of automatically judging competitiors' submissions via an online judge, competitors are given the input file and six minutes to generate the answer for that input file locally. This difference in submission format is very peculiar and allows for unorthodox methods of solving problems. For example, you could use MATLAB or Wolfram Mathematica. You could use parallel processing across multiple devices (which isn't just theorycrafting, it's actually doable as shown in [this recent blog](https://codeforces.com/blog/entry/95295)). All of this can make the competition more interesting, but it also begs the question of fairness.

The thing is, when you have to produce the output locally, your hardware plays a significantly larger role in your running time. What made online judging fair was that everyone's submission ran in the same environment under the same conditions (or as close as possible, since some online judges use multiple machines for judging). And for anyone wondering, there certainly is a difference in hardware performance, even just between a laptop vs a PC. I can personally attest that when I used to use my laptop, I was experiencing almost 5 second compile times when using `#include <bits/stdc++.h>`, reduced slightly by precompiling headers. After getting a PC, my compile times even without precompiling headers was easily under 1 second, sometimes 2. Actual execution times also received similar boosts. And I can only imagine the difference if using a more powerful machine or multiple. When I interned at a machine learning lab last year, we used supercomputers with GTX Titan Blacks for faster training of the neural networks, and I can only imagine applying such computational power to FHC.

Most of the time, this difference in hardware won't matter if you come up with the intended solution. FHC problem bounds are small enough that if you code a solution with correct complexity, you can expect to finish running on the input under a minute even with the crappiest hardware. Where it starts matter is when you have less optimal complexities, like $\mathcal O(n^2)$ for $n \leq 10^5$. I feel like cheesing with high computational power is definitely a viable strategy, just that no one has opted to do so yet because FHC is primarily targeted towards competitive programmers who focus more on solving problems with thinking. And if this strategy ends up being viable, these contests can quickly devolve into being pay-to-win instead of being based on algorithm solving prowess.

Regardless, I don't think this is a serious threat to the competition at the moment. As I mentioned before, FHC is mainly targeted towards competitive programmers, not machine learning scientists with access to insane cloud computing from their lab. It's just a thought I had. In the worst case, imagine if FHC problems started adapting to account for increased computational power. Problems start having $\mathcal O(n^2)$ as the intended solution for $n \leq 10^5$. That would be funny.

---

Thanks for reading! I apologize for posting what is essentially a filler article. I'm currently working on a longer tutorial article, but between classes and other responsibilities it'll take more time to complete, so be on the look out for that.

---
layout: post
title: "How to Understand Hard Tutorials"
tags: [tutorial, opinion]
usemathjax: true
---

*Hooray, it's my first post of October, meaning I've kept this blog running for at least a month!*

Tell me if this sounds familiar: you see a new tutorial blog posted on Codeforces about some crazy advanced topic. You read through it, and you find it goes way over your head really quickly. Or you open an editorial for some problem, and you get lost after reading the first few lines. While I can't guarantee you'll be able to understand every tutorial after reading this article, I can provide some insight on how I approach reading and understanding tutorials.

## 1. Make sure you meet the prerequisites.
Every tutorial assumes some amount of previous knowledge before reading it. For example, most tutorials assume you know how to code and possess an understanding of mathematics up to a high school level. A tutorial that builds on an existing topic will also usually require knowledge of that previous topic (i.e. a tutorial on persistent segment trees likely requires you to know what a segment tree is). Authors usually don't outright tell you what prerequisites are required, but you can infer them by skimming through the tutorial and looking for keywords. If a tutorial suddenly states "and now this is just a trivial application of convex hull trick" without further elaboration, "convex hull trick" is probably a prerequisite assumed by this tutorial.

Let's give an example. Say you're reading [this article on subtree lazy propagation for link-cut trees](https://codeforces.com/blog/entry/80145). What are the prerequisites?
1. Know what a link-cut tree is. It's literally in the title.
    1. Digging into link-cut trees reveal that you'll need to know what a splay tree is, since those are used to implement a link-cut tree is. More fundamentally, knowledge of what trees are and common properties of trees can help.
2. How to maintain subtree aggregate data for a link-cut tree.
3. How lazy propagation works, which you can pick up from a tutorial about lazy propagation for segment trees, since that's usually the simplest context where lazy propagation is introduced.

Most of those were just deduced from the title/first paragraph, which is quite convenient.

Here's another example: [this action-packed blog about generating functions](https://codeforces.com/blog/entry/77468). What are the prerequisites?
1. Being very comfortable with manipulating algebraic expressions and summations, given the high volume of LaTeX I see.
2. Knowledge of common math functions like $\exp$, $\ln$, and $\binom{n}{k}$.
3. You don't actually need to understand how to perform operations on polynomials such as $P(x) Q(x)$, $\sqrt{P(x)}$, or $\ln P(x)$ in subquadratic time, just know that that's possible.

I'd say that this blog isn't intimidating because of the knowledge required, but because of its density and how quickly it escalates. I believe that if you meet the prerequisites (most of which is likely acquired in your high school or undergrad depending on where you live), you can understand this blog by slowly working through the blog line by line. Which leads me to my second point...

## 2. Don't skip sections!
This is the fastest way to get lost. Tutorials are often self-building, where each section feeds understanding of the next. If you don't understand a section, I would highly recommend not moving on to the next section until you understand it, because it is very unlikely that a later section will clarify what you don't understand, and more likely that you'll just get lost.

**There is one exception to this rule: proofs.** Proofs are slightly different, because they're sometimes offered as a supplement for completeness or satisfaction rather than understanding. I've often found that if I understand the general intuition behind the concept, I can skip the proofs with little consequence (though usually I read the proof anyways out of curiosity). I wouldn't recommend always skipping over proofs though. A good proof can sometimes make things more clear.

## 3. Actively think about what you're reading.
Sometimes, you followed steps 1 and 2, but you still get stuck on a section. This might be because the step that the author is taking is not intuitive, or because the author skipped a few steps in between. While the author could be to blame for not doing enough handholding, the reader can also try to think on their own about why a step is true. In fact, it can sometimes be more satisfying to figure out why a step holds true on your own rather than getting it all from the author. Coming up with your own explanation or intuition for some concept is a great way to improve your thinking abilities. This isn't an invitation to authors to be as terse as possible in their writing though: a tutorial that skips too much is simply not helpful to anyone.

## 4. You rarely read a good tutorial once.
There are some blogs that I frequently refer back to when I want to recall how to do something. I don't have the best memory, so if I read a blog about ["queue undo trick"](https://codeforces.com/blog/entry/83467) a year ago, the most I might remember when I see such a problem is "oh, this is possible, I read some blog about this." Then, I'll pull up the blog from my bookmarks and figure out the implementation from there. Or other times, I'll see a cool blog but conclude I don't meet the prerequisites for it yet, so I'll store the second half of the article for future use. In fact, one such example is the [generating functions blog](https://codeforces.com/blog/entry/77468) I mentioned earlier, which I still have yet to fully read through even part 1, mostly because I'm too lazy to read and actively think about the later sections at the moment. The point is, I recommend saving blogs you like so you can come back to it when you have a better understanding of the prerequisites, or when you just want to recall the information.

## 5. Finally, ask if stuck.
I put this as the final step because I feel people often skip to this step too often before considering the previous ones. But indeed, sometimes the author simply didn't explain a part as clearly as they could have. Or they skipped over a non-trivial step thinking it was trivial. **My biggest pet peeve is when someone asks to simply "explain it again."** Too often, people ask something to the effect of "can you explain it again in more detail?" The issue here is it's not targeted. I have no idea which section/line you're confused about. Asking something like "I don't get how X is true in section Y because Z" is way more useful because the author immediately knows which section was unclear and can tailor their response to that question.

## Extra: Taking Notes?
I leave this as an extra section because I personally don't do this, but I know other people do. I have heard that taking notes yourself can help you retain the information better and also lets you organize the information in a way that helps with recall (step 4). However, when I personally tried it once, I deemed note taking to be too much effort for marginal benefit in return. Ultimately, it depends on what type of person you are. We all learn differently, so do what works for you.

---

That's all I have. I know some of this might sound like common sense, but one of my favorite quotes is "common sense is not common practice," and I hope this article can encourage some more common practice in your learning. Let me know if you agree or disagree with any of my suggestions.

---
layout: post
title: "Self Reflection and What's Next?"
tags: [personal, opinion]
---

It's been a month since my last major milestone in CP, and high time for some self reflection. There are two major CP competitions I competed in this month: Facebook Hacker Cup, and Kotlin Heroes 8.

## FHC

![image 1]({{ site.baseurl }}/assets/images/reflection-1.png)

In last weekend's round 3, I got a frustrating 222nd place, narrowly missing the top 200 badge on the t-shirt. To think that if I just did A and D1 faster, I could have gotten it! Or if only they counted top 200 for round 2 as well T_T.

Saltiness aside, my downfall in this contest was unfortunately implementation. In both my implementations for A and D1, I had a critical bug that required stress testing against a brute force solution to find. Those types of bugs are the worst, because it's so time consuming to write another solution AND a generator, and that's just the time taken to find the test case. When you add on the time it takes to analyze the test case, print debug output, and think about what's wrong, it's easy to lose a significant amount of time debugging. Thus, I submitted A at the 40 min mark and D1 at the 1 hr 50 min mark, both very undesirably late and not leaving much time to work on the other problems. A similar situation happened with C2 in round 2, so implementation woes is definitely a common trend with my FHC experience.

Practicing implementation is a strange thing. For me, writing clean implementations in a contest environment vs practice are two very different things. In practice, I'm usually able to keep my implementation relatively clean. On the other hand, in contest I'm often worried about other variables like time left in the contest, and I approach code writing in more of an additive manner, where I'll keep adding in more code on top of the old code to fix issues instead of refactoring, resulting in nonsense like this:

<div markdown="1" style="text-align: center; margin-bottom: 5%">
![image 2]({{ site.baseurl }}/assets/images/reflection-2.png)

<em>TFW you run out of ideas for variable names so you go with "yyyy"</em>
</div>
Perhaps the strat is to just approach implementation the same way I do in practice. Don't worry too much about the time, because writing sloppy code will probably consume more time overall anyways.

## Kotlin Heroes

![image 3]({{ site.baseurl }}/assets/images/reflection-3.png)
Kotlin heroes went slightly better for me, cause at least I got the shirt. Aside from the slow start from figuring out the kinks of Kotlin, I was on a pretty good pace for A-F, snagging me within top 50 off of speedforces. That being said, G was an annoying problem to not get because I felt that type of problem (DP with data structures) is within my ballpark. I got the naive DP recurrence: `dp[i][j] = max(dp[i-1][j] + a[i][j], dp[i-1][j+1])`, where `dp[i][j]` is the maximum damage after the first `i` warriors made moves and there are `j` barricades left, and `a[i][j]` is equal to the amount of damage warrior `i` can do if there are `j` barricades left (usually equal to 0). But the way I went about optimizing it was wrong: I tried to think of each row of the DP table as an array of segments of equal values. Then, the transition can be modeled as having each segment extend one unit to the left if it's larger than the left segment and updating a single point (which potentially creates a new segment). So kind of like a slope trick-esque approach. See the diagram below:

![image 4]({{ site.baseurl }}/assets/images/reflection-4.png)

Unfortunately, I couldn't find any good properties of this function, which made it difficult to speed it up past quadratic complexity. A better intuition is to map `dp[i][j]` as 2D coordinate points, and mark the transitions as querying a triangle:

![image 5]({{ site.baseurl }}/assets/images/reflection-5.png)

Essentially, most DP transitions don't change the DP value. Only a linear number of states actually change value by adding `a[i][j]`, so we can consider the transitions more directly between value changing points. `dp[i][j]` can transition from any previous state `dp[k][l]` if `k + l < i + j`. So a state can transition from any previous state within a triangle. Once you apply that intuition, the way to speed up the DP recurrence with data structures is far more apparent.

Missing the necessary intuition is probably the most common reason I fail to solve a problem, and unfortunately it's also the most nebulous to try to improve on. It's hard to practice to just "become smarter." In my experience, "becoming smarter" usually happens from the result of just doing a lot of relatively difficult problems (so above your rating range). Once you've done enough, you start to notice patterns in trains of thought and get better intuition for which ideas are good and which aren't (I talked extensively about this in a [post from last month]({{ site.baseurl }}/difficulty)). So perhaps I'd have to start grinding hard problems again if I want to get over this plateau.

## What's Next?
I'm not entirely sure. To be honest, I don't practice or spend anywhere close to as much time on CP as I used to, mainly because I've now found interest in other hobbies and things. Reaching red was kind of a "final boss" for me in CP, and now that I've hit red, I don't know how much effort I'd want to commit to the next milestone. But at the very least, I can assure you this blog will still be updated. Writing this blog hasn't gotten boring yet :)

How about you? Where are you in your CP journey, what's your next goal, and what specific aspects of CP do you need to work on? Feel free to comment them below.

---
layout: post
title: "Trivialize Linear Recurrence Problems with Berlekamp-Massey!"
tags: [tutorial, math]
featured: true
usemathjax: true
---

Berlekamp-Massey is a powerful tool that can knock out almost all linear recurrence problems, but it's often explained in the context of BCH decoding in many online tutorials, making it difficult to understand in a more general sense. There's [TLE's Codeforces blog](https://codeforces.com/blog/entry/61306), which contains all the core concepts of the algorithm but is a bit terse in my opinion. There's also [Massey's paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1054260), which is more on the mathematically rigorous side, and I have incorporated some of Massey's proofs into the proof section of this article. The goal of this article is to explain Berlekamp-Massey using my intuition/understanding of it.

**I promise this article isn't hard, just dense.** You won't need an understanding of mathematics beyond high school, but it's very easy to get lost if you don't read line-by-line and just skim it. I tried my best to make it as clear as possible, but there's only so much I can convey with words.

## Table of Contents
1. [Definition of a Linear Recurrence](#definition)
2. [Finding the $k$th Term of a Linear Recurrence](#kth)
3. [What is the Berlekamp-Massey algorithm?](#berlekamp-massey)
4. [An Example](#example)
5. [How does our process for choosing $d$ work?](#d-process)
6. [Which $b$ do we choose?](#b-process)
7. [The First Step](#first-step)
8. [Putting it all together in code…](#code)
9. [Test for Understanding](#test)
10. [Proofs](#proofs)
11. [Applications in Problems](#applications)
12. [Problems](#problems)
13. [References](#references)

## Definition of a Linear Recurrence <a name="definition"></a>

Let's say we have some arbitrary sequence of numbers:

$$
\{1, 2, 4, 8, 13\}
$$

Now, let's say we use the following rule to generate an infinite sequence from this initial sequence:

$$
s_i = s_{i-1} + 2s_{i-2} + 5s_{i-3} - 3s_{i-4} - s_{i-5} \\
s_0 = 1 \\ s_1 = 2 \\ s_2 = 4 \\ s_3 = 8 \\ s_4 = 13
$$

Using this rule, what will the next element of this sequence be? What will $s_5$ be?

$$
\begin{align*}
s_5 &= s_4 + 2s_3 + 5s_2 - 3s_1 - s_0 \\
&= 13 + 2 \cdot 8 + 5 \cdot 4 - 3 \cdot 2 - 1 \\
&= 42
\end{align*}
$$

And what will $s_6$ be?

$$
\begin{align*}
s_6 &= s_5 + 2s_4 + 5s_3 - 3s_2 - s_1 \\
&= 42 + 2 \cdot 13 + 5 \cdot 8 - 3 \cdot 4 - 2 \\
&= 94
\end{align*}
$$

Following this rule, here are the first 10 elements of our sequence:

$$
\{1, 2, 4, 8, 13, 42, 94, 215, 566, 1327, \dots\}
$$

We say the following sequence is defined by the **linear recurrence** $s_i = s_{i-1} + 2s_{i-2} + 5s_{i-3} - 3s_{i-4} - s_{i-5}$. The first 5 terms, $s_0, s_1, \dots, s_4$, are the **base case** or **initial conditions** of the recurrence. In general, a linear recurrence is a recurrence relation of the form:

$$
s_i = \sum_{j=1}^n c_j s_{i-j}
$$

where $c_j$ are constants, and $n$ is the length of the linear recurrence. Technically, what we defined above is a **homogeneous** linear recurrence. A linear recurrence could also be non-homogeneous, such as if we tack on a constant:

$$
s_i = p + \sum_{j=1}^n c_j s_{i-j}
$$

However, we will only be dealing with homogeneous linear recurrences in this article. After all, we can transform a $n$-length linear recurrence with an added constant into a $(n+1)$-th length homogeneous linear recurrence anyways.

<details markdown="1" style="margin-bottom: 5%"><summary>How?</summary>

Shift the linear recurrence and subtract from itself:

$$
\begin{align*}
s_i &= p + c_1 s_{i-1} + c_2 s_{i-2} + c_3 s_{i-3} + \dots + c_n s_{i-n} \\
s_{i+1} &= p + c_1 s_{i} + c_2 s_{i-1} + c_3 s_{i-2} + \dots + c_n s_{i-n+1} \\
s_{i+1} - s_i &= c_1 s_{i} + (c_2 - c_1) s_{i-1} + (c_3 - c_2) s_{i-2} + \dots + (c_n - c_{n-1}) s_{i-n+1} - c_n s_{i-n} \\
s_{i+1} &= (c_1 + 1) s_{i} + (c_2 - c_1) s_{i-1} + (c_3 - c_2) s_{i-2} + \dots + (c_n - c_{n-1}) s_{i-n+1} - c_n s_{i-n}
\end{align*}
$$

---

</details>

Here are some more examples (and one non-example) of linear recurrences:

<details markdown="1" style="margin-bottom: 5%"><summary>Example 1</summary>

$$
F_i = F_{i-1} + F_{i-2} \\
F_0 = 0 \\
F_1 = 1
$$

This linear recurrence is the well-known Fibonacci sequence.

---

</details>

<details markdown="1" style="margin-bottom: 5%"><summary>Example 2</summary>

$$
s_i = 2s_{i-2} \\
s_0 = 1 \\
s_1 = 3
$$

Despite this linear recurrence appearing to only have 1 term, we will still refer to it as having length 2. There is an implicit $0 \cdot s_{i-1}$ term. Essentially, we define length as the number of base case terms necessary.

---

</details>

<details markdown="1" style="margin-bottom: 5%"><summary>Example 3</summary>

$$
s_i = 0
$$

This is technically a homogeneous linear recurrence, albeit a trivial example.

---

</details>

<details markdown="1" style="margin-bottom: 5%"><summary>Example 4</summary>

$$
s_i = s_{i-1} s_{i-2} + 2s_{i-3} \\
s_0 = 0 \\
s_1 = 1 \\
s_2 = 1
$$

The above recurrence is **not** a linear recurrence. This is because the $s_{i-1} s_{i-2}$ term violates the linear condition, as it's not a constant times a single previous term. This would be a [quadratic recurrence](https://mathworld.wolfram.com/QuadraticRecurrenceEquation.html).

---

</details>

Once we obtain the linear recurrence, we can compute the $k$th term of a $n$ length linear recurrence in $\mathcal O(nk)$ naively. Or, if $k$ is larger (say $k \leq 10^{18}$), we can instead compute it using [matrix exponentiation](https://usaco.guide/plat/matrix-expo?lang=cpp) in $\mathcal O(n^3 \log k)$ time. However, what might be less known is an even faster algorithm that works in $\mathcal O(n \log n \log k)$. Let's learn about it in the next section.

## Finding the $k$th Term of a Linear Recurrence <a name="kth"></a>

This isn't part of the Berlekamp-Massey algorithm, but it's still useful to know. [TLE's blog](https://codeforces.com/blog/entry/61306) covers this algorithm pretty well, but I'll include it here for the sake of completeness.

First, for a polynomial $f(x) = \sum_{i=0}^n c_i x^i$, we define $G(f) = \sum_{i=0}^n c_i s_i$, where $s_i$ are the terms of our infinite sequence generated by our recurrence. $G$ satisfies the properties $G(f + g) = G(f) + G(g)$ for polynomials $f$ and $g$ and $G(kf) = k G(f)$ for scalar $k$. The $k$th term of our sequence is just $G(x^k)$:

$$
\begin{align*}
G(x^k) &= G(1 \cdot x^k + 0 \cdot x^{k-1} + 0 \cdot x^{k-2} + \dots + 0 \cdot x + 0 \cdot 1) \\
&= 1 \cdot s_k + 0 \cdot s_{k-1} + 0 \cdot s_{k-2} + \dots + 0 \cdot s_1 + 0 \cdot s_0 \\
&= s_k
\end{align*}
$$

This alone is kind of silly, because $G(x^k)$ requires knowledge of $s_k$ to evaluate, which is what we're trying to find. However, let's consider a special polynomial. For a linear recurrence $\\{c_1, c_2, \dots, c_n\\}$, we define the **characteristic polynomial** of that recurrence as $f(x) = x^n - c_1 x^{n-1} - c_2 x^{n-2} - \dots - c_{n-1} x - c_n$.

The characteristic polynomial has interesting properties when applying $G$. What is $G(f)$?

<details markdown="1" style="margin-bottom: 5%"><summary>Answer</summary>

$$
\begin{align*}
G(f) &= G(x^n - \sum_{j=1}^n c_j x^{n-j}) \\
&= 1 \cdot s_n - c_1 \cdot s_{n-1} - c_2 \cdot s_{n-2} - \dots - c_{n-1} \cdot s_1 - c_n \cdot s_0 \\
&= 0
\end{align*}
$$

The last line is because $s_n$ satisfies the recurrence relation $s_i = \sum_{j=1}^n c_j s_{i-j}$.

---

</details>

And similarly, what is $G(x f(x))$? Or $G(x^2 f(x))$? Or $G(f(x) g(x))$ for any arbitrary polynomial $g(x)$?

<details markdown="1" style="margin-bottom: 5%"><summary>Answer</summary>

$$
\begin{align*}
G(x f(x)) &= G(x^{n+1} - \sum_{j=1}^n c_j x^{n+1-j}) \\
&= 1 \cdot s_{n+1} - c_1 \cdot s_n - c_2 \cdot s_{n-1} - \dots - c_{n-1} \cdot s_2 - c_n \cdot s_1 \\
&= 0 \\
G(x^2 f(x)) &= G(x^{n+2} - \sum_{j=1}^n c_j x^{n+2-j}) \\
&= 1 \cdot s_{n+2} - c_1 \cdot s_{n+1} - c_2 \cdot s_n - \dots - c_{n-1} \cdot s_3 - c_n \cdot s_2 \\
&= 0
\end{align*}
$$

A similar proof applies to $G(x^k f(x))$ for any arbitrary $k$. And thus, $G(f(x) g(x)) = 0$ as well because we can distribute over the terms of $g$, pull out the scalar, and apply our knowledge of $G(x^k f(x)) = 0$.

---

</details>

What is the implication of this? Consider $x^k / f(x)$. We can express any two polynomials $a(x)$ and $b(x)$ as $a(x) = b(x) q(x) + r(x)$, where $q(x)$ is the quotient after dividing $a(x)$ by $b(x)$ and $r(x)$ is the remainder. Applying this to $x^k / f(x)$, we get

$$
\begin{align*}
x^k &= f(x) q(x) + r(x) \\
G(x^k) &= G(f(x) q(x) + r(x)) \\
&= G(f(x) q(x)) + G(r(x)) \\
&= G(r(x))
\end{align*}
$$

We use the notation $r(x) = a(x) \bmod b(x)$. So it suffices to compute $G(x^k \bmod f(x))$! If we use binary exponentiation, we can compute this in $\mathcal O(n \log n \log k)$:

```c++
template<typename T>
T solve(const vector<T> &c, const vector<T> &s, long long k) {
    int n = (int) c.size();
    assert(c.size() <= s.size());

    vector<T> a = n == 1 ? vector<T>{c[0]} : vector<T>{0, 1}, x{1};
    for (; k>0; k/=2) {
        if (k % 2)
            x = mul(x, a);  // mul(a, b) computes a(x) * b(x) mod f(x)
        a = mul(a, a);
    }
    x.resize(n);

    T ret = 0;
    for (int i=0; i<n; i++)
        ret += x[i] * s[i];
    return ret;
}
```

If you want to know how to compute $a(x) \bmod b(x)$ in $\mathcal O(n \log n)$, you can refer to this [cp-algorithms article](https://cp-algorithms.com/algebra/polynomial.html#toc-tgt-5).

In practice, we usually perform `mul` naively in $\mathcal O(n^2)$, giving us an $\mathcal O(n^2 \log k)$ algorithm instead. This is for two reasons.
1. Berlekamp-Massey works in $\mathcal O(n^2)$ and is usually the bottleneck anyways, so the $\mathcal O(n \log n \log k)$ algorithm has limited usage.
2. The $\mathcal O(n \log n \log k)$ algorithm uses FFT, so we would have to implement FFT with arbitrary modulus if our modulus was not FFT-friendly (e.g. $10^9 + 7$).

<details markdown="1" style="margin-bottom: 5%"><summary>Naive Implementation of "mul"</summary>

```c++
vector<T> mul(const vector<T> &a, const vector<T> &b) {
    vector<T> ret(a.size() + b.size() - 1);
    // ret = a * b
    for (int i=0; i<(int)a.size(); i++)
        for (int j=0; j<(int)b.size(); j++)
            ret[i+j] += a[i] * b[j];
    // reducing ret mod f(x)
    for (int i=(int)ret.size()-1; i>=n; i--)
        for (int j=n-1; j>=0; j--)
            ret[i-j-1] += ret[i] * c[j];
    ret.resize(min((int) ret.size(), n));
    return ret;
}
```

---

</details>

Now it's time for the meat of the article: obtaining the linear recurrence with Berlekamp-Massey.

## What is the Berlekamp-Massey algorithm? <a name="berlekamp-massey"></a>

Given some finite sequence of numbers $s$, the Berlekamp-Massey algorithm can find the shortest linear recurrence that satisfies $s$. For example, if we plug in the sequence $\\{0, 1, 1, 3, 5, 11, 21\\}$, the algorithm will return the sequence $\\{1, 2\\}$, corresponding to the linear recurrence $s_i = s_{i-1} + 2s_{i-2}$.

Before I explain the algorithm, I will unfortunately have to do a quick definition/notation dump. This is mainly so that I can make the rest of the article less verbose. Don't worry, most of the notation is very natural in my opinion.

- Sequences will be denoted using curly brackets (e.g. $\\{1, 2, 3\\}$ or $\\{43, 123, 2, 10\\}$).
- 0-based indexing will be used for the sequence, and 1-based indexing will be used for the sequence of recurrence relation coefficients. The sequence will be $s$, and the sequence of recurrence relation coefficients will be $c$. So we will have $\\{s_0, s_1, s_2, \dots\\}$ and $\\{c_1, c_2, \dots, c_n\\}$ satisfying $s_i = \sum_{j=1}^n c_j s_{i-j}$ for $i \geq n$.
- The length of a sequence is denoted as $\|c\|$ or $n$.
- Say I have some recurrence relation sequence $c = \\{c_1, c_2, \dots, c_n\\}$. I will denote "evaluating the recurrence relation at index $i$" as $c(i) = \sum_{j=1}^n c_j s_{i-j}$. If the recurrence relation correctly evaluates $s_i$, then $c(i) = s_i$.
    - A recurrence relation is correct if it successfully evaluates $c(i) = s_i$ for all $i \geq n$.
    - To avoid negative indices for $c(i)$ when $i < n$, we will simply say $c(i) = s_i$ is always true for $i < n$. This is an acceptable definition since those $s_i$ form the base case anyways.
    - If $c$ is empty, $c(i) = 0$.
- Say we have two sequences $c$ and $d$. I define adding two sequences $c$ and $d$ as $c + d = \\{c_1 + d_1, c_2 + d_2, \dots, c_n + d_n\\}$.
    - If $c$ and $d$ are of different lengths, we pad with zeros. So say $\|c\| = n, \|d\| = m > n$. Then $c + d = \\{c_1 + d_1, c_2 + d_2, \dots, c_n + d_n, d_{n+1}, \dots, d_m\\}$.
    - From this definition, we have the identity $c(i) + d(i) = (c + d)(i)$. You can prove this by explicitly writing the terms out.
- I define scalar multiplication with a sequence $c$ as $kc = \\{kc_1, kc_2, \dots, kc_n\\}$.
    - From this definition, we have the identity $k \cdot c(i) = (kc)(i)$. You can prove this by explicitly writing the terms out.
- The general gist of Berlekamp-Massey is as follows: we have some initial $c$. Each time we process the next element $s_i$, we will check if $c$ correctly evaluates $s_i$. If it's correct, we keep $c$. If it's wrong, we know $c$ isn't the correct recurrence relation for our sequence. We will have $c(i) \neq s_i$. Then, we will adjust $c$ to some new $c'$ such that $c'(j) = s_j$ for all $j \leq i$.
    - We say a recurrence relation sequence $c$ **fails** at index $i$ if $i$ is the first index where $c(i) \neq 0$.

Phew! Ok, let's start.

## An Example <a name="example"></a>

It is much easier to explain with an example. So let's use the sequence $s = \\{1, 2, 4, 8, 13, 20, 28, 215, 757, 2186\\}$. We will begin with an empty recurrence relation sequence $c = \\{\\}$.

Let's start by processing $i = 0$. We have $c(0) = 0 \neq 1 = s_0$ (recall that if $c$ is empty, $c(i) = 0$). Hmm, that's not right. Since this is the first time we're initializing $c$, we will merely set $c = \\{1\\}$. It's very likely that we'll have to change $c$ in the next step anyways, so the main purpose of initializing $c$ like this is simply to ensure $s_0$ is the base case. For all we care, we could set $c = \\{0\\}$ or $c = \\{69\\}$ and it wouldn't matter.

Next, let's go to $i = 1$. We have $c(1) = c_1 \cdot s_{1-1} = 1 \cdot 1 = 1 \neq 2 = s_1$. Ok, unfortunately the recurrence relation was wrong. We'll have to adjust. Fortunately, there's a simple fix to this: change to $c = \\{2\\}$. Now, $c(1) = 2 \cdot 1 = 2$ like we wanted.

Next, we process $i = 2$. We have $c(2) = c_1 \cdot s_{2-1} = 2 \cdot 2 = 4 = s_2$. It appears our $c$ works, so there's no need to modify it.

Next, $i = 3$. $c(3) = c_1 \cdot s_{3-1} = 2 \cdot 4 = 8 = s_3$. This works, so we make no changes.

Next, $i = 4$. $c(4) = c_1 \cdot s_{4-1} = 2 \cdot 8 = 16 \neq 13 = s_3$. We need to adjust. What if we made $c = \\{13 / 8\\}$? The issue is, now $c(4) = 13$ like we wanted, but $c(3)$ is wrong! We can no longer assume the linear recurrence is just of length 1. We'll need something more sophisticated.

Let's be more specific about what we want. We want some sequence $c' = c + d$ such that $c'(i)$ evaluates correctly for all $i \leq 4$. So we want some $d$ such that $d(i) = 0$ for $i < 4$ and $d(4) = s_4 - c(4) = -3$. We denote our desired value for $d(4)$ as $\Delta$.

Here's the trick to generate such a $d$: let's keep track of each previous version of $c$ and which index it failed on. So for example, we have $\\{\\}$ which failed at index 0 and $\\{1\\}$ which failed at index 1. Let's consider the second version of $c$, the $\\{1\\}$ sequence that failed at index 1. Let's denote the failure index as $f$. Here's what we'll do:
1. Set $d$ equal to that sequence.
2. Multiply the sequence by $-1$.
3. Insert a 1 on the left.
4. Multiply the sequence by $\frac{\Delta}{d(f + 1)} = \frac{-3}{1} = -3$.
5. Insert $i - f - 1 = 4 - 1 - 1 = 2$ zeros on the left.

<details markdown="1" style="margin-bottom: 5%"><summary>$d$ after each step</summary>

After Step 1: $d = \\{1\\}$

After Step 2: $d = \\{-1\\}$

After Step 3: $d = \\{1, -1\\}$

After Step 4: $d = \\{-3, 3\\}$

After Step 5: $d = \\{0, 0, -3, 3\\}$

---

</details>

So we have $d = \\{0, 0, -3, 3\\}$. I know this seems super arbitrary, but let's evaluate $d$ and see what happens:

$$
d(4) = d_1 s_{4-1} + d_2 s_{4-2} + d_3 s_{4-3} + d_4 s_{4-4} = 0 \cdot 8 + 0 \cdot 4 - 3 \cdot 2 + 3 \cdot 1 = -3
$$

And $d$ isn't defined for $i < 4$, since it's a length 4 sequence. So I guess this $d$ works. Our new recurrence is thus $c + d = \\{2\\} + \\{0, 0, -3, 3\\} = \\{2, 0, -3, 3\\}$.

Let's keep going. Our new $c$ will work for $i = 5$ and $i = 6$, but fail for $i = 7$.

$$
c(7) = c_1 s_{7-1} + c_2 s_{7-2} + c_3 s_{7-3} + c_4 s_{7-4} = 2 \cdot 28 + 0 \cdot 20 - 3 \cdot 13 + 3 \cdot 8 = 41 \neq 215 = s_7
$$

We need some $d$ to add to $c$ such that $d(i) = 0$ for $i < 7$ and $d(7) = s_7 - c(7) = 174$.

Once again, let's look at the past versions of $c$:
- $\\{\\}$ which failed at index 0.
- $\\{1\\}$ which failed at index 1.
- $\\{2\\}$ which failed at index 4.

This time, we'll consider the third version of $c$, the $\\{2\\}$ sequence that failed at index 4 (I'll explain which version you pick in a moment). We will apply the following seemingly arbitrary steps:
1. Set $d$ equal to our chosen sequence.
2. Multiply the sequence by $-1$.
3. Insert a 1 on the left.
4. Multiply the sequence by $\frac{\Delta}{d(f + 1)} = \frac{174}{-3} = -58$.
5. Insert $i - f - 1 = 7 - 4 - 1 = 2$ zeros on the left.

<details markdown="1" style="margin-bottom: 5%"><summary>$d$ after each step</summary>

After Step 1: $d = \\{2\\}$

After Step 2: $d = \\{-2\\}$

After Step 3: $d = \\{1, -2\\}$

After Step 4: $d = \\{-58, 116\\}$

After Step 5: $d = \\{0, 0, -58, 116\\}$

---

</details>

Does our $d$ work? Let's find out.

$$
\begin{align*}
d(7) &= d_1 s_{7-1} + d_2 s_{7-2} + d_3 s_{7-3} + d_4 s_{7-4} = 0 \cdot 28 + 0 \cdot 20 - 58 \cdot 13 + 116 \cdot 8 = 174 \\
d(6) &= d_1 s_{6-1} + d_2 s_{6-2} + d_3 s_{6-3} + d_4 s_{6-4} = 0 \cdot 20 + 0 \cdot 13 - 58 \cdot 8 + 116 \cdot 4 = 0 \\
d(5) &= d_1 s_{5-1} + d_2 s_{5-2} + d_3 s_{5-3} + d_4 s_{5-4} = 0 \cdot 13 + 0 \cdot 8 - 58 \cdot 4 + 116 \cdot 2 = 0 \\
d(4) &= d_1 s_{4-1} + d_2 s_{4-2} + d_3 s_{4-3} + d_4 s_{4-4} = 0 \cdot 8 + 0 \cdot 4 - 58 \cdot 2 + 116 \cdot 1 = 0
\end{align*}
$$

Holy cow this actually works! So we add $d$ to our old $c$ to get our new $c$: $\\{2, 0, -3, 3\\} + \\{0, 0, -58, 116\\} = \\{2, 0, -61, 119\\}$.

Finally, we process $i = 8$ and $i = 9$, and find the recurrence is correct for both of them. So $c = \\{2, 0, -61, 119\\}$ is our final answer.

## How does our process for choosing $d$ work? <a name="d-process"></a>

Let's examine those "arbitrary" steps closely, shall we?
1. Set $d$ equal to our chosen sequence.
2. Multiply the sequence by $-1$.
3. Insert a 1 on the left.
4. Multiply the sequence by $\frac{\Delta}{d(f + 1)}$.
5. Insert $i - f - 1$ zeros on the left.

First, we choose some older version of $c$, which we will denote as $b$ and having failed at index $f$. Notice that $b(j) = s_j$ for $j < f$ and $b(f) \neq s_f$. That's true by the definition of a failed sequence. Let's instead consider it as $s_j - b(j) = 0$ for $j < f$ and $s_f - b(f) \neq 0$. That's essentially what steps 1-3 are doing: they set $d$ such that $d(j + 1) = s_j - b(j)$.

<details markdown="1" style="margin-bottom: 5%"><summary>Written out more obviously</summary>

After step 1: $d = b = \\{b_1, b_2, \dots, b_n\\}$

After step 2: $d = \\{-b_1, -b_2, \dots, -b_n\\}$

After step 3: $d = \\{1, -b_1, -b_2, \dots, -b_n\\}$

And evaluating $d(j + 1)$:

$$
\begin{align*}
d(j + 1) &= \sum_{k=1}^n d_k s_{j+1-k} \\
&= 1 \cdot s_j - b_1 s_{j-1} - b_2 s_{j-2} - \dots - b_n s_{j-n} \\
&= s_j - (b_1 s_{j-1} + b_2 s_{j-2} + \dots + b_n s_{j-n}) \\
&= s_j - b(j)
\end{align*}
$$

---

</details>

We know that $d(j) = 0$ for $j \leq f$ and $d(f + 1) \neq 0$. After multiplying the sequence by $\frac{\Delta}{d(f + 1)}$ per step 4, we get that $d(j) = 0$ for $j \leq f$ still holds true, but now $d(f + 1) = \Delta$.

<details markdown="1" style="margin-bottom: 5%"><summary>Why?</summary>

You didn't forget the identity $k \cdot c(i) = (kc)(i)$, did you?

---

</details>

Finally, step 5 makes us insert $i - f - 1$ zeros on the left. To understand the effect of that, let's start by inserting one zero. Let $d'$ be $d$ with an extra zero inserted on the left. We have that $d'(j) = d(j - 1)$. To prove this, just write out the terms explicitly. So inserting zeros on the left is just a shift! If we insert $k$ zeros, we get $d'(j) = d(j - k)$. If we insert $i - f - 1$ zeros, we get $d'(j) = d(j - (i - f - 1))$. Specifically, $d'(i) = d(i - (i - f - 1)) = d(f + 1) = \Delta$, and $d'(j) = 0$ for $j < i$. This is exactly the condition we wanted to impose on our $d$ sequence!

## Which $b$ do we choose? <a name="b-process"></a>

While choosing any $b$ will work, let's not forget that in addition to finding a linear recurrence that works, we want a linear recurrence of minimal length.

Take a look at the diagram below.

![image 1]({{ site.baseurl }}/assets/images/berlekamp-massey-1.png)

We are currently processing $i = 8$. The current linear recurrence sequence is the red one, is of length 4, and has just failed at index 8. We have three previous versions of $c$: a yellow sequence of length 1 that failed at index 1, a green sequence of length 2 that failed at index 4, and a blue sequence of length 3 that failed at index 6. You can also imagine there's one more sequence: an empty sequence that failed at index 0. I didn't draw it since it's of length 0.

Let's say we pick the blue sequence as our $b$. I have now drawn a purple segment on our diagram:

![image 2]({{ site.baseurl }}/assets/images/berlekamp-massey-2.png)

This purple segment represents the length of $d$. You can verify that one unit of increased length comes from the inserted 1, and one unit of increased length comes from the $i - f - 1 = 8 - 6 - 1 = 1$ zeros inserted. And the final updated $c'$ comes from overlaying the red and purple segments, representing us adding $c$ and $d$.

This tells us what the optimal $b$ is: the one whose segment has the rightmost left endpoint! So we only need to keep track of that best $c$ throughout the algorithm, not every previous version.

## The First Step <a name="first-step"></a>

Selecting a previous version of $c$ to use only works if you're not updating $c$ for the first time. So what do you do if you do update $c$ for the first time? Let's find out with the following example: $s = \\{0, 0, 0, 0, 1, 0, 0, 2\\}$. Once again, $c = \\{\\}$ initially.

Process $i = 0$. $c(0) = 0 = s_0$. So we keep going.

Process $i = 1$. $c(0) = 0 = s_1$. So we keep going.

This repeats until we reach $i = 4$. $c(0) = 0 \neq 1 = s_4$. Since we are updating $c$ for the first time, there are no previous versions of $c$ to rely on. So we will initialize $c$ to an arbitrary length 5 sequence. The reason why it must be at least length 5 is because that is the minimal length required to include $s_4$ in the base case. If $s_4$ was not part of the base case, then it must satisfy some recurrence $s_4 = \sum_{j=1}^n c_j s_{4-j}$. But since $s_0, s_1, s_2, s_3$ are all zero, such a recurrence would always evaluate to 0. So $s_4$ must be part of the base case. For simplicity, we will assign $c = \\{0, 0, 0, 0, 0\\}$, although like I stated in the previous example, the values could be arbitrary.

Now, when you update $c$ a second time when failing at index 7, you can rely on $b = \\{\\}$ with failure index 4. You can try by hand to confirm that this works.

<details markdown="1" style="margin-bottom: 5%"><summary>What $c$ becomes at index 7</summary>

$$
\{0, 0, 2, 0, 0\}
$$

Yes, the extra trailing 0s matter! Without them, $s_4$ would not be part of the base case!

---

</details>

Notice that if the sequence is all 0s, this algorithm will simply return $c = \\{\\}$, which is fine since we defined $c(i) = 0$ for an empty sequence.

## Putting it all together in code... <a name="code"></a>

```c++
template<typename T>
vector<T> berlekampMassey(const vector<T> &s) {
    vector<T> c;    // the linear recurrence sequence we are building
    vector<T> oldC; // the best previous version of c to use (the one with the rightmost left endpoint)
    int f = -1;     // the index at which the best previous version of c failed on
    for (int i=0; i<(int)s.size(); i++) {
        // evaluate c(i)
        // delta = s_i - \sum_{j=1}^n c_j s_{i-j}
        // if delta == 0, c(i) is correct
        T delta = s[i];
        for (int j=1; j<=(int)c.size(); j++)
            delta -= c[j-1] * s[i-j];   // c_j is one-indexed, so we actually need index j - 1 in the code
        if (delta == 0)
            continue;   // c(i) is correct, keep going
        // now at this point, delta != 0, so we need to adjust it
        if (f == -1) {
            // this is the first time we're updating c
            // s_i was the first non-zero element we encountered
            // we make c of length i + 1 so that s_i is part of the base case
            c.resize(i + 1);
            mt19937 rng(chrono::steady_clock::now().time_since_epoch().count());
            for (T &x : c)
                x = rng();  // just to prove that the initial values don't matter in the first step, I will set to random values
            f = i;
        } else {
            // we need to use a previous version of c to improve on this one
            // apply the 5 steps to build d
            // 1. set d equal to our chosen sequence
            vector<T> d = oldC;
            // 2. multiply the sequence by -1
            for (T &x : d)
                x = -x;
            // 3. insert a 1 on the left
            d.insert(d.begin(), 1);
            // 4. multiply the sequence by delta / d(f + 1)
            T df1 = 0;  // d(f + 1)
            for (int j=1; j<=(int)d.size(); j++)
                df1 += d[j-1] * s[f+1-j];
            assert(df1 != 0);
            T coef = delta / df1;   // storing this in outer variable so it's O(n^2) instead of O(n^2 log MOD)
            for (T &x : d)
                x *= coef;
            // 5. insert i - f - 1 zeros on the left
            vector<T> zeros(i - f - 1);
            zeros.insert(zeros.end(), d.begin(), d.end());
            d = zeros;
            // now we have our new recurrence: c + d
            vector<T> temp = c; // save the last version of c because it might have a better left endpoint
            c.resize(max(c.size(), d.size()));
            for (int j=0; j<(int)d.size(); j++)
                c[j] += d[j];
            // finally, let's consider updating oldC
            if (i - (int) temp.size() > f - (int) oldC.size()) {
                // better left endpoint, let's update!
                oldC = temp;
                f = i;
            }
        }
    }
    return c;
}
```

I tried to annotate the code as best as I can, using the same terminology as in the article. The method uses generic types, so you can plug in modint if you're computing something under mod, or double, if you're working with real numbers (warning that numerical stability is not guaranteed). If you need a working modint template, you can find mine [here](https://github.com/mzhang2021/cp-library/blob/master/implementations/math/Modular.h). The complexity of this algorithm is clearly $\mathcal O(n^2)$.

## Test for Understanding <a name="test"></a>

Do you really understand everything I just explained? If so, try your hand at some of these exercises! For the purpose of this exercise, assume the first step fills with $i + 1$ zeros instead of random values.

1. Find the intermediate sequences of $c$ after each step of the algorithm on $s = \\{0, 2, 3, 4, 5, 6, 7, 8\\}$.

    <details markdown="1" style="margin-bottom: 5%"><summary>Answer</summary>

    $c$ after each $i$ is processed:

    $$
    \begin{align*}
    i = 0&: c = \{\} \\
    i = 1&: c = \{0, 0\} \\
    i = 2&: c = \{3/2, 0\} \\
    i = 3&: c = \{3/2, -1/4\} \\
    i = 4&: c = \{3/2, -1/4, -1/8\} \\
    i = 5&: c = \{2, -1, 0\} \\
    i = 6&: c = \{2, -1, 0\} \\
    i = 7&: c = \{2, -1, 0\}
    \end{align*}
    $$

    ---

    </details>

2. Find the intermediate sequences of $c$ after each step of the algorithm on $s = \\{1, 8, 10, 26, 46\\}$.

    <details markdown="1" style="margin-bottom: 5%"><summary>Answer</summary>

    $c$ after each $i$ is processed:

    $$
    \begin{align*}
    i = 0&: c = \{0\} \\
    i = 1&: c = \{8\} \\
    i = 2&: c = \{8, -54\} \\
    i = 3&: c = \{1, 2\} \\
    i = 4&: c = \{1, 2\}
    \end{align*}
    $$

    ---

    </details>

3. Find the intermediate sequences of $c$ after each step of the algorithm on $s = \\{1, 3, 5, 11, 25, 59, 141, 339\\}$.

    <details markdown="1" style="margin-bottom: 5%"><summary>Answer</summary>

    $c$ after each $i$ is processed:

    $$
    \begin{align*}
    i = 0&: c = \{0\} \\
    i = 1&: c = \{3\} \\
    i = 2&: c = \{3, -4\} \\
    i = 3&: c = \{1, 2\} \\
    i = 4&: c = \{1, 1, 3\} \\
    i = 5&: c = \{3, -1, -1\} \\
    i = 6&: c = \{3, -1, -1\} \\
    i = 7&: c = \{3, -1, -1\}
    \end{align*}
    $$

    ---

    </details>

## Proofs <a name="proofs"></a>

I know not everyone will be interested in this section, so I'll put them under a spoiler.

<details markdown="1" style="margin-bottom: 5%"><summary>Click Me!</summary>

The proofs come from [Massey's paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1054260) and will actually allow us to prove an even stronger condition on the minimum length recurrence relation, which will let us implement a conciser solution.

**Definition:** Let $L_i$ denote the minimum length recurrence relation for the prefix of the first $i$ elements of $s$. Let $r_i$ denote that recurrence relation of length $L_i$.

**Theorem:** If $r_{i-1}$ works for all positions up to $i - 1$ but not for position $i$ (i.e. $r_{i-1} \neq r_i$), then $L_i \geq \max(L_{i-1}, i + 1 - L_{i-1})$.

To prove this, we will prove the two arguments inside the $\max$ function separately.

**Lemma 1:** $L_i \geq L_{i-1}$

This lemma is kind of self-evident. By definition, $r_{i-1}$ must work for all positions up to $i - 1$ and is of minimal length for position $i - 1$. And $r_i$ must work for all positions up to $i$ and is of minimal length for position $i$. Say $L_i < L_{i-1}$. Then that contradicts the definition of $r_{i-1}$, because there exists a recurrence of shorter length that works for all positions up to $i - 1$: $r_i$. **Notice that this lemma is true for all $i$, not just when $r_{i-1} \neq r_i$.**

**Lemma 2:** $L_i \geq i + 1 - L_{i-1}$

Case 1: $L_{i-1} \geq i$

This case is self-evident, because then we get $L_i \geq 1$. There's no way $L_i = 0$ because that would imply $r_i$ is the empty sequence. $L_{i-1}$ would also have to be 0 because $L_{i-1} \leq L_i$ and $L_{i-1}$ can't be negative since you can't have negative length. So $L_{i-1}$ would also have to be the empty sequence, and $r_{i-1} = r_i$, meaning the theorem no longer applies since the theorem assumes $r_{i-1}$ fails for position $i$.

Case 2: $L_{i-1} < i$

Let the coefficients of $r_{i-1}$ be $c_j$ and the coefficients of $r_i$ be $d_j$. By definition:

$$
\sum_{j=1}^{L_{i-1}} c_j s_{k-j} \begin{cases}
= s_k & L_{i-1} \leq k < i \\
\neq s_k & k = i
\end{cases}
$$

and

$$
\sum_{j=1}^{L_i} d_j s_{k-j} \begin{array}{cc} = s_k & L_i \leq k \leq i \end{array}
$$

We will do a proof by contradiction. Assume $L_i < i + 1 - L_{i-1}$. Thus,

$$
\begin{align*}
\sum_{j=1}^{L_{i-1}} c_j s_{i-j} &= \sum_{j=1}^{L_{i-1}} c_j \left( \sum_{k=1}^{L_i} d_k s_{i-j-k} \right) \\
&= \sum_{j=1}^{L_{i-1}} \sum_{k=1}^{L_i} c_j d_k s_{i-j-k} \\
&= \sum_{k=1}^{L_i} \sum_{j=1}^{L_{i-1}} d_k c_j s_{i-k-j} \\
&= \sum_{k=1}^{L_i} d_k \left( \sum_{j=1}^{L_{i-1}} c_j s_{i-k-j} \right) \\
&= \sum_{k=1}^{L_i} d_k s_{i-k} \\
&= s_i
\end{align*}
$$

But this is a contradiction, since we forced by definition that $\sum_{j=1}^{L_{i-1}} c_j s_{i-j} \neq s_i$. Thus, $L_i \geq i + 1 - L_{i-1}$. Notice that the index $i - j - k$ never dips negative thanks to us assuming $L_i < i + 1 - L_{i-1}$.

Combining the above two lemmas provides us with our theorem.

### Proving the Algorithm

We will initiate a proof by induction. Let the base case be the first position of a non-zero element in $s$ (if all the elements in $s$ are 0, the recurrence is just the empty sequence and we are done). As proven under ["The First Step"](#first-step) section of this article, the optimal length of a first non-zero element at index $i$ is $i + 1$.

Now for the induction step: assume for all $j < i$, if $r_{j-1} \neq r_j$, then $L_j = \max(L_{j-1}, j + 1 - L_{j-1})$, and otherwise when $r_{j-1} = r_j$, then $L_j = L_{j-1}$. Then that same condition holds true for $j = i$. There are two cases.

Case 1: $r_{i-1}$ works for index $i$. In this case, we keep the same recurrence for the next index, and this is optimal per lemma 1.

Case 2: $r_{i-1}$ does not work for index $i$. Since this is not the first step, our algorithm chooses a previous version and use it to construct $d$. Recall from the section ["Which $b$ do we choose?"](#b-process) that it is optimal to pick the version with the rightmost left endpoint. From here, we spawn two more cases:

Case 2a: The left endpoint of the old version is greater than or equal to the left endpoint of our current recurrence. In this case, the length of our new recurrence sequence does not change, which is optimal.

Case 2b: The left endpoint of the old version is less than the left endpoint of our current recurrence. In this case, the length of our new recurrence does change! Recall our theorem, which states that $L_i \geq \max(L_{i-1}, i + 1 - L_{i-1})$ when our recurrence changes. Since the length is getting longer, it must be that $L_i \geq i + 1 - L_{i-1}$. Let's prove that our algorithm always yields $L_i = i + 1 - L_{i-1}$.

Let the index where our previous version failed be $m$. Notice that every time we update the previous version, it's when the recurrence got longer. So $L_{m-1} < L_m = L_{m+1} = \dots = L_{i-1}$. By induction, $L_m = \max(L_{m-1}, m + 1 - L_{m-1})$. Since $L_m > L_{m-1}$, $L_m = m + 1 - L_{m-1}$, or $L_{m-1} = m + 1 - L_m = m + 1 - L_{i-1}$. Now, note that if the previous version failed at index $m$ and we are currently at index $i$, the length of the new recurrence will be $i - m + L_{m-1}$ (look at the diagram of segments above if you're unsure). Finally,

$$
\begin{align*}
L_i &= i - m + L_{m-1} \\
&= i - m + (m + 1 - L_{i-1}) \\
&= i + 1 - L_{i-1}
\end{align*}
$$

### A More Concise Implementation

Our theorem above provided a stronger condition on $L_i$, which lets us greatly shorten our implementation. Take a look at the code below (courtesy of [KACTL](https://github.com/kth-competitive-programming/kactl/blob/main/content/numerical/BerlekampMassey.h)):

```c++
template<typename T>
vector<T> berlekampMassey(const vector<T> &s) {
    int n = (int) s.size(), l = 0, m = 1;
    vector<T> b(n), c(n);
    T ld = b[0] = c[0] = 1;
    for (int i=0; i<n; i++, m++) {
        T d = s[i];
        for (int j=1; j<=l; j++)
            d += c[j] * s[i-j];
        if (d == 0)
            continue;
        vector<T> temp = c;
        T coef = d / ld;
        for (int j=m; j<n; j++)
            c[j] -= coef * b[j-m];
        if (2 * l <= i) {
            l = i + 1 - l;
            b = temp;
            ld = d;
            m = 0;
        }
    }
    c.resize(l + 1);
    c.erase(c.begin());
    for (T &x : c)
        x = -x;
    return c;
}
```

Some notes about this implementation:
- `n` is the length of the $s$ sequence, not the $c$ sequence
- `c` is our current recurrence relation
- `b` is our best previous version
- `l` is the length of our current recurrence relation
- `m` is the number of steps since the last time we increased the length of our recurrence relation
- `ld` is the last $\Delta$ computed with our previous version
- In this implementation, we instead define the recurrence as $s_i + \sum_{j=1}^n c_j s_{i-j} = 0$, so we negate all the coefficients at the end.
- We also append a 1 at the beginning of `c` and `b`. You can figure out why this works by tracing the code.
- This implementation also takes care of the first step, all with the same logic!

---

</details>

## Applications in Problems <a name="applications"></a>

Ok, this is what you probably came for. Aside from contrived problems asking us to explicitly find the minimum length linear recurrence satisfying a sequence, what can we do with Berlekamp-Massey? The answer is that we pretty much never have to figure out an explicit recurrence ever again!

Take this [Codeforces problem](https://codeforces.com/contest/1511/problem/F) for example. First, let's come up with a naive DP solution. Feel free to think for a while, then click the spoiler.

<details markdown="1" style="margin-bottom: 5%"><summary>Naive DP Solution</summary>

Let $dp[i][a][b][c][d]$ be the number of chainwords of length $i$, where the final character in the chainword is the $b$th character of the $a$th word and the $d$th character of the $c$th word. Essentially, we maintain what segment we are on for the top and bottom hints. Let $L \leq 5$ be the maximum length of a dictionary word. The complexity of this algorithm is something like $\mathcal O(n^4 L^2 m)$, depending on how you implement your transitions.

---

</details>

This is obviously way too slow. But we can make the following observation: the transitions are the same for all $i$. When this is the case, we can turn the $m$ into a $\log m$ factor with matrix exponentation or the polynomial method. The number of states we have per $i$ is $\mathcal O(n^2 L^2) \leq 1600$. Matrix exponentiation will be too slow, but our new method using polynomials can handle this just fine!

<details markdown="1" style="margin-bottom: 5%"><summary>Wait, this isn't a linear recurrence...</summary>

You're right, when we look at the transition $dp[i][a][b][c][d] \to dp[i+1][e][f][g][h]$, this doesn't look like a linear recurrence because the states have 5 dimensions instead of 1. I claim $\\{dp[0][a][b][c][d], dp[1][a][b][c][d], \dots, dp[m][a][b][c][d]\\}$ for any $a, b, c, d$ satisfies a linear recurrence of length $\mathcal O(n^2 L^2)$. I actually don't know how to prove this claim, but I'll include a train of thought I had below and maybe someone can let me know if that train leads anywhere. Who knows, maybe it's not even true, although it seems to be true from past experience and self-generated examples.

First, we will convert our recurrence down to a 2D recurrence by compressing the indices. The technique for reducing a dimension is as follows. Say you have some 2D table $dp[i][j]$ with dimensions $n \times m$. You can instead express this as a 1D table $dp[i \cdot m + j]$ with dimension $n \cdot m$. Essentially, we express our 2D pair as a base $m$ number. Refer to the diagram below to see how the conversion from 2D to 1D coordinates works.

![image 3]({{ site.baseurl }}/assets/images/berlekamp-massey-3.png)

This same idea generalizes to multiple dimensions. For our example, $dp[a][b][c][d] \to dp[a \cdot L \cdot n \cdot L + b \cdot n \cdot L + c \cdot L + d]$.

So using this idea, we can compress $dp[i][a][b][c][d]$ down to 2 dimensions, $dp[i][\dots]$. We can express $dp[0], dp[1], \dots, dp[m]$ each as vectors of length $n^2 L^2$. For transitions, we can treat it as multiplying $dp[i]$ by some transition matrix $A$ of size $n^2 L^2 \times n^2 L^2$ to get $dp[i+1]$. And our desired result is $dp[m] = A^m dp[0]$.

The [Cayley-Hamilton theorem](https://en.wikipedia.org/wiki/Cayley%E2%80%93Hamilton_theorem) states that if $A$ is of dimension $n \times n$, then $A^i$ satisfies a linear recurrence of length $n$ with coefficients of the characteristic polynomial. Specifically, it states $A^n = \sum_{j=1}^n c_j A^{n-j}$, and then we can generalize that to a linear recurrence for $A^i$ by multiplying both sides by $A^{i-n}$.

~~Now, I'm stuck, because I don't think a linear recurrence for $A^m$ necessarily signifies a linear recurrence for a specific entry of $dp[m]$. If anyone knows something about this, I'd be dying to hear about it. As a bonus, it's worth noting that you don't have to rely on faith that a linear recurrence exists for a specific entry of $dp[m]$, and instead just compute $A^m dp[0]$ directly as explained in [this blog](https://codeforces.com/blog/entry/85969). The idea is to generate some $1 \times n$ vector $\vec w$ to left multiply with $A^m dp[0]$ to convert from vector to scalar. Then, we can plug these scalar values into Berlekamp-Massey.~~

From here, showing a linear recurrence exists for $dp[m]$ can be done with some algebra. User shioko on Codeforces has worked out the details [here](https://codeforces.com/blog/entry/96199?#comment-903177).

---

</details>

Ok, so we know a linear recurrence of size $\mathcal O(n^2 L^2)$ exists, but what exactly is it? Let's ask Berlekamp-Massey! Essentially, we will run the naive DP to generate the first few terms, then plug them into Berlekamp-Massey to get the full linear recurrence. It's magic! Here's a [submission](https://codeforces.com/contest/1511/submission/131888684).

By the way, how many terms do we need to plug into Berlekamp-Massey before we can be sure we got the right recurrence? For example, say we're working with the Fibonacci sequence $\\{1, 1, 2, 3, 5, \dots\\}$. If we just feed $\\{1, 1, 2\\}$ into Berlekamp-Massey, the algorithm could return $s_i = s_{i-1} + s_{i-2}$. But it could also return $s_i = 3s_{i-1} - s_{i-2}$. Or $s_i = 4s_{i-1} - 2s_{i-2}$. How do we ensure Berlekamp-Massey gives us the right recurrence?

It turns out, for a linear recurrence of length $n$, you need to feed at least $2n$ terms into Berlekamp-Massey to guarantee getting the same or equivalent recurrence.

<details markdown="1" style="margin-bottom: 5%"><summary>Proof</summary>

Let's say you only provide $2n - 1$ terms of the sequence. Say $n = 5$, the sequence is $\\{0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, \dots \\}$, and the recurrence is $s_i = s_{i-5}$ for $i \geq 5$ (this counterexample is generalizable for any $n$). Another possible recurrence of length $n$ is $s_i = 0$ for $i \geq 5$ and base case $s_0 = s_1 = s_2 = s_3 = 0, s_4 = 1$. And we wouldn't notice that this recurrence is wrong until we process $a_9$, or the $2n$th element.

On the other hand, if we have at least $2n$ elements, all $n$ length linear recurrences we could generate are equivalent. Say we have two recurrence sequences $c$ and $d$. We want to show if $s_i = \sum_{j=1}^n c_j s_{i-j} = \sum_{j=1}^n d_j s_{i-j}$ for $n \leq i < 2n$, then $s_i = \sum_{j=1}^n c_j s_{i-j} = \sum_{j=1}^n d_j s_{i-j}$ for all $i \geq 2n$ as well.

We initiate a proof by induction. The base case is $s_i = \sum_{j=1}^n c_j s_{i-j} = \sum_{j=1}^n d_j s_{i-j}$ for all $i < 2n$. Now for the induction step: if the condition is true for $i < k$, then the condition is true for $i = k$. Now we just crank out the math:

$$
\begin{align*}
s_k &= \sum_{j=1}^n c_j s_{k-j} \\
&= \sum_{j=1}^n c_j \left(\sum_{l=1}^n d_l s_{k-j-l} \right) \\
&= \sum_{j=1}^n \sum_{l=1}^n c_j d_l s_{k-j-l} \\
&= \sum_{l=1}^n \sum_{j=1}^n d_l c_j s_{k-l-j} \\
&= \sum_{l=1}^n d_l \left(\sum_{j=1}^n c_j s_{k-l-j} \right) \\
&= \sum_{l=1}^n d_l s_{k-l} \\
&= \sum_{j=1}^n d_j s_{k-j}
\end{align*}
$$

Notice that $k - j - l$ never becomes negative since $k \geq 2n$, which is why this proof doesn't work when we only provide $2n - 1$ or less elements for Berlekamp-Massey.

---

</details>

Now as an aside, it's worth noting that in this problem, you need way less than $\mathcal O(n^2 L^2)$ terms. According to the editorial, there are only at most 241 states that matter, because we never reach certain combinations of states. Furthermore, in practice we don't even need to calculate an exact bound. Since extra terms can't hurt, it's much easier to just generate as many terms as the problem's time limit will allow, then plug all of them into Berlekamp-Massey and hope it gets AC. How easy is that?

## Problems <a name="problems"></a>

While none of these problems require Berlekamp-Massey to solve, Berlekamp-Massey trivializes them significantly in my opinion.

[Codeforces Round 286, Div 1 E: Mr. Kitayuta's Gift](https://codeforces.com/contest/506/problem/E)

[Atcoder Beginner Contest 198F: Cube](https://atcoder.jp/contests/abc198/tasks/abc198_f)

[Codechef July Challenge 2021, PARTN01: Even Odd Partition](https://www.codechef.com/JULY21A/problems/PARTN01)

## References <a name="references"></a>

[https://codeforces.com/blog/entry/61306](https://codeforces.com/blog/entry/61306)

[https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1054260](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1054260)

---
layout: post
title: "Maintaining a CP Library"
tags: [opinion]
---

I love maintaining a competitive programming library. You get to implement things attuned to your own personal style, and it's super satisfying when you use something from your library to successfully pass a problem. There's also just something satisfying about working on a project over a long period of time, and having it grow and evolve alongside your CP journey. I started my library 3 years ago[^1] in high school, and it's come a long way since then. And so now, I write this article so that I can ~~[plug my library](https://github.com/mzhang2021/cp-library)~~ explain how I design my implementations and how they integrate into my setup.

[^1]: If you check the stats for my Github repo, it was created in February 2020, but that was only the date when I put my implementations on Github. Before then, I used to store them locally on my laptop like a bum.

## Coding Style

The main reason I maintain a library is to have templates that integrate with my coding style. In CP, my code usually follows these certain guidelines:

- I don't use global variables. Instead, everything is in the main method, and I use lambda functions to substitute normal functions. This way, I don't ever worry about not resetting global variables in between test cases.
- I use vectors over C-style arrays so that out-of-bounds errors are caught earlier in samples rather than later in large tests.

To abide by these guidelines, you'll notice that my templates are usually in self-contained structs [^2], so that I can declare a new instance of them per test case.

[^2]: Some of my implementations use an outdated style like global arrays with fixed sizes. Those are usually implementations leftover from a long time ago that I never bothered to revamp because I don't use them frequently enough.

## An Example

Here's an example of a template for strongly connected components:

<details markdown="1" style="margin-bottom: 5%"><summary>Click Me!</summary>

```c++
// https://github.com/mzhang2021/cp-library/blob/master/implementations/graphs/SCC.h
struct SCC {
    int n, ti;
    vector<int> num, id, stk;
    vector<vector<int>> adj, dag, comp;

    SCC(int _n) : n(_n), ti(0), num(n), id(n, -1), adj(n) {}

    void addEdge(int u, int v) {
        adj[u].push_back(v);
    }

    void init() {
        for (int u=0; u<n; u++)
            if (!num[u])
                dfs(u);
        dag.resize(comp.size());
        for (auto &c : comp)
            for (int u : c)
                for (int v : adj[u])
                    if (id[u] != id[v])
                        dag[id[u]].push_back(id[v]);
        for (auto &v : dag) {
            sort(v.begin(), v.end());
            v.erase(unique(v.begin(), v.end()), v.end());
        }
    }

    int dfs(int u) {
        int low = num[u] = ++ti;
        stk.push_back(u);
        for (int v : adj[u]) {
            if (!num[v])
                low = min(low, dfs(v));
            else if (id[v] == -1)
                low = min(low, num[v]);
        }
        if (low == num[u]) {
            comp.emplace_back();
            do {
                id[stk.back()] = (int) comp.size() - 1;
                comp.back().push_back(stk.back());
                stk.pop_back();
            } while (comp.back().back() != u);
        }
        return low;
    }
};
```

---

</details>

The way you use this template is quite straightforward: you declare an instance of `SCC` initialized with the number of nodes. You add each edge with the `addEdge()` method. When you're done adding edges, you call `init()`, and afterwards `comp` will be populated with the nodes partitioned based on strongly connected components, and `dag` will be an adjacency list for the condensation graph. Importantly, I don't have to remember how the SCC algorithm actually works. I can just paste it in my global scope, then use it as a black box for solving a problem.

More generally, packing templates into structs like this makes life easier if I have to use multiple templates. For instance, my source file for a implementation-heavy tree problem might look something like this:

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![image 1]({{ site.baseurl }}/assets/images/library-1.png)

</div>

Pretty clean if you ask me.

## How Generic Should Templates Be?

This is something that I've gone back and forth on. Should I have to modify the internals of my implementation, or should I be able to abstract away the details and use it like an external library? The philosophy I've gone by is to try to make it as generic as possible, without making it too clunky or unnatural. For example, the following is a template for sliding window minimum/maximum:

<details markdown="1" style="margin-bottom: 5%"><summary>Click Me!</summary>

```c++
// https://github.com/mzhang2021/cp-library/blob/master/implementations/data-structures/MinDeque.h
template<typename T>
struct MinDeque {
    int l = 0, r = 0;
    deque<pair<T, int>> dq;

    void push(T x) {
        while (!dq.empty() && x <= dq.back().first)
            dq.pop_back();
        dq.emplace_back(x, r++);
    }

    void pop() {
        assert(l < r);
        if (dq.front().second == l++)
            dq.pop_front();
    }

    T min() {
        assert(!dq.empty());
        return dq.front().first;
    }
};
```

---

</details>

This is an implementation that's easy to make generic. The following code will work for any data type with comparison defined. On the other hand, lazy segment tree is something that's notoriously difficult to make generic. Here's my variant of lazy segment tree:

<details markdown="1" style="margin-bottom: 5%"><summary>Click Me!</summary>

```c++
// https://github.com/mzhang2021/cp-library/blob/master/implementations/data-structures/SegmentTreeNodeLazy.h
struct SegmentTree {
    struct Node {
        int ans = 0, lazy = 0, l, r;

        void leaf(int val) {
            ans += val;
        }

        void pull(const Node &a, const Node &b) {
            ans = a.ans + b.ans;
        }

        void push(int val) {
            lazy += val;
        }

        void apply() {
            ans += (r - l + 1) * lazy;
            lazy = 0;
        }
    };

    int n;
    vector<int> a;
    vector<Node> st;

    SegmentTree(int _n) : n(_n), a(n), st(4*n) {
        build(1, 0, n-1);
    }

    SegmentTree(const vector<int> &_a) : n((int) _a.size()), a(_a), st(4*n) {
        build(1, 0, n-1);
    }

    void build(int p, int l, int r) {
        st[p].l = l;
        st[p].r = r;
        if (l == r) {
            st[p].leaf(a[l]);
            return;
        }
        int m = (l + r) / 2;
        build(2*p, l, m);
        build(2*p+1, m+1, r);
        st[p].pull(st[2*p], st[2*p+1]);
    }

    void push(int p) {
        if (st[p].lazy) {
            if (st[p].l != st[p].r) {
                st[2*p].push(st[p].lazy);
                st[2*p+1].push(st[p].lazy);
            }
            st[p].apply();
        }
    }

    Node query(int p, int i, int j) {
        push(p);
        if (st[p].l == i && st[p].r == j)
            return st[p];
        int m = (st[p].l + st[p].r) / 2;
        if (j <= m)
            return query(2*p, i, j);
        else if (i > m)
            return query(2*p+1, i, j);
        Node ret, ls = query(2*p, i, m), rs = query(2*p+1, m+1, j);
        ret.pull(ls, rs);
        return ret;
    }

    int query(int i, int j) {
        return query(1, i, j).ans;
    }

    void update(int p, int i, int j, int val) {
        if (st[p].l == i && st[p].r == j) {
            st[p].push(val);
            push(p);
            return;
        }
        push(p);
        int m = (st[p].l + st[p].r) / 2;
        if (j <= m) {
            update(2*p, i, j, val);
            push(2*p+1);
        } else if (i > m) {
            push(2*p);
            update(2*p+1, i, j, val);
        } else {
            update(2*p, i, m, val);
            update(2*p+1, m+1, j, val);
        }
        st[p].pull(st[2*p], st[2*p+1]);
    }

    void update(int i, int j, int val) {
        update(1, i, j, val);
    }
};
```

---

</details>

To use this for a general problem, I'd have to modify the `Node` class, the returned parameter of `query()`, and maybe the `push()` method if I modify the `lazy` parameter. So that's quite a bit of internals that I have to mess with, although I think I prefer this to fully generic attempts at lazy segment tree. For contrast, the Atcoder Library has a [fully generic variant](https://github.com/atcoder/ac-library/blob/master/atcoder/lazysegtree.hpp) of lazy segment tree, and it ends up being kind of clunky because you have to define a struct for the data and lazy values, and define functions for interactions between them. Ultimately, it's up to personal preference, and I just think it's easier to not bother trying to make a lazy segment tree fully generic.

## Does It Always Make Sense to Use Structs?

Consider something like combinatoric functions (factorial, choose function, etc.). Currently, I have the following template for it:

<details markdown="1" style="margin-bottom: 5%"><summary>Click Me!</summary>

```c++
// https://github.com/mzhang2021/cp-library/blob/master/implementations/math/Combo.h
#include "Modular.h"

using M = ModInt<998244353>;

const int MAX = 1e5 + 5;

M fact[MAX], inv[MAX];

M choose(int n, int k) {
    if (k < 0 || n < k) return 0;
    return fact[n] * inv[k] * inv[n-k];
}

void preprocess() {
    fact[0] = 1;
    for (int i=1; i<MAX; i++)
        fact[i] = fact[i-1] * i;
    inv[MAX-1] = inverse(fact[MAX-1]);
    for (int i=MAX-2; i>=0; i--)
        inv[i] = inv[i+1] * (i + 1);
}
```

---

</details>

In this case, it doesn't make much sense to wrap it in a struct, because these things values are usually precomputed once for the entire instance of execution, not once for each test case. And since it's not really a data structure, it would be strange to treat it as one by wrapping it around a struct. Right now, I just put this in the global scope, but this does violate my guidelines by creating global variables `fact` and `inv`. One alternative that I've considered, which I've seen others do, is use **namespaces**. While I could, I've decided namespaces aren't worth the effort, because in practice I'll do `using namespace template_name;` anyways, and then it'll effectively be global variables anyways. I guess it could provide an extra layer of organization, since I can minimize namespaces in my IDE, but that's not too big of a deal.

## Where to Test Templates?

In order of preference:

1. [Yosupo Library Checker](https://judge.yosupo.jp/) - This has a wide array of well-known problems, and test cases are added by community members, so you can be pretty confident in their strength and coverage.
2. [CSES](https://cses.fi/problemset/list/) - Similar to Yosupo Library Checker, has a lot of well-known problems and test cases are added by community members.
3. Problems from Various OJs (Codeforces, etc.) - Codeforces does have hacks, but there's no guarantee a contestant added such a hack back when the contest was live, and there have certainly been instances of Codeforces problems with weak test cases. Other online judges don't even support hacks. And it can be harder to find a problem that tests for a specific template.

## Is Building a Personal Library Worth It?

It's definitely not necessary. Plenty of people have gotten very far from copy pasting off KACTL. I just personally maintain a library because it's fun, and it's a tangible project I can point to and be proud of.

---

---
layout: post
title: "Historic Information on Segment Trees"
tags: [tutorial, algo]
usemathjax: true
---

It's been a while, but I'm back with some more estoric knowledge! Historic information is a concept I've only seen briefly mentioned in [this tutorial](https://codeforces.com/blog/entry/57319) on segment tree beats, and the section only covers one example of it, so I'll cover some more examples in this article.

## What is Historic Information?

Let's look at the following problem: You're given an array of $n$ integers $a_i$, and you have another array $b_i$ initially equal to $a_i$. You want to support the following operations.

1. For $l \leq i \leq r$, add $x$ to $a_i$ ($x$ may be negative).
2. Query for $\max_{i=l}^r a_i$.
3. Query for $\max_{i=l}^r b_i$.

After each operation, update $b_i$ to $\max(b_i, a_i)$ for all $1 \leq i \leq n$. $b_i$ is essentially the maximum $a_i$ over time, and is an example of historic information.

The first two operations are pretty standard lazy segment tree. We use a single lazy variable to store the sum of all updates applied to a node. What about the third operation? One option would be to add the lazy value to $b_i$ once you're pushing down at a node, but that isn't exactly right. Consider the following scenario:

1. You add $2$ to $a_i$.
2. You add $6$ to $a_i$.
3. You add $-15$ to $a_i$.

The total lazy value is adding $2 + 6 - 15 = -7$ to $a_i$, so $a_i \to a_i - 7$. But what about $b_i$? It's incorrect to say $b_i \to b_i - 7$, because actually $b_i \to b_i + 8$. This is because the value was changed three times: $b_i \to b_i + 2 \to b_i + 2 + 6 \to b_i + 2 + 6 - 15$. The maximum of all of those would be $b_i + 2 + 6 = b_i + 8$. And here lies the issue: just storing the sum of all updates as our lazy value is not sufficient to track the history of changes across multiple operations.

To remedy this, we introduce another lazy value, dubbed `hlazy` (short for "historic lazy"), which stores the maximum prefix of lazy updates applied. So say there were four updates applied to a node: $\\{+2, +3, -6, +9\\}$. Then `hlazy` would equal $\max(0, +2, +2 + 3, +2 + 3 - 6, +2 + 3 - 6 + 9) = 8$. When we push down node $u$ onto node $v$, we have `v.hlazy = max(v.hlazy, v.lazy + u.hlazy)`. And now, we update `b[i] = max(b[i], a[i] + hlazy)` which gives the correct new value of $b_i$.

<details markdown="1" style="margin-bottom: 5%"><summary>Code</summary>

```c++
// based on how I implement lazy segment tree
// https://github.com/mzhang2021/cp-library/blob/master/implementations/data-structures/SegmentTreeNodeLazy.h
struct Node {
    int maxA, maxB, lazy, hlazy, l, r;

    void leaf(int val) {
        maxA = maxB = val;
        lazy = hlazy = 0;
    }

    void pull(const Node &a, const Node &b) {
        maxA = max(a.maxA, b.maxA);
        maxB = max(a.maxB, b.maxB);
    }

    void push(const Node &other) {
        hlazy = max(hlazy, lazy + other.hlazy);
        lazy += other.lazy;
    }

    void apply() {
        maxB = max(maxB, maxA + hlazy);
        maxA += lazy;
        lazy = hlazy = 0;
    }
};
```

---

</details>

## A Harder Example

Now let's look at another problem: You're given an array of $n$ integers $a_i$, and you have another array $b_i$ initially equal to $a_i$. You want to support the following operations.

1. For $l \leq i \leq r$, add $x$ to $a_i$.
2. Query for $\sum_{i=l}^r a_i$.
3. Query for $\sum_{i=l}^r b_i$.

After each operation, add $a_i$ to $b_i$ for all $1 \leq i \leq n$. So this time our historic information is an accumulation of all $a_i$ over time.

Operation 3 is a bit trickier to deal with this time. To deal with it, we introduce a new concept: a timestamp. We number each of the operations as happening at times $1, 2, \dots, m$. For each node, we store a timestamp denoting the time of latest operation we've applied to this node (or $0$ if no operations have been applied to this node yet). Now, let's take a look at how $b_i$ changes as we apply updates to it. For sake of simplicity, say we're updating a leaf node. Let's say we apply $+x$ at time $t_x$, $+y$ at time $t_y$, and $+z$ at time $t_z$, where $t_x < t_y < t_z$. The initial timestamp before applying these updates is $t_i$.

- $b_i \to b_i + a_i \cdot (t_x - t_i) + (a_i + x) \cdot (t_y - t_x) + (a_i + x + y) \cdot (t_z - t_y)$
- $a_i \to a_i + x + y + z$
- $t_i \to t_z$

Let's simplify the update to $b_i$:

$$
b_i \to b_i + a_i \cdot (t_x - t_i) + (a_i + x) \cdot (t_y - t_x) + (a_i + x + y) \cdot (t_z - t_y) \\
= b_i + a_i \cdot t_x - a_i \cdot t_i + a_i \cdot t_y - a_i \cdot t_x + x \cdot (t_y - t_x) + a_i \cdot t_z - a_i \cdot t_y + (x + y) \cdot (t_z - t_y) \\
= b_i + a_i \cdot (t_z - t_i) + x \cdot (t_y - t_x) + (x + y) \cdot (t_z - t_y)
$$

Let's denote $x + y + z$ as `lazysum` and $x \cdot (t_y - t_x) + (x + y) \cdot (t_z - t_y)$ as another lazy value `cumsum` (short for "cummulative sum").

How does push down work? Say we have a node with updates $\\{+u, +v, +w\\}$ and we push down another node with updates $\\{+x, +y, +z\\}$. Then we get:

- Initial `cumsum`: $u \cdot (t_v - t_u) + (u + v) \cdot (t_w - t_v)$
- After push down:

$$
u \cdot (t_v - t_u) + (u + v) \cdot (t_w - t_v) + (u + v + w) \cdot (t_x - t_w) + (u + v + w + x) \cdot (t_y - t_x) + (u + v + w + x + y) \cdot (t_z - t_y) \\
= \text{cumsum} + (u + v + w) \cdot (t_x - t_w + t_y - t_x + t_z - t_y) + x \cdot (t_y - t_x) + (x + y) \cdot (t_z - t_y) \\
= \text{cumsum} + \text{lazysum} \cdot (t_z - t_w) + \text{other.cumsum}
$$

I will leave it as an exercise to figure out the update for a non-leaf node and how to merge two segment tree nodes. Once you've figured it out, you can check your answers with the code below:

<details markdown="1" style="margin-bottom: 5%"><summary>Code</summary>

```c++
// based on how I implement lazy segment tree
// https://github.com/mzhang2021/cp-library/blob/master/implementations/data-structures/SegmentTreeNodeLazy.h
struct Node {
    int sumA, sumB, timestamp, lazysum, cumsum, lazyTimestamp, l, r;
    bool lazy;

    void leaf(int val) {
        sumA = val;
        sumB = timestamp = lazysum = cumsum = lazyTimestamp = lazy = 0;
    }

    void pull(const Node &a, const Node &b) {
        sumA = a.sumA + b.sumA;
        timestamp = max(a.timestamp, b.timestamp);
        sumB = a.sumB + b.sumB + a.sumA * (timestamp - a.timestamp) + b.sumA * (timestamp - b.timestamp);
    }

    void push(const Node &other) {
        if (lazy) {
            cumsum += lazysum * (other.lazyTimestamp - lazyTimestamp) + other.cumsum;
            lazysum += other.lazysum;
        } else {
            cumsum = other.cumsum;
            lazysum = other.lazysum;
            lazy = true;
        }
        lazyTimestamp = other.lazyTimestamp;
    }

    void apply() {
        sumB += sumA * (lazyTimestamp - timestamp) + (r - l + 1) * cumsum;
        sumA += (r - l + 1) * lazysum;
        timestamp = lazyTimestamp;
        lazysum = cumsum = lazyTimestamp = lazy = 0;
    }
};
```

---

</details>

## Applications

Ok, when on earth would you ever need this? Aside from contrived problems designed to test this technique, I've found historic information most useful when doing some sort of "all pairs" problem via linesweep. Take [this problem](https://codeforces.com/gym/103069/problem/G) from the 2020 ICPC Asia East Continent Final for example.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>

The algorithm I describe below is very similar to the offline algorithm for solving [SPOJ DQUERY](https://www.spoj.com/problems/DQUERY/), which you can see [here](https://codeforces.com/blog/entry/8962?#comment-146571). For sake of brevity, I will assume that you have read and understood that solution in the paragraphs below.

We will use an offline algorithm. As we sweep from left to right, we maintain an array $p$ where each index's value is 1 if a subarray starting at that index is valid, and 0 otherwise. So say $a = [3, 9, 4, 3, 2, 9, 9, 3, 2, 3, 2, 2, 2]$. Then $p = [0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]$. Now say you process the next element in your linesweep. There are two cases:

1. You've never seen this element before. In our example, say the next element is a $6$, so $a = [3, 9, 4, 3, 2, 9, 9, 3, 2, 3, 2, 2, 2, 6]$. Then now $p = [1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1]$. Notice that we basically just did a range flip operation.
2. You've seen this element before. In our example, say the next element is a $9$, so $a = [3, 9, 4, 3, 2, 9, 9, 3, 2, 3, 2, 2, 2, 9]$. First, we delete the last occurrence of $9$ (we basically pretend it doesn't exist and set $p_i = p_{i+1}$ at that index). So $p = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]$. And now we add our new last occurrence of $9$ at the end, giving us $p = [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]$. So we've essentially done just two range flip operations.

If we make $p$ a segment tree, we can query for the number of valid subarrays for each right endpoint. But our original problem needs the number of valid subarrays with left endpoint $\geq l$ for each right endpoint for ALL right endpoints in $[l, r]$. So to remedy this, we instead query for the historic sum of $p_i$ for all $l \leq i \leq r$. So we need a segment tree that supports range flip and historic sum. I leave the details of what values to store in the segment tree node as an exercise for the reader, because I think being able to figure this out will serve as a good test of understanding. If you're stuck, you can refer to my implementation [here](https://ideone.com/aADK7p) (ideone link because Codeforces gym links aren't public).

---

</details>

## Problems

Here are some other problems where I've used historic information. I don't have many at the moment, so if you have any more feel free to let me know.

### [SPOJ GSS2: Can you answer these queries II](https://www.spoj.com/problems/GSS2/)

<details markdown="1" style="margin-bottom: 5%"><summary>Solution Sketch</summary>

Similar to the 2020 ICPC Asia East Continent Final problem explained above: linesweep, store only last occurrence, support range add and historic maximum. Explained in more detail [here]({{site.baseurl}}/gss/).

---

</details>

### [Codeforces Round 751, Div 1 C: Optimal Insertion](https://codeforces.com/contest/1601/problem/C)

<details markdown="1" style="margin-bottom: 5%"><summary>Solution Sketch</summary>

Compress the input to values in the range $1, 2, \dots, n + m$. It's optimal to sort $b$ and have them be in the same relative order in $c$ to minimize the number of inversions. Now for each $b_i$, we can insert them in at the beginning, at the end, or between any $(a_i, a_{i+1})$ pair. Let's sweep through each of $a_i$ from left to right, and update a segment tree maintaining the number of inversions each element would add if inserted in that gap. Finally, for each element in $b$, query for the historic minimum of $b_i$ in the segment tree. So we need to support range add and historic minimum.

[Submission for Reference](https://codeforces.com/contest/1601/submission/133199732)

---

</details>

### [UOJ 164](https://uoj.ac/problem/164)

Translation: Support the following operations:

1. For $l \leq i \leq r$, add $x$ to $a_i$.
2. For $l \leq i \leq r$, set $a_i$ to $\max(a_i - x, 0)$.
3. For $l \leq i \leq r$, set $a_i$ to $x$.
4. Query for $a_y$.
5. Query for $b_y$.

<details markdown="1" style="margin-bottom: 5%"><summary>Solution Sketch</summary>

This is the example problem for historic information described [here](https://codeforces.com/blog/entry/57319). The blog also contains the solution.

[Submission for Reference](https://uoj.ac/submission/519426)

---

</details>

### [XXII Open Cup, Grand Prix of Nanjing, Problem E: Paimon Segment Tree](https://codeforces.com/gym/103470/problem/E)

<details markdown="1" style="margin-bottom: 5%"><summary>Solution Sketch</summary>

It literally says "historical values" in the problem statement 🤔. Maintaining the historical squared sum is just adding some extra values on top of maintaining the historical sum, so I'll let you work out the details. If you're stuck, you can refer to my code [here](https://ideone.com/2zIHRs).

P.S. Don't actually use persistent segment trees as the problem suggests, just split the query into $\text{query}(1, r) - \text{query}(1, l - 1)$.

---

</details>

---

And that's a wrap! Thanks for reading!

My next article idea (which will probably be published in a month from now) is to make a tier list of online judges. But this isn't gonna be just a baby tier list. I've made an insane number of accounts on different online judges, so this list will be as comprehensive as possible. Currently, I have the following list, and if you have any others to add, let me know. The only requirement is that the online judge must be in English.

<details markdown="1" style="margin-bottom: 5%"><summary>The List</summary>

- [Codeforces](https://codeforces.com/)
- [Atcoder](https://atcoder.jp/)
- [Codechef](https://www.codechef.com/)
- [SPOJ](https://www.spoj.com/)
- [UVa](https://onlinejudge.org/)
- [ICPC Live Archive](https://icpcarchive.ecs.baylor.edu/)
- [USACO](http://www.usaco.org/)
- [USACO Training](https://train.usaco.org/)
- [HackerRank](https://www.hackerrank.com/)
- [HackerEarth](https://www.hackerearth.com/)
- [LeetCode](https://leetcode.com/)
- [binarysearch](https://binarysearch.com/)
- [DMOJ](https://dmoj.ca/)
- [Aizu Online Judge](https://judge.u-aizu.ac.jp/onlinejudge/index.jsp)
- [PKU Online Judge](http://poj.org/)
- [Timus](https://acm.timus.ru/)
- [LightOJ](https://lightoj.com/)
- [oj.uz](https://oj.uz/)
- [CSA Academy](https://csacademy.com/)
- [Kattis](https://open.kattis.com/)
- [CodeDrills](https://codedrills.io/)

I decided not to add sites like [CSES](https://cses.fi/problemset/list/) or [Yosupo Judge](https://judge.yosupo.jp/) because they serve a different purpose than traditional online judges (e.g. CSES is a database of classical problems). I also might exclude interview prep sites like [LeetCode](https://leetcode.com/) since they also serve a different purpose, though I'm not sure at the moment.

---

</details>

---
layout: post
title: "A Competitive Winter Break"
tags: [personal]
usemathjax: true
---

Happy new year! It's been a minute since I last posted, but here's my first post of 2022. I was trying to go for a title similar to [Petr's blog posts](https://petr-mitrichev.blogspot.com/) with the adjective + "week" format, but it doesn't really sound that good now that I've typed it out.

Firstly, I must apologize that the tier list I mentioned at the end of the [last post]({{site.baseurl}}/historic-segtree/) is not ready yet. I originally planned on working on that over winter break, but ended up putting it off and falling back on unproductive habits instead.

For the time being, I figured I'd write an update post about CP competitions I did over winter break, because I had some more time to participate in several of them.

## Rated Codeforces

Ever since I first hit red in September, I've taken a hiatus from competing in rated contests. But I have to say, playing out of competition just didn't hit the same. Aside from it out of competition contests being div 2 instead of div 1, competing unrated just feels more low stake and not as fun, so it was refreshing to compete in rated contests again over break.

I did three rated contests over break. The first was Global Round 18, where I paid an unexpected visit to master rating again 😬.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/winter-break-1.png)

</div>

My contest experience consisted of a lot of hopping around between problems and just not seeing simple observations (when I say simple, I mean that based on my average performances, I would typically be able to find observations of this difficulty). I'll include a retelling of my contest performance in a spoiler below:

<details markdown="1" style="margin-bottom: 5%"><summary>The Story</summary>

Problem A and B were quick solves. I got the right ideas and submitted them relatively quickly. Next is problem C. On first read, I misunderstood the problem as only flipping adjacent candles instead of all other candles, because problem C reminded me of a different problem I saw before involving flipping adjacent candles. After working through the samples and realizing I misunderstood the problem, I switch to thinking about flipping all candles. I play with the samples and other small cases for a while, but no obvious pattern emerges. A significant amount of time passes without me solving it, and I decide I need to skip C for now to salvage my performance.

I open problem D. I get the observation about the parity of the bit count also being xor'ed pretty quickly, which means I only need to assign the unassigned edges 0 or 1. But I'm stuck on how to recover the answer. Again, I think about different ways of assigning, including some weird greedies, but nothing concrete comes to mind.

At this point, my morale and self-confidence aren't exactly high, because I've just bricked on two early problems and am on track to getting specialist performance if another problem doesn't get solved soon. I could move onto E, but the solve counts weren't giving me any confidence that E was any easier than C or D. I also did a quick sweep through the entire problem set to see if there was some data structure problem I could knock out instead, but there didn't appear to be any (yes, I later found out that problem G was general matching and problem H was a well-known flow problem, but I'm not too familiar with flows and matchings).

So I return to problem C. I finally come up with the following idea: if I form pairs between the characters at the same indices in the two strings, then only the count of 00, 01, 10, and 11 pairs matter and not the exact positions they're at. So if I compress the string into a 2x2 frequency table, maybe I could do BFS on this state graph. But the number of states appeared huge, so I made another observation that would turn out to be wrong: I thought maybe we could cap the frequency table values at just 2 or 3 to reduce the number of states, because all of the pairs except for the one you're operating on will flip. However, a wrong answer on pretest 2 disproved that claim. I code up a generator for a stress test because there's not much else I can do, and I find that even when I increase the cap to 5 or 10, the solution is still wrong. However, I notice something strange as I increase the bounds: the naive BFS with no cap still runs fast even for $n = 10^5$. With no better options left, I submit the naive BFS, and to my surprise it passes pretests! At this stage, I was certain there might be some countertest that could fail my solution, and I was just hoping problemsetters would miss that case in the systests. I later found out that such a countertest doesn't exist, because you can [prove](https://codeforces.com/blog/entry/98253?#comment-870707) the BFS solution runs in $\mathcal O(n)$.

Finally, I return to problem D. After more thinking and still no concrete ideas on how to recover the answer, I attempt to cheese the problem with [hill climbing](https://en.wikipedia.org/wiki/Hill_climbing). Unfortunately for me, the performance of hill climbing on this problem is really bad, to the point where it doesn't even consistently get the samples right. I also read E in the middle of doing this, but at this point I have little motivation left to think properly about yet another greedy problem that I'll likely miss the key insight for. And so the contest ends, and I'm left with the largest rating drop I've ever received.

---

</details>

If I were to reflect on my performance, I'd say my biggest mistake was **getting tilted.** I actually thought I'd gotten over that since my div 2 days, as I've gotten good at skipping problems. For example, I've had two performances ([ex. 1](https://codeforces.com/contest/1503/standings/participant/111989857#p111989857) and [ex. 2](https://codeforces.com/contest/1552/standings/participant/117714327#p117714327)) in the past that could have been much worse but were somewhat salvaged by me skipping a problem I was bricking on. But bricking on not one, but two problems in a row was just too frustrating, and once I got annoyed it only went downhill from there.

Next was Goodbye 2021, which definitely went much better than Global Round 18.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 2]({{site.baseurl}}/assets/images/winter-break-2.png)

</div>

No funny business happened this time. I got the right observations on the early problems and rolled through them quickly enough to get IGM performance despite not solving any of the GM+ difficulty problems. I did get one wrong submission due to integer overflow on problem E, which cost me roughly 20 places. Just speedforces things 😩.

To explain the -10 penalty on problem F, that was because I tried to cheese the problem with hill climbing again after not being able to think of a proper solution. It looked more promising than problem D from last round as the bounds were much smaller and I got to pretest 7, but I could not get it to work before the contest ended. As a side note, I'm annoyed I didn't get the intended solution because the intended solution only had one observation: the "all colors pairwise different or equal" condition was equivalent to the colors summing to 0 mod 3. After that, you just turn each triangle in the graph into an equation and plug into Gaussian elimination. I also learned my Gaussian elimination library code was objectively worse than everyone else's as [my practice submission](https://codeforces.com/contest/1616/submission/141131180) got uphacked.

The last Codeforces contest I did was Hello 2022.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 3]({{site.baseurl}}/assets/images/winter-break-3.png)

</div>

Despite getting back to GM, this was an extremely frustrating performance for me because of one reason: **my submission to problem G was one typo away from AC.**

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 4]({{site.baseurl}}/assets/images/winter-break-4.png)

*Ahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh!!!*

</div>

To clarify what happened: I made my first submission to G at 2 hours and 5 minutes. I'm greeted with a wrong answer on pretest 2. At this point, there are less than 10 minutes left in the contest, and I panicked because there's like no time left to debug. I quickly rethink through my idea again and comb through my code, and I see no issues. At this point, I decide to stress test, which in hindsight was a fatal mistake. Stress testing takes too long. I would have to write a generator and a naive solution, and after finding the countercase I would have to add debug statements to my code and pinpoint where the countercase is tripping me up. Indeed, by the time I find the countercase via stress testing, there's like 1 minute left and I'm unable to debug in time. What I should have done is just try to find the bug by inspection. Meticulously read each line and try to spot the bug, because that was the only chance I'd be able to debug in time.

This mistake was frustrating because I missed out on what could have been one of my best performances, and I missed out on solving a 3000+ rated problem in contest for the first time. I realize these are completely arbitrary milestones I've set for myself, but it still feels bad. I attribute this mistake to making a poor decision under pressure. I think going for G instead of F was the right call, because I heard F was a tedious casework greedy while G was much cleaner, and my mistake was trying to stress test in under 10 minutes.

Also, I don't have much right to complain about this performance anyways, because I still went positive and it definitely could have gone much worse. For instance, I was lucky I got the hit-or-miss observation on D quickly, and on a different day it could have easily been another Global Round 18 situation.

## Advent of Code

Advent of Code was running last month, and I was curious why so many [good CP'ers](https://adventofcode.com/2021/leaderboard) were doing it. So I decided to give it a try. After following along for a few days, I concluded that I simply wasn't the intended audience for these problems. From a CP standpoint, there were no interesting problems; almost every problem was solved with some form of brute force or "translate the problem statement to code." I guess day 8's second star was a nice ad hoc task where you had to deduce the digits from the set of segments turned on. But for the most part, the problem statements were overly wordy and complex, and half the challenge was understanding what the freaking problem was.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 5]({{site.baseurl}}/assets/images/winter-break-5.png)

*I'm sorry, but after seeing day 19 was yet another brute force problem, I couldn't do these anymore.*

</div>

And just to make sure I wasn't missing out on elegant solutions, I checked the AoC [subreddit](https://www.reddit.com/r/adventofcode/), where I found out the average software developer loves brute force and finds it highly stimulating and engaging. Take day 16 for example, where the entire challenge was parsing the infinitely long problem statement and translating the procedure to code. Apparently that was one of the subreddit's [favorite](https://www.reddit.com/r/adventofcode/comments/rhot41/2021_day_16_just_a_thank_you_to_the_aoc_creator/) [problems](https://www.reddit.com/r/adventofcode/comments/rhsq96/2021_day_16_that_was_surprisingly_a_lot_of_fun/). To be clear, I'm not saying these people's opinions are wrong. I'm just amused at the difference in opinion. If you enjoyed the AoC tasks, more power to you, because having fun is the most important part of any programming competition.

From watching [Neal's](https://www.youtube.com/c/nealwuprogramming/videos) AoC videos, I'm guessing the fun part for top CP'ers is speedforcing these problems. But personally, these problems aren't even fun to speedforce, because they're so overly tedious and verbose with almost no slick observations needed, making it quite literally a reading comprehension and typing contest. I guess the one cool thing coming out of this was that I got slightly more proficient at Python. I decided since the format of this contest was "submit output only," I would use Python and try to write as clean code as possible. I must say, some tasks like day 18 are *much* easier to handle with Python due to their input format, and I felt badass whenever I successfully used list comprehension to write one liners in certain probelms. All in all, the concept of Advent of Code is neat, but the problems just aren't my type, and that's perfectly ok.

## A Practice Story

Since this post is already getting overly long and rambly, I'll wrap it up soon and conclude with a short story about a problem I recently solved in practice. I found the process of solving this problem amusing because it ended up being a lesson in debugging and hashing out the details.

<details markdown="1" style="margin-bottom: 5%"><summary>The Story</summary>

So I was solving [this Codeforces problem](https://codeforces.com/contest/843/problem/E) that I pulled from this [hard problem list](https://codeforces.com/blog/entry/98630). The problem didn't seem too bad on first read, and I got most of the details for the solution right off the bat.

1. Putting non-zero flow through each edge with $g_i = 1$ can be done by finding any arbitrary path of $g_i = 1$ edges from the source to the sink going through that edge and adding 1 to the flow of each edge on the path.
2. To minimize the number of saturated edges, it suffices to only saturate edges in the minimum cut. This ensures any path from the source to the sink contains at least one saturated edge and thus no more flow can be pushed. To avoid including $g_i = 0$ edges in the minimum cut, my first idea was to just remove them from the network, which is incorrect.

I code up this idea, submit it, and am greeted with a wrong answer on test 3. I realize that my initial idea of excluding $g_i = 0$ edges from the network was wrong, because then it's possible that there exists a path of mostly $g_i = 0$ edges and one unsaturated $g_i = 1$ edge from the source to the sink. Luckily, this issue is quickly remedied by adding the $g_i = 0$ edges to the flow network with capacity $\infty$ so that they won't be included in the minimum cut.

However, after fixing what I thought was the bug, I still got wrong answer on test 3. Now I'm slightly confused. After rehashing through my idea and looking at all the details, I can't see anything wrong with the idea. I figured it must be a bug with my DFS. I try reimplementing the same procedure with different variations and make multiple submissions, all succumbing to the same wrong answer on test 3. And after almost an hour, I finally cave and begrudging turn to external resources. I decided since I felt so close to the AC idea, I didn't want to look at the editorial yet and just wanted a hint, so I scrolled through the comment section of the announcements blog. And I found my answer in [this thread](https://codeforces.com/blog/entry/54008?#comment-380734).

Ah, the devil is in the details! The nuance I was missing was that I didn't properly consider reverse edges. I'd forgotten that part of what makes the flow algorithm work is that augmenting paths can travel through reverse edges and "cancel" out flow. If I include a $g_i = 1$ edge going backwards from the $T$-side to $S$-side, it's possible for me to still have an augmenting path by going through a bunch of unsaturated edges from the source, going through that backwards $g_i = 1$ edge in **reverse** and cancelling out the flow already in it, and then going through more unsaturated edges to reach the sink. Luckily, the fix for this is simple: for each $g_i = 1$ edge, set the reverse capacity to $\infty$ so that the edge is never included backwards in the minimum cut.

Now here's the kicker: my solution is still wrong! And still on wrong answer on test 3! I continue to make more tweaks and trying alternative ways of implementing the same thing. For example, I tried accounting for $g_i = 1$ edges in cycles by sending a unit of flow through the cycle it's contained in instead of through a path from source to sink. All submissions succumb to the same wrong answer. And then, it hits me. My clown ass was finding the minimum cut wrong! In my DFS method for finding the minimum cut, I wasn't considering reverse edges in the flow network. Heck, I wasn't even considering $g_i = 0$ edges in my DFS because it was the same DFS method that I never changed since my first submission! This is so spectacularly wrong that it's a miracle my solution even passed test 2. I fix my DFS method and finally claim my AC.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 6]({{site.baseurl}}/assets/images/winter-break-6.png)

*My submission history after this fiasco...*

</div>

Where was I going with this story? I'm not sure. I just felt like writing about it. I guess my biggest takeaways are:

1. Fully understand the nuances of the algorithm you're tweaking. I usually black box maximum flow, so this was a cool problem that had me better understand how flow worked.
2. When debugging, actually debug. Look at all parts of the code, even parts you're sure are right.

</details>

---

I'm not sure what the future of this blog will be. It'll probably be a while until I write another long tutorial because those take time. Maybe I'll get around to finishing that tier list. In any case, thanks for reading!

---
layout: post
title: "A Shift in Perspective"
tags: [tutorial]
usemathjax: true
---

I've seen numerous problems recently where the solution comes from a shift in perspective, and I've found them amusing enough to write a whole blog around this topic. I wouldn't consider this a technique but rather just an explanation of visual intuition for problem solving.

## [Codeforces Round 767, Div 1 D2: Game on Sum (Hard Version)](https://codeforces.com/contest/1628/problem/D2)

Firstly, I will assume familiarity with the $\mathcal O(nm)$ DP solution that solves the Easy Version, which you can read about in the [official editorial](https://codeforces.com/blog/entry/99276).

Let $dp[i][j]$ be the answer when there are $i$ turns left and Bob has to choose $j$ more add operations. The transitions are as follows:

$$
\begin{align*}
j = 0 &\implies dp[i][j] = 0 \\
j = i &\implies dp[i][j] = i \cdot k \\
0 < j < i &\implies dp[i][j] = \frac{dp[i-1][j-1] + dp[i-1][j]}{2}
\end{align*}
$$

<details markdown="1" style="margin-bottom: 5%"><summary>Intuition for the DP</summary>

It would be ironic for a blog post about intuition to not explain the intuition for this recurrence. The editorial already covers the intuition pretty well, but I'll include it here for sake of completeness.

Let's knock out the straightforward cases first. If $j = 0$, the answer is $0$ because Bob can just subtract every time, so it would be suboptimal for Alice to pick any positive number. If $j = i$, the answer is $i \cdot k$ because Bob is forced to add for all of his remaining operations, so Alice can cash in a maximum of $+k$ on each operation.

Now let's consider another small case like $i = 2, j = 1$. Say Alice picks $x$. If Bob subtracted $x$ now, he would have no subtract operations left and Alice would cash in $+k$ in the final operation, so the resulting score would be $k - x$. If Bob added $x$ now, he would be able to subtract on his final operation and Alice would optimally state $0$ as her final number, so the resulting score would be $x$. Since Bob wants to minimize the score, he would pick the choice resulting in $\min(x, k - x)$. Now Alice wants to pick the $x$ that maximizes this minimum. The function is maximized when $x = k - x$, or $x = \frac{k}{2}$, and the final score would be $\frac{k}{2}$.

There's no reason we can't extend this logic to the general case. Say we're at any arbitrary state of the game $(i, j)$. If Alice selects $x$, Bob makes the choice that attains $\min(dp[i-1][j] - x, dp[i-1][j-1] + x)$<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>. For Alice to maximize this minimum, she selects the $x$ such that $dp[i-1][j] - x = dp[i-1][j-1] + x$, or $x = \frac{dp[i-1][j] - dp[i-1][j-1]}{2}$. The final score would be $\frac{dp[i-1][j-1] + dp[i-1][j]}{2}$. You can prove that this is always a valid move (i.e. $0 \leq \frac{dp[i-1][j] - dp[i-1][j-1]}{2} \leq k$) with some induction and algebra (written out in detail in this [comment](https://codeforces.com/blog/entry/99276?#comment-880412)).

---

<div class="footnotes">
<ol>
<li id="fn:1">
I always find DP solutions for game theory problems humorous because it's funny to imagine Alice and Bob mentally computing DP tables in their head while playing a game with each other.
<a href="#fnref:1" class="reversefootnote">&#8617;</a>
</li>
</ol>
</div>

---

</details>

Ok, now we must solve the hard version with constraints $m \leq n \leq 10^6$. To me, it isn't immediately obvious how to optimize this DP to linear time just by staring at the formulas. So as the title suggests, it's time for a shift in perspective!

Let's draw out the DP table, because visualizing stuff is great.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/perspective-1.png)

*You know, I'm something of an artist myself.*

</div>

Yes, I realize I have $i$ as the columns and $j$ as the rows in this diagram, but just roll with it. Each cell $(i, j)$ contains the value of $dp[i][j]$. So for cells of the form $(i, 0)$, their values are $0$. For cells of the form $(i, i)$, their values are $i \cdot k$. And for the remaining cells, I've drawn two green arrows representing the transitions from $(i, j)$ to $(i - 1, j)$ and $(i - 1, j - 1)$. Now here's the cool part: since this looks like a grid, let's think about grid paths. Say we initially start at cell $(n, m)$. We have a $\frac{1}{2}$ probability of moving to the cell on the left or diagonally up and left. If we ever reach a cell of the form $(i, i)$, we gain value $i \cdot k$ and stop moving. Now this problem becomes calculating the expected value of any path we take starting from $(n, m)$. Pretty neat!

This new interpretation is much easier to come up with an $\mathcal O(n)$ solution to. We precompute factorials and powers of $2$, and we iterate over each of the ending cells. If we end at $(i, i)$, we will take $n - i$ steps and move up exactly $m - i$ times. There are $\binom{n - i}{m - i}$ such paths, and we multiply by $\left(\frac{1}{2}\right)^{n - i}$ for the probability of making the right decision at each step. But this isn't exactly correct, because we have to exclude the paths that go through cell $(i + 1, i + 1)$. So we subtract $\binom{n - i - 1}{m - i - 1}$ from our number of paths. The final answer is thus

$$
\sum_{i=0}^m \left(\binom{n - i}{m - i} - \binom{n - i - 1}{m - i - 1}\right) \cdot \left(\frac{1}{2}\right)^{n - i} \cdot i \cdot k \\
= \sum_{i=0}^m \binom{n - i - 1}{m - i} \cdot \left(\frac{1}{2}\right)^{n - i} \cdot i \cdot k
$$

In summary, we've optimized a DP solution by looking at it under a combinatorics lens!

## [Kotlin Heroes Episode 8 G: A Battle Against a Dragon](https://codeforces.com/contest/1571/problem/G)

I already wrote about this problem [4 months ago]({{site.baseurl}}/reflection/#kotlin-heroes), but I'm mentioning it again as another example of a shift in perspective. The naive $\mathcal O(nm)$ DP approach is $dp[i][j] = \max(dp[i-1][j] + a[i][j], dp[i-1][j+1])$, where $dp[i][j]$ is the maximum damage done by the first $i$ warriors with $j$ barricades left and $a[i][j]$ is the amount of damage the $i$th warrior can do if there are exactly $j$ barricades. Again, optimizing this DP is best understood if you draw out the DP states on a grid, because then you'll notice that the transitions can be expressed as 2D triangle queries and the problem becomes a data structures exercise. You can read more details in my [older post]({{site.baseurl}}/reflection/#kotlin-heroes) about this problem.

<details markdown="1" style="margin-bottom: 5%"><summary>Data Structures Note</summary>

I realized I never explained how to actually solve the "data structures exercise" at hand in my older post. So I'll briefly explain it here. Treat the inputs as 2D points $(i, b_{ij})$ and sort them based on the diagonal $i + b_{ij}$ they lie on. Now perform a linesweep from smaller to larger diagonals and for each point, perform the following steps:

1. Calculate its DP value by querying in a segment tree on the range $[b_{ij}, m]$.
2. Insert the point into the segment tree at index $b_{ij}$.

By considering points in diagonal order, only points below the diagonal of the current point we're considering will have already been inserted into the data structure, giving us the desired triangle query.

[Submission for Reference](https://codeforces.com/contest/1571/submission/131573499)

---

</details>

## [2019-2020 ICPC Southern and Volga Russian Regional Contest D: Conference Problem](https://codeforces.com/contest/1250/problem/D)

Bounds are small which usually signify a DP solution is worth thinking about. However, the intervals are kind of annoying and don't easily lend themselves to a recurrence in my opinion. We will eventually arrive at a recurrence that can be explained using the intervals, but here's another way of arriving at it with a shift in perspective:

Let's turn to the 2D plane (Noticing a theme here? I get all of my intuition from drawing stuff as geometric points or grids). Express an interval $[l, r]$ as a 2D point $(l, r)$. Another interval $[l', r']$ intersects $[l, r]$ if $l' \leq r$ and $r' \geq l$. In other words, the point $(l', r')$ lies to the upper left of the point $(r, l)$.

Now, let's treat the countries as colors. So we can reformulate this problem as the following:

*You are given $2n$ 2D points, some already colored. The points come in pairs of $(l, r)$ and $(r, l)$. We will refer to the $(l, r)$ points as "regular points" and $(r, l)$ points as "query points." Points in the same pair must be the same color. Denote a query point as "bad" if it contains no different colored regular points to its upper left. Assign colors in the range $1 \dots 200$ to the uncolored points to maximize the number of bad points.*

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 2]({{site.baseurl}}/assets/images/perspective-2.png)

*In this diagram, I've labeled the regular and query points with R and Q respectively. I used different colors to distinguish points with different c values. In this example, only the green point is bad, so our answer is 1.*

</div>

Finally, here's the observation that leads to our DP: Peform a linesweep to process the points from left to right. We only care about the y-coordinate of the highest two previously processed regular points of different colors. If we have a regular point with $c = 1$ at $y = 10$ and a regular point with $c = 2$ at $y = 9$, we couldn't care less if there's a regular point with $c = 3$ and $y = 7$, because any query point containing that third point in its upper left will also contain the first two points and thus already be guaranteed to have a point of different color in its upper left. Armed with that knowledge, we can formulate an $\mathcal O(n^3 C^2)$ solution: let $dp[i][j][k][l][m]$ be the maximum number of bad points if we've processed the first $i$ points, our highest regular point is at coordinate $j$ with color $k$, and our second highest regular point with a different color than the first is at coordinate $l$ with color $m$. Note that we've compressed our coordinates down to just $\mathcal O(n)$ distinct values instead of $10^6$.

We can do better. Storing the highest point is unnecessary, because we know that if our query point y-coordinate is below the y-coordinate of the second highest point of a different color, then we cover at least two distinct colors, and otherwise we only cover one color. So we can knock off two dimensions from our DP and get $\mathcal O(n^2 C)$ which fits under the constraints of this problem. One more note: we will instead store the lowest y-coordinate $j$ such that all regular points above $j$ are color $k$. In other words, we will store the lowest point of the highest color instead of the highest point of the second highest color. This is so we can determine if a query point is bad or not if its y-coordinate is greater than or equal to $j$.

To summarize, our $\mathcal O(n^2 C)$ DP is as follows: $dp[i][j][k]$ is the maximum number of bad points if we've processed the first $i$ points and all points with y-coordinate greater than or equal to $j$ are color $k$.

The transitions aren't exactly straightforward, but they're also not too hard and can be worked out with enough conviction and casework. I'll leave the transitions as an exercise to the reader 🙂.

<details markdown="1" style="margin-bottom: 5%"><summary>Boooo!</summary>

Ok fine. I really don't feel like typing out the transitions because it's just casework about if point $i$ is above or below $j$ and if it matches or doesn't match color $k$. I'll link a [submission for reference](https://ideone.com/lxZxoa) and if you're still confused you can leave a comment below. In my code I used a binary indexed tree for convenience, but it's unnecessary because bounds are small enough that you can do everything with for loops.

---

</details>

## Finale: Optimizing Bitmask DP with Graph Interpretation

I don't have an official problem link for this problem, but it's problem 6 on [this blog](https://codeforces.com/blog/entry/99649). I usually don't pay attention to blog posts about coding interview problems, but I read problem 6 and felt solving it for $n \leq 20$ was kind of non-trivial. Maybe I'm wrong and the solution is standard for others.

The problem is as follows:

*You're given two arrays $a$ and $b$, both of size $n$. In one operation, you can choose an integer $x$ and two indices $i$ and $j$, then add $x$ to $a_i$ and subtract $x$ from $a_j$. Find the minimum number of operations to transform array $a$ into $b$, or print $-1$ if it's not possible.*

**Constraints:** $n \leq 20, -10^9 \leq a_i, b_i \leq 10^9$

(The constraints on $a_i$ might be smaller, but I can't read the image in the blog post very clearly and the complexity won't depend on $a_i$ anyways.)

If the sum of $a_i$ and the sum of $b_i$ are not equal, then the answer is obviously $-1$ because the sum of $a_i$ remains constant after each operation. Otherwise, it's always possible. Select some index $i$, add $b_i - a_i$ to $a_i$ and subtract $b_i - a_i$ from some other arbitrary $a_j$, then delete $a_i$ and $b_i$ from their respective arrays and solve the problem recursively. This procedure also provides an upper bound of $n - 1$ on the number of operations we perform.

To reduce the number of operations, we can partition the set of indices $U = \\{1, 2, \dots, n\\}$ into disjoint subsets, where for each subset $S$ is **good**. We define a subset $S$ to be good if $\sum_{i \in S} a_i = \sum_{i \in S} b_i$. Now we can fix the entire array just by performing our above procedure on each individual subset of indices. A subset $S$ can be fixed in $\|S\| - 1$ operations using the above procedure. So the answer is just $n - k$ where $k$ is the maximum number of disjoint subsets we can partition into.

This problem can be solved with a standard $\mathcal O(3^n)$ bitmask DP: let $dp[S]$ denote the maximum number of disjoint good subsets $S$ can be partitioned into. Iterate over all submasks and transition from that submask. Specifically:

$$
dp[S] = \max_{S' \subset S, S' \text{ is good}} (dp[S \setminus S'] + 1)
$$

The answer is $n - dp[U]$. But $\mathcal O(3^n)$ is too slow for $n \leq 20$. Can we do better?

<div class="spoiler" style="margin-bottom: 5%">Of course we can, that's why I'm writing about this problem.</div>

Here's the key observation: if $S$ is good, and $S' \subset S$ is good, then $S \setminus S'$ is also good. So instead of finding a partition of $U$ into disjoint subsets, let's find a sequence of good subsets $S_1, S_2, \dots, S_k$ such that $S_1 \subset S_2 \subset \dots \subset S_k \subset U$. Now just take set differences between adjacent subsets in the sequence and we have our partition of disjoint good subsets!

It's more clear how to optimize finding a maximum length sequence $S_1 \subset S_2 \subset \dots \subset S_k \subset U$. Let's take a shift in perspective! Let's treat all the good subsets as nodes in a graph, and draw an edge from $S$ to $T$ if $S \subset T$. We want to find a maximum length path from $\emptyset$ to $U$. However, this graph still could have up to $\mathcal O(3^n)$ edges. Instead, let's also add in the non-good subsets as nodes so that now every subset of $U$ is a node and there are $2^n$ nodes. And instead of adding edges for all subset relations, just add an edge from a subset to every other subset with exactly one new bit turned on (i.e. we draw an edge from $S$ to $T$ if $S \subset T$ and $\|T \setminus S\| = 1$). This way, all subset relations are represented indirectly as nodes being reachable in this graph. Each node only has at most $n$ outgoing edges for each of its bits, so there are $\mathcal O(n \cdot 2^n)$ edges in this graph. And we can find the path with the maximum number of good nodes on it with DP, since it's a directed acyclic graph. So we've successfully solved the problem in just $\mathcal O(n \cdot 2^n)$!

---

This blog post could go on forever because there are an infinite number of problems that benefit from shifting perspectives. In fact, sometimes problemsetters even start with a clear perspective, but then intentionally present you with a different perspective to hide the solution (e.g. the trick of this [AtCoder problem](https://atcoder.jp/contests/agc040/tasks/agc040_c) is to flip A and B in the odd positions, which makes the operations much clearer). These perspective shifts aren't meant to be black magic; I'm a huge proponent of visuals, and I typically find intuition just by drawing stuff. For example, if I'm doing a problem about arrays and intervals, you might see this on my paper:

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 3]({{site.baseurl}}/assets/images/perspective-3.png)

*I can't believe I'm providing such high quality diagrams for free* 🙄

</div>

Thanks for reading. If you have any other cool problems, feel free to share!

---
layout: post
title: "My First Ever ICPC Regionals Experience!"
tags: [personal]
usemathjax: true
---

This past weekend, I attended my first ever ICPC regionals! It was an incredibly fun and enlightening experience for me, so I'd like to share with you some takeaways I had.

First, let me provide a bit of background, since I think I've only referred to myself by handle on my blog so far. My name is Max, and I'm a first year CS major at Georgia Tech. My teammates for this regional were [Arvind](https://codeforces.com/profile/arvindr9) and [Jeffrey](https://codeforces.com/profile/RandomKami). We were team "Acceptable" at the [2021 Southeast US Regional](https://seusa21-d1.kattis.com/standings).

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/first-ever-regionals-1.png)

*We ranked 3rd overall, or 2nd after removing duplicate schools, which qualifies us for the NAC.*

</div>

In the following spoiler, I've put a play-by-play of how our contest went by the hour, reconstructed with help of our submission history, scoreboard, and my personal memory. The exact times might be slightly off because it was extremely hectic to keep track of all these things during the actual contest. This play-by-play is also only from my perspective; I might not mention what my teammates were working on at a given time because I wasn't paying attention to what they're doing.

<details markdown="1" style="margin-bottom: 5%"><summary>Play-by-play</summary>

So as you can see on the scoreboard, the two teams that beat us were both from UCF. Going into this contest, our coach [chenjb](https://codeforces.com/profile/chenjb) had already warned us that UCF would be the major threat in this competition and they were extremely good at speedforces, so we knew we had our work cut out for us.

The contest begins. Arvind and Jeffrey start flipping through the problems while I type out our template and set up an alias command for compiling with warning flags. Our first impression of the problemset was that it didn't look hard enough to not be speedforces, so we knew we needed a strong pace to have a chance of winning.

About 5 or so minutes in, Jeffrey tells me [problem F](https://seusa21-d1.kattis.com/problems/na21.hopscotch500) looks like an easy DP. I quickly skim problem F and confirm that it indeed looks like an easy DP. I code the most naive shit possible and submit it at 8 minutes in:

```c++
for (int i=0; i<k-1; i++)
    for (auto [x, y] : spots[i])
        for (auto [a, b] : spots[i+1])
            dp[a][b] = min(dp[a][b], dp[x][y] + min((x - a) * (x - a), (y - b) * (y - b)));
```

We submit and eagerly await the verdict, hoping that we got first blood on this contest. And we're greeted with... time limit exceeded? I look over what I submitted and facepalm: our solution was $\mathcal O(n^4)$ when $n \leq 500$! Somehow, I had convinced myself our solution was amortized $\mathcal O(n^3)$ or something when I coded it, but no, it's $\mathcal O(n^4)$ when there are lots of points in two consecutive layers. I rethink the details and come up a horrible solution using linesweep and maintaining prefix/suffix minimums. If you know this problem, you'll know this is not the intended solution, and there's a **much, much, much** easier solution that can be coded in less than 10 lines. But anyways, for now the linesweep bash solution is the only one I have in mind, and I tell my teammates it'll take a while to implement, so we put problem F aside for now.

The next problem we attempt is [J](https://seusa21-d1.kattis.com/problems/na21.treehopping), which Arvind goes to code. He finishes the code extremely quickly and we submit at the 12 minute mark. We eagerly await our verdict and get... wrong answer? We reread the problem and facepalm again: we misread the problem! The problem isn't asking if adjacent vertices in the tree are more than distance 3 apart in the permutation. It's asking if adjacent vertices in the permutation are more than distance 3 apart in the tree!

At this point, we're down bad. It hasn't even been 30 minutes in and we're already losing on penalty to UCF with no ACs yet. We think for a bit longer, but we don't see how to take advantage of it being distance 3 or less. The only solution we can think of is brainless LCA bash that would work for any distance $k$ apart. I decide to just go ahead and implement it.

I hammer out a solution using binary lifting to find the LCA, run it on samples, and it... crashes. I'm so confused. I've literally never messed up binary lifting in practice before, but it appears there's a first time for everything. If only that first time wasn't at ICPC regionals :/ . After staring at my code and commenting out parts of the code to find the location of the crash, I still can't find the bug. Jeffrey is also ready to code [problem H](https://seusa21-d1.kattis.com/problems/na21.tetrisgeneration), so I print my code and pass the computer over to him.

Staring at my code on paper still doesn't help. Luckily, Arvind also takes a look at the code and points out my silly mistake: in my DFS method, I referenced the wrong variable, `p` instead of `u`, for a certain line. Ahhhhhhhh!!! I quickly make the edit on the computer and submit to claim our first AC 30 minutes into the contest. But this first mistake would be a sign. Because you can bet this was certainly not the last mistake I made on this contest, but just the first of many...

Shortly after our AC on J, Jeffrey completes his code for H and claims AC on first submit. Nice! The next problem he attempts to code is [I](https://seusa21-d1.kattis.com/problems/na21.tournamentseeding). After one wrong submit and a quick bug fix, we claim AC on problem I.

I should also note that sometime in the midst of all this chaos, we implemented [problem A](https://seusa21-d1.kattis.com/problems/na21.blackandwhite). I don't remember if this was before or after we got problem I (I think it was after). What I do remember is that we saw on the scoreboard that several teams had solved problem A, so we figured it couldn't be that hard. And indeed, upon reading we immediately thought of bitmask DP, which is the intended solution for this problem. I came up with some formulas and coded problem A. It worked on the first two samples, but to my dismay it failed on the third. Problem A will end up being an on and off thing that we think about whenever the computer was free, and it will serve as a bottleneck for the next 2 hours...

At this time, we also decide to code my linesweep bash idea for F, since the computer was available. Honestly, with my track record on this contest so far, I should have been banned from coding. I implement F, run it on samples, and what a surprise, I fail the second sample. LOL. I print the code and stare at it while contemplating life.

Jeffrey now codes [problem B](https://seusa21-d1.kattis.com/problems/na21.circlebounce). He asks me if we have a modint template we can use, and I attempt to code one for him to use. Unfortunately, my modint template doesn't compile because of weird C++ const lvalue/rvalue crap (seriously, why am I still coding in this contest?), so we give up and just manually mod after each intermediate calculation. He submits at almost 2 hours into the contest and gets... time limit exceeded? We reread the problem and realize $n \leq 10^{12}$, and Jeffrey was trying to code an unoptimized version of his idea that ran in $\mathcal O(n)$. I notice that the linear time part of his code:

```c++
for(int x=1;x<=n;x++) {
    cos[x] = (cos[x-1]*cos[0]-sin[x-1]*sin[0])%mod;
    sin[x] = (sin[x-1]*cos[0]+sin[0]*cos[x-1])%mod;
}
```

looks like a linear transformation. So it can be optimized with matrix exponentiation. I add matrix exponentiation to his code and resubmit, giving us AC on problem B 2 hours and 13 minutes into the contest (and first solve on B!). We're finally starting to bounce back a little, albeit we're still missing some easier problems (according to the scoreboard) such as problem A.

The next target is [problem E](https://seusa21-d1.kattis.com/problems/na21.failthemall). Arvind tries implementing a backtracking solution because he guesses there might be good properties of the problem that allows the backtracking to run faster. We also write a generator to create some large cases which seem to verify this claim. However, our claim turned out to be false, as we got time limit exceeded on submit.

A little after this, Arvind comes up with an alternative set of formulas for implementing the transitions in problem A. He goes ahead and implements it, and gets AC first try. Nice. Still don't know why my formulas are wrong, but I guess that works.

Shortly after that, I notice we can use 2-sat to solve problem E. Specifically, KACTL's version of [2-sat](https://github.com/kth-competitive-programming/kactl/blob/main/content/graph/2sat.h), which we have in our team reference document, has a convenient function `atMostOne` that does exactly what we want. And to find the lexicographically smallest solution, we can just try placing characters one at a time and repeatedly run the 2-sat algorithm after each character placed. I code this solution and thankfully get AC first try (given my ratio of correct to incorrect code written on this contest, I might have actually lost my mind if I wrote yet another buggy code). And shortly after that, Arvind codes his idea for [problem G](https://seusa21-d1.kattis.com/problems/na21.shortestmissingsubsequences) and also gets AC on first try! Now at 7 problems 3 hours and 30 minutes into the contest, we were finally catching up to the front of the leaderboard.

After all of this, we finally knock out problem F. I realize that my linesweep bash idea was wrong because I wrongly assumed I could use two-pointers in one section when I actually couldn't. And we also realize the much easier intended solution at this time: for each cell, just brute force over the $x$ or $y$ value that we transition from in the previous layer and take the minimum of all cells in the previous layer with that $x$ or $y$ value. This works because the cost function for moving is a minimum of $x$ and $y$, so any "wrong" transitions we considered won't be optimal anyways. The code is super concise and clearly runs in $\mathcal O(n^3)$. We code it up and claim AC 4 hours in. And shortly after this, Jeffrey codes and claims AC on [problem D](https://seusa21-d1.kattis.com/problems/na21.dormroomdivide).

Now, we enter the final 30 minutes of the contest. Our hopes are to get one more problem, because we were certainly losing on penalty among teams with 9 solves. After some pondering about [problem C](https://seusa21-d1.kattis.com/problems/na21.diagonals), we figure maybe backtracking will work because the problem guarantees there exists exactly one solution, so the search space is likely much smaller than the naive bound of $2^{n^2}$. I code up a recursive backtracking solution and it... doesn't work on samples. I was unable to debug it successfully during contest, likely because I was panicking due to there being little time left. The solution did pass when I recoded it again in upsolving, so my guess is I just made a typo during the onsite :/ .

With 10 minutes left, Jeffrey tells me he has the solution to [problem K](https://seusa21-d1.kattis.com/problems/na21.xorisland). But it was unfortunately too late, and we could not finish our code before the end of the contest. And even when we submitted immediately after the end of the contest, we got WA, so we would have needed way more time. The contest was over.

---

</details>

Hopefully that play-by-play wasn't too dry to read (or if you didn't read it, that's ok too). Anyways, here are some of the takeaways I have from this experience:

**1. I've forgotten how great in-person events are.**

I'm opening with this point because I want to make clear that despite my frustration expressed at some moments in the play-by-play, I genuinely had fun at regionals. I complain because this wouldn't be a smax post-contest reflection post without some form of complaining, and because I'm generally quite critical of myself, but I still want the main takeaway from this post to be how great my experience was.

The vast majority of CP competitions such as Codeforces, AtCoder, or even many team competitions I did in high school, are online. I sit in my room and log onto my computer to compete. In the case of a team competition, I join a Discord call to coordinate with my friends. But the atmosphere is very different when you're actually sitting in a room with other competitors and hearing different teams discuss amongst themselves. And on our site, the runners bring a balloon to your table whenever you get AC on a problem, which adds to the hype (though I will say I got super psyched out at the beginning because the div 2 teams in our room were accruing balloons at a super fast rate near the beginning of the contest). It's just that much more fun and satisfying when you see a physical reward for your ACs. I have no doubt it will be extremely hype to attend NAC in-person and sit in the same auditorium as the strongest CPers from across North America.

And while we're on the topic of positives, I'll say that while I didn't get the chance to properly appreciate it in contest, [problem K](https://seusa21-d1.kattis.com/problems/na21.xorisland) was a really fun problem to upsolve. If you want to solve it yourself, then as a hint,

<div class="spoiler" style="margin-bottom: 5%">
read about the "blue eye brown eye riddle" first, because that personally helped me motivate the solution to problem K.
</div>

**2. I didn't realize how reliant I was on my own setup.**

At regionals, our team used VSCode, but the setup was very different from the VSCode setup we used in practice. You can't access the internet to install extensions on VSCode at regionals, so VSCode simply operated as a glorified text editor for our use case. The lack of a linter meant I had no idea if my code would compile until I ran the compilation command, so the number of uncaught typos I made definitely increased. Also, the number of times I hit "ctrl + Q" out of habit and accidentally closed the VSCode window (because on my personal setup I have "ctrl + Q" mapped to something else) was stupidly high. Finally, the keyboard they gave us at regionals was a big keyboard with the arrow keys on the far right. So something like the image below:

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 2]({{site.baseurl}}/assets/images/first-ever-regionals-2.png)

</div>

You might wonder what difference this makes. Well, the keyboard I personally use on both my laptop and my PC is a more concise keyboard with the arrow keys directly underneath the "shift" key:

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 3]({{site.baseurl}}/assets/images/first-ever-regionals-3.png)

*In this keyboard, the arrow keys can be reached without moving my right hand, while I can't do that in the larger keyboard.*

</div>

Yes, that's right. I use the arrow keys when coding. Instead of closing the right parenthesis by actually typing it, I use the arrow keys and rely on the autocomplete in my code editor to fill in the right parenthesis. In my defense, it's unnatural for me to type the right parenthesis when I already see it autocompleted on the screen, but this difference definitely messed with me during regionals and made even typing a simple for loop a much slower procedure. Of course, I'm not saying that we would have been significantly faster if I typed faster, because the bottleneck for our slow performance was still buggy code. It's just a small difference on top of many mistakes I was already making.

**3. I need to be more consistent.**

It would be disingenuous for me to say "oh if I didn't make a typo on J and if I didn't code A wrong and if I didn't code F wrong and if I was basically a perfect human being, we would have won." Committing mistakes and debugging are all part of the game, and the team that's best able to adapt and mitigate the damage of these mistakes is the one that comes out on top. Looking at the scoreboard, UCF also made wrong submits, which means their best case performance would have smoked our best case performance regardless.

The issue is that I'm really bad at recovering from a rough start. I described a similar pitfall in [my reflection post in January]({{site.baseurl}}/winter-break), where I never recovered from a rough start on Global Round 15 and lost 139 rating points. I think in this regionals, I also suffered from a similar issue where previous mistakes snowballed and made me more prone to committing future mistakes. If you didn't know me and judged my behavior this round, you would have thought I was a candidate master.

One way to alleviate this is to be more consistent in the first place. Reach a point where even my worst case performance is somewhat acceptable. I think this is an attribute that people overlook when discussing how to get better. If you look at performances of the top 10 competitors on CF, you'll notice that they hit consist LGM performance not just every once in a while, but almost every single time. And even if they brick on an early problem, they can usually salvage their performance with a solve on a harder problem to compensate, so their losses are never that devastating. Personally, I've gotten IGM performances before, but an ideal state would be to be able to hit IGM performances consistently, which will be my next goal.

Another way is to just get better at keeping my cool. Don't let early mistakes affect the rest of the contest. This is of course easier said than done, but it's a skill that would benefit me not just in CP, but in life.

---

Congrats for making it this far! If you're still here, here are some pics taken at the event. Cheers, and hopefully we'll do better at the NAC!

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 4]({{site.baseurl}}/assets/images/first-ever-regionals-4.png)

*Clowning in 4K*

![Image 5]({{site.baseurl}}/assets/images/first-ever-regionals-5.png)

*Awards ceremony! I am second from the left, the one wearing the dark green hoodie.*

![Image 6]({{site.baseurl}}/assets/images/first-ever-regionals-6.png)

*School picture! Georgia Tech sent 6 teams in total.*

![Image 7]({{site.baseurl}}/assets/images/first-ever-regionals-7.png)

*Productive use of balloons :)*

</div>

---
layout: post
title: "2022 ICPC North American Championship"
tags: [personal]
usemathjax: true
---

*Yay the blog has been revived!*

Hey everyone! This past weekend, I attended the 2022 ICPC North American Championship, which might just be the most fun CP in person events I've attended so far. I represented Georgia Tech alongside fellow teammates [Arvind](https://codeforces.com/profile/arvindr9) and [Jeffrey](https://codeforces.com/profile/RandomKami). We got to fly down to Florida, meet CP people in person, and get all expenses paid for by our school! Literally the perfect formula for fun. In a similar spirit to my post about [ICPC regionals]({{site.baseurl}}/first-ever-regionals), I'd like to write about my first ever NAC experience.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/nac-1.png)

</div>

*The next few sections will be me talking about the competitive programming aspects of NAC. If you don't care for that, you can just scroll to the bottom where I talk about more social aspects and dumped some photos.*

## Expectations

If you take a look at the [NAC teams list](https://codeforces.com/blog/entry/101221) this year, you might notice that there are some really stacked teams this year. Over a dozen teams have at least one red person, some of which are IGMs, and many of them have done ICPC before and are therefore probably more consistent than us. Thus, our goal going in was simple: secure a spot in the world finals.

It's a little hard to predict how many world finals slots there will be, because there are only two data points to go off of (2020 and 2021). Last year they cleared 19 teams, but I've been told that isn't/won't be the norm. The year before, it seems [17 teams](https://codeforces.com/blog/entry/73791) were chosen, but it wasn't simply just the top 17 teams on the NAC leaderboard. I found a [list of the different criteria](https://codeforces.com/blog/entry/70439) used to select teams that year, and it seems there were regional considerations as well. In particular, the maximum length prefix of teams on the [leaderboard](https://web.archive.org/web/20200615000000*/http://nac.icpc.global/scoreboard/) that all qualled in 2020 was only 10. So to secure a world finals spot, one would guess you want to be in the top 10, maybe 10-12 range.

<!-- ## Initial Impressions from NAPC

NAC is actually a multi-day event where the teams engage in a brief [programming camp](https://www.cecs.ucf.edu/NAC-NAPC/) and socialize for the first few days, and compete in the real NAC competition on the last day. The NAPC was our opportunity to actually compare ourselves to the other teams and see how team performances mirror their team ratings. Unfortunately, you can't access the NAPC leaderboards anymore because they were all hosted on [nac22.kattis.com](https://nac22.kattis.com/) and have since been overwritten with the real competition, so this part is me going off of memory.

In total, we did 4 contests aside from the real one:
1. A standard contest (but shorter, 4 hrs)
2. A half-keyboard contest (you can only code in the second half of each hour, 3 hrs total)
3. NSA challenge
4. Dress rehearsal

The NSA challenge isn't a helpful data point for evaluating NAC because the type of problems is too different (just [take a look yourself](https://nsachallenge22.kattis.com/problems)). Dress rehearsal also wasn't the most reliable data point because some teams focused more on testing the environment instead of AK'ing first, but enough teams tried that it can still be partially considered. The main takeaways I got were:

1. We do not want a speedforces NAC. A lot of teams are good at speedforces. We are not one of them.
2. I definitely slept on some of the teams with lower team rating on the [list](https://codeforces.com/blog/entry/101221).
3. Half-keyboard contest proved I still struggle to implement stuff correctly in the last 30 minutes at times.
4. During the dress rehearsal, I found out that sitting in the giant ballroom with the timer on the big screen is a very different vibe. I won't lie, I felt a bit of adrenaline during the dress rehearsal already despite it not being the real thing. And you can bet I felt it on the final day.

None of these things are things we could change or account for before NAC anyways. They're just things I observed. A lot of teams have the same build (a standard Algo/DS build), as teams with the same number of solves generally solved the same subset of problems and there weren't many holes in the leaderboard (so basically, the opposite of a CF global round leaderboard). -->

## The Contest

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 2]({{site.baseurl}}/assets/images/nac-2.jpg)

![Image 3]({{site.baseurl}}/assets/images/nac-3.jpg)

</div>

I'll say right now, competing on site is a very different feeling from doing a team practice in a random room. At the start, you wait outside the ballroom anxiously as they filter teams into the room one by one. When you walk in, they announce your school through the speakers. You walk across the enormous ballroom to find your seat as Imagine Dragons' "Whatever It Takes" is blaring through the speakers. You sit down and immediately notice the countdown timer on the big screen, counting down mere minutes before the contest starts. Different speakers and sponsors come on stage to give words of inspiration or talk about career opportunities, but everything is a blur because you're only fixated on that timer.

When the timer hits zero, everyone scrambles for the keyboard and the problem set. I hastily configure VS Code and type out our template as my teammates start reading the set. The contest has begun.

In similar fashion to my regionals post, I'll put a play-by-play in spoilers reassembled from memory, the scoreboard, and the [livestream](https://www.youtube.com/watch?v=1iez5djXwE4). Don't worry, I assure you I'll be far less salty than last time.

<details markdown="1" style="margin-bottom: 5%"><summary>Play-by-play</summary>

A link to all problems can be found [here](https://nac22.kattis.com/problems).

The first few minutes are all devoted to setting up the environment and combing through the problems. The first AC from MIT comes in at 9 minutes for problem J. Upon reading this, we flip to problem J. It's literally just brute force, simulate tic-tac-toe. I go to code up a simple recursive brute force. Unfortunately, it gets wrong answer. Oops. Luckily, I realize the fix shortly after:

In problem J, you also have to detect when a state is unreachable and print $-1$ in those cases. I handled most of the cases correctly but missed one: if the board has $3$ X tokens and $3$ O tokens, and X has $3$ in a row while O does not, then this is actually impossible. This is because X goes first, and after X wins, the game immediately ends, so it would have been impossible for O to place a third token. After correctly accounting for those types of cases, we get our first AC at the 38 minute mark.

The next problem we get AC on is M. Jeffrey comes up with a construction based on breaking a length 10 string into 3 sections and cycling AAAA -> AAAB -> AABB -> ... That one fortunately gets AC first try at 56 minutes.

After that, Arvind attempts a solution for E. It unfortunately gets wrong answer. Arvind and I go to debug that solution while Jeffrey independently codes problem A. The next hour is pretty dry as both our A and E are wrong for the longest time. After a few bugs, we finally get AC on problem A 1 hr and 59 minutes in.

Unfortunately, the issue with problem E ran more deep than a simple implementation bug. Our initial idea was to let $dp[u][v]$ denote the minimum cost of making the subtree rooted at node $u$ valid and having its value be $v$. That formulation is fine. The problem was that we assumed $v$ would be bounded by some constant multiple of $100$, because the input values were bounded by $100$, and we used that assumption for a $\mathcal O(nv^2)$ solution. An upper bound on the total answer is of course $2n$ (by simply changing all node values to $2$), but maybe each individual value never needs to be that big in an optimal solution? Unfortunately, that's not the case. A counterexample would be a root with $n - 1$ children, around half of which are $89$ and half of which are $97$. Then an optimal solution is to change the root to $89 \cdot 97 = 8633$. It took us a while and three wrong submissions to come to this conclusion...

Eventually, after realizing this, Arvind reformulates the DP to work for $v \leq 2n$, and we get to a $\mathcal O(n^2 \log n)$ solution from iterating over the prime divisors for the transition. After another wrong submit from a typo, we finally get AC on problem E.

At this point, we're in a tricky spot. It's 2 hours and 40 minutes into the contest, and we only have 4 problems and dubious penalty. 2 hours and 40 minutes for the 4 easiest problems in the set does not bode well for our ability to get the harder problems in time... I won't lie, at this stage of the contest I was concerned we weren't going to make it.

The next problem we solve is problem G. At this point, we were just chasing the scoreboard, and several problems (F, G, L) seemed roughly tied at this point. Problem G effectively gives us a functional graph and asks us to find a path on this graph visiting the most number of distinct sightseeing spots. The issue with a straightforward DP is accounting for overcount of sightseeing spots, so the best you could do with that idea is $\mathcal O((rc)^2)$ with reduced constant from bitset.

Fortunately, you can just reverse the edges, which gives you a tree rooted at either a single node or a cycle. And once you convert the problem to a tree problem, the rest of the solution flows quite naturally: DFS from the root and collect the sightseeing spots in some hashmap/frequency array for a pure $\mathcal O(rc)$ solution. There is a bit of code to hammer out to handle the 6 different types of characters in the grid and the contraction of cycles into single nodes in the tree, but it's not too bad, and we're able to get it with minimal dirt at the 3 hour 24 minute mark.

While I was implementing G, Jeffrey also read and worked out the solution to problem F, and he explains the solution to me after we AC G. The solution ends up being a clean $\mathcal O(nt \cdot 2^n)$ bitmask DP. We get AC on that problem just after the 4 hour mark.

We're now in the final hour, and the scoreboard is frozen. We have 6 problems and still slightly dubious penalty. Maybe we'll be ok if we don't get one more, but it's hard to say. The final problem we go for is L. L was a problem I read earlier because we saw MIT solved it ridiculously early and quickly, but I dismissed it because at the time I didn't even know how to solve it in 1D, let alone 2D. But the trick to the problem actually turns out to be quite simple (courtesy of Arvind): if you have too many points in your rectangle, then the answer is always 1. There will always be some triplet $(a, b, c)$ such that $a \leq b \leq c, a + b > c$. Specifically, the worst case grows like the Fibonacci sequence $\{1, 1, 2, 3, 5, 8, \dots\}$. So if you just set a threshold of $45$ points, then you can always print 1 if there are more than that many points in the rectangle, and use brute force otherwise.

The cleanest way enumerate all the points in a rectangle is with a merge sort tree, where you descend on the segment tree for the first dimension and binary search in a sorted vector for the second. Unfortunately, I kind of forgot that existed during the contest, so I thought you needed something more complex like a raw 2D segment tree with pointers and whatnot. Raw 2D segment tree is notorious for using a ton of memory, and it has been forever since I last implemented one, so I wasn't confident in my ability to code one. Fortunately, I also realized you can do square root decomposition instead. Just partition the first dimension into blocks containing sorted vectors and binary search on the second dimension. Funny enough, this is just a strictly worse version of the optimal segtree + sorted vector approach, so I'm not sure how I didn't think of that after thinking of square root, but whatever.

I go to implement this solution, and after getting a RTE due to setting incorrect array bounds, I get a WA still. Uh oh. But we remain calm. There's still like 40 minutes left. As long as we debug like we always do, it's literally impossible for us to not get such a raw DS problem before the contest ends. Also, since this is the last problem we plan on ACing, Arvind and Jeffrey had already created small cases to try while I was implementing, so we had plenty to go off of.

And fortunately, we do get it. I manage to find my typo after combing through my printed code. And after fixing, submitting, and anxiously waiting, we get our AC verdict! This is it! We're at 7 problems! We should be clean!

As a bonus, we also thought we might have gotten the solution to problem I near the very end as well, but we did not finish implementing in time. We also decided to spam submits on every problem at the last minute for fun, since the scoreboard was frozen and it would show a million question marks, but we could only get submits in on problems B and I before we got rate limited lmao.

---

</details>

Ultimately, we ended up solving 7 problems, which we were almost certain coming out of the contest was enough to qualify for world finals! And after asking around and finding out that a lot of teams were actually at 6, we realized we might medal too!

## The Results

<div markdown="1" style="text-align: center; margin-bottom: 5%">

<iframe width="886" height="498" src="https://www.youtube.com/embed/EHtnn7WC8V0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

*Resolving the scoreboard one-by-one is genius.*

</div>

I must say, that was a very anxious closing ceremony to sit through. But after listening to some long speeches and going through the awards for the NSA challenge and first solves, we finally get to the real deal. They announce there will only be 6 medals. Oh dear. That's going to be tight then, because we've already identified that some teams who would have better penalty than us if they were on 7[^1]. How many teams will actually hit 7 though?

It turns out, few enough that we're perfectly 6th! I just remember being so ecstatic upon finding out. The commitment to ICPC at the start of this year and the practices we did were all worth it. The effort was worth it. There's no feeling more rewarding than that.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 4]({{site.baseurl}}/assets/images/nac-4.jpg)

*From left to right: Jingbang (coach), Arvind, Jeffrey, Me*

<img src="{{site.baseurl}}/assets/images/nac-5.jpg" alt="Image 5" width="50%"/>

</div>

## The Experience

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 6]({{site.baseurl}}/assets/images/nac-6.jpg)

*There's a bigger group pic out there that exists but I have yet to get my hands on it.*

<img src="{{site.baseurl}}/assets/images/nac-7.jpg" alt="Image 7" width="50%"/>

*Omg who dis*

</div>

Regardless of the results, this was already one of the best competitive programming experiences I've had so far. There's something surreal about meeting someone who you only knew by their handle previously, but now have a real name and face to match it to. There's something surreal about chatting with others about CP in real life. Like you'll mention a Codeforces round or blog, and others will actually know what you're talking about. Heck, there's something surreal about finding out there are people who actually read this blog!

I also found out I suck at Poker. And Secret Hitler. And lockout. Luckily (or unfortunately), I have like a year[^2] to get better at all of these before world finals 😅

But with that, I think I will end it here. I originally had a whole other section about NAPC, the programming camp that happens before NAC, but that section ended up being too rambly and not going anywhere. I want to keep this post shortened to just the highlights and my main thoughts.

Also, as a reward for making it this far, I have a sneak peak for my next educational blog that will be crossposted to Codeforces: [click me!](https://dmoj.ca/problem/ds5) So stay tuned for that.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 8]({{site.baseurl}}/assets/images/nac-8.jpg)

![Image 9]({{site.baseurl}}/assets/images/nac-9.jpg)

<!-- ![Image 10]({{site.baseurl}}/assets/images/nac-10.jpg) -->

<img src="{{site.baseurl}}/assets/images/nac-11.jpg" alt="Image 11" width="50%"/>

![Image 12]({{site.baseurl}}/assets/images/nac-12.jpg)

![Image 13]({{site.baseurl}}/assets/images/nac-13.jpg)

<img src="{{site.baseurl}}/assets/images/nac-14.jpg" alt="Image 14" width="50%"/>

</div>

[^1]: Specifically, Jeffrey spent like the entire time in between the contest and the closing ceremony asking around and figuring out which teams were on 6 or 7 solves, and ended up narrowing it down to confirmed 6th or 7th, depending on whether or not the University of Washington solved 7 problems.

[^2]: I heard rumors 2022 world finals will be in November 2023? I also heard it will be in Egypt? Or Budapest? Who knows.

---
layout: post
title: "Fully Dynamic Trees Supporting Path/Subtree Aggregates and Lazy Path/Subtree Updates"
tags: [tutorial, algo]
featured: true
usemathjax: true
---

## Motivation

We will solve [this problem](https://dmoj.ca/problem/ds5).

## Prologue

The original name of this problem is [SONE1](https://darkbzoj.cc/problem/3153), originating from the fiery depths of Chinese OI. Amazingly, the Chinese have figured out how to augment the link/cut tree to solve this problem (ref [1](https://www.cnblogs.com/clrs97/p/4403244.html), [2](https://blog.csdn.net/iamzky/article/details/43494481)). Maintaining subtree aggregates with a LCT isn't difficult, and there's a well-known [Codeforces article](https://codeforces.com/blog/entry/67637) by [ouuan](https://codeforces.com/profile/ouuan) about it, but supporting lazy subtree updates is much harder. There's [this](https://codeforces.com/blog/entry/80145), but it requires the updates to be invertible. I haven't found any English resources explaining how to augment LCT to solve this problem, so this article is my attempt at doing that. Most of the content is credited to this [article](https://www.cnblogs.com/clrs97/p/4403244.html), and the implementation is built upon a clean [ordinary LCT implementation](https://codeforces.com/blog/entry/75885) by [bicsi](https://codeforces.com/profile/bicsi).

**EDIT**: As an aside, some readers have pointed out that this is essentially what's referred to in English as a ["Top Tree"](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.87.9754&rep=rep1&type=pdf). The articles I referenced referred to it as an "AAA Tree". I initially assumed they were different, but after skimming the paper in question there indeed seem to be similarities, just differences in what stuff are called.

## Prerequisites

- Link/Cut Tree
    - [https://en.wikipedia.org/wiki/Link/cut_tree](https://en.wikipedia.org/wiki/Link/cut_tree)
    - [https://www.youtube.com/watch?v=XZLN6NxEQWo](https://www.youtube.com/watch?v=XZLN6NxEQWo)
- Supporting Subtree Aggregates (But Not Lazy) with LCT
    - [https://codeforces.com/blog/entry/67637](https://codeforces.com/blog/entry/67637)

## Terminology

I will use the same terminology used in the resources linked in the prerequisites section. Here is a complete list of them to ensure we are all on the same page before proceeding:

<details markdown="1" style="margin-bottom: 5%"><summary>Terminology</summary>

- Link/cut trees (LCT) maintains a **preferred path decomposition** of a tree.
- A preferred path decomposition of a tree is a decomposition of its nodes into node-disjoint paths, known as **preferred paths**.
- Nodes on the same preferred path are connected with **preferred edges**. All other edges are **virtual edges**.
- We also have **preferred children** vs **virtual children** depending on the type of edge.
- The **represented tree** refers to the real tree graph we are representing. This is not to be confused with the structure of the LCT data structure.
- The preferred paths will be stored as **splay trees** (referred to in some literature as "auxiliary trees", but I will not use that term in this article).
- The entire collection of splay trees concatenated with parent pointers is referred to as just the **LCT** (referred to in some literature as "tree of aux trees", but not a fan of that phrasing).
- All operations in the LCT (adding an edge, removing an edge, rerooting the tree, etc.) revolve around the **access** operation. `access(u)` makes the path from root to $u$ preferred and splays $u$ to the top of the LCT.
- Subtree aggregates are stored in **virtual subtrees**, which are all subtrees in the LCT except for within the splays.
- When doing lazy propagation, pushing down lazy tags to children is referred to as a **push** operation. Recalculating the aggregates in a node based on children is referred to as a **pull** operation.

---

</details>

## The Core Idea

I have a LCT, and I store the aggregates of each subtree in **virtual subtrees**. Any child of a node is either a **preferred child** or a **virtual child**. The issue with doing lazy propagation directly on this model is that a node could have several (up to $\mathcal O(n)$) virtual subtrees hanging from it, and we cannot push down the lazy tags to each of its virtual children each time. The fix is surprisingly simple: **binarize** the tree. This way, when we push down lazy tags, we only have at most two virtual children to push to, and the complexity is fixed. To understand what I mean by binarize, take a look at the diagram of LCTs below:

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/ds5-1.png)

*The solid lines represent preferred edges, and the dotted lines are unpreferred/virtual edges. The nodes 2, 7, 1, 3 form a preferred path in that order. Node 1 has both a left and right preferred child, and it has 3 virtual children.*

</div>

The LCT on the left is what an ordinary LCT might look like. Node 1 has 3 virtual children, which is more than 2. Therefore, we binarize its virtual children by introducing a new **fake** vertex to connect both 4 and 5. Now as seen in the LCT on the right, each node has at most 2 preferred children and 2 virtual children (up to 4 children in total). So simple!

If only... the devil is in the details...

If you were to go and start coding this idea right now, you might realize there are quite few pitfalls to keep in mind while coding. The next few sections will be dedicated to talking about some of the implementation details in depth.

## The Access Method

Since everything in LCTs revolve around access, let's first characterize the access method. Here's what a typical access method might look like:

```c++
// tr is a global array storing each of the LCT nodes
// p is the parent and ch is a size 4 array of its children
void attach(int u, int d, int v) {
    tr[u].ch[d] = v;
    tr[v].p = u;
    pull(u);
}

// adds v as a virtual child of u
void add(int u, int v) {
    tr[u].vir += tr[v].sum;
}

// removes v as a virtual child of u
void rem(int u, int v) {
    tr[u].vir -= tr[v].sum;
}

void access(int u) {
    splay(u);
    add(u, tr[u].ch[1]);        // the preferred path ends at u, so any node with greater depth should be disconnected and made virtual instead
    attach(u, 1, 0);            // sets the right preferred child of u to null (0)
    while (tr[u].p) {
        int v = tr[u].p;
        splay(v);
        add(v, tr[v].ch[1]);    // disconnect any node with greater depth than v, so result is a preferred path ending at v
        rem(v, u);              // u is no longer a virtual child of v
        attach(v, 1, u);        // u is now the right preferred child of v
        splay(u);
    }
}
```

You'll be glad to hear this isn't far from the final product. The fundamental process of accessing a node remains the same. The only difference is that the `add` and `rem` methods become more complex, because we may have to introduce more fake nodes for binarization.

## The Add Method

Firstly, let's take a look at `add`.

```c++
// attaches v as a virtual child of u
void add(int u, int v) {
    if (!v)
        return;
    for (int i=2; i<4; i++)
        if (!tr[u].ch[i]) {
            attach(u, i, v);
            return;
        }
    int w = newFakeNode();
    attach(w, 2, tr[u].ch[2]);
    attach(w, 3, v);
    attach(u, 2, w);
}
```

The first case is straightforward: if one of our 2 virtual children is null, simply set $v$ as that virtual child instead.

```c++
for (int i=2; i<4; i++)
    if (!tr[u].ch[i]) {
        attach(u, i, v);
        return;
    }
```

The second case is when $u$ already has 2 virtual children. In that case, we create a new fake node connecting both $v$ and one of our old virtual children, and substituting that as the new virtual child.

```c++
int w = newFakeNode();
attach(w, 2, tr[u].ch[2]);
attach(w, 3, v);
attach(u, 2, w);
```

The `rem` method is slightly more complex and requires the introduction of several other concepts first, so we'll come back to that in a minute.

## Bound on Fake Node Creation

Firstly, while we're on the subject of fake nodes, **how many fake nodes will we create?** If we only create fake nodes without destroying, we could create as many as the number of preferred child changes, which is $\mathcal O(n \log n)$ with significant constant. That's often an undesirable amount of memory to use. Fortunately, observe that at any point, we only need at most $n$ fake nodes to binarize the entire tree. Therefore, as long as we recycle memory and destroy fake nodes in the `rem` method when they become unnecessary, we will limit our memory usage to just $2n$ nodes.

## Fake Nodes in Our Preferred Paths?

If we just splay willy-nilly in our access method, we might connect fake nodes into our preferred paths. This is undesirable because it becomes messy to keep track of where fake nodes are and when we can destroy them for preserving memory. Thus, we'll need to make the following change to access.

<details markdown="1" style="margin-bottom: 5%"><summary>Old Access</summary>

```c++
void access(int u) {
    splay(u);
    add(u, tr[u].ch[1]);
    attach(u, 1, 0);
    while (tr[u].p) {
        int v = tr[u].p;
        splay(v);
        add(v, tr[v].ch[1]);
        rem(v, u);
        attach(v, 1, u);
        splay(u);
    }
}
```

---

</details>

Instead of `int v = tr[u].p`, we will do `int v = par(u)`, where `par` is a method giving us the lowest ancestor of $u$ that is **not** a fake node.

```c++
int par(int u) {
    int v = tr[u].p;
    while (tr[v].fake)
        v = tr[v].p;
    return v;
}
```

However, this is not good enough. There is no height guarantee on our binary tree formed from fake nodes, so we could be repeating $\mathcal O(n)$ work each time and break complexity. The solution? Make it a splay. So we have **two splays**: the usual one in ordinary LCT and one on the fake nodes. The new `par` method would thus look like the following:

```c++
int par(int u) {
    int v = tr[u].p;
    if (!tr[v].fake)
        return v;
    fakeSplay(v);
    return tr[v].p;
}
```

<details markdown="1" style="margin-bottom: 5%"><summary>Technically...</summary>

So I said the old `par` method wasn't ok because the height could be $\mathcal O(n)$, but when I copy pasted it into my code and submitted it, it still got AC. My guess is it's very difficult to construct a sequence of operations to lead to a degenerate fake node binary tree, so in practice it still runs fast for random data? Either way, just use the second version to be safe.

---

</details>

## Fake Splay

There's no need to remake a new set of splay functions for the fake splay cause that's wasteful. Instead, strive to write reusable splay code that works for both versions depending on a parameter passed in. I don't want to get too into the weeds of the splay implementation because that's more about splay trees and less about LCT, and because everyone implements splay differently, so I'll just copy paste my before and after showing how I adapted splay tree code to work for both versions:

<details markdown="1" style="margin-bottom: 5%"><summary>Before</summary>

```c++
void attach(int u, int d, int v) {
    tr[u].ch[d] = v;
    tr[v].p = u;
    pull(u);
}

int dir(int u) {
    int v = tr[u].p;
    return tr[v].ch[0] == u ? 0 : tr[v].ch[1] == u ? 1 : -1;
}

void rotate(int u) {
    int v = tr[u].p, w = tr[v].p, du = dir(u), dv = dir(v);
    attach(v, du, tr[u].ch[!du]);
    attach(u, !du, v);
    if (~dv)
        attach(w, dv, u);
    else
        tr[u].p = w;
}

void splay(int u) {
    push(u);
    while (~dir(u)) {
        int v = tr[u].p, w = tr[v].p;
        push(w);
        push(v);
        push(u);
        int du = dir(u), dv = dir(v);
        if (~dv)
            rotate(du == dv ? v : u);
        rotate(u);
    }
}
```

---

</details>

<details markdown="1" style="margin-bottom: 5%"><summary>After</summary>

```c++
void attach(int u, int d, int v) {
    tr[u].ch[d] = v;
    tr[v].p = u;
    pull(u);
}

int dir(int u, int o) {
    int v = tr[u].p;
    return tr[v].ch[o] == u ? o : tr[v].ch[o+1] == u ? o + 1 : -1;
}

void rotate(int u, int o) {
    int v = tr[u].p, w = tr[v].p, du = dir(u, o), dv = dir(v, o);
    if (dv == -1 && o == 0)
        dv = dir(v, 2);
    attach(v, du, tr[u].ch[du^1]);
    attach(u, du ^ 1, v);
    if (~dv)
        attach(w, dv, u);
    else
        tr[u].p = w;
}

// call splay(u, 0) for ordinary and splay(u, 2) for fake
void splay(int u, int o) {
    push(u);
    while (~dir(u, o) && (o == 0 || tr[tr[u].p].fake)) {
        int v = tr[u].p, w = tr[v].p;
        push(w);
        push(v);
        push(u);
        int du = dir(u, o), dv = dir(v, o);
        if (~dv && (o == 0 || tr[w].fake))
            rotate(du == dv ? v : u, o);
        rotate(u, o);
    }
}
```

---

</details>

## How to Do the Lazy Propagation

A first thought on doing lazy propagation is to simply maintain 2 lazy values: one for the path, and one for the subtree. However, it's important to ensure the two lazy values cover a disjoint partition of nodes, because otherwise there's no way to properly order the combination of lazy values affecting the intersection of the two partitions. To clarify what I mean, consider the range set operation applied to a simple array.

$$
[1, 2, 3, 4, 5]
$$

Say we range set all values in the interval $[1, 3]$ to $7$, range set the interval $[3, 5]$ to $9$, and then range set the interval $[1, 3]$ again to $10$. Say we maintain two lazy values, one for the interval $[1, 3]$ and one for the interval $[3, 5]$, which would be $9$ and $10$ respectively. The issue is that it isn't obvious what the lazy effect on index $3$ is. In this case, we could maintain the time stamp of both lazy values and use the later one to determine the effect, but you can see how if we use some other lazy operation where intermediate operations, not just the last one, have an effect (e.g. applying an affine function $x := ax + b$ to a range), then this breaks down.

The same applies for the LCT model. If a LCT node has two lazy values, one for the path and one for subtree (in the represented tree), then we need them to cover a disjoint partition of all nodes in the subtree (in the LCT, not in the represented tree). For brevity, let's denote the first lazy value as `plazy` (short for "path lazy") and the second value as `slazy` (short for "subtree lazy"). We will also define 3 aggregates in each node: `path`, `sub`, and `all`. `path` is the standard path aggregate used in ordinary LCT, so it is `tr[tr[u].ch[0]].path + tr[tr[u].ch[1]].path + tr[u].val`. `sub` will be everything else not covered in `path`, so `tr[tr[u].ch[0]].sub + tr[tr[u].ch[1]].sub + tr[tr[u].ch[2]].all + tr[tr[u].ch[3]].all`. And finally, `all` is just a combination of both to get everything in this LCT subtree, or `tr[u].all = tr[u].path + tr[u].sub`. Putting it all together, we get the following `pull` method for recalculating the aggregates in a node:

```c++
void pull(int u) {
    if (!tr[u].fake)
        tr[u].path = tr[tr[u].ch[0]].path + tr[tr[u].ch[1]].path + tr[u].val;
    tr[u].sub = tr[tr[u].ch[0]].sub + tr[tr[u].ch[1]].sub + tr[tr[u].ch[2]].all + tr[tr[u].ch[3]].all;
    tr[u].all = tr[u].path + tr[u].sub;
}
```

Now for pushing down the lazy tags! Pushing `plazy` is the same as ordinary LCT: push down to the 0th and 1st child. Pushing `slazy` is slightly less obvious. Specifically, we will only update `slazy` for the 0th and 1st child, and we will update **both** `plazy` and `slazy` for the 2nd and 3rd child. For the 2nd and 3rd child, we update both lazy values because all values in that LCT subtree represent nodes affected in the real subtree, so both `path` and `sub` must be updated. But why don't we update both lazy values in the 0th and 1st child? It's because if we were to receive an update for this entire subtree from the parent, then there are 2 cases:

1. The current node is a preferred child of its parent. In this case, recursively go up until we hit case 2.
2. The current node is a virtual child of its parent. In this case, when we push down from the parent, the current node would have gotten both its `plazy` and `slazy` values updated. So the `path` part of its LCT subtree is already fully updated. If we were to update `plazy` of its 0th and 1st children again when pushing down from the current node, then we would be applying the update twice to the path.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 2]({{site.baseurl}}/assets/images/ds5-2.png)

*Say node 1 is following case 2, meaning it is a virtual child of its parent. Node 1 just received a tag pushed down from its parent, so its plazy and slazy values are updated. The nodes highlighted in red are the ones already correctly accounted for by plazy. If we were to push down and update node 2's plazy with node 1's slazy as well, then node 2 would be receiving the update twice which is incorrect.*

</div>

So putting everything together gives us the following push method:

```c++
void pushPath(int u, const Lazy &lazy) {
    if (!u || tr[u].fake)
        return;
    tr[u].val += lazy;
    tr[u].path += lazy;
    tr[u].all = tr[u].path + tr[u].sub;
    tr[u].plazy += lazy;
}

void pushSub(int u, bool o, const Lazy &lazy) {
    if (!u)
        return;
    tr[u].sub += lazy;
    tr[u].slazy += lazy;
    if (!tr[u].fake && o)
        pushPath(u, lazy);
    else
        tr[u].all = tr[u].path + tr[u].sub;
}

void push(int u) {
    if (!u)
        return;
    if (tr[u].plazy.lazy()) {
        pushPath(tr[u].ch[0], tr[u].plazy);
        pushPath(tr[u].ch[1], tr[u].plazy);
        tr[u].plazy = Lazy();   // empty constructor for lazy resets it
    }
    if (tr[u].slazy.lazy()) {
        pushSub(tr[u].ch[0], false, tr[u].slazy);
        pushSub(tr[u].ch[1], false, tr[u].slazy);
        pushSub(tr[u].ch[2], true, tr[u].slazy);
        pushSub(tr[u].ch[3], true, tr[u].slazy);
        tr[u].slazy = Lazy();
    }
}
```

<details markdown="1" style="margin-bottom: 5%"><summary>Note</summary>

This code uses a style of lazy where the values are correctly updated at the same time that the node receives the tag. I usually implement lazy in a different style, where the values are correctly updated when the node pushes down its tag, but I found implementing lazy propagation the first way yields a much better constant factor.

---

</details>

## The Rem Method

I promised you we would come back to the `rem` method. Well, we're finally ready to tackle it!

```c++
int dir(int u, int o) {
    int v = tr[u].p;
    return tr[v].ch[o] == u ? o : tr[v].ch[o+1] == u ? o + 1 : -1;
}

void recPush(int u) {
    if (tr[u].fake)
        recPush(tr[u].p);
    push(u);
}

void rem(int u) {
    int v = tr[u].p;
    recPush(v);
    if (tr[v].fake) {
        int w = tr[v].p;
        attach(w, dir(v, 2), tr[v].ch[dir(u, 2) ^ 1]);
        if (tr[w].fake)
            splay(w, 2);    // fake splay
        free(v);
    } else {
        attach(v, dir(u, 2), 0);
    }
    tr[u].p = 0;
}
```

What's happening here? Let's break it down. $u$ is the node that we are disconnecting from its parent, $v$. First consider the easier of the two cases:

```c++
} else {
    attach(v, dir(u, 2), 0);
}
```

This clause triggers when $v$ is not a fake node. In this case, it's exactly the same as ordinary LCT. Remove $u$ as a virtual child of $v$.

We could handle the second case in the same way, but there's an extra consideration. If $v$ is fake, then after removing $u$, $v$ only has one virtual child and is therefore obsolete. We can destroy the node and attach $v$'s other child directly to $v$'s parent instead! This step is necessary to ensure our number of fake nodes remains under our bound of $n$.

There's one more consideration: correct application of pushes and pulls. Consider the following scenario:

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 3]({{site.baseurl}}/assets/images/ds5-3.png)

*We are calling rem on node 4 and attaching it as a preferred child of 1 instead. The red fake node has a pending lazy tag waiting to be pushed down. The left and right diagrams show before and after the procedure.*

</div>

See the issue? When we remove $u$ and reattach it above, it could "dodge" a lazy tag that it's supposed to receive. So to remedy this, we need to call push on all fake ancestors of $u$ up to the next non-fake node, which is what the method `recPush` does in my code. Finally, notice that after removing $u$, we would need to call `pull` again on all fake ancestors up to the next non-fake node to correctly set their values. We could create another method `recPull` with similar structure to `recPush`. Or, we could just call `splay` at the end to correctly set all the node values moving up. Calling `splay` would also fix any amortized complexity concerns from calling `recPush`.

And that's it! We've successfully implemented our new version of `access`!

## The Rest of the Methods

Once you have access, every other method is sincerely trivial. I'll include them here for sake of completeness.

Reroot, link, cut, lca, and path query/update are all completely identical to ordinary LCT.

<details markdown="1" style="margin-bottom: 5%"><summary>Recap of Those Methods</summary>

`reroot(u)`:
- Introduce a lazy flip tag that swaps the 0th and 1st child of a node
- Access $u$
- Apply lazy flip tag to $u$

`link(u, v)`:
- Reroot $u$
- Access $v$
- Call `add(v, u)`

`cut(u, v)`:
- Reroot $u$
- Access $v$
- Disconnect $v$'s 0th child
- Pull $v$

`lca(u, v)`:
- Modify the access method to return the last parent of $u$ in the while loop before it becomes the root
- Access $u$
- Return access $v$

`path(u, v)`:
- Reroot $u$
- Access $v$
- Return $v$'s `path`/update $v$'s `plazy`

---

</details>

Finally, for subtree queries/updates. Say we are querying for the subtree of $u$ given that $r$ is the root. First:

- Reroot $r$
- Access $u$

Now, $u$'s subtree in the represented tree consists of all the virtual subtrees hanging off of $u$, plus $u$ itself. So in other words, `tr[tr[u].ch[2]].all + tr[tr[u].ch[3]].all + tr[u].val`. For query, we just return that value. For update, we update `tr[u].val` directly, and then we update both the `plazy` and `slazy` values of its two virtual children. We do not touch the 0th or 1st child, since those are not part of $u$'s subtree.

## The Code

I've pasted snippets here and there, but if you're still fuzzy on some of the implementation details, you can see my complete code [here](https://github.com/mzhang2021/cp-library/blob/master/implementations/graphs/TopTree.h).

## Performance

<div markdown="1" style="text-align: center; margin-bottom: 5%">

<img src="{{site.baseurl}}/assets/images/ds5-4.png" alt="Image 4" width="25%"/>

</div>

Presumably, the complexity of this augmentation of LCT is still $\mathcal O(\log n)$, but with a significant constant factor ~~([this article](https://www.cnblogs.com/clrs97/p/4403244.html) estimates the constant factor to be $97$)~~. On DMOJ, it runs in around 0.6-0.7 seconds for $n, q \leq 10^5$. On Library Checker, it runs in a [little over 1 second](https://judge.yosupo.jp/submission/91913) for $n, q \leq 2 \cdot 10^5$. So its performance is closer to $\mathcal O(\log^2 n)$ in practice. Some of the implementations in the Chinese articles run faster than mine, but that seems to come at the cost of readability and some strange code, and my implementation is fast enough for almost all reasonable competitive programming purposes.

## Applications

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 5]({{site.baseurl}}/assets/images/ds5-5.png)

</div>

---
layout: post
title: "September 2022 Update"
tags: [personal, opinion]
---

Hey everyone, it's been too long since I last took the time to sit down and write a blog post. There's a multitude of reasons why I haven't written anything lately. Mainly, once college started, any time to devote to CP instantly evaporated thanks to classes, clubs, and socializing. But also, I just didn't have anything to write. Typically, when I write a blog post, I write with some topic or intent in mind, usually to educate or as an opinion piece. I've started a number of drafts for my next article but have tentatively scrapped all of them, simply because I realized midway that I either didn't have too much to say or I didn't feel qualified enough to write on the subject.

**So what is this post then?** To be honest, this is just a post for myself. To write down my thoughts on CP right now. My progress in my CP journey and where I want to go next. And also where I want to go with this blog.

# The Current State of CP

Should qualify this section with the obvious disclaimer that this is my opinion, of course. This is from the perspective of someone who first started actively doing competitive programming and Codeforces in mid 2019. I have seen the platform change significantly over the course of those 3 years.

Right now, it feels like CP is in a plateau. I'm not talking about problem quality; problem quality has always been fine. Rather, I'm referring to innovation in the sport.

To get my point across, let me compare to the state of CP during the inception of COVID. When COVID hit, people had no choice but to turn to indoor activities, such as competitive programming. Codeforces saw a huge influx of people, with the average number of registrants for Div. 2s skyrocketing from below 10k to 20k+. Quarantine also marked the start of many high rated users' YouTube channels and streaming careers. More community Discord servers and big events like [Lockouts](https://codeforces.com/blog/entry/81395) began cropping up. It might be nostalgia talking, but it really did feel like the CP community was more alive at the time.

CF rounds also felt more memorable back in the day, maybe a big part due to community engagement. I still vividly remember two CF rounds I took part in 2 years ago: [Global Round 9](https://codeforces.com/blog/entry/79620) and [Codeforces Round 659](https://codeforces.com/blog/entry/80422). When Global Round 9 dropped, that was an experience. Before the round even started, there was the peak moment of the ["as a tester" memes](https://codeforces.com/blog/entry/79620?#comment-654440), marking the last time that meme would be funny as it proceeded to get beaten into to the ground and buried six feet under in the following 2 years. As for the round itself, in my mind the round forever marked a shift in the tone of Codeforces problemsetting and paved the way towards the acceptance of troll problems and problems with a huge [solution search space]({{site.baseurl}}/difficulty/) but simple solution. My friends and I collectively lost our shit in our group chat and the comment section of the announcements blog was flooded by exasperated or amused competitors. CF Round 659 was somehow so unbalanced that it was hilarious. In Div. 1, just solving AB got you a red performance and C-F were all 2800+, while Div. 2 got a 2200 rated problem B. And D1A was also super submit bait as it was too easy to come up with [something wrong](https://codeforces.com/blog/entry/80422?#comment-667993) for that problem. Personally, I solved a whopping 0 problems as a wee Candidate Master that round. I think that was the first time I ever solved 0 problems in a round. They just don't make CF rounds the same anymore 😔.

In many ways, this golden time obviously could not last forever. Eventually, quarantine would end and people would go back to the real world. And don't get me wrong, that's for the better. Personally, despite the CP boon, I prefer and am much healthier physically and mentally post COVID. It's just that with most of my old CP friends no longer doing CP and me no longer recognizing most of the handles on the leaderboards these days, I can't help but look fondly upon CP in the past. Most YT channels don't upload anymore, streams aren't a thing anymore, and CP for me has lost a lot of the "magic" it used to have when I first started.

I'm also just growing old. I'm about to turn 20 in a few months. I find the majority of posts on CF nowadays cringe or unfunny. The average age on CF is probably around the high school range, 16-18. And I've also just probably been doing CP for too long and am starting to feel burn out.

# My Progress in CP

It has been one year since I first hit red, and I'm still around the same rating one year later 😅. But to be fair, I believe myself to be much stronger than a year ago. In my mind (even if the data and contest results don't always back me up), I am now stable GM and essentially on the cusp, if not already over, the IGM line. So I certainly won't be quitting until I reach IGM, because I do firmly believe I have the capability to do so.

At the same time, I no longer grind anywhere close to as much as in the past. During quarantine, I devoted an unhealthy amount of time to CP and would do every contest possible. My practice was also unfocused; I was just trying to solve as many problems as I can, majority of which were editorial assists, so I was still hardstuck purple for half a year despite the volume of problems I was solving. Fast forward to now, and all of my engagement with CP is now either doing the occasional CF contest if it fits in my schedule, running my university's CP club, or teaching classes on the side. I hardly ever practice now, mostly because it's hard to find time to, and also because a part of me feels I shouldn't. When all of my HS friends have moved on from CP and gone to enjoy other activites and hobbies, I can't help but feel a little foolish still clinging onto CP as a primary hobby of mine. To avoid getting rusty, I've switched my practice focus now to mainly thinking, something I can do passively while walking to class or doing other daily activities. I'm more likely now to read some problems but not bother implementing them, which I feel ok with doing because I'm reasonably confident in my implementation skills up to this point.

As a tangent, I am starting to find the "magic" in other things I do. For example, I recently started climbing (specifically bouldering), and it's so much fun because it integrates thinking and puzzle solving (figuring out the best approach to a course and which limbs to use where) with getting a good workout. Also, because I'm just starting out, I'm in the golden phase where I feel improvement every session, so I feel motivated to keep trying. The feeling of joy from trying out a tip from a YouTube video or from a friend during my next session can be compared to the feeling of unlocking a new set of problems you can now solve after learning a new algorithm in competitive programming. I think this will be great for my personal development but also for my relationship with CP. During quarantine, CP was the only thing on my mind. I would literally do every contest possible on every platform, whether it be Codeforces, AtCoder, CodeChef, or even HackerEarth and LeetCode. I did CP so much because it was my only hobby during quarantine, and without CP I would feel empty. But if I can shift my focus to other activities where I'm making more progress, then I can maintain a healthy relationship with CP where I don't do it all the time and therefore still find it fun whenever I occasionally pick up the keyboard. It's all about balance.

# What's Next for the Blog?

Frankly, I'm not sure. Now that I've had a blog for a while, I can see why people stop making YT videos or streaming after a while. When you first start out, you have all these great ideas, and you just make banger after banger to share on the internet. But after you've exhausted all your great ideas, you don't know where to go next, and keeping the blog updated starts to feel more like an obligation than a form of enjoyment.

That being said, I do have some ideas to post next. I just need to sit down some day and type them out. So hopefully you'll stick around for that! And of course, if you have any ideas on what you want to see, or any thoughts about anything I wrote, please leave a comment! I assure you, I read every comment on all of my blogs. Thanks for reading, and I'll see you next time (in hopefully less than a month).

---
layout: post
title: "Kruskal Reconstruction Tree"
tags: [tutorial, algo]
usemathjax: true
---

The Kruskal Reconstruction Tree (KRT) is something that is prevalent in folklore but scarcely documented, and most high rated users have probably either used this trick inadvertently or know of some other way of solving the problems that this DS solves. The most complete resource I know of is this [CF blog](https://codeforces.com/blog/entry/85714), and this blog is basically me adding a few more uses of this DS that I've seen since the other blog was published. There's also something similar called a [line tree](https://codeforces.com/blog/entry/71568?#comment-559304) which seems to solve the same class of problems, but has a slightly different construction (an array instead of a tree). The name "Kruskal Reconstruction Tree" is one that I've heard others refer to it as, and is mentioned [here](https://codeforces.com/blog/entry/88669).

## The Concept

Let's say we want to preprocess some tree in $\mathcal O(n \log n)$ and answer path max queries in $\mathcal O(1)$. Most people might answer queries in $\mathcal O(\log n)$ with binary lifting or even $\mathcal O(\log^2 n)$ with HLD. But how to do $\mathcal O(1)$? This [blog](https://codeforces.com/blog/entry/71568) contains a bunch of cool methods, one of which is the aforementioned line tree and one of which is [KRT](https://codeforces.com/blog/entry/71568?#comment-559341).

The KRT construction is as follows: start with a DSU containing $n$ isolated vertices and process the edges in increasing order of weight. When we insert a new edge $(u, v)$, we create a new vertex representing that edge and set it as the parent of $u$ and $v$'s components in the DSU. The initial sort of edges is $\mathcal O(n \log n)$ and we build the structure using a DSU with path compression (but **not** union by size/rank, as we care about parent-child relationships!), so the overall build time is $\mathcal O(n \log n)$. Our final KRT will have $2n - 1$ edges ($n$ vertices as leaves, and $n - 1$ vertices representing tree edges). Take a look at the diagrams below for a concrete example.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/kruskal-1.png) ![Image 2]({{site.baseurl}}/assets/images/kruskal-2.png)

*The original tree is on the left and its KRT is on the right. All newly created vertices representing edges are labelled with the pair of vertices they connect in the original tree.*

</div>

From the diagram, we can discern a key property of the KRT: the maximum edge between $u$ and $v$ is their LCA in the KRT! It's well-known how to compute LCA in $\mathcal O(1)$ using euler tour and RMQ, so we've successfully answered path max queries in $\mathcal O(1)$. The code is super simple:

<details markdown="1" style="margin-bottom: 5%"><summary>Code</summary>

```c++
// id is the next unused id to assign to any new node created
// par is the standard definition of dsu parent
// adj will store the KRT
int id, par[2 * MAXN];
vector<int> adj[2 * MAXN];

int find(int u) {
    return par[u] == u ? u : par[u] = find(par[u]);
}

void unite(int u, int v) {
    u = find(u), v = find(v);
    if (u == v)
        return;
    par[u] = par[v] = par[id] = id;
    adj[id] = {u, v};
    id++;
}
```

---

</details>

Let's now look at some real applications of this DS!

## [Codeforces Round 809, Div 2 E: Qpwoeirut and Vertices](https://codeforces.com/contest/1706/problem/E)

**Statement:** Given an undirected graph, answer queries of the form $(l, r)$ where we find the smallest $k$ such that edges $1, \dots, k$ are sufficient for all vertices $l, \dots, r$ to be connected.

If we build the KRT, then the answer is simply the LCA of all vertices $l, \dots, r$. Of course, finding the LCA of a group of nodes by iterating over each of them is too slow. However, it suffices to find the nodes with the smallest and largest DFS times and take the LCA of those two nodes to get the LCA of the entire group. So the full solution is to build an RMQ on indices to query for the smallest and largest DFS times (in the KRT, not the original tree) within some range $[l, r]$, and then query the KRT to find the LCA edge, giving us an $\mathcal O((n + m) \log n + q)$ solution.

## [Codeforces Round 767, Div 1 E: Groceries in Meteor Town](https://codeforces.com/contest/1628/problem/E)

**Statement:** Given a weighted tree with all nodes initially deactivated, answer queries of three types:
1. Activate all nodes in some range $[l, r]$.
2. Deactivate all nodes in some range $[l, r]$.
3. For some node $x$, find the maximum weight edge from $x$ to any activated node.

Build the KRT again. That third query is simply asking for the LCA of all activated nodes and $x$. We can use the same trick as the previous problem. It suffices to find the activated node with the smallest and largest DFS times, and take the LCA of those nodes and $x$, to get the LCA of $x$ and all activated nodes. So we simply need a data structure that can range activate, range deactivate, and query for the minimum and maximum DFS times among all activated indices, which sounds like a job for a segment tree. Bam, Div 1E has never been so clean!

## [Codechef TULIPS: Tiptoe Through the Tulips](https://www.codechef.com/problems/TULIPS)

**Statement:** Given a weighted tree with all nodes initially containing a tulip, answer queries of the form $(d, u, k)$ where on day $d$, we perturb all nodes reachable from $u$ via edges with weight $\leq k$ and collect tulips from all nodes with tulips, and we want to know how many tulips we collect on that day. After a node is perturbed, it grows a tulip if it remains unperturbed in the next $X$ days.

Here we discover the true potential of KRT: we can turn component queries into subtree queries. Once the KRT is built, the set of nodes reachable from $u$ via edges with weight $\leq k$ correspond to a subtree in the KRT. To find the root of that subtree, we need to find the highest ancestor of $u$ with edge weight $\leq k$, which can be done using binary lifting as the edge weights increase monotonically going up the KRT. And if some DFS times are assigned, then subtree queries become range queries!

Now that we're in the realm of range queries, we can explore our vast toolbox of range query structures. We need some data structure that can count indices in a range with time $\leq d - X$ and range set times to $d$. That's an annoying formulation, so let's consider a simpler one: count zeros in a range and range add. Whenever we process a query, we count zeros in that range, and then we add $1$ to that range and mark a future event to be processed at time $d + X$ to add $-1$ from that same range. This is much more tractable, as values are always non-negative so counting zeros is equivalent to [counting minimums](https://usaco.guide/adv/count-min?lang=cpp), so all necessary operations can be handled by segment tree.

## [IOI 2018: Werewolf](https://oj.uz/problem/view/IOI18_werewolf)

**Statement:** Given an undirected graph, process queries of the form $(S, E, L, R)$ where we want to know if it's possible to reach $E$ from $S$ by first going through vertices with indices $\geq L$, reaching some vertex with index in between $[L, R]$, and going through vertices with indices $\leq R$.

It would be sacrilegious to talk about KRT and not mention the problem that spurred the trick, IOI Werewolf. So let's look at this problem now. This time, our KRT is going to look a little different, since our edges are unweighted. For processing $\geq L$ ($\leq R$ is analogous), start with an empty DSU and process the vertices in decreasing order of index. When we process a vertex, find the components of all previously processed vertices and create a new vertex as the DSU parent of all of those components. Now for a query $S$, the set of vertices it can reach that are $\geq L$ correspond to some subtree in the KRT.

If we build two KRTs, one for $\geq L$ and the other for $\leq R$, then we can find the sets of vertices reachable from $S$ and $E$ that are $\geq L$ or $\leq R$ respectively, and then check if those sets intersect. Since these are subtree queries, we can again turn them into range queries with DFS times, and the problem reduces to the following: given two permutations $A$ and $B$, answer queries of checking if there exists a common element between $A[l_1, r_1]$ and $B[l_2, r_2]$.

This can be formulated as a sweepline problem. We can find something stronger, the count of common elements between $A[l_1, r_1]$ and $B[l_2, r_2]$. This equals the count of common elements between $A[l_1, r_1]$ and $B[1, r_2]$ minus the count of common elements between $A[l_1, r_1]$ and $B[1, l_2 - 1]$. So we sweep on $B$, and when we process an element, add $1$ to its corresponding position in $A$ in a binary indexed tree. Then, when we reach the right endpoint of a query in $B$, answer it by querying for the sum between $[l_1, r_1]$ in $A$ with the binary indexed tree. Voilà!

## Closing

The power of KRT is giving us a way to handle queries based on partitioning components by edge weights into tree queries, something we are more familiar with and have more tools for. Of course, often times these queries can also be processed offline and answered while merging in DSUs, but that method isn't always nice, and the KRT gives us a clean online method of answering them.

---
layout: post
title: "Transition Then State"
tags: [tutorial, algo]
featured: true
usemathjax: true
---

When we think about the classic model for dynamic programming, typically we think about first enumerating the state, and then enumerating the transitions from that state. But sometimes that perspective doesn't yield itself to a sufficiently fast solution. Let's look at some examples.

## Bellman-Ford

Let's begin with a classic example, the [Bellman-Ford algorithm](https://en.wikipedia.org/wiki/Bellman%E2%80%93Ford_algorithm). The code for that typically looks something like this:

```c++
for (int iter = 0; iter < n - 1; iter++)
    for (auto [u, v, w] : edges)
        dist[v] = min(dist[v], dist[u] + w);
```

One perspective of this algorithm is that it is dynamic programming. Suppose there are no negative cycles, so the shortest distance from the source node $s$ to every other vertex in the same component is well-defined. Then each shortest path is always simple (no repeated vertices).

<details markdown="1" style="margin-bottom: 5%"><summary>Why?</summary>

Suppose the shortest path from $s$ to some other vertex $v$ was not simple, meaning the shortest path contains repeated vertices. Suppose vertex $w$ appears twice on the path. Then the segment of the path from the first to second occurrence of $w$ is a cycle. Because there are no negative cycles, the cycle has non-negative weight, so we can remove the cycle from the path and get a path with less edges and at least as low of total weight. We repeat this until the path becomes simple.

---

</details>

A simple path can contain at most $n$ vertices and therefore at most $n - 1$ edges. Initially, the only distance correctly computed is $dist[s] = 0$. In each iteration, we iterate over our edge list and perform what is known as a "relaxation", where we update $dist[v]$ based on the shortest path to $u$, plus the edge from $u$ to $v$. The important thing is that after the first iteration, all shortest paths with one edge will be correctly computed. In general, after the $k$th iteration, all shortest paths with $k$ edges will be correctly computed, because prior to that iteration all shortest paths with $k-1$ edges were correctly computed, and every shortest path with $k$ edges consists of a $k-1$ length path plus an extra edge covered in the $k$th iteration. So after $n - 1$ iterations, all shortest paths with at most $n-1$ edges (all of them) will be correctly computed, so the algorithm is correct.

If we look at this through the lens of the classic dynamic programming model, what's really happening here is there is an implicit second dimension. The actual state is $dp[k][v]$ denoting the shortest distance from $s$ to $v$ with at most $k$ edges. And the transition would be

$$
dp[k][v] = \min(dp[k-1][v], \min_{uv \in E(G)} dp[k-1][u] + w(u, v))
$$

The first part of that transition happens implicitly by reusing the same table for each iteration, and the second part of that transition is covered by the relaxation in each iteration. Bellman-Ford is essentially a memory efficient implementation of this recurrence.

If we code the DP explicitly with a 2D table, we get $\mathcal O(n(n + m))$. Compared to $\mathcal O(nm)$ Bellman-Ford, this way is worse when $m \ll n$.

## DP to Maintain Convex Polygon

This is actually the problem that kickstarted this blog post. [Problem K](https://qoj.ac/contest/1096/problem/5443) from the first stage of the 2022-23 Universal Cup gives us a simple polygon with $n \leq 200$ vertices and asks us to count the number of subsets of vertices (of size at least 2) such that for every pair of vertices in the subset, the line segment between them lies inside (or on the boundary of) the polygon.

We can start by precomputing for every pair of vertices whether or not the line segment between them lie inside the polygon in $\mathcal O(n^3)$. This subproblem itself is actually an ICPC World Finals problem, namely [problem A](https://qoj.ac/contest/414/problem/2767) from the 2017-18 WF, so you can refer to the solution to that for more details.

We can also observe that the subset of vertices we choose should form a convex polygon. If there are three adjacent vertices $u, v, w$ that form a concave section of the subset polygon, then some part of the line segment from $u$ to $w$ would lie outside the original polygon.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

<img src="{{site.baseurl}}/assets/images/transition-1.png" alt="Image 1" width="50%"/>

</div>

On the other hand, if the subset polygon is convex, then it suffices to check that the edges of that polygon lie within the original polygon, and we don't have to check all pairs of vertices.

These observations alone bring us to an $\mathcal O(n^4)$ DP: $dp[s][u][v]$ denotes the number of subsets where the first vertex was $s$, the last 2 vertices taken were $u$ and $v$, and we have an $\mathcal O(n)$ transition for enumerating the next vertex to take. A transition to vertex $w$ is valid if $u \to v \to w$ is a counterclockwise turn. That's not fast enough. But let's consider what makes this DP redundant: storing both of the last 2 vertices in order to track whether or not the next turn is counterclockwise.

Instead, let's redefine our DP: let $dp[s][v]$ denote the number of subsets where the first vertex was $s$ and the last vertex was $v$. And instead of iterating first state, then transition, let's enumerate the transitions as the outer loop. Collect all vectors between all pairs of vertices (so an edge $uv$ is included twice, once as $u \to v$ and once as $v \to u$) and sort them based on polar angle. Now iterate over the vectors in that order, and when we encounter vector $u \to v$, iterate over all $s$ and perform a relaxation $dp[s][v] \mathrel{+}= dp[s][u]$. So the code would look something like this:

```c++
// vecs contains all vectors u -> v that lie within the original polygon
sort(vecs.begin(), vecs.end(), cmp);
for (auto [u, v] : vecs)
    for (int s = 0; s < n; s++)
        dp[s][v] += dp[s][u];
```

This is very clearly $\mathcal O(n^3)$. As for correctness, after processing the $i$th vector, all DP states will represent having built part of a polygon with the last edge at an angle no more than the angle of the $i$th vector. So the condition of enforcing counter-clockwise turns in our subset polygon is automatically satisfied by the order in which we process the vectors.

Now unfortunately, the code I wrote above wouldn't actually get AC on the problem I linked, because the original problem allows collinear points, so you have to do some extra work to avoid reusing the same edge twice, once in each direction. But that's an implementation detail and not the core idea behind the solution.

Again, if we were to look at this from the classic dynamic programming model perspective, there is an implicit extra dimension. Our actual state is $dp[i][s][v]$ denoting the number of subsets where we have processed up to the $i$th vector, the first vertex was $s$, and the last vertex was $v$. And simulating this DP directly would consume $\mathcal O(n^4)$ time. So by saving memory, we also saved complexity.

This particular DP for computing convex polygons has actually appeared in a number of other problems.
- [Problem H: Satanic Panic from Codeforces](https://codeforces.com/contest/1146/problem/H) (with explanation of the trick [here](https://codeforces.com/blog/entry/66639?#comment-506612))
- [Problem F: Integer Convex Hull from AtCoder](https://atcoder.jp/contests/abc202/tasks/abc202_f) in $\mathcal O(n^3)$ instead of $\mathcal O(n^4)$
- [Pegs from NENA 2013](https://open.kattis.com/problems/peggame) in $\mathcal O(n^6)$ instead of $\mathcal O(n^8)$

## Genius - A Codeforces Problem

Finally, [this problem](https://codeforces.com/contest/1497/problem/D) is probably the first time I truly saw this idea, and I remember reading the editorial at the time and thinking it looked very similar to Bellman-Ford.

IQ is always increasing, so one approach is $dp[i][j]$ denoting the maximum number of points if the last 2 problems you solved were $i$ and $j$. Your IQ would therefore be $\|c_i - c_j\|$. And you have $\mathcal O(n)$ transition by enumerating the next problem you solve with larger complexity gap and a different tag, giving an $\mathcal O(n^3)$ solution overall, which is too slow.

Similar to the geometry problem, we will change from storing the last 2 problems to just the last problem solved and instead enumerate the edges in a better order. Because $c_i = 2^i$, a gap $(i, j)$ is greater than gap $(k, l)$ for $i > j, k > l$ iff $i > k$ or $i = k, j > l$. So the correct order to enumerate these edges in to get increasing order gaps is increasing $i$ as the outer loop, then decreasing $j$ as the inner loop.

```c++
for (int i = 1; i < n; i++)
    for (int j = i - 1; j >= 0; j--)
        if (tag[i] != tag[j]) {
            long long di = dp[i], dj = dp[j];
            dp[i] = max(dp[i], dj + abs(s[i] - s[j]));
            dp[j] = max(dp[j], di + abs(s[i] - s[j]));
        }
```

The condition of enforcing increasing IQ is automatically taken care of by the order in which we enumerate these edges. Or alternatively, the dimension of current IQ has become an implicit dimension. So again, we cut memory to also cut complexity and get $\mathcal O(n^2)$.

## More Problems

- [Problem F: Brave CHAIN from AtCoder](https://atcoder.jp/contests/abc176/tasks/abc176_f)

---
layout: post
title: "Segment Tree Merging"
tags: [tutorial, algo, ds]
featured: true
usemathjax: true
---

Segment tree merging often serves as an alternative to small-to-large while also cutting an extra log factor, or can be used for "directed" merging (i.e. the merging is not symmetric w.r.t. both sets). The extent to which this technique can be applied is actually quite extensive, and a lot of it is not documented in English (the only English resource I've found is [this cf post](https://codeforces.com/blog/entry/49446)), so I hope to bridge the gap with this post.

## The Technique

Suppose you have several [dynamic segment trees](https://usaco.guide/plat/sparse-segtree?lang=cpp). For our example, our segment trees will support point update and range sum.

```c++
const int NODES = 1e7 + 5;

int id = 1, sum[NODES], pl[NODES], pr[NODES];

int query(int p, int l, int r, int i, int j) {
    if (i > r || j < l || !p)
        return 0;
    if (i <= l && r <= j)
        return sum[p];
    int m = (l + r) / 2;
    return query(pl[p], l, m, i, j) + query(pr[p], m + 1, r, i, j);
}

int update(int p, int l, int r, int i, int v) {
    if (!p)
        p = id++;
    if (l == r) {
        sum[p] = v;
        return p;
    }
    int m = (l + r) / 2;
    if (i <= m)
        pl[p] = update(pl[p], l, m, i, v);
    else
        pr[p] = update(pr[p], m + 1, r, i, v);
    sum[p] = sum[pl[p]] + sum[pr[p]];
    return p;
}
```

Some notes on this implementation:
- Instead of C++ pointers, I prefer to implement dynamic segment trees by allocating a giant pool of nodes and using `int` as "pointers" (really indices to nodes in the pool).
- In the function signatures, `p` is the pointer to the current segtree node, `l` and `r` are the node range, `i` and `j` are the query range, and `v` is the updated value at index `i` in the update method.
- In the global scope, `id` is the next unused node in our pool. `sum` is the sum of our segment tree node and `pl` and `pr` are pointers to the node's left and right child.
- One neat thing about this implementation (and the reason why I prefer it over a C++ pointer approach) is that "null" is represented by index 0 and has sum 0, so we do not need to worry about dereferencing null pointers.
- Both of these methods are clearly $\mathcal O(\log n)$ by the same logic as normal segment tree.

Ok, now suppose I want to support merging two segment trees into one. Here is what that method looks like:

```c++
int merge(int p1, int p2) {
    if (!p1)
        return p2;
    if (!p2)
        return p1;
    pl[p1] = merge(pl[p1], pl[p2]);
    pr[p1] = merge(pr[p1], pr[p2]);
    sum[p1] = sum[pl[p1]] + sum[pr[p1]];
    return p1;
}
```

Essentially, we traverse down both segtrees at the same time. If either segtree is empty, we return the other. If both exist, then we recursively merge both children and we return the node of the first segtree.

What is the complexity of this? While a single merge may do up to $\mathcal O(n)$ work, the total complexity is bounded by $\mathcal O(n \log n)$ assuming the total sizes of all our segtrees add up to $n$. This is because whenever we keep recursing down in the merge method, we destroy the node of the second segtree (`p2` is no longer needed). So the total amount of work done by the merge method is bounded by the number of segtree nodes we can destroy, and we can only create at most $\mathcal O(n \log n)$ segtree nodes.

## POI Tree Rotations

Links to [$n \leq 2 \cdot 10^5$](https://szkopul.edu.pl/problemset/problem/sUe3qzxBtasek-RAWmZaxY_p/site/?key=statement) and [$n \leq 10^6$](https://szkopul.edu.pl/problemset/problem/b0BM0al2crQBt6zovEtJfOc6/site/?key=statement)

In this problem, we are given a binary tree, we have the option of swapping the children of each internal node of the tree, and we want to minimize the number of inversions in the inorder traversal of this tree.

Firstly, we might observe that each internal node's decision is independent of other internal nodes. Specifically, for each internal node, if $y$ was the number of pairs of elements in the left and right subtrees where the one in the left is greater than the one in the right, then the number of inversions contributed is $\min(y, sz[left] \cdot sz[right] - y)$, as we can either swap or not swap the children. So it remains to calculate $y$ quickly for each internal node.

One approach is small-to-large merging. We start at the leaves and maintain some data structure that stores elements and can query for the number of elements less than $x$ in the data structure. This could be a [pbds ordered set](https://codeforces.com/blog/entry/11080) or some other binary search tree. We merge these data structures bottom up with small-to-large merging and count the number of inversions between two subtrees as we merge into their parent. The complexity will be $\mathcal O(n \log^2 n)$.

But that's not the best we could do for this problem. Consider choosing a dynamic segment tree as our data structure. And instead of merging small-to-large, we use the merge method outlined above. Then the complexity becomes $\mathcal O(n \log n)$.

But wait, how do we count the number of inversions as we merge? The small-to-large merging approach looked something like this:

```c++
const int MAXN = 2e5 + 5;

long long inv;
ordered_set st[MAXN];

int merge(int a, int b) {
    bool flip = false;
    if (st[a].size() < st[b].size()) {
        st[a].swap(st[b]);
        flip = true;
    }
    for (int x : st[b]) {
        int cnt = st[a].order_of_key(x);
        inv += flip ? cnt : st[a].size() - cnt;
    }
    for (int x : st[b])
        st[a].insert(x);
    return a;
}
```

The key thing is that we had to iterate over every element in the smaller of the two sets and query for the number of inversion pairs it forms with the other set, which already consumes $\mathcal O(n \log^2 n)$ time.

The fix is to embed the inversion counting logic into our segtree merge. The "indices" of our segtrees are the values of its elements, and the aggregate is sum. Suppose that `p1` and `p2` are the segtrees of our left and right subtrees respectively. When we are at a stage in the merge method where both `p1` and `p2` exist, the number of inversions increases by `sum[pl[p2]] * sum[pr[p1]]`. All other inversion pairs will be counted at a later stage in the recursion. You can refer to the code below for more clarity:

```c++
long long inv;

int merge(int p1, int p2) {
    if (!p1)
        return p2;
    if (!p2)
        return p1;
    inv += (long long) sum[pl[p2]] * sum[pr[p1]];
    pl[p1] = merge(pl[p1], pl[p2]);
    pr[p1] = merge(pr[p1], pr[p2]);
    sum[p1] = sum[pl[p1]] + sum[pr[p1]];
    return p1;
}
```

So with this, we can solve the problem in just $\mathcal O(n \log n)$!

*Note that even though the complexity is correct for the $n \leq 10^6$ version of this problem, the memory limit is too tight for $\mathcal O(n \log n)$ memory, so to pass that version you have to merge a less sparse data structure such as splay trees or treaps.*

Now let's check out some other problems solvable with this technique which are a little harder to solve with small-to-large.

## [PKUWC2018 Minimax](https://loj.ac/p/2537)

**Translation:** You are given a **binary tree** of $n$ nodes rooted at $1$. If a node is a leaf, it is assigned an input weight. Otherwise, it is assigned some probability $p_i$. It has a $p_i$ chance of getting set to the maximum of its children and $1 - p_i$ chance of getting set to the minimum of its children. **It is guaranteed that all leaf node weights are distinct.** Now, if the root could become $m$ different values, then let $V_i$ denote the $i$th smallest value and $D_i$ denote the probability of the root becoming it, compute

$$
\sum_{i=1}^m i \cdot V_i \cdot D_i^2 \mod 998244353
$$

**Input format:** First line is $n \leq 3 \cdot 10^5$. Second line contains $n$ integers, the $i$th integer is the parent of node $i$ or $0$ if $i = 1$. Third line contains $n$ integers, the $i$th integer equals its weight $w_i \leq 10^9$ if it is a leaf node or $p_i \cdot 10^4$ otherwise (guaranteed to be an integer).

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>

Let's maintain for each node $i$ the set of values that node $i$ could become. For transition, say you're iterating over all values in a child's list. Accumulate the probabilities of the values in the other list that are larger than it, then multiply $p_i$ with that as well as its current probability to get the probability it is chosen. Same for smaller than it. This is $\mathcal O(n^2)$ as we need to iterate over both children's lists, and finding the position in the other can be maintained with some two pointers.

One could hope to speed this up with some sort of small-to-large merging, but unfortunately there isn't an easy way to update the probabilities in the big list. Instead, consider using a merging segment tree. Each node maintains its set of values as a segment tree sorted by values. The leaf nodes store the probabilities of attaining each of those values. The segment tree also maintains a lazy multiply value.

The merge function is more nuanced. As we traverse down the merge, we maintain lazy values $v_1$ and $v_2$ denoting the contribution of segment tree $1$ and $2$ to the other segment tree respectively. When we reach a point in the merge function where one exists but not the other, we lazy multiply the segment tree leftover. When both exist, consider the effect they have on each other. If we descend left down the segment tree, then we add the sum of the right segment tree all multiplied by $1 - p_i$ as all of those values are larger, so you need their sum of probabilities times $1 - p_i$ to contribute to the probability of selecting the values on the left. Analogously, when we descend right, we add sum of left segment tree multiplied by $p_i$. The complexity is $\mathcal O(n \log n)$.

To get a better sense of what I'm talking about it, you can refer to [my submission](https://loj.ac/s/1387916).

---

</details>

## Range Sort

Given an array of $n \leq 10^5$ integers, process $q \leq 10^5$ queries where you sort some subarray $[l, r]$ either increasing or decreasing. Output the final sequence after all queries.

I'm not aware of any submission link to this problem at the moment ([http://www.lydsy.com:808/JudgeOnline/problem.php?id=4552](http://www.lydsy.com:808/JudgeOnline/problem.php?id=4552) used to exist but that online judge is offline now). You could also submit as an overkill solution to [https://atcoder.jp/contests/abc237/tasks/abc237_g](https://atcoder.jp/contests/abc237/tasks/abc237_g).

<details markdown="1" style="margin-bottom: 5%"><summary>Solution</summary>

The key is to represent the array as a set of contiguous intervals, each one sorted either increasing or decreasing. So initially the array can have up to $\mathcal O(n)$ segments. With each query, we cut up to two segments on the ends, erase all segments fully contained within our query segment, and insert the query segment into the set. So the number of new segments in our set is at most $3$.

As for representing the segments, we can represent them as dynamic segment trees. And we just need to be able to merge and split these segments trees. Merge is the same as above, split is not difficult either and only creates at most $\mathcal O(\log n)$ new segtree nodes. The code below shows how to split a segtree into two segtrees, one containing the $k$ smallest elements and the other containing the remaining.

```c++
pair<int, int> split(int p, int k) {
    if (!p)
        return {0, 0};
    int q = id++;
    if (sum[pl[p]] >= k) {
        tie(pl[q], pl[p]) = split(pl[p], k);
        sum[p] -= k;
        sum[q] = k;
        return {q, p};
    } else {
        tie(pr[p], pr[q]) = split(pr[p], k - sum[pl[p]]);
        sum[q] = sum[p] - k;
        sum[p] = k;
        return {p, q};
    }
}
```

---

</details>

## More problems

- [https://codeforces.com/contest/1515/problem/H](https://codeforces.com/contest/1515/problem/H)
- [https://loj.ac/p/2553](https://loj.ac/p/2553)
- [https://loj.ac/p/2722](https://loj.ac/p/2722)
- [https://loj.ac/p/3340](https://loj.ac/p/3340)
- [https://www.spoj.com/problems/COT3/](https://www.spoj.com/problems/COT3/)

---
layout: post
title: "Simulating Cost Flow"
tags: [tutorial, algo]
featured: true
usemathjax: true
---

This post assumes knowledge of [min/max cost flow](https://en.wikipedia.org/wiki/Minimum-cost_flow_problem) and a rough understanding of the high-level methods for solving (successive shortest path and cycle canceling). Knowledge of specific algorithms and implementation details are not expected. Also solutions to some problems will use [Aliens trick](https://web.archive.org/web/20210511092429/http://www.serbanology.com/).

---

The cost flow problem is a well-known problem in literature that a whole slew of competitive programming problems can be formulated as. Unfortunately, competitive programming problems typically have constraints set too high for directly running even the most state of the art cost flow algorithm. However, in certain problems the flow network will have a very specific setup that allows us to figure out greedily the optimal flow (i.e. proving greedy with flow) or "simulate" the flow algorithm faster with data structures. That is what this post will be about.

I will say that I do not claim novelty over these ideas. This idea is (unsurprisingly) well-known in China. This particular post and some of the example problems are taken from [this Chinese blog](https://www.luogu.com.cn/blog/command-block/mu-ni-fei-yong-liu-xiao-ji).

I will also describe everything below in terms of min cost flow, max cost is of course analogous. When describing flow network constructions, I will shorthand with $(cost, capacity)$ for describing edges.

## High-level Methods for Cost Flow

Most cost flow algorithms fall into one of these categories of high-level approaches. The two in particular that we will look at are:

1. **Successive Shortest Path**: Repeatedly find a min cost augmenting path (a path with minimum total cost where all edges can still contain more flow) from source to sink and send flow through that path.
2. **Cycle Cancelling**: Repeatedly find a negative cost cycle (a cycle with negative total cost where all edges can still contain more flow) and send flow through that cycle.

Note that in the cycle cancelling paradigm, there is no concept of source and sink, but you can solve for source and sink by adding an edge from sink to source with cost of $-\infty$ and capacity of $k$ for the min cost $k$-flow (or $\infty$ capacity if you want min cost max flow).

## Min Cost Flow is Convex

Min cost flow as a function of amount of flow is convex. This makes intuitive sense, as you can always rearrange the augmenting paths you choose to pick less steep ones (lower cost) before more steep ones (higher cost). This fact is useful as it allows us to use Aliens trick.

## [Codeforces 802O: April Fools' Problem](https://codeforces.com/contest/802/problem/O)

**Statement Summary:** You are preparing and printing $k$ problems for a contest over the course of $n$ days. On the $i$th day, you can prepare a problem for cost $a_i$ and print a problem for cost $b_i$. You must prepare a problem before printing it. In other words, if you prepare a problem on day $i$, you can only print it on some day $j \geq i$. You can prepare and print at most one problem per day. What is the minimum cost of preparing and printing $k$ problems?

**Constraints:** $1 \leq k \leq n \leq 5 \cdot 10^5, 1 \leq a_i, b_i \leq 10^9$

---

Let's first think about solving this with flows. One straightforward way is to create $2n + 2$ nodes: a source $s$ and sink $t$, plus nodes representing $a_i$ and $b_i$ for each $i$. We add the following edges:

- $s$ to each $a_i$ node with $(a_i, 1)$
- each $b_i$ node to $t$ with $(b_i, 1)$
- each $a_i$ node to $b_j$ node where $i \leq j$ with $(0, 1)$

The answer is the min cost $k$-flow of this network.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/simulating-cost-flow-1.png)

*Example for $n = 3$*

</div>

Now let's try to optimize using one of two methods. Both are possible and lead to different solutions for this problem.

### Successive Shortest Path

This is the method described in the official editorial.

First, let's simplify this network to not use $\Omega(n^2)$ edges. Consider a slightly different setup with $n + 2$ nodes: a source $s$ and sink $t$, plus nodes $1$ through $n$. We add the following edges:

- $s$ to each $i$ with $(a_i, 1)$
- each $i$ to $t$ with $(b_i, 1)$
- $i$ to $i + 1$ with $(0, \infty)$

I claim the answer is also the min cost $k$-flow of this network. Essentially, a matching consists of picking some $a_i$, waiting zero or more days (represent by taking some number of $i \to i + 1$ edges), and then pick some $b_j$ where $i \leq j$.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 1]({{site.baseurl}}/assets/images/simulating-cost-flow-2.png)

*Example for $n = 4$*

</div>

An augmenting path consists of sending flow from $s$ to some $i$, then from $i$ to $j$, and finally from $j$ to $t$.

- $s \to i$ and $j \to t$ must of course have no flow going through them.
- If $i \leq j$, this is ok. We add $+1$ flow to each edge on the path from $i$ to $j$.
- If $i > j$, then we are sending flow along a reverse edge and cancelling out existing flow, so every edge on the path from $i$ to $j$ must have at least $1$ flow, and the augmenting path adds $-1$ flow to each path edge.

And in the successive shortest path algorithm, we simply need to choose the augmenting path with minimum cost each time. So we need to choose a pair of indices $i, j$ with minimum $a_i + b_j$, and either $i \leq j$ or there exists flow on every edge on the path between $i$ and $j$.

So we need to be able to add $+1$ or $-1$ to a range and recompute the minimum cost valid $(i, j)$ pair. This sounds like a job for lazy segment tree! We just repeat $k$ times of querying the segtree for $(i, j)$ and either add $+1$ or $-1$ to all edges in between $i$ and $j$, giving us an $\mathcal O(k \log n)$ solution.

The details for determining the minimum cost valid $(i, j)$ pair are quite tedious, but the TLDR is that you have to store first and second minimums of the amount of flow through any edge in a subarray. I have an [old submission](https://www.luogu.com.cn/record/79319016) that implements this idea with a ton of variables in the segtree node. It can probably be reduced with more thought.

### Cycle Cancelling

To drastically simplify our analysis, we will apply Aliens trick. Binary search on some reward $c$, so now there is no requirement of at least $k$ problems anymore, but instead you get a reward of $c$ for completing one problem. So preparing a problem on day $i$ and printing on day $j$ costs $a_i + b_j - c$. So there is an edge from $t$ to $s$ with $(-c, \infty)$. We binary search for the minimum $c$ that we complete at least $k$ problems. This is valid because we know min cost flow is convex with respect to the amount of flow.

Now consider incrementally adding in nodes in decreasing order (from $n$ to $1$) and consider how the optimal flow changes after each additional node. When we add node $i$, we add edges $s \to a_i$, $b_i \to t$, and $a_i \to b_j$ for all $i \leq j$. Upon adding node $i$, we consider the possibilities for how a new negative cycle could form.

1. $s \to a_i \to b_j \to t \to s$ with cost $a_i + b_j - c$. This is essentially matching $a_i$ with some unmatched $b_j$.
2. $s \to a_i \to b_j \to a_k \to s$ with cost $a_i - a_k$. This is cancelling out some existing flow passing through $s \to a_k \to b_j$ and is equivalent to changing the matching to match $b_j$ with $a_i$ instead of $a_k$.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 3]({{site.baseurl}}/assets/images/simulating-cost-flow-3.png) ![Image 4]({{site.baseurl}}/assets/images/simulating-cost-flow-4.png)

*Examples of negative cycles of the first and second types*

</div>

Those are actually the only two cases we need to consider. For example, you might think we need to consider some cycle of the form $s \to a_i \to b_j \to t \to b_k \to a_l \to s$. This cycle would represent taking some matched pair $(a_i, b_j)$ instead of $(a_l, b_k)$. However, if $(a_l, b_k)$ was originally matched in the first place, that would imply $a_l + b_k \leq c$, as otherwise $t \to b_j \to a_i \to s \to t$ would be a negative cycle with weight $c - a_l - b_k$ and we would have had to handle that before adding node $i$. And if $s \to a_i \to b_j \to t \to b_k \to a_l \to s$ was a negative cycle, that would imply $a_i + b_j < a_l + b_k$ and therefore $a_i + b_j < c$, so we could instead take the negative cycle $s \to a_i \to b_j \to t \to s$. In fact, we would ultimately converge to that state regardless, because if we took $s \to a_i \to b_j \to t \to b_k \to a_l \to s$ first, then $s \to a_l \to b_k \to t \to s$ would form a negative cycle afterwards, so the end result after resolving all negative cycles is identical.

You can also confirm that after taking a negative cycle of one of the two types listed above (whichever one is more negative), we will not create any more additional negative cycles. So essentially, each time we add a node $i$ to the network, we take at most one additional negative cycle.

<details markdown="1" style="margin-bottom: 5%"><summary>Example: Can taking a type 2 cycle lead to creating a type 1 cycle?</summary>

If we break up some old pair $(a_k, b_j)$ and instead match $(a_i, b_j)$, is it then possible for $a_k$ to be paired with some other $b_l$?

If we are breaking up the pair, that implies $a_i < a_k$. Now suppose we pair $a_k$ with $b_l$, implying $a_k + b_l < c$. Substituting $a_i < a_k$ yields $a_i + b_l < c$. Furthermore, if we compare $a_i - a_k$ and $a_i + b_l - c$, because $-a_k > b_l - c$, $a_i + b_l - c < a_i - a_k$, so it is more optimal to match $a_i$ with $b_l$ in the first place.

---

</details>

Therefore, we can simply maintain a priority queue of potential negative cycles and iterate $i$ from $n$ to $1$. When we match $(a_i, b_j)$, we insert $-a_i$ into the priority queue to allow for breaking up that matching and pairing $b_j$ with something else in the future. When we leave $b_i$ unmatched, we insert $b_i - c$ into the priority queue to allow for matching with that $b_i$ in the future. And at each step, if $a_i$ plus the minimum value in the priority queue is less than $0$, then we take that option. The overall complexity is $\mathcal O(n \log n \log A)$ (but with much better constant than the successive shortest path solution).

<details markdown="1" style="margin-bottom: 5%"><summary>Code for Clarity</summary>

```c++
auto solve = [&] (int c) -> pair<int, long long> {
    int cnt = 0;
    long long cost = 0;
    priority_queue<pair<int, int>, vector<pair<int, int>>, greater<pair<int, int>>> pq;
    for (int i=n-1; i>=0; i--) {
        pq.emplace(b[i] - c, 0);
        if (a[i] + pq.top().first <= 0) {
            cnt += pq.top().second == 0;
            cost += a[i] + pq.top().first;
            pq.pop();
            pq.emplace(-a[i], 1);
        }
    }
    return {cnt, cost + (long long) k * c};
};

int l = 0, r = 2e9 + 5;
while (l < r) {
    int m = midpoint(l, r);
    if (solve(m).first >= k)
        r = m;
    else
        l = m + 1;
}

cout << solve(l).second << "\n";
```

The priority queue stores negative cycles as a pair of cost and what type it is (swapping with existing matching or creating a new matching). Note that `solve` tries to maximize the number of matchings created for tiebreakers, which is necessary to avoid reconstruction issues with Aliens trick (touched on in more detail in the Aliens trick resource I linked at the top).

---

</details>

Notice that the final solution looks awfully similar to a greedy algorithm. And indeed, we can apply a greedy interpretation to this: each $a_i$ can be matched with some $b_j$ where $i \leq j$ for cost $a_i + b_j - c$, and we want cost as negative as possible. Then the two types of negative cycles can be interpreted as "options" when introducing a new index $i$: either match $a_i$ with some unmatched $b_j$ or swap it with some existing $(a_k, b_l)$ matching. In essence, the flow interpretation serves as a **proof** for the correctness of the greedy.

<details markdown="1" style="margin-bottom: 5%"><summary>Do we need Aliens trick?</summary>

Maybe not, but the solution certainly gets significantly messier without it. If we don't apply Aliens trick and instead analyze the network directly, we have an edge from $t$ to $s$ with $(-\infty, k)$.

For types of negative cycles introduced upon adding a new index $i$, the two types described above still apply, but now $s \to a_i \to b_j \to t \to b_k \to a_l \to s$ is also possible, because the $t \to s$ edge no longer has infinite capacity, so it may not be possible to choose $s \to a_i \to b_j \to t \to s$ instead.

Furthermore, not only is $s \to a_i \to b_j \to t \to b_k \to a_l \to s$ possible, but there are even more complicated types of cycles that could spawn now. Consider the following case:

```
5 3
9 1 10 10 4
6 7 8 2 9
```

After processing $i = 5, 4, 3$, our matchings will be $(10, 8), (10, 2), (4, 9)$ for total cost of $43$.

When we introduce $i = 2$, we can no longer take any more matchings as $k = 3$, so we will have to consider swapping with existing matchings instead. The most negative cycle available is swapping out $(1, 7)$ for $(10, 8)$ for cost $1 + 7 - 10 - 8 = -10$. This is a negative cycle of the third type that didn't exist with Aliens trick, namely $s \to a_2 \to b_2 \to t \to b_3 \to a_3 \to s$. And so now we have $(1, 7), (10, 2), (4, 9)$ with total cost $33$.

Now introduce $i = 1$. The most negative cycle is $s \to a_1 \to b_4 \to a_4 \to s$ with cost $9 - 10 = -1$. So after swapping, we have matchings $(9, 2), (1, 7), (4, 9)$. However, we are not done. There's still a long negative cycle that exists in this situation: $t \to b_2 \to a_2 \to b_4 \to a_1 \to b_1 \to t$ with cost $-7 + 6 = -1$. What this does is this matches the $9$ with the $6$ instead and then matches the $1$ with the $2$ instead of the $7$. So this converges to $(9, 6), (1, 2), (4, 9)$ with the minimum cost of $31$.

If we were to do this in one negative cycle after adding $i = 1$, it would be $s \to a_1 \to b_1 \to t \to b_2 \to a_2 \to b_4 \to a_4 \to s$ with cost $9 + 6 - 7 - 10 = -2$. That's absurd!

In short, when there is a capacity $k$ restriction, it is no longer the case that the optimal solution containing $i$ from $2$ to $n$ changes just a little bit when adding $i = 1$. It could completely morph.

---

</details>

So with this first example, we can see two different solutions that arise from thinking about flows. The first one is probably impossible to think of without flows. For the second one, it might be difficult to prove Aliens trick can be applied here without formulating it as flow. Once Aliens trick is applied, the greedy is relatively easier to come up without thinking about flows, but having the flow network still gives us confidence in its correctness.

## [Codeforces 866D: Buy Low Sell High](https://codeforces.com/contest/866/problem/D)

**Statement Summary:** You have knowledge of the price of a stock over the next $n$ days. On the $i$th day it will have price $p_i$. On each day you can either buy one share of the stock, sell one existing share you currently have, or do nothing. You cannot sell shares you do not have. What is the maximum possible profit you can earn?

**Constraints:** $2 \leq n \leq 3 \cdot 10^5, 1 \leq p_i \leq 10^6$

---

This problem has several interpretations that lead to the same code. You can think of it as slope trick. Or as greedy. Let's think about it as flows.

Firstly, we can loosen the restrictions by allowing for both buying and selling a stock on a given day. This won't change the answer since both buying and selling on the same day is equivalent to doing nothing.

Now we design the following flow network with $n + 2$ nodes: a source $s$ and sink $t$ and $n$ nodes representing days $1$ through $n$. We add the following edges:

- $s$ to $i$ for all $1 \leq i \leq n$ with $(-p_i, 1)$
- $i$ to $t$ for all $1 \leq i \leq n$ with $(p_i, 1)$
- $i$ to $i + 1$ for all $1 \leq i < n$ with $(0, \infty)$

The answer is thus the max cost flow of this network.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 5]({{site.baseurl}}/assets/images/simulating-cost-flow-5.png)

*Example for $n = 3$*

</div>

Now consider the cycle cancelling method. Add an edge from $t$ to $s$ with $(0, \infty)$. We are using a cost of $0$ instead of $\infty$ because we do not require the max amount of flow, and forcing the max flow is not always optimal.

This network is extremely similar to the previous problem. We add the nodes from $n$ to $1$. When we add node $i$, the possible positive (since we are doing max cost flow) cycles are:

- $s \to i \to \dots \to j \to t \to s$ for $i \leq j$ with cost $p_j - p_i$
- $s \to i \to \dots \to j \to s$ for $i \leq j$ also with cost $p_j - p_i$

So the code ends up being quite simple and may resemble some other greedy code you've seen.

<details markdown="1" style="margin-bottom: 5%"><summary>Code</summary>

```c++
long long ret = 0;
priority_queue<int> pq;
for (int i=n-1; i>=0; i--) {
    pq.push(p[i]);
    if (pq.top() > p[i]) {
        ret += pq.top() - p[i];
        pq.pop();
        pq.push(p[i]);
    }
}
```

---

</details>

## [CSES Programmers and Artists](https://cses.fi/problemset/task/2426)

**Statement Summary:** You have $n$ people, the $i$th person has $x_i$ amount of programming skill and $y_i$ amount of art skill. Each person can be assigned to be a programmer, artist, or neither. You need exactly $a$ programmers and $b$ artists. What is the maximum sum of skills attainable by some matching?

**Constraints:** $1 \leq n \leq 2 \cdot 10^5, a + b \leq n, 1 \leq x_i, y_i \leq 10^9$

---

You get the drill now. Let's model this problem as a flow network. Here is one possible construction using $n + 4$ nodes.

- $s$ to $i$ with $(0, 1)$
- $i$ to $prog$ with $(x_i, 1)$
- $i$ to $art$ with $(y_i, 1)$
- $prog$ to $t$ with $(0, a)$
- $art$ to $t$ with $(0, b)$

The answer is the max cost max flow of this network. Notice that in this case the maximum flow is also optimal as all augmenting paths will have positive weight.

<div markdown="1" style="text-align: center; margin-bottom: 5%">

![Image 6]({{site.baseurl}}/assets/images/simulating-cost-flow-6.png)

*Example for $n = 3$*

</div>

Let's try approaching this with successive shortest path! Essentially, we repeatedly find an augmenting path from $s$ to $t$ with maximum cost and send $1$ unit of flow through. After some pondering, we conclude we only have $4$ types of paths to consider.

- $s \to i \to prog \to t$ with cost $x_i$
- $s \to i \to art \to t$ with cost $y_i$
- $s \to i \to prog \to j \to art \to t$ with cost $x_i - x_j + y_j$
- $s \to i \to art \to j \to prog \to t$ with cost $y_i - y_j + x_j$

The first and fourth types of paths decrease the capacity of $prog \to t$ by $1$, and the second and third types of paths decrease the capacity of $art \to t$ by $1$.

Do we need to consider any more complicated paths? Such as $s \to i \to prog \to j \to art \to k \to prog \to t$? Turns out we don't. Suppose path $s \to i \to prog \to j \to art \to k \to prog \to t$ has the maximum cost. The cost is $x_i - x_j + y_j - y_k + x_k$. Now consider a different augmenting path that would also be valid, $s \to i \to prog \to t$ with cost $x_i$. The fact that the other path was chosen implies $x_i - x_j + y_j - y_k + x_k > x_i$, or $y_j - x_j + x_k - y_k > 0$. This implies that there exists a positive cycle $j \to art \to k \to prog \to j$ before introducing our new augmenting path, contradicting the property of successive shortest paths which ensures optimal cost flow with each new augmenting path added.

Therefore, we just need to maintain some heaps to repeatedly find the maximum cost path out of those $4$ options:

- pair unmatched $i$ to programmer
- pair unmatched $i$ to artist
- pair unmatched $i$ to programmer, and switch some existing programmer $j$ to artist
- pair unmatched $i$ to artist, and switch some existing artist $j$ to programmer

My code is a bit verbose but attempts to implement this idea as straightforwardly as possible:

<details markdown="1" style="margin-bottom: 5%"><summary>Code</summary>

```c++
#include <bits/stdc++.h>
using namespace std;

#ifdef LOCAL
#define DEBUG(...) debug(#__VA_ARGS__, __VA_ARGS__)
#else
#define DEBUG(...) 6
#endif

template<typename T, typename S> ostream& operator << (ostream &os, const pair<T, S> &p) {return os << "(" << p.first << ", " << p.second << ")";}
template<typename C, typename T = decay<decltype(*begin(declval<C>()))>, typename enable_if<!is_same<C, string>::value>::type* = nullptr>
ostream& operator << (ostream &os, const C &c) {bool f = true; os << "["; for (const auto &x : c) {if (!f) os << ", "; f = false; os << x;} return os << "]";}
template<typename T> void debug(string s, T x) {cerr << "\033[1;35m" << s << "\033[0;32m = \033[33m" << x << "\033[0m\n";}
template<typename T, typename... Args> void debug(string s, T x, Args... args) {for (int i=0, b=0; i<(int)s.size(); i++) if (s[i] == '(' || s[i] == '{') b++; else
if (s[i] == ')' || s[i] == '}') b--; else if (s[i] == ',' && b == 0) {cerr << "\033[1;35m" << s.substr(0, i) << "\033[0;32m = \033[33m" << x << "\033[31m | "; debug(s.substr(s.find_first_not_of(' ', i + 1)), args...); break;}}

template<typename T>
struct FastSet {
    priority_queue<T> pq, pending;

    void add(T x) {
        pq.push(x);
    }

    void rem(T x) {
        pending.push(x);
        while (!pq.empty() && !pending.empty() && pq.top() == pending.top()) {
            pq.pop();
            pending.pop();
        }
    }

    T max() {
        assert(!pq.empty());
        return pq.top();
    }
};

int main() {
    ios_base::sync_with_stdio(false);
    cin.tie(NULL);

    int a, b, n;
    cin >> a >> b >> n;
    vector<int> x(n), y(n);
    for (int i=0; i<n; i++)
        cin >> x[i] >> y[i];

    FastSet<pair<int, int>> pqA, pqB;
    priority_queue<pair<int, int>> pqA2, pqB2;
    for (int i=0; i<n; i++) {
        pqA.add({x[i], i});
        pqB.add({y[i], i});
    }

    long long ret = 0;
    while (a > 0 || b > 0) {
        int mx = 0;
        if (a > 0) {
            if (!pqA.pq.empty())
                mx = max(mx, pqA.max().first);
            if (!pqB.pq.empty() && !pqB2.empty())
                mx = max(mx, pqB.max().first + pqB2.top().first);
        }
        if (b > 0) {
            if (!pqB.pq.empty())
                mx = max(mx, pqB.max().first);
            if (!pqA.pq.empty() && !pqA2.empty())
                mx = max(mx, pqA.max().first + pqA2.top().first);
        }
        assert(mx > 0);

        ret += mx;
        if (a > 0) {
            if (!pqA.pq.empty() && mx == pqA.max().first) {
                int i = pqA.max().second;
                pqA.rem({x[i], i});
                pqB.rem({y[i], i});
                pqA2.emplace(y[i] - x[i], i);
                a--;
                continue;
            }
            if (!pqB.pq.empty() && !pqB2.empty() && mx == pqB.max().first + pqB2.top().first) {
                int i = pqB.max().second, j = pqB2.top().second;
                pqA.rem({x[i], i});
                pqB.rem({y[i], i});
                pqB2.pop();
                pqB2.emplace(x[i] - y[i], i);
                pqA2.emplace(y[j] - x[j], j);
                a--;
                continue;
            }
        }
        if (b > 0) {
            if (!pqB.pq.empty() && mx == pqB.max().first) {
                int i = pqB.max().second;
                pqA.rem({x[i], i});
                pqB.rem({y[i], i});
                pqB2.emplace(x[i] - y[i], i);
                b--;
                continue;
            }
            if (!pqA.pq.empty() && !pqA2.empty() && mx == pqA.max().first + pqA2.top().first) {
                int i = pqA.max().second, j = pqA2.top().second;
                pqA.rem({x[i], i});
                pqB.rem({y[i], i});
                pqA2.pop();
                pqA2.emplace(y[i] - x[i], i);
                pqB2.emplace(x[j] - y[j], j);
                b--;
                continue;
            }
        }
        assert(false);
    }

    cout << ret << "\n";

    return 0;
}
```

---

</details>

## More Problems

- [Codeforces 280D: k-Maximum Subsequence Sum](https://codeforces.com/contest/280/problem/D)
- [NOI 2019: Sequence](https://loj.ac/p/3158)
- [CCPC Finals 2021 Problem I: Reverse LIS](https://codeforces.com/gym/103860/problem/I)
- Any other example problem from [this blog](https://www.luogu.com.cn/blog/command-block/mu-ni-fei-yong-liu-xiao-ji)
"

----------

	Oleksandr Kulkov

C++ STL: Policy based data structures

By adamant, 10 years ago, translation, In English
Hi everyone! After a relatively long lull, I decided that my contribution growing too slowly the hour has come to please you with another article in the blog :)

2 months ago user Perlik wrote an article, in which he described a very interesting STL implemented data structure that allows you to quickly perform various operations with substrings. Some time after I tested it on various tasks and, unfortunately, tend to get a negative result — rope was too slow, especially when it came to working with individual elements.

For some time, I forgot about that article. Increasingly, however, I was faced with problems in which it was necessary to implement set with the ability to know ordinal number of item and also to get item by its ordinal number (ie, order statistic in the set). And then I remembered that in the comments to that article, someone mentioned about the mysterious data structure order statistics tree, which supports these two operations and which is implemented in STL (unfortunately only for the GNU C++). And here begins my fascinating acquaintance with policy based data structures, and I want to tell you about them :)


Let's get started. In this article I will talk about IMO the most interesting of the implemented structures — tree. We need to include the following headers:

#include <ext/pb_ds/assoc_container.hpp> // Common file
#include <ext/pb_ds/tree_policy.hpp> // Including tree_order_statistics_node_update
After closer inspection you may find that the last two files contained in the library

#include <ext/pb_ds/detail/standard_policies.hpp>
Namespace, which we will have to work in newer versions of C++ is called __gnu_pbds;, earlier it was called pb_ds;

Now let's look at the concrete structure.

The tree-based container has the following declaration:

	  template<
	  typename Key, // Key type
	  typename Mapped, // Mapped-policy
	  typename Cmp_Fn = std::less<Key>, // Key comparison functor
	  typename Tag = rb_tree_tag, // Specifies which underlying data structure to use
	  template<
	  typename Const_Node_Iterator,
	  typename Node_Iterator,
	  typename Cmp_Fn_,
	  typename Allocator_>
	  class Node_Update = null_node_update, // A policy for updating node invariants
	  typename Allocator = std::allocator<char> > // An allocator type
	  class tree;
	
Experienced participants may have already noticed that if initialize the template only the first two types, we obtain almost exact copy of the container map. Just say, that this container can be set, for this you just need to specify the second argument template type as null_type ( in older versions it is null_mapped_type).

By the way Tag and Node_Update are missing in map. Let us examine them in more detail.

Tag — class denoting a tree structure, which we will use. There are three base-classes provided in STL for this, it is rb_tree_tag (red-black tree), splay_tree_tag (splay tree) and ov_tree_tag (ordered-vector tree). Sadly, at competitions we can use only red-black trees for this because splay tree and OV-tree using linear-timed split operation that prevents us to use them.

Node_Update — class denoting policy for updating node invariants. By default it is set to null_node_update, ie, additional information not stored in the vertices. In addition, C++ implemented an update policy tree_order_statistics_node_update, which, in fact, carries the necessary operations. Consider them. Most likely, the best way to set the tree is as follows:

typedef tree<
int,
null_type,
less<int>,
rb_tree_tag,
tree_order_statistics_node_update>
ordered_set;
If we want to get map but not the set, as the second argument type must be used mapped type. Apparently, the tree supports the same operations as the set (at least I haven't any problems with them before), but also there are two new features — it is find_by_order() and order_of_key(). The first returns an iterator to the k-th largest element (counting from zero), the second — the number of items in a set that are strictly smaller than our item. Example of use:


    ordered_set X;
    X.insert(1);
    X.insert(2);
    X.insert(4);
    X.insert(8);
    X.insert(16);

    cout<<*X.find_by_order(1)<<endl; // 2
    cout<<*X.find_by_order(2)<<endl; // 4
    cout<<*X.find_by_order(4)<<endl; // 16
    cout<<(end(X)==X.find_by_order(6))<<endl; // true

    cout<<X.order_of_key(-5)<<endl;  // 0
    cout<<X.order_of_key(1)<<endl;   // 0
    cout<<X.order_of_key(3)<<endl;   // 2
    cout<<X.order_of_key(4)<<endl;   // 2
    cout<<X.order_of_key(400)<<endl; // 5
Finally I would like to say about the performance of order_statistics_tree in STL. For this, I provide the following table.

Solution\Problem	1028	1090	1521	1439
order_statistics_tree, STL	0.062	0.218	0.296	0.468
Segment tree	0.031	0.078	0.171	0.078
0.859*
Binary Indexed Tree	0.031	0.062	0.062	
-
* The final task requires direct access to the nodes of the tree for the implementation of solutions for O (mlogn). Without it, the solution works in O (mlogn*logn).

As you can see from all this , order_statistics_tree relatively little behind handwritten structures, and at times ahead of them in execution time. At the same time the code size is reduced considerably. Hence we can conclude is that order_statistics_tree — it is good and it can be used in contests.

Besides tree, I also wanted to describe here trie. However , I was confused by some aspects of its implementation, greatly limiting its usefulness in programming olympiads, so I decided not to talk about it. If anyone want he is encouraged to try to learn more about this structure by himself.

Useful links:
— Documentation of pb_ds
— Testing of pb_ds
— Using of pb_ds
— Demonstration of order_statistics_tree
— Demonstration of trie with prefix search
— Operations with intervals with handwritten update policy class
— More examples from that site

P.S. Sorry for my poor English :)

Tags stl, k-th order statistic, set, map

Anti-doublehash test.

By adamant, 10 years ago, translation, In English
Hi everyone! Many saw entry of user Zlobober which stated that the Thue-Morse string with terrifying force knocks hash modulo 2^64 . Single hashes were too unsafe because of the birthday paradox. The only remaining salvation from infernal suffix structures was double hashing. However, soon it will come to an end. Thoroughly studying Thue-Morse string and its possible modifications, I came to the conclusion that the test that breaks even double hash solutions is possible.

Here I described in detail the construction of the test, its use against real solutions from various contests and proof of its work.

Now I propose to users of codeforces to discuss what to do in this situation. Is there really no more ways to solve the problems with polynomial hash? Who has any thoughts on this?

About ordered set

By adamant, 10 years ago, translation, In English
Hi everyone! I think many of you know about such data type as set. It must support such operations like:

Insert an element into set;
Check if the set contain some element;
Delete element from set.
If we talk about ordered set, then elements (what a surprise!) must be kept in some order. As I think, there is also very important next operations:

Find the k-th element in a set;
Find the order of element in a set.
Most of all, for the implementation of such functional some may use binary balanced trees (AVL- tree , red-black tree , cartesian tree , etc). However, in this article I would like to talk about some of the features in the implementation of an ordered set with other structures. In particular, I'll show the implementation with the segment tree and with the Binary Indexed Tree (Fenwick tree). But I just want to note that it only allows you to build the set of natural numbers. For all other types of elements such methods will not work :(

The basic idea:

Let the index of cell in the array be the value of element in the set and let the value in the cell be the amount of element in cell. Then we can easily do all required operations in the logarithmic time. Really, the insertion of element x in the set is the same as increment of arr[x]. Deleting of element is the same as decrement of arr[x]. To check if the set contains element we should check if arr[x]>0, i.e. sum(x, x) > 0. We should also mention the operation of finding K-th element. To do this we will have to use the technique of binary expansion. In the segment tree we should store the sizes of the subtrees, like in BST, and in binary indexed tree we can watch to the following image showing the distribution of elements in tree and derive an algorithm:


    int get(int x)
    {
        int sum=0;
        int ret=0; // Number of element in array, for which the sum is equal to sum
        for(int i=1<<MPOW;i && ret+(i-1)<N;i>>=1) // Loop through the powers of two, starting with the highest possible
        {
            if(sum+arr[ret+(i-1)]<x) // Trying to expand the current prefix
                sum+=arr[ret+(i-1)],
                ret+=i;
        }
        return ret;
    }
Easy to see that while using this approach ret always has a value such that the next time a number from the interval [0; ret] will be met not earlier than at ret + i, however, due to decreasing of i, we will never reach this point, and therefore, none of the elements in the prefix will be counted twice. This gives us the right to believe that the algorithm is correct. And the check on some problems confirms it :)

The basic idea is presented, for now let's talk about the advantages and disadvantages of this approach.

Pros:

We can write it really fast;
Intuintive (especially in the case of the segment tree);  
Works fast (in practice often overtakes BST);
Contras:  

Limited functionality (supports only integers in the set);  
In its simplest form takes O(C) memory, where C — the maximum possible number in the set;
The second drawback is VERY essential. Fortunately, it can be eliminated. You can do this in two ways.

Compression of coordinates. If we can solve the problem in an offline, we read all queries and learn all possible numbers, which we will have to handle. Then we sort them, and associate smallest element with one, the second smallest — with two, etc. This reduces memory complexity to O(M), where M is the number of queries. However, unfortunately, it does not always work.
Dynamic growth of the tree. The method is not to store int the tree the vertices, which we do not need. There are at least three ways to do this. Unfortunately, for binary indexed tree there is only one of them is suitable. The first option — to use a hash map. Bad, very bad option. More precisely, with the asymptotic point of view it is good, but the hash map have very large constant, so I do not recommend this method. Unfortunately, it is that only way that we can use in BIT :) . The second option — extension with pointers. When we need a new tree vertex, we simply create it. Cheap and cheerful. However , it is still not the fastest option. The third option — statically allocate a block of MlogC vertices, and then for each vertex store the indexes of its children (as in trie). In practice, it is the fastest way. Memory complexity is the same in all three methods and is reduced to O(MlogC), which are generally acceptable.
I hope that I was interesting/useful to someone. Good luck and have fun! :)

P.S. Sorry for my poor English. Again ;)

Tags set, segment tree, binary indexed tree, fenwick tree

Suffix tree. Basics. Building in O(nlogn)

By adamant, 10 years ago, translation, In English
Hi everyone! I was always wondered how cleverly interwoven so-called string algorithms. Six months ago I wrote here article about the possibility of a rapid transition from Z-function to prefix-function and backward. Some experienced users already know that such transitions are possible between the more complex string structures — suffix tree and suffix automaton. This transition is described at e -maxx.ru. Now I would like to tell you about such data structure as a suffix tree, and share the simple enough (theoretically) way of its fast building — obtain a suffix tree from suffix array .

I remind you that a suffix tree is a prefix tree, that contains all suffixes of specified string. In the simplest implementation it will require O(n2) time and memory — we simply add in the trie all suffixes one by one until we get what we want. In most cases it's too large. We will try to do something with it.

At first we will reduce the memory complexity to O(n). In order to do this we need to get the following idea: if we have a group of edges that are connected in series and do not have branches, we can combine them into one, which will be presented with some substring, but not a single character. Thus, we obtain a compact trie (also known as radix tree or patricia tree). But that's not all. Finally, we note that we have no need to store the entire substring on the edge, so we can store only the indices of its beginning and its ending in the source string. That is what will give us the desired linear complexity. Really, vertices in our tree will now appear only in places separating lexicographically consecutive suffixes, and there will be no more than n - 1 of such places.

Finally, let's reduce time complexity to O(n). In order to do this, we will approach the following strategy:
1) Add to the trie lexicographically minimal suffix.
2) For each of the next suffixes we will rise to lcp[i] and continue building trie from here.

Surprisingly, this will be enough. In fact, the actions we will do are identical to the depth-first traversal of tree, which obviously runs in O(n).

Wait a minute, but in the title is written "building in O(nlogn)", and you got O(n), wtf?

Indeed, in fact, if we have a lcp array, suffix tree can already be built in O(n). However, there still remains one problem — we need to get in some way lcp array. And here we come to the aid of suffix array, which can be used in order to get lcp. Relatively simple method for this is described on the site e-maxx.ru. We also can get it in O(n) using Kasai algorithm. Finally, we can combine it with some linear suffix array algo in order to get suffix tree with O(n) complexity.

Advantages of this method:

Simple to understand.
Acceptable time and memory complexity.
disadvantages:  

Algorithm only works offline.  
The amount of code. It took me almost 300 lines (100 of which — for a suffix array), and then the whole evening to do something that will work. It was the first time for me building the suffix tree, so I can not say for sure whether it is possible to implement this algorithm easily.
Here, you can also see an example of code that does all these atrocities to create a suffix tree. Good luck to everyone and see you soon, I hope the article will be interesting :)

To check the correctness of the code were used the following tasks:
1393 — validation of building lcp.
1590 — validation of building suffix tree.

Tags suffix tree, suffix array, lcp array, string algorithms

Google Code Jam 2014, round 1B

By adamant, 10 years ago, translation, In English
Hi everyone! The countdown is on. Round 1B begins in under 24 hours. If you preferred to sleep instead of competing in round 1A (just like me!), then this round is for you! The top 1000 contestants from sub-round will advance to Online Round 2 and won't be able to compete in further sub-rounds.

I remind you that round will take place on Saturday, May 3 at 16:00 UTC at the site, that everybody knows.

Manacher's algorithm and code readability

By adamant, 10 years ago, translation, In English
Hi everyone again! Recently, solving the problem 1937 from Timus (by the way, I recommend it to you too! It is a great opportunity to improve yourself in string algorithms), I was faced with the necessity of finding all subpalindromes in the string. Experienced programmers already know that one of the best algorithms for this is Manacher's algo that allows you to get all subpalindromes in compressed form without any additional structures. The only problem — the algorithm is kinda hard to implement. That is, its idea is simple and straightforward, but the implementation usually are heaps of code to the widespread consideration of where to write +1, and where -1.

For those who do not know or do not remember, I will describe briefly the Manacher's algorithm. Conciseness in memory is achieved through the fact that we do not store the indices of palindromes, and for each item in the string we store the half of the length of the largest palindrome, for which item is central. If we are talking about palindromes of even length, we consider the central element to the right of exact center.

Further, we keep the left and the right ends of the rightmost palindrome and, just as it done in the Z-function, we firstly try to initialize the value of the array at this point, using information that is already computed, and then we continue with naive algorithm. To be precise, we can initialize the value of p[i] from the point that is symmetric to it in the rightmost palindrome, i.e. from p[l + (r - i)]. If we talk about palindromes of even length (you already hate them just like me, right?), Then we are interested in the value of p[l + (r - i) + 1]. And yes, it is necessary not to forget that after initialization we should not have to go beyond the known part of the string, i.e. it is necessary to initialize with something that is not more than r - i ( or r - i + 1 for palindromes of even length).

That's how e-maxx offers to use Manacher's algo:


vector<int> d1 (n);
int l=0, r=-1;
for (int i=0; i<n; ++i) {
	int k = (i>r ? 0 : min (d1[l+r-i], r-i)) + 1;
	while (i+k < n && i-k >= 0 && s[i+k] == s[i-k])  ++k;
	d1[i] = k--;
	if (i+k > r)
		l = i-k,  r = i+k;
}
vector<int> d2 (n);
l=0, r=-1;
for (int i=0; i<n; ++i) {
	int k = (i>r ? 0 : min (d2[l+r-i+1], r-i+1)) + 1;
	while (i+k-1 < n && i-k >= 0 && s[i+k-1] == s[i-k])  ++k;
	d2[i] = --k;
	if (i+k-1 > r)
		l = i-k,  r = i+k-1;
}
Two almost similar pieces of code for palindromes of even and odd length. And, if my memory serves me, and it's still not fixed, they also contain an error (can you find it?)

A little thought and writing out some cases on a piece of paper, I was able to squeeze two separate pieces into one. At the same time, I tried to keep the maximum similarity with the Z-function algorithm.

    vector<vector<int>> p(2,vector<int>(n,0));
    for(int z=0,l=0,r=0;z<2;z++,l=0,r=0)
        for(int i=0;i<n;i++)
        {
            if(i<r) p[z][i]=min(r-i+!z,p[z][l+r-i+!z]);
            while(i-p[z][i]-1>=0 && i+p[z][i]+1-!z<n && s[i-p[z][i]-1]==s[i+p[z][i]+1-!z]) p[z][i]++;
            if(i+p[z][i]-!z>r) l=i-p[z][i],r=i+p[z][i]-!z;
        }
    }
However, long enough string with while spoils the whole picture. I propose to fight with it like this:

    vector<vector<int>> p(2,vector<int>(n,0));
    for(int z=0,l=0,r=0;z<2;z++,l=0,r=0)
        for(int i=0;i<n;i++)
        {
            if(i<r) p[z][i]=min(r-i+!z,p[z][l+r-i+!z]);
            int L=i-p[z][i], R=i+p[z][i]-!z;
            while(L-1>=0 && R+1<n && s[L-1]==s[R+1]) p[z][i]++,L--,R++;
            if(R>r) l=L,r=R;
        }
As it seems to me, this is the most readable version of the code. And what do you think about this? How to implement Manacher's algo least painful? And in general, how do you solve the problem of finding all subpalindromes in the string? Share your ideas in the comments below :)

P.S. It looks like my English is especially poor in this entry. My apologies about it :(

UPD: droptable said in the comments on how to support arrays from Manacher's algorithm in online. Here I put an example of the algorithm itself, with some modifications, which will be discussed below.

The basic idea of ​​the algorithm: at any time we keep the center of the longest palindrome relating to the right end of the line. When we add character to the end, we have two options for what happens next:

Main palindrome expands. Then we simply increase its radius value and return.
Main palindrome not expands. This means that we will need to traverse an array forward to find new primary palindrome. Thus, each time a new cell initializing with number in the cell that is symmetrical to the center of the previous main palindrome. Once it turned out that we can extend the current palindrome, which means that it is leftmost of the related to the end of the string and then it will become the new main palindrome.
Now about the changes in the code. First, were deleted the two empty characters that were inserted into the string at the begin. As it turned out, they are not necessary, however, but their addition to their complexity algorithm.

Second, was added get(). Mikhail mentioned in his comment about some problems with online. The problem was that, in any moment, we know only the final values ​​in the elements, which are earlier than the center of the main palindrome. However, you will notice that the values ​​after it, we can get in O(1) if we will refer to symmetric from the center of the main palindrome element.

Tags string algorithms, subpalindromes, manacher algo

Obtaining suffix array from suffix automaton

By adamant, 10 years ago, translation, In English
Hi everyone! 2 years ago Logvinov_Leon asked about way to get the suffix array from suffix automata. Oddly enough, he had not received answer. However, as you may have noticed, I'm interested in the topic of transformations of some string structures to others, so I decided to shed some light on how this is done.

So, you will notice that suffix automaton — essentially a suffix tree with a few other compression principle. While in compressed tree we just squeeze the edges, when we construct the suffix automaton we getting something like two-way trie. We are compressing not only common prefixes, but also common suffixes. From this we can draw the following interesting conclusion: if we will traverse through automaton with dfs, ignoring the used-array, we will get the same thing as when traversing suffix tree. Now remember that the suffix array from suffix tree obtained by the usual DFS — we just write down the suffixes in the order in which we encounter them in the trie. Thus, we already have an algorithm for constructing suffix array from automata for O(n2).

But, of course, it does not suit us and we want to go further. If we want to have linear time, we can't ignore the used-array. Easy to see that even so, starting in depth tour we will meet a few (at least one) suffixes before the first stop in the visited state. Now our task when we visit the state is to quickly get all the suffixes that are hidden behind it, without traversing all the ways from this state again. I suggest the following solution: let each state in the machine to keep indices of the first and last encountered after it suffixes in suffix array. Then, when we came in the used state, we will add to the array all the suffixes in which we would have got from it, taking into account the already traversed length. Obviously, this solution will work for the O(n), since we totally add exactly n suffixes, and all the rest of the time we just dfs through the suffix automaton.

P.S. my implementation of the algorithm.
P.P.S. I can't say for sure, but it seems that in practice, this algorithm is not applicable due to the very large constants.

Tags suffix array, suffix automata

Suggestion: DOT graph description language on Codeforces

By adamant, 10 years ago, translation, In English
Hi everyone!

It's no secret that in competitive programming we often have to work with graphs[citation needed]. We also often have to draw graphs. And maybe some people already know that there is tool for visualization of graphs called GraphViz, which parses the DOT code into visual image with the corresponding graph.

Example of a graph that can be obtained using graphviz — compressed suffix tree for the string abaabbaaa. Yes, the graph contains a small mistake, but it is not important :)

 

Now, actually, to the subject. How do you feel about adding DOT language support to the Codeforces editor? I think it would be cool, and you? :)

Skip-list

By adamant, 9 years ago, translation, In English
Hi everyone!

I recently read about such an interesting data structure as skip-list. It seemed to me, the structure is very interesting and at the same time easy to use. That is the reason I decided to experiment with the structure in various problems and try to implement the various modifications of it. "Basic" operations are well described in this entry from wikipedia. So I will not give them too much time.

Basic implementation.

Here is "pure" variant of skip-list — implemented only the basic operations — find, insert, erase.

code

Adding order statistics functions.

Now we need to add two new functions find_by_order() and order_of_key(). In order to do this, in balanced binary trees we used to store the size of subtrees. But here we just should store the length of links. After this both functions can be implemented easy enough. In first function we change binary predicate and comparing not elements themselves, but length of the prefixes they are ending in. In the second function we do everything just like in usual find(), but we also store the length of prefix that lead to current element.

code

Make the structure use implicit key.

When we added find_by_order() we also got an opportunity to make skip-list use implicit key, i.e. key=order of element in set. Everything we need in order to use it is using find_by_order() instead of find().

code

P.S. What do you think about this data structure? How wise is it to use skip-lists on the contests? What other modifications do you know? Also, my implementation is pretty weak, do you know the better one?

Tags skip-list, list, set

C++ STL: Policy based data structures. Part 2

By adamant, 9 years ago, translation, In English
Hi everyone!

Some of you may remember my entry about policy based data structures. In addition to a review article on these structures, I also wanted to write about the possibility of using your own Node_Update class. Then I hadn't enough time for it, but now I can and I want to catch up and share with you some new knowledge.


So, let's start. Node_update class should looks like this:

template<class Node_CItr,
	 class Node_Itr,
	 class Cmp_Fn,
	 class _Alloc>
struct my_node_update
{
    typedef my_type metadata_type;

    void operator()(Node_Itr it, Node_CItr end_it)
    {
        ...
    }
};
Let's consider how this class works. Policy based tree, which is using update policy will additionally keep in node one variable of type my_type. When the tree is rebuilt, part of the metadata spoils, so for each node, which could be damaged applied the operator (). At the same time this operator begins to be applied firstly to the leaves, that is, we guarantee that if () is applied to the node, its metadata can be damaged, but the metadata of its children will be intact. The function has two arguments let's learn them.

Node_Itr it & mdash; iterator pointing to the node from which was called (). Node_CItr end_it & mdash; const-iterator to the node referenced by all NIL-tops. Node_iterator has the following methods:

get_l_child () & mdash; returns node_iterator on left child or node_end, if it does not exist.
get_r_child () is the same for the right child.
get_metadata () & mdash; returns const reference to metadata stored in the node.
* & mdash; dereference. Returns reference to a regular iterator of this item in set.
For clarity, I give an example of the function () to support the size of the subtree starting at the node:

    void operator()(Node_Itr it, Node_CItr end_it)
    {
        auto l=it.get_l_child();
        auto r=it.get_r_child();
        int left=0,right=0;
        if(l!=end_it) left =l.get_metadata();
        if(r!=end_it) right=r.get_metadata();
        const_cast<int&>(it.get_metadata())=left+right+1;
    }
Well, we learned how to update invariants. Now let's learn how to use them.In order to do this we note that any public-methods of node_update automatically become public in tree. In addition, we have access to all of the virtual methods in the base class, if we declare them as virtual in node_update. In this regard, we add to our class the following lines:

    virtual Node_CItr
    node_begin() const = 0;
 
    virtual Node_CItr
    node_end() const = 0;
This will give us the opportunity to gain direct access to the tree. In particular, node_begin() will point to the root of the tree, and node_end() to the place where all NIL-nodes are. Finally, we are ready to entirely manage the metadata at the nodes of our tree. For example, here is the function that finds order of int key in set:

    int order_of_key(int x)
    {
        int ans=0;
        auto it=node_begin();
        while(it!=node_end())
        {
            auto l=it.get_l_child();
            auto r=it.get_r_child();
            if(Cmp_Fn()(x,**it))
            {
                it=l;
            }
            else
            {
                ans++;
                if(l!=node_end()) ans+=l.get_metadata();
                it=r;
            }
        }
        return ans;
    }
Currently I have two examples of problems solved with node_update it is task D (7485682) from recent contest and solution of the problem GCD2010 from the timus (sol). In the near future I will try to find and solve more problems with update policy and add to the article. If there is a problem that you think can be well solved by the structure & mdash; I will be glad if you lay them out now :)

Tags policy based, advanced, pbds, data structures

Petrozavodsk summer camp 2014

By adamant, 9 years ago, translation, In English
Hi everyone!

Today summer Petrozavodsk summer training camp 2014 has been started. I hope that I don't violate any unofficial taboo by writing about it :)

If you, just like me, are in a group of people who do not have the opportunity to participate in this celebration of life, you can follow the camp, for example, at SnarkNews or acm.math.spbu.ru.

Tags summer, petrozavodsk, ptz

Palindromic tree: behind the scenes

By adamant, 9 years ago, translation, In English
Hi everyone!

As some of you may know, on this summer camp in Petrozavodsk droptable presented a new data structure, palindromic tree. I had the honor to participate in the study of the structure for the six months before that, and I want to tell about it now :)

But firstly a brief explanation. If you alredy know basic ideas of the structure, you may go to the interest part. Let's to each palindrome assign corresponding string equal to its right half, i.e., its radius and the boolean variable indicating its parity. Now let's merge all our radii of subpalindromes of string S in two prefix trees for even and odd lengths separately. Claim: such trie will be  in memory. Indeed, there can be no more than n distinct subpalindromes in the string, and each node in trie corresponds with only one unique palindrome. Therefore, we have no more than n + 2 nodes.

Let's show that such structure can be constructed in . Let each vertex in the tree store suffix link that leads to the node that corresponds to the radius of maximum suffix palindrome of the whole palindrome of current node. For example, there is a vertex corresponding to the radius "bacaba" with odd length, i.e., to the palindrome "abacabacaba". Suffix link will lead in the node corresponding to the radius of maximum suffix palindrome "abacaba", i.e., "caba".

Now let us assume that we have a palindromic tree for a string S and we want to add to the end of the string some character. We will maintain a pointer to the node corresponding to the maximum suffix-palindrome of string on every step. Further actions are similar to those in the construction of suffix automaton. We are look at the new character in the string and at the one that stands in a string before palindrome, which corresponds to the current state. If they do not match, then let's go by the suffix link and repeat the check. When they match, we look at whether there is an edge to the new character in the node. If yes, then move along the edge and enjoy, if not palindrome that we should add was not in a string before. So, let's create for it a new node. Now we need to find for it suffix link. Well, let's go by the suffix links again until you find the correct position in a second time for the same symbol. When we found it — this is the suffix link to a new node. That's all. Now we have an algorithm that works amortized in , since at each stage we firstly several times reducing our current string, and then increase it by only one character. And obviously there will be no more erases than inserts (which are no more than n).

And yes, a few words about the implementation. You may have noticed that I do not consider the case when occurs the loss outside of the tree. This is due to the fact that such a loss never occurs :). How to achieve this & mdash; since we have two tries, we have two roots. For one of them let's make an initial length 0 (for even palindromes), and for the second — -1 (respectively for odd). And by default let's do a suffix link from the first one to the second. Thus, every time we are at the root with the length of -1, we always find that the extension is possible, as the new symbol and the symbol before the suffix-palindrome are the same in this case.

Code: link
Written by me as the most similar to the suffix automaton for the purpose of easier memorization :). There is also great implementation from Merkurev link.

Now I will talk about how it happened that I was involved in the preparation of the problem for Petrozavodsk camp. In the comments to my article about the Manacher's algorithm droptable wrote about the online version of the algorithm and asked to add it to the article, which I did, after some discussion. Just then Michael told me about the problem from the APIO, which, in an amazing coincidence, was held at the same time. Brief statement: find refrain-palindrome in the given string.

Then Mikhail hinted that they have a cool structure that quickly and easily solves that problem. Since I was young and inexperienced and still wasn't familiar with the hard suffix structures, I decided to think about that kind of constructive alternative solution. So I'm somewhat reinvented palindromic tree :) In fact, I was very far from the solution, because I did not use the suffix links, and wanted to build it with hashes, but nevertheless I came up with the general structure. And when I told about my tree Mikhail it turned out that it is exactly the structure he told about earlier :)

Furthermore, it was a lot of productive discussion, the results of which I would like to share. For example, I figured out a way to build a tree in , which, however, in practice is faster than many algorithms with other structures that work in . The idea was similar to an online Manacher's algorithm & mdash; let's store in each item of the array the pointer to the vertex, which corresponds to the maximum palindrome centered at this item. Further, when calculated palindrome in the new cell, we take either vertex from symmetric to the center of maximum suffix-palindrome cell or some of its ancestor with less length, if it "climbs" over the current end of the string. Initially, it seemed to me that the solution will be linear even if you just go up to the correct level every time, but it turned out that it is not :(

Using the technique of binary expansion, I was able to reduce time to . Also there are algorithms for finding LA (Level Ancestor) in , which would solve this problem in , but, obviously, it would be unreasonable because of the complexity of algorithm. So...

challenge 1: Can you improve the idea of this algorithm to the , without using common methods of solving LA problem?

Code:
: link
: link

Oddly enough, it's not all what I wanted to tell about. In the original version of the problem, which was planned to be at Petrozavodsk one had to not only add character to the end of string and count the number of new palindromes, but also to delete the character from the end of the string. Later, this part has been removed since problem was already complicated enough. Now I would like to describe the possible algorithms for the solution of the task, which was to be originally.

Firstly let's understand & mdash; what is a problem for us? It seems that we have everything to solve the problem, why wouldn't we just do a rollback to the previous state? However, this conclusion is false, because the evaluation of adding a new symbol  is amortized. And if we for example take a string aaaaaaa...aab, and then start each time to remove and return b. It is easy to see that at each step we will make  iterations that is of course, unacceptable. Both methods, which will be described have the goal to make an appending of character with strict but not amortized complexity. So:

1) "Smart" suffix links. Let's at each vertex store additional suffix link that leads to the vertex, which have different preceding character in string as in the usual suffix link. Now we can use ordinary and "smart" suffix links in order to find the next state faster. According to Lemma 11 of droptable's Article (article not yet available, however, it has already been accepted for publication. Since it will soon be performing at this conference), the path to the root with "smart" links will pass through  vertices. Thus, every time we will do no more than  operations.

2) full automaton. Nor, for that matter, why do we bathe with complex theorems, and one link that takes you to a different character? Let's keep at each vertex array on  elements & mdash; one such link for each letter in the alphabet. It is easy to show that we can now answer the query in . Let us add to the string symbol 't'. It will be approximately like this:

v = last
v = smartlink[v][t]
new = to[v][t]
v = smartlink[v][t]
suflink[new] = to[v][t]
smartlink[new] = smartlink[ suflink[new] ]
smartlink[new][p] = suflink[new]
Now we can solve the problem in  time and memory. Let's improve the result to . For this we need persistent array. It can be constructed on the segment tree. So, we create a persistent segment tree with Σ elements. Next, let's create a new vertex. We can fork in  desired root of the tree, and in  make a corresponding change. Here our quick structure is ready.

Challenge 2: Do you see other ways to solve this problem? For example, saving amortized evaluation with removal or other ways to solve the problem in less than ?

For dessert, I want to offer you a work-out in the use of this structure and try to solve two problems. The first of them & mdash; is the same problem that was offered this summer at Petrozavodsk training camp, but with more serious constraints. By the way, only one team solved it. IMCS of Siberian FU Bizons with some terrible bitmask magic compressed in memory suffix automaton and passed it on the first try for three minutes before the end of the contest. Kudos to them and respect :). The second task is already quite old. It was presented for the first time at the winter training camp 2006, where the author's solution reduced the problem to LCA on prefix-function automaton. However...

We offer you to solve it in a slightly different interpretation, in which the author's solution, apparently, is not going to work. Imagine that you are given the first line is the line that should go to the second, and the second given is generator. You need to calculate the answer to each of its prefix. Dare :)

So, the problems:
timus. Palindromes and Super Abilities
e-olimp. Palindromic factory

Tags palindromes, palindromic tree, suffix structures, automatons, subpalindromes

Aho-Corasick algorithm. Construction

By adamant, 9 years ago, translation, In English
Hi everyone!

This time I would like to write about the Aho-Corasick algorithm. This structure is very well documented and many of you may already know it. However, I still would try to describe some of the applications that are not so well known.

This algorithm was proposed by Alfred Aho and Margaret Corasick. Its is optimal string pattern matching algorithm. e.g. given the string set {"a", "abba", "acb"} and given text, say, "abacabba". With Aho-Corasick algorithm we can for each string from the set say whether it occurs in the text and, for example, indicate the first occurrence of a string in the text in , where |T| is the total length of the text, and |S| is the total length of the pattern. But in fact it is a drop in the ocean compared to what this algorithm allows.

To understand how all this should be done let's turn to the prefix-function and KMP. Let me remind you, the prefix function is called array π[i] = max(k): s[0..k) = s(i - k..i], ie, π[i] is the length of the longest own suffix that matches the prefix of the substring [0..i]. Consider the simplest algorithm to obtain it. Suppose we have counted all the π values on the interval from 0 to i - 1. In this case, we can repeatedly "jump" to positions π[i - 1], π[π[i - 1] - 1], π[π[π[i - 1] - 1] - 1]... and so on. Let the moment after a series of jumps, we are in a position of t. If s[t + 1] = s[i] and t is maximum possible, then π[i] = t + 1. If we will count the π as described above, we will get it in .

Now, let's build automaton that will allow us to know what is the length of the longest suffix of some text T which is also the prefix of string S and in addition add characters to the end of the text, quickly recounting this information. So, let's "feed" the automaton with text, ie, add characters to it one by one. If we can make transition now, then all is OK. Otherwise, we go through suffix link until we find the desired transition and continue. Let's say suffix link is a pointer to the state corresponding to the longest own suffix of the current state. It is easy to see that suffix links in such automatons is the same as π from KMP. So now for given string S we can answer the queries whether it is a substring of text T.

Finally, let us return to the general string patterns matching. Firstly may seem that this is just the beginning of a long and tedious description of the algorithm, but in fact the algorithm has already been described, and if you understand everything stated above, you'll understand what I write now.

So let's generalize automaton obtained earlier (let's call it a prefix automaton) Uniting our pattern set in trie. Now let's turn it into automaton — at each vertex of trie will be stored suffix link to the state corresponding to the largest suffix of the path to the given vertex, which is present in the trie. You can see that it is absolutely the same way as it is done in the prefix automaton. It remains only to learn how to obtain these links.

I suggest doing it this way: run a breadth-first search from the root. Then we "push" suffix links to all its descendants in trie with the same principle, as it's done in the prefix automaton. This solution is appropriate because if we are in the vertex v in a bfs, we already counted the answer for all vertices whose height is less than one for v, and it is exactly requirement we used in KMP. There are also some other methods, as "lazy" dynamics, they can be seen, for example, at e-maxx.ru.

Basic implementation: http://ideone.com/J1XjX6
Alternative one: http://ideone.com/0cMjZJ
You can easily see the KMP in the push_links().

Recommended problems:

UVA — I love strings!!
Timus 1269 — Obscene Words Filter
Timus 1158 — Censored!
MIPT El Judge 014 — War-cry
SPOJ — Morse
Later, I would like to tell about some of the more advanced tricks with this structure, as well as an about interesting related structure. So stay tuned :)

Tags strings, aho-corasick, pattern_matching

Dynamic connectivity problem

By adamant, 9 years ago, translation, In English
Hi everyone!

Recently, at the MIPT: The Fall training camp on the contest from Alexander Milanin was a problem from Petr Mitrichev Contest 7. We were given a graph and a set of queries like "suppose we removed from the graph k ≤ 4 edges. Check whether graph is still connected?" I want to talk further about the solution of a more general problem, when the edges are added and removed without additional constraints in  offline. The first algorithm with such an assessment was offered by David Eppstein in 1992, reducing it to fully dynamic minimum spanning tree problem, but here we will focus on a simple algorithm, proposed in 2012 by Sergei Burunduk1 Kopeliovich.

Let's assume that there are three types of queries & mdash; add the edge (+), remove the edge (-) and find out some information about the graph (?) (in this case, let it be the number of connected components of the graph). We assume that we received a k queries. Consider the k + 1 points of time & mdash; initial and k points after each query. For convenience, we transform the requests of the first kind in the queries of the form "i-th edge is present in the column from the time l to the time r" (!).

Thus, suppose we have a graph G =  < V, E >  and the set of queries. Let  be a set of edges, which are always present in it (that were originally there and were no requests for their removal). Let's compress each connected component formed from such edges in one vertex and construct a new graph of these vertices. Also delete all vertices that are not mentioned in the list of requests (to work with the graph of k vertices). Remade queries so that if in the initial graph query was assigned to a pair of vertices (a, b), now it will be assigned to a pair of vertices (comp(a), comp(b)). We see that the execution of ? requests in new graph will have exactly the same result as in the initial. It is further proposed an algorithm: divide the time interval which is currently being processed in two halves and recursively solve firstly the left and then the right side, and thus obtain answers for the entire set of queries. Base & mdash; a single point of time, answered trivially & mdash; at this point we fed to the input graph without edges, therefore, for any query answer will be the number of vertices in the graph. At each step, the query processing subsegment [l;r), we will keep only those vertices that are mentioned on this subsegments, then the request [l;r) will be processed in the O(r - l), which will be  in the sum over all subsegments.

Solution by Burunduk1. 

Sergei also proposed a similar idea of algorithm for a biconnected components (and bridges) in a dynamically changing graph in  offline. You can read about it in his diploma, which is attached below.

To summarize, I would like to offer traditionally solve several problems on the topic. Especially for this was made training. Good Luck & Have Fun!

P.S. More details about the structure and other algorithms for solving the problem (as well as the proof of some trivial facts that have been omitted) can be found in Burunduk1's diploma.

 Discussion of Dynamic connectivity contest
Tags dynamic connectivity, dynamic, offline, bridges, 2-edge-connectivity, connectivity

Suffix tree. Ukkonen's algorithm

By adamant, 9 years ago, translation, In English
First part, I guess. Even if you think that you are familiar with suffix tree, please, take a look at the code below. It may be interesting to you.

Hi everyone! Finally I learnt this one :)

In this entry I would like to avoid long and complex theory which scared me from suffix tree for a long time. So straight to the point. I will not prove algorithm if you want some proofs, you may check stackoverflow or Dan Gusfield's book... Or somewhere else, I don't know.

Suffix tree is a compressed suffix trie, so all vertices which are not corresponding to suffixes and which have only one descendant are omitted.

Now about the algorithm. At each iteration, it makes implicit suffix tree. In implicit suffix tree all vertices which have only one descendant are omitted. Usually edges are stored as a pair of [Li, Ri]. Personally I am not very convenient to work with them in this way, so I suggest to store in each node some data corresponding to the edge from its ancestor to it — fposi & which is the left position of first edge occurence in the string and leni which is the length of the edge. In this case, the length of the edges, leading to leaves will by default be considered equal to inf. So we can be sure that at any time the edges to the leaves are correct. Root of the tree will be the vertex numbered 0.

Let's at each step of the algorithm keep the longest non-unique suffix of the string. To do this, let's keep a pair of numbers (node, pos) & mdash; vertex in the suffix tree and the number of characters that you need to pass down from it to have this suffix. By default node = pos = 0. When you append a new symbol, let's increase pos by 1 and add all of new unique suffix of the string that appear after adding a new character.

Also, we need the concept of suffix links. It is defined for internal nodes of the tree. Following suffix link will lead to the vertex corresponding to the same substring, but without first character. For the root vertex suffix link is not defined.

Appending of new character consists of the following stages:

If pos = 0, then all suffixes are added. Return. Otherwise let's find the vertex after which new suffix will be added (it is not neccessarily node because edge from node may be too short). So while pos greater then edge from node let's follow this edge and substract its length from pos.
Now let's try to add new suffix. We will have three options here:
If we do not have needed outgoing edges at this node, we simply create a new vertex and hung it to the current one.
If there is an edge and a suffix that we want to add lies entirely on it then this and further suffixes are not unique. Return.
If there is an edge and suffix doesn't lie entirely on it then it differs in only one character, this means that we need to create a new vertex in the middle of the edge and then create another one new vertex (which will be new suffix) and hung it to the vertex in the middle of splitted edge.
If you have not returned on the previous step, go to the next suffix. If node is root, then we reduce the pos with 1, otherwise we just follow the suffix link node = link(node) without changing pos. After that, we go to step 1.
And about siffix links. On the i-th step we will set suffix link of internal vertex created on i - 1-th step. If we create a new internal vertex (i.e. split some edge), then the suffix link will lead into it. In two other cases, the suffix link will lead to the node (I am too lazy to write truly marvellous proof of this, so it is left to the curious reader as an exercise).

And, finally the implementation.

Code: #sT8Vd1

I tried to make the code as simple and clear as possible :) I do not know if I managed to do so and hope for your feedback and questions.

Tags suffix tree, suffix trie, ukkonen, strings

A bit more about palindromes

By adamant, 8 years ago, translation, In English
Hi everyone!

Today I want to talk about one quite famous and interesting problem.

So, here it is: given string S. Split it in the minimum possible amount of palindromic strings. Rather unpretentious, isn't it? You can find this problem here or here. But whenever you see it, in the best case intended solution would be a quadratic (or even cubic). Here will be described a solution to this problem in  online (ie, answer will be received for each prefix).

Generally speaking, this solution is relatively new and it is not unique (here is the other one quite new though). You can see original droptable's entry here. In this solution palindromic tree will be used, its description can be found in this article. This implementation will be taken as basic.

Let's get started :) To begin with, consider the following naive algorithm runs in O(n2). We will maintain the dp of ans(i) — the minimum number of palindromes, in which you can split the strings prefix, ending at position i. For it will be built palindromic tree and at each step the whole suffix path will be traversed, moving from the vertex to its suffix link.

    for(v = last; len[v] > 0; v = link[v])
        ans[i] = min(ans[i], ans[i - len[v]] + 1);
To solve the problem quickly, let's introduce two new values that will be stored in the tree vertices difference of vertex diff(v) = len(v) - len(link(v)), and serial link slink(v). Serial link will lead from the vertex v in the vertex u, corresponding to the maximum suffix palindrome of v, which satisfies diff(v) ≠ diff(u). It is easy to see that it is possible to maintain when you create a new node as follows:

    if(diff[v] == diff[link[v]])
        slink[v] = slink[link[v]];
    else
        slink[v] = link[v];
Approval 1: path within serial links to the root contain only  vertices. You can see the proof in droptable's entry.

Knowing this fact you can use following algorithm:
Starting from the maximum suffix palindrome (last), quickly improve answer along all palindromic suffixes till the serial link, and then move on by the serial link and repeat the procedure. It is easy to see that all palindrome suffixes will be considered. Let us learn how to quickly process described set of palindromes (let's call it a series). For this we need

Approval 2: Let us consider suffix palindrome v and link(v) ≠ slink(v) in some step of the algorithm. Then, the previous occurrence of link(v) in a string was in position i - diff(v), while in this position there is no suffix-palindrome with length len(link(v)) + diff(link(v)), ie, link(v) was the beginning of the series in that position.

Proof: Since the suffix of palindrome is also a prefix for him, we can specify the occurence of link(v) in mentioned position as a prefix of v. Let's show that there is no occurence of link(v) between i and i - diff(v). Let's assume there is. Then the intersection of occurences in this position and in the position i - diff(v) is also a palindrome (since it is itself a border of palindrome) with length greater than len(v) - 2·diff(v). So, diff(link(v)) neqdiff(v), it's a contradiction.

Let's show that string with length len(v) ending at i - diff(v) is not a palindrome. Let v = DTDT, link(v) = TDT = DT. From here you can see that if we append diff(v) characters to the front of link(v) and got a palindrome, then appended characters are equal D. But due to the fact that the DT is a palindrome, we see that DDTDT is also a palindrome. Therefore, v is not the beginning of the series, so you will not consider this string on any step of the algorithm.

Based on the approval 2, let's use following dp: Let series(v) is a series, starting at the vertex v, then the value of the dp in it equal
 where i is the latest (among already used) position in a string, in which vertex v corresponded to the longest palindrome in series. You can see that when we are in the position i, then series_ans(link(v)) due to approval 2 covers all the values ​​we are interested in, but one in which the length of the suffix is ​​$len(slink(v)) + diff(v)$, ie, considered all palindrome suffixes, except the smallest in the series. It can be considered separately.

Finally we have following algorithm:

    for(v = last; len[v] > 0; v = slink[v])
    {
        series_ans[v] = ans[i - (len[slink[v]] + diff[v])];
        if(diff[v] == diff[link[v]])
            series_ans[v] = min(series_ans[v], series_ans[link[v]]);
        ans[i] = min(ans[i], series_ans[v] + 1);
    }
Obviously, it works in O(nt), where t — is the maximum possible length of "serial path" ie total complexity is .

An example of a program that prints for each prefix minimal number of palindromes, by which it can be split: #xE2k6Y

Problems on Online Judge: 2058 2044

On suffix automaton (and tree)

By adamant, history, 8 years ago, translation, In English
Hi everyone!

This summer in Moscow IPT was held Moscow International Workshop ACM ICPC. I gave a lecture on suffix structures there (it was only about suffix automaton and suffix tree actually). In this regard, I would like to present to you the lecture notes. So here are Russian and English versions. Enjoy and Happy Holidays :)

P. S. My very apologies for my bad English :(

UPD: Thanks to Burunduk1 for help in code formatting.

UPD2: Please also refer to Wikipedia article on this topic. I'm the main author of its current (29 Oct 2021) version.

Tags suffix automaton, suffix tree, tutorial

Codeforces Round #349 (Div. 1 & Div. 2)

By adamant, 8 years ago, translation, In English
Hi everyone!

Yeah, you guessed it right, after a long four month break after the latest div. 1 round which was not dedicated to any official competition, you will once again have the unique opportunity to participate in usual codeforces round. No t-shirts for top-x competitors! No multi-level selection system for the opportunity to compete in final! No esoteric languages or marathon-like problems! We even will not tell you the scoring untill the very end of the round! That's it, like the good old days.

So this round was prepared for you by Ivan Smirnov (ifsmirnov) and me (adamant). We want to thank Max Akhmedov (Zlobober), Alex Frolov (fcspartakm), Edvard Davtyan (Edvard) and Mike Mirzayanov (MikeMirzayanov) for the help in round preparing, and useful advice. Special thank to Edvard for taking the role of coordinator this time and traditionally MikeMirzayanov for polygon and codeforces systems.

Good luck to everyone! We really hope that you will have a lot of fun participating in this round :)

UPD. Score distribution:

Div. 2: 500-1000-1500-2000-3000

Div. 1: 500-1000-2000-2000-3000

UPD. 2. Also thanks a lot to Alex Fetisov (AlexFetisov). Forgive me, please, I have totally forgotten about you :)

UPD. 3. If you missed the editorial, you can find it here.

 Announcement of Codeforces Round 349 (Div. 1)
 Announcement of Codeforces Round 349 (Div. 2)

Question about suffix automaton/tree

By adamant, history, 7 years ago, In English
Hi everyone!

As you may know, it is possible to build a suffix automaton for a set of strings as well as suffix tree consisting of all suffixes of strings from the set. Question is as follows: for each string Sk consider set Vk of vertices/states of tree/automaton that corresponds to the substrings of Sk. Is it true that ? Can you prove it or make a counter-example?

Quaternion algebra and geometry

By adamant, history, 7 years ago, translation, In English
Hi everyone! As you may already know (if you don't then I advice you to learn it), in 2D geometry it is convenient to use complex numbers to handling points operations and rotations. Now I would like to tell you about similar construction, which allows you to work efficiently with 3D geometry, in particular to use vectors and linear operations on them, calculate many popular operations (like dot or cross products) and maintain rotations in 3D space.

Let  — are vectors in 3D space. Following notation from analytical geometry will be used:

Dot product: , where  — is the angle between  and  in their common plane,  — length of vector .

Cross product: , where  — unit vectors, directed along ,  and  axes. In 3D space cross product is a vector having length  and directed orthogonally to their common plane in such way that if you'll watch on vectors from the endpoint of cross product, shortest rotation from  to  will be counter-clockwise.

So, quaternion — is hypercomplex number, which can be represented as , where  — are real numbers and  — are imaginary units. We can define multiplication operation on quaternions with following equality . The whole quaternionic multiplication table can be derived from this identity:


Assuming that quaternion multiplication is distributive over addition (), we can multiply the quaternions, "opening parenthesis". Note that quaternionic multiplication will be associative (), but in common case not commutative (there are  such that ).

Quaternions can be also represented as , where  — is vector of 3D space with unit vectors . Components  and  are called scalar and vector parts of quaternions. Assume we have quaternions  and . Then their multiplication can be represented as . Consider multiplication of two quaternions with zero scalar parts.

.

Note that it can be shortly written as . Thus we have following formula for quaternionic multiplication:


Finally note that since , quaternion  can be represented in form , where  — are complex numbers. Also note that since , we have identity  or, introducing notation : . Thus multiplication of quaternions  and  can be represented as . Number , introduced earlier, is called conjugated to complex number . So we have another one formula for quaternion multiplication:


Let's show that any non-zero quaternion is invertible, i.e. for each quaternion  there is inverse  such that . In this case we can divide quaternions by multiplicating with inverse element. Like in complex numers we can introduce for quaternion  conjugated quaternion . From that formula we can see that . Thus we can introduce quaternionic norm (which is generalisation of length):  and inverse element . Note that . Indeed from multiplication formula we get . From here we can see  and .

Now we are ready to learn about the most useful property of quaternions which is handling rotations in 3D space. From now on we will consider vectors to be quaternions with zero scalar part. Let's introduce conjugation operation of vector a by quaternion g the result of which will be vector [1] . This identity is equivalent to the following: . Let , then we can get . Considering scalar and vector parts separately we will get:


Note that because ||ab|| = ||a||·||b|| we can get that norms of vectors  and  are equal. First equation of the system means that orthogonal projections on axis  are equal for  and . Set of vectors with same norm and projection on some direction form a circumference around that direction. Thus we see that  can be obtained from  by its rotation around  on some angle. Let's find it. From now on we will say that rotation is clockwise or counter-clockwise depending on what it's like if we watch it from the endpoint of vector around which rotation is happening.

Let's assume that . If it's not true, we can divide  by square root of its norm, it will not affect . Now we can assume that , where . Also using first equation and using notation  and , we can transform second equation in following way: . (On the other hand we subtracted  from both sides, on the other one since  we can see that )

Note that  and  are lying in the plane orthogonal to , thus, this is the rotation plane. Now we should see that  is vector  rotated  around vector  clockwise. Thus left part of second equation is , rotated around  clockwise on angle , and the right one is , rotated around  counter-clockwise on . Thus we see that  is  rotated around  counter-clockwise on .

To sum up: quaternion with unit norm  has such property that for each  expression  specifies vector  rotated around  on  counter-clockwise.
[1] . Scalar part of this product equals to .

And now the implementation. Ideone: #BBOx9H

// To begin, we introduce the notation for the class and its elements, which we will use.
typedef double ftype;
typedef complex<ftype> point;
typedef complex<point> quater;
#define qA real()
#define qB imag()
#define qs qA.real()
#define qx qA.imag()
#define qy qB.real()
#define qz qB.imag()

const ftype pi = acos(-1.);
const ftype eps = 1e-9;

// Multiplication via formulas in complex terms.
quater operator * (quater a, quater b)
{
    return {{a.qA * b.qA - a.qB * conj(b.qB)},
            {a.qA * b.qB + a.qB * conj(b.qA)}};
}

// Conjugated element
quater conj(quater a)
{
    return {conj(a.qA), - a.qB};
}

// Quaternionic norm.
ftype norm(quater a)
{
    return (a * conj(a)).qs;
}

// Length.
ftype abs(quater a)
{
    return sqrt(norm(a));
}

// Dividing via inverse element.
quater operator / (quater a, quater b)
{
    return a * conj(b) / point(norm(b));
}

// Quaternion from vector coordinates.
quater vec(ftype x, ftype y, ftype z)
{
    return {{0, x}, {y, z}};
}

// Basis vectors.
const quater ex = vec(1, 0, 0);
const quater ey = vec(0, 1, 0);
const quater ez = vec(0, 0, 1);

// Vector part of quaternion.
quater vec(quater a)
{
    return a -= a.qs;
}

// Dot product.
ftype dot(quater a, quater b)
{
    return -(a * b).qs;
}

// Cross product.
quater cross(quater a, quater b)
{
    return vec(a * b);
}

// Triple (mixed) product.
ftype mix(quater a, quater b, quater c)
{
    return dot(a, cross(b, c));
}

// Conjugation of vector a by quaternion g.
quater conj(quater a, quater g)
{
    return g * a / g;
}

// Quaternion representing rotation around i on phi counter-clockwise.
quater rotation(quater i, ftype phi)
{
    return point(cos(phi / 2)) + i * point(sin(phi / 2));
}

// Rotate a around i counter-clockwise.
quater rotate(quater a, quater i, ftype phi)
{
    return conj(a, rotation(i, phi));
}

// Check if two vectors are equal.
bool cmp(const quater &a, const quater &b)
{
    return abs(a - b) < eps;
}

// Any vector which is orthogonal to v.
quater ortho(quater v)
{
    if(abs(v.qy) > eps)
        return vec(v.qy, -v.qx, 0);
    else
        return vec(v.qz, 0, -v.qx);
}

// Quaternion representing rotation from a to b via minimal angle.
quater min_rotation(quater a, quater b)
{
    a /= abs(a);
    b /= abs(b);
    if(cmp(a, -b)) // Degenerate case :(
        return rotation(ortho(b), pi);
    else
        return conj(a * (a + b));
}

// Angle of rotation from [-pi; pi] defined by quaternion.
ftype get_angle(quater a)
{
    a /= abs(a);
    return remainder(2 * acos(a.qs), 2 * pi);
}

// Quaternion representing rotation which turns nx in x, ny in y and [nx, ny] in z.
quater basis_rotation(quater nx, quater ny)
{
    nx /= abs(nx);
    ny /= abs(ny);
    quater a = min_rotation(nx, ex);
    ny = conj(ny, a);
    quater b = min_rotation(ny, ey);
    if(cmp(ny, -ey))
        b = rotation(ex, pi);
    return b * a;
}
Tags geometry, quaternions

Week of Code 23 on HackerRank

By adamant, 7 years ago, translation, In English
Hi everyone!

23rd edition of Week of Code will start soon. This time challenges were set by me (adamant) and tested by wanbo. Also thanks CherryTree for some help. This is first contest which entirely consists of my problems and I tried my best to make them interesting to you. Hope everyone will find at least one problem that matches ones taste :)

P. S. And some rules if you don't know them yet: Monday through Sunday, one new challenge will be unlocked each day for you to solve. The maximal score decreases by 10% at the end of every 24 hours. Your submissions will run on hidden test cases at the end of every 24 hours. Only the last submission time counts. And as usual, the top 10 hackers on the leaderboard win exclusive HackerRank T-shirts.

Tags w23, hackerrank, weekly, week of code

Math exercise

By adamant, history, 7 years ago, In English
is it true that ?

Tags math, fun

General ideas

By adamant, 7 years ago, translation, In English
// Finally translated!

Hi everyone!

Do you like ad hoc problems? I do hate them! That's why I decided to make a list of ideas and tricks which can be useful in mane cases. Enjoy and add more if I missed something. :)

1. Merging many sets in  amortized. If you have some sets and you often need to merge some of theme, you can do it in naive way but in such manner that you always move elements from the smaller one to the larger. Thus every element will be moved only  times since its new set always will be at least twice as large as the old one. Some versions of DSU are based on this trick. Also you can use this trick when you merge sets of vertices in subtrees while having dfs.

2. Tricks in statements, part 1. As you may know, authors can try to hide some special properties of input to make problem less obvious. Once I saw constraints like . Ha-ha, nice joke. It is actually .

3.  on subsegments. Assume you have set of numbers in which you add elements one by one and on each step calculate  of all numbers from set. Then we will have no more than  different values of gcd. Thus you can keep compressed info about all  on subsegments of :

code
4. From static set to expandable via . Assume you have some static set and you can calculate some function  of the whole set such that , where  is some function which can be calculated fast. For example,  as the number of elements less than  and . Or  as the number of occurences of strings from  into  and  is a sum again.

With additional  factor you can also insert elements into your set. For this let's keep  disjoint sets such that their union is the whole set. Let the size of  be either  or  depending on binary presentation of the whole set size. Now when inserting element you should add it to  set and rebuild every set keeping said constraint. Thus  set will tale  operations each  steps where  is the cost of building set over  elements from scratch which is usually something about . I learned about this optimization from Burunduk1.

5. -subsets. Assume you have set of numbers and you have to calculate something considering xors of its subsets. Then you can assume numbers to be vectors in -dimensional space over field  of residues modulo 2. This interpretation useful because ordinary methods of linear algebra work here. For example, here you can see how using gaussian elimination to keep basis in such space and answer queries of  largest subset xor: link. (PrinceOfPersia's problem from Hunger Games)

6. Cycles in graph as linear space. Assume every set of cycles in graph to be vector in -dimensional space over  having one if corresponding edge is taken into set or zero otherwise. One can consider combination of such sets of cycles as sum of vectors in such space. Then you can see that basis of such space will be included in the set of cycles which you can get by adding to the tree of depth first search exactly one edge. You can consider combination of cycles as the one whole cycle which goes through 1-edges odd number of times and even number of times through 0-edges. Thus you can represent any cycle as combination of simple cycles and any path as combination as one simple path and set of simple cycles. It could be useful if we consider pathes in such a way that going through some edge twice annihilates its contribution into some final value. Example of the problem: 724G - Xor-matic Number of the Graph. Another example: find path from vertex  to  with minimum xor-sum.

7. Mo's algorithm. Variant of sqrt-decomposition. Basic idea is that if you can do non-amortized insert of element in the set (i.e. having opportunity to revert it), then you can split array in sqrt blocks and consider queries such that their left ends lie in the same block. Then for each block you can add elements from its end to the end of the array. If you found some right end of query in that block you can add elements from the end of block to left end of query, answer the query since all elements are in the set and revert those changes then.

8. Dinic's algorithm in . This algorithm in  is very fast on the majority of testcases. But you can makes its asymptotic better by very few new lines of code. For this you should add scaling idea to your algorithm, i.e. you can iterate powers of 2 from k to 0 and while it is possible to consider only edges having capacity at least . This optimization gives you  complexity.

9. From expandable set to dynamic via . Assume for some set we can make non-amortized insert and calculate some queries. Then with additional  factor we can handle erase queries. Let's for each element x find the moment when it's erased from set. Thus for each element we will wind segment of time  such that element is present in the set during this whole segment. Now we can come up with recursive procedure which handles  time segment considering that all elements such that  are already included into the set. Now, keeping this invariant we recursively go into  and  subsegments. Finally when we come into segment of length 1 we can handle the query having static set. I learned this idea from Burunduk1, and there is a separate entry about it (on dynamic connectivity).

10. Linear combinations and matrices. Often, especially in dynamic programming we have to calculate the value wich is itself linear combination of values from previous steps. Something like . In such cases we can write  into the  matrix and use binary exponentiation. Thus we get  time instead of .

11. Matrix exponentiation optimization. Assume we have  matrix A and we have to compute  several times for different m. Naive solution would consume  time. But we can precalculate binary powers of A and use  multiplications of matrix and vector instead of matrix and matrix. Then the solution will be , which may be significant. I saw this idea in one of AlexanderBolshakov's comments.

12. Euler tour magic. Consider following problem: you have a tree and there are lots of queries of kind add number on subtree of some vertex or calculate sum on the path between some vertices. HLD? Damn, no! Let's consider two euler tours: in first we write the vertex when we enter it, in second we write it when we exit from it. We can see that difference between prefixes including subtree of v from first and second tours will exactly form vertices from v to the root. Thus problem is reduced to adding number on segment and calculating sum on prefixes. Kostroma told me about this idea. Woth mentioning that there are alternative approach which is to keep in each vertex linear function from its height and update such function in all v's children, but it is hard to make this approach more general.

13. Tricks in statements, part 2. If k sets are given you should note that the amount of different set sizes is  where s is total size of those sets. There is even stronger statement: no more than  sets have size greater than . Obvious example is when we are given several strings with total length s. Less obvious example: in cycle presentation of permutation there are at most  distinct lengthes of cycles. This idea also can be used in some number theory problems. For example we want calculat . Consider two groups: numbers less than  we can bruteforce and for others we can bruteforce the result of  And calculate how many numbers will have such result of division.
Another interesting application is that in Aho-Corasick algorithm we can consider pathes to the root in suffix link tree using only terminal vertices and every such path will have at most  vertices.

14. Convex hull trick. Assume we have dp of kind , then we can maintain convex hull of linear functions which we have here and find the maximum with ternary search.

15. xor-, and-, or-convolutions. Consider ring of polynomials in which  or  or . Just like in usual case  we can multiply such polynomials of size  in . Let's interpret it as polynomial from  variables such that each variable has power  ≤ 1 and the set of variables with quotient  is determined by binary presentation of . For example, instead of  we will consider the polynomial . Now note that if we consider values of this polynomial in the vertices of cube  then due to , we can see that product of such polynomials will use exactly xor rule in powers. or-convolution can be done in the same way considering vertices of  and having . and-convolution you can find yourself as an excercise.

xor-convolution
or-convolution
Finally I may note that or-convolution is exactly sum over all submasks and that inverse transform for xor-convolution is the same with initial one, except for we have to divide everything by n in the end. Thanks to Endagorion for explaining me such interpretation of Walsh-Hadamard transform.

16. FFT for two polynomials simultaneously. Let  be the polynomials with real quotients. Consider . Note that , thus .

Now backwards. Assume we know values of  and know they have real quotients. Calculate inverse FFT for . Quotients for A will be real part and quotients for B will be imaginary part.

17. Modulo product of two polynomials with real-valued FFT. If mod is huge we can lack accuracy. To avoid this consider  and calculate . Using the previous point it can be done in total of two forward and two backward FFT.

Easiest HLD with subtree queries

By adamant, 6 years ago, In English
Hi everyone!

tl;dr. If you write the following code:

void dfs_sz(int v = 0) {
    sz[v] = 1;
    for(auto &u: g[v]) {
        dfs_sz(u);
        sz[v] += sz[u];
        if(sz[u] > sz[g[v][0]]) {
            swap(u, g[v][0]);
        }
    }
}

void dfs_hld(int v = 0) {
    in[v] = t++;
    for(auto u: g[v]) {
        nxt[u] = (u == g[v][0] ? nxt[v] : u);
        dfs_hld(u);
    }
    out[v] = t;
}
Then you will have such array that subtree of  correspond to segment  and the path from  to the last vertex in ascending heavy path from  (which is ) will be  subsegment which gives you the opportunity to process queries on pathes and subtrees simultaneously in the same segment tree.


Inspired by Vladyslav's article and Alex_2oo8's problem Persistent Oak. You can find my solution to this problem here to learn how it can be used to maintain persistent hld over the tree.

Tags hld, euler tour, decomposition, hot lady decomposition

Add Policy Based Data Structures in the C++ standard

By adamant, history, 6 years ago, translation, In English
Hi everyone!

I suggested adding policy based data structures from SGI STL into C++ standard. You can know this useful library from here. What do you think of it?

Invitation to CodeChef October Lunchtime 2017!

By adamant, 6 years ago, translation, In English
Hello CodeForces Community!

The clock is ticking and Chef is ready with Lunchtime meal of October. So get ready to knuckle down October Lunchtime and mark your calendars for below. Joining me on the problem setting panel, we have:

Problem Setter: chemthan (Trung Nguyen)
Problem Tester & Editorialist: adamant (Alexander Kulkov)
Contest Admin: kingofnumbers (Hasan Jaddouh)
Russian Translator: CherryTree (Sergey Kulik)
Mandarin Translator: huzecong (Hu Zecong)
Vietnamese Translator: Team VNOI
I hope you will enjoy solving them. Please give your feedback on the problem set in the comments below after the contest.

So, note down the details and be there when the contest starts:

Time: 28th October 2017 (1930 hrs) to (2230 hrs). (Indian Standard Time — +5:30 GMT) — Check your timezone.

Details: https://www.codechef.com/LTIME53

Registration: You just need to have a CodeChef handle to participate. For all those, who are interested and do not have a CodeChef handle, are requested to register in order to participate.

Prizes: * Top 10 performers in Global and Indian category will get CodeChef laddus, with which the winners can claim cool CodeChef goodies. Know more here: https://www.codechef.com/laddu. (For those who have not yet got their previous winning, please send an email to winners@codechef.com)

Good Luck! Hope to see you participating!!

On Fast Fourier Transform

By adamant, history, 6 years ago, translation, In English
Hi everyone!

On this Saturday I'm giving a lecture on Fast Fourier Transform on Moscow International Workshop ACM ICPC. Due to this I wrote lecture notes which anybody can use as reference for Fast Fourier Transform. I added there almost anything one should need while using FFT in contests. Even if you suppose you know the algorithm I dare you to look this paper through since there still can be some new ideas for you. Also you can see Russian version here.

Tags fft, lecture, tutorial

Codeforces Round #453 (Div. 1 & Div. 2)

By adamant, 6 years ago, translation, In English
Hi everyone!

Missed me? I bet no! In either way, here goes another one round which I dared to spoil by my participation in problemsetting. Welcome to the world of extremely unoriginal problems, awkwardly long and boring statements and trifling jokes in anouncements.

This time round was prepared by Ildar Gainullin (300iq) and me (I'm ashamed to place link of such color here, you know who). We want to thank Vladislav Isenbaev (winger), Konstantin Semenov (zemen), Alexey Shmelev (ashmelev), Ivan Smirnov (ifsmirnov) and Alex Fetisov (AlexFetisov) for testing problems and help in preparation. Also special thanks goes to Nikolay Kalinin (KAN) for his help as coordinator and, of course, MikeMirzayanov for polygon and codeforces.

We hope that you will enjoy the problems, good luck and have fun!

UPD. Also thanks to akvasha for completing our problemset by his problem.

UPD 2. The contest is over, congratulations to winners!

Div. 1:

dotorya
Um_nik
Radewoosh
ainta
dreamoon_love_AA
Div. 2:

UoA_Kaori
noelnadal
mtw
Kroma
Yjsu
Here are editorials.

 Announcement of Codeforces Round 453 (Div. 1)
 Announcement of Codeforces Round 453 (Div. 2)

 On convex hull trick and e-maxx-eng

By adamant, history, 6 years ago, In English
Hi everyone!

Perhaps you heard about github project on translating e-maxx. The thing is that project is actually more than just translating it. You see, there are bunch of algorithms and approaches which either do not have proper elaborations or have but they're written in some weird uncommon languages like, you know, russian or chinese. And there are some sites which just don't fit for this purpose for some reasons.

Years ago when I started doing competitive programming e-maxx.ru was the main resource to learn things. Things changed a bit now. E-maxx is Russian only and it wasn't updated for years. And now I hope that e-maxx-eng will manage to fill this gap of common resource for everyone to learn new things and keep updated on recent competitive programming tricks and new algorithms.

So I encourage everyone to collaborate in making e-maxx-eng comprehensive guide into competitive programming, and not only on hacktoberfests :). And to begin with I would like to share with you my article on convex hull trick and Li Chao tree I wrote for this resource. Enjoy!

If you were too lazy to read it thoroughly: There is a link to CHT and Li Chao tree article just above this sentence!

Tags convex hull, e-maxx

Counting sums of powers

By adamant, history, 5 years ago, In English
Hi there! Imagine you're participating in codechef long challenge and you see a problem from chemthan asking you to calculate some sums of powers like 1p + 2p + ... + np for all p from 0 to k. You immediately understand what's going on here and take Faulhaber's formula from your wide pants, do ya? Just take a look at it!


Beautiful, right? Wrong! This formula is dumb and it's hard to understand and remember! Why would you ever think about Bernoulli's number on any contest which lasts less than a week? And what the hell are those Bernoulli numbers?! Here is what you should do:

Let Sp = 1p + ... + np. Consider its exponential generating function (EGF):


Now you can simply find S0, ..., Sk by finding inverse series for  and multiplying it with . Enjoy your power sums without Stirling and/or Bernoulli numbers!

Exercise: Solve this problem in .

P.S. Yes, you basically perform the same calculations as in Faulhaber's formula, but now you hopefully understand what you're doing.

Tags power

The history of some recurring problem

By adamant, history, 5 years ago, In English
Hi everyone! This one will be long and contain a lot of off-topic, prepare yourself (or skip down to solution of mentioned problem)!

Intro
In this blog post I would like to talk about some problem that somehow was on my mind for several years and yet only now I have some more or less complete understanding of how to deal with it. The problem is as follows:

You're given string S and q queries. In each query you have to count amount of distinct substrings of S[l, r].

Since then and for a long time this one was probably the hardest string problem I could ever imagine. In particular I saw some partial cases of it on several judges, which supported my assumption that problem is particularly tough. Some notable examples:

To Queue or not to Queue on codechef by Gerald. It is the same problem, but queries are formed as queue, i.e. you have to add letter to the right, delete letters from the left and write number of distinct substrings on each turn. This one is from year 2013. This particular problem can be solved in O(n) with some modification of Ukkonen's algorithm to make it work on a queue.
Chef and Substrings also on codechef by Gerald. This one asks for number of distinct substrings which start on position between l and r. Actual editorial for this one is presumably lost, so I'll appreciate if anyone can share which ideas were used in it. This one is also from year 2013. Meanwhile its constraints make me feel that intended solution had to run in , though it's possible to solve it in  which I will elaborate later on.
A problem on Petrozavodsk Summer Programming Camp 2017 asking about number of distinct subpalindromes of substring which also has somehow interesting story behind. I came up with  idea using eertree and some kind of Mo's Algorithm back in 2015 and shared it with droptable who invented  solution based purely on some scientific palindromic properties. I tried to propose this problem to csacademy then and waited for some feedback in good faith for roughly a year. I didn't receive one but later on Barcelona Programming Bootcamp I was approached by wefgef who said that he remembers that I sent a problem and gave me cool csacademy t-shirt which I oftenly wear now. Well, since then another half a year passed and after all problem was given in Petrozavodsk by droptable.
I also prepared partial case of this problem for Petrozavodsk Summer Camp 2015, MIPT contest. You can find it on timus: 1799. This one asks you to count distinct substrings on all edges of suffix tree and can be solved by compacted suffix automaton in O(n).
And finally there was a problem How many substrings? on hackerrank by SkyDec in 2016. It was unnoticed by me till year 2017 though when izban shared it with me and told that solution is some heavy link-cut tree stuff and that niyaznigmatul knows some better solution which I still don't know. I didn't really studied that solution and I wasn't satisfied since it was  and it used that link-cut tree stuff which I don't know. Up to that time I was coming back to that problem several times but couldn't come up with anything better than  so it still was better than anything I could imagine. So I just marked that the problem has some fast solution and didn't think about it afterwards.

After that came problem Sum of Squares of the Occurence Counts from XVIII Open Cup, Grand Prix of Korea. In this problem you have to sum up squares of number of occurences of all distinct substrings for all prefixes. I came up with heavy-light decomposition on suffix automaton solution, which I wrote down in corresponding comment. Later discussing this problem 300iq shared with me HLD-based solution for distinct substrings on segment, which was too hard for me to understand then.

And most recently I discussed that problem again with izban and 300iq, who suggested to me the problem which was stated as follows:

You're given strings s1, s2, s3 and q queries: how many distinct substrings you can obtain if you cancatenate some arbitrary substrings of s1[l1, r1], s2[l2, r2] and s3[l3, r3].

Which is from some unnamed asian contest if I understood it correctly. We discussed distinct substrings problem once again and during discussion came up with following approach which doesn't use any HLD and works... Well, still in .

Solution
Let's recall one of offline solutions for distinct elements on segment queries. You can group queries by their right end and then, at step r we will keep bi = 1 if i is the last occurrence of ai before r and 0 otherwise. Then you can count distinct numbers by summing up bi for i from l to r. Let's utilize this technique for our problem. To begin with we'll build suffix automaton for whole string beforehand. Remember that suffix link tree of suffix automaton is suffix tree of reversed string!

Now we will drop our string and add letters one by one to the end of string s, but using automaton and its suffix link tree for whole string. We will keep bi to be equal amount of substrings s[i, j] such that j ≤ r and i is the last position when s[i, j] occurs in s[1, r]. Thus to answer queries you'll have to simply sum up numbers from l to r. But how to maintain bi?

For which strings positions will be updated when you add new letter c to the end of s? Well, for all new suffixes, of course, so we'll add 1 to the whole string. Now we should find positions in which those of such strings occurred last time before that. You may note that in suffix link tree you will update position for all substrings on the path from current prefix of s to the root. So to find which substrings you will move you may say that you color path from v to root in color i and if you bump into vertex u colored in j, then it was colored when you were considering prefix s[1, j]. Thus you find next vertex w when the color will be changed (let its color be k) and you subtract 1 on segment [j - lenu, j - lenw). After this you'll repeat the same operation with j → k. In the end you'll fix some amount of segments on the path to the root colored in the wrong color. Each time you'll have to subtract 1 on corresponding segment and find next position in which color will be changed. To do this after each update you should place number i in vertex v and get maximum number in subtree of u, this will give you color of u. And finally you should keep array up_toi in which for color i you will keep first vertex in which path of this color from its initial vertex to the root was interrupted.

You will take only  amortized steps to get to the root when you update colors of vertices and spend  operations per each vertex thus complexity still will be  amortized but you don't have to use any heavy stuff like link-cut trees or heavy-light decomposition.

What, you ask why we will only take  steps to get to the root? Well, I lied a bit when I said that you're not going to use heavy-light decomposition. We will use it to prove this claim. You can do the following thing to recolor vertices. Keep in each light edge stack of colors which lie on its corresponding heavy path. Then when you come into this vertex you will remove some colors which are completely vanished and you will put you current color on top afterwards. Thus on each iteration you will put color in the stack at most  times, which provides amortized estimation for complexity  of number colors you'll have to change while traveling to the root.

This all could be done in pretty simple for cycle! (given that you already implemented segment tree and euler tour stuff):

code
Challenge
Can you improve this solution to ?

Kudos
And now since this blog is recollection of significant part of my experience with string algorithms, I would like to pay some special thanks to people who, probably not even realizing that, made essential contribution in me getting involved with all that beautiful stuff :)

I_love_natalia, for being that kind of weirdo who accepts friend requests from strangers, and answers ridiculous questions after half-year of silence from them. And for introducing concept of suffix automaton to me of course, since I were too stupid back then (and I am now) to learn things from e-maxx myself and not from other's words :) That's really awesome, most people I know would never waste their time on trying to explain such kind of basic but vast topic in comprehensive manner to literally some random guy from Internet.
Burunduk1 for this neat gym and great discussions on many algorithmic topics.
droptable for bringing to me the concept of eertree, it was kind of useful when I had too little understanding of suffix links.
YuukaKazami for Cyclical Quest. It was somehow the first problem that introduced this kind of stuff with suffix structures to me.
MinakoKojima for Stringology is Magic. It also was essential in my learning.
CherryTree for problems Substrings on tree which introduced generic automata on trie to me and Two Strings game which introduced concept of Alice and Bob playing with substrings of particular strings, I was inspired to make two, in my opinion, neat problems after this one. Problems are first and second.
Radewoosh for calling me his favorite blog writer. I decided to finish this article which was on hold for seven weeks after I read this comment again :D
Tags stringology is magic, tutorial, strings

Mindbun goes Telegram!

By adamant, history, 5 years ago, In English
Hi everyone!

As you may know, I'm an attention whore seeker. For this (well, not only this) reason year and almost a half ago I've created public page on vk.com: Зайчатки разума. Name is some old Russian semi-linguistic meme without adequate English translation, thus I decided to use 'Mindbun' as presumably closest to literal translation as English name. There was announce on codeforces, which didn't get any particularly positive or negative reaction. Or any reaction at all :)

On that page I mostly post maths things interesting to me and information of any similar activities by me on other places (like, here). It serves as some kind of public notebook to me. Why won't I just post stuff on codeforces, as before? Well, my tastes may be very specific and I'm greatly afraid that I will bother community by posting dozens of short posts on topics which only partially relevant to competitive programming and strongly imbalanced (on mentioned page there is like half of content is about polynomials and/or complex numbers). Thus I decided to throw my thoughts on some other channel and leave only 'big' content for codeforces.

And it was pretty successful as for me. VK page has 506 followers currently and I really enjoy making that stuff! You can see compilation of my older posts and newer ones (Russian, sorry!). So now I decided that it's good time for i18n. Thus I'm going to crosspost English versions of original VK public page in special telegram channel. Welcome! Also since I really like to chat with people, I also created Mindbun-related chat for anyone interested in it :)

And to give you some example of what's it like, I'll just provide you first post from the channel :)


P.S. Channel has not any content yet. I'll start by translating some newer posts from VK page, and will post any future entries there. I'd like to translate all the previous stuff, but there's vast of it and I really can't afford it right now, and I feel shy about some of my older posts :(

P.P.S. I would really appreciate your feedback on the following question: how should I inform on updates in Mindbun here? There are several variants I'm considering right now.

Post only big stuff on codeforces and keep notes to Mindbun only
Make some kind of digest with updates once a... Week? Month? Year? Random moment of time?..
Any other formats I haven't considered yet?

Elementary symmetric polynomial

By adamant, history, 5 years ago, In English
Hi there!

Consider the following problem: You're given set of n items with weights a1, ..., an. How many ways are there to select k items (order of choosing matters) with total weight of m (let's denote it as bm)? There are two main variants of the problem:

You may take any item arbitrary number of times. In this case bm = [xm](xa1 + ... + xan)k.
You may take each item exactly once. In this case bm = m![xmyk](1 + yxa1)...(1 + yxan)
First case is quite explicit and allows you to calculate answer in like  as .

But what about the second? If you define P(x) = xa1 + ... + xan and Qk(x) = b0 + b1x + b2x2 + ..., you may say for example that Q1(x) = P(x), Q2(x) = P2(x) - P(x2) or Q3(x) = P3(x) - 3P(x)P(x2) + 2P(x3) which allows to calculate Qk(x) for small k quickly. But does anybody know any fast way to calculate Qk(x)? Newton's identities seem to allow something like  if I'm not mistaken. Can anybody suggest any faster algorithm?

Tags polynomials

Oleksandr Kulkov Contest 1 in gym

By adamant, history, 5 years ago, In English
Hi everyone!

I gave a contest in winter Petrozavodsk programming camp and now I'd like to share it with codeforces by making it a contest in codeforces gym: 2018-2019 Зимние Петрозаводские сборы, Oleksandr Kulkov Contest 1. It was my first experience giving a contest to the camp and I'm pretty much excited about it!

In the camp only 7 out of 11 problems were solved, so there should be something in the contest for everyone. To make the contest more interesting I suggest you to participate in it as live contest on Saturday, 9 March, 12:00 (UTC+3), which may be changed in case of overlap with some other contest or if it's inconvenient for many participants. After this I suggest to gather here and discuss problems (if anyone's going to participate, of course). I will also post editorial which may (or may not) contain some neat stuff.

Uhm, good luck and have fun :)

P.S. It appears you already may see problems if you have coach mode enabled, I'd ask you to not do this unless you're not going to participate in contest!

UPD: Gentle reminder that it's less than 24 hours before the contest and it's hopefully not going to be rescheduled this time.

UPD 2: Thanks everyone for participating in the contest! Here are editorials: link

UPD 3: Google drive link got broken, so I uploaded the editorial to the contest directly.

 Announcement of 2018-2019 Winter Petrozavodsk Camp, Oleksandr Kulkov Contest 1
Tags gym, petrozavodsk

WTF or another complain on compilers

By adamant, history, 5 years ago, In English
Okay, so compare these two submissions: 51053654 and 51053605

The only difference is that first one made via GNU C++17 and the second one via MS C++ 2017. Code is same, but first gets RE 16 and second one gets AC.

WTF, GNU C++??

Operations on polynomials (on cp-algorithms)

By adamant, history, 5 years ago, In English
Hi there!

During preparation of Oleksandr Kulkov Contest 1 I started writing some template for polynomial algebra (because 3 problems in contest in one or another way required some polynomial operations). And with great pleasure I'd like to report that it resulted in this article on cp-algorithms.com (English translation for e-maxx) and this mini-library containing all mentioned operations and algorithms (except for Half-GCD algorithm). I won't say the code is super-optimized, but at least it's public, provides some baseline and is open for contribution if anyone would like to enhance it!

Article also provides some algorithms I didn't mention before. Namely:

Interpolation: Now the described algorithm is  and not  as it was before.
Resultant: Given polynomials A(x) and B(x) compute product of A(μi) across all μi being roots of B(x).
Half-GCD: How to compute GCD and resultants in  (just key ideas).
Feel free to read the article to know more and/or use provided code :)

tl;dr. article on operations with polynomials and implementation of mentioned algorithms.

Oleksandr Kulkov Contest 2 in gym

By adamant, history, 4 years ago, In English
Hi everyone!

This summer I gave another contest in summer Petrozavodsk programming camp and (although a bit lately) I want to share it with codeforces community by adding it to codeforces gym: 2018-2019 Summer Petrozavodsk Camp, Oleksandr Kulkov Contest 2. To make it more fun I scheduled it on Sunday, 5 january, 12:00 (UTC+3). Feel free to participate during scheduled time or, well, whenever you're up to. Good luck and have fun :)

Problems might be discussed here afterwards, I even may write some editorials for particular problems (per request, as I don't have them prepared beforehand this time).

UPD: 17h reminder before the start of the contest

UPD2: It wasn't an easy task to do, but I managed to add ghost participants to the contest! Enjoy!

 Announcement of 2018-2019 Summer Petrozavodsk Camp, Oleksandr Kulkov Contest 2
Tags gym, petrozavodsk

Recovering rational number from its remainder modulo huge integer

By adamant, 4 years ago, In English
Hi everyone!

It's been a while since I posted anything. Today I'd like to talk about problem I from Oleksandr Kulkov Contest 2. Well, on some similar problem. Problem goes as follows: There is a rational number 𝑥=𝑝𝑞
, and you know that 1≤𝑝,𝑞≤𝐶
. You want to recover 𝑝
 and 𝑞
 but you only know number 𝑟
 such that 𝑟≡𝑝𝑞−1(mod𝑚)
 where 𝑚>𝐶2
. In original problem 𝑚
 was not fixed, instead you were allowed to query remainders 𝑟1,…,𝑟𝑘
 of 𝑥
 modulo several numbers 𝑚1,…,𝑚𝑘
, which implied Chinese remainder theorem.

To solve this problem let's start with the observation that we always can recover answer uniquely. Indeed, assume that there are numbers 𝑝′
 and 𝑞′
 such that 1≤𝑝′,𝑞′≤𝐶
 and 𝑝′𝑞′−1≡𝑟(mod𝑚)
. That would mean that 𝑝𝑞′≡𝑝′𝑞(mod𝑚)
. But initial constraints claim that 𝐶2<𝑚
, thus this equation holds in integers as well, so 𝑝𝑞=𝑝′𝑞′
. There are two major ways to approach this problem now, one associated with Euclidean algorithm and the other associated with continued fractions.

Euclidean algorithm. Now since we know that any 𝑝,𝑞
 such that 1≤𝑝,𝑞≤𝐶
 and 𝑝𝑞−1≡𝑟(mod𝑚)
 would provide a solution, we can find some particular one by solving the following optimization problem:

𝑟𝑞(mod𝑚)→min,1≤𝑞≤𝐶
To solve it we should look on how 𝑞,2𝑞,3𝑞,…
 sequence behaves modulo 𝑚
. We may see that it actually can be split into several arithmetic progressions with step 𝑞
: we increase current number until it's not less than 𝑚
, that is 𝑘1𝑞≥𝑚
. At this point we subtract 𝑚
 from this number and obtain number 𝑘1𝑞−𝑚
. Since this number is not greater than 𝑞
 (otherwise we'd subtract 𝑚
 on the previous step) we may say that this number is equal to 𝑞′≡−𝑚(mod𝑞)
. Next time we subtract 𝑚
 happens when 𝑘2𝑞≥2𝑚
 and we obtain 𝑘2𝑞−2𝑚
, which is same as 2𝑞′≡−2𝑚(mod𝑞)
. Let's draw 𝑘𝑞
 on the grid where 𝑦
 axis stands for ⌊𝑘𝑞𝑚⌋
 and 𝑥
 axis stands for 𝑘𝑞(mod𝑚)
:


You can clearly see that while the main progression has a step 3
 modulo 7
, there is also a progression of starting points with step −1
 modulo 3
. Since we look for the minimum element, we're only interested in starting points, thus we reduced the problem to this:

−𝑘𝑚(mod𝑞)→min,1≤𝑘𝑚≤𝐶𝑟
Wow, it looks like 𝑞
 and 𝑚
 just swapped and we now may deal with 𝑚(mod𝑞)
 as in Euclidean algorithm! Right? Err, not exactly. We switched from (𝑚,𝑟)
 pair to (𝑟,−𝑚mod𝑟)
, while in Euclidean algorithm transition is to (𝑟,𝑚mod𝑟)
. And this is the problem here, as (𝑟,−𝑚mod𝑟)
 transition can't always guarantee logarithmic amount of steps. Thus, we need to make (𝑟,𝑚mod𝑟)
 transition. Luckily it turns out to be pretty simple as instead of minimizing problem we can solve maximizing problem:

𝑘𝑚(mod𝑞)→max,1≤𝑘𝑚≤𝐶𝑟
And then return the result subtracted from 𝑞
. This makes proper transition and the maximization problem may be reduced to the minimization one in the similar manner. In the end it all may be written in a pretty short code piece:

int min_rem(int m, int r, int c) {
	if(c < 1) {
		return inf;
	} else if(r == 0) {
		return 0;
	} else {
		int step = m % r;
		int mx = c * r / m;
		int t = max_rem(r, step, mx);
		return r - t;
	}
}
 
int max_rem(int m, int r, int c) {
	if(r == 0 || c <= m / r) {
		return r * c;
	} else {
		int step = m % r;
		int mx = (c + 1) * r / m;
		int t = min_rem(r, step, mx);
		return m - t;
	}
}
Continued fractions. We have to find the solution of 𝑟𝑞−𝑝=𝑘𝑚
 with 𝑝,𝑞≤𝐶
. Let's look on continued fraction for 𝑟𝑚
 and its convergents 𝑝1𝑞1,𝑝2𝑞2,…,𝑝𝑛𝑞𝑛
. It's known from continued fraction theory that if ∣∣𝑟𝑚−𝑃𝑄∣∣<12𝑄2
 then 𝑃𝑄
 is one of convergents for 𝑟𝑚
. If we rewrite this inequality, we would obtain something more familiar:

|𝑟𝑄−𝑃𝑚|<𝑚2𝑄
In our case we know that there are 𝑝,𝑞
 such that 𝑞≤𝐶
 and 𝑟𝑞−𝑘𝑚=𝑝≤𝐶<𝑚𝐶≤𝑚𝑞
. Thus if we strengthen the constraint to be 𝑚>2𝐶2
 it would be safe to assume that 𝑞
 is always the denominator of some 𝑟𝑚
 convergent! Note that here 𝑚>2𝐶2
 bound matters, as there are 𝑝,𝑞
 pairs for which you won't find solution restricting to 𝑚>𝐶2
 only, so this solution is less generic.

The solution here is a bit sketchy as I only heard some continued fraction solution in Petrozavodsk and tried to recover it here using basic facts about continued fractions from Wikipedia, sorry for that. There may also be 𝑀>𝐶2
 solution for this case, but I couldn't figure it out.

P.S. I'd like to write some comprehensive article about continued fractions, but I need to solve some problems on it first. I would highly appreciate if you suggest some of those in the comments! Also feel free to share if you have any other solutions to this problem.

UPD: Very similar problem was also set by dreamoon_love_AA: link.

Tags #number theory, #tutorial, continued fraction

On continued fractions. Part 1: Introduction

By adamant, 4 years ago, In English
Hi everyone!

After writing this article I've decided to write another one being comprehensive introduction into continued fractions for competitive programmers. I'm not really familiar with the topic, so I hope writing this entry will be sufficient way to familiarize myself with it :)

Part 1: Introduction
Part 2: Properties and interpretation

Definitions. To begin with, any rational number 𝑟=𝑝𝑞
 may be uniquely represented as a finite nested fraction of the following kind:


𝑟=𝑎0+1𝑎1+1𝑎2+…
Where 𝑎0
 is integer number and 𝑎1,𝑎2,…,𝑎𝑛
 are positive integer numbers and either 𝑛=0
 or 𝑎𝑛≠1
. It may be written shortly as 𝑟=[𝑎0,𝑎1,…,𝑎𝑛]
. Let's investigate it a bit. In the given constraints, [𝑎1,𝑎2,…,𝑎𝑛]
 is either absent or it is greater than 1
, which means that 𝑟−𝑎0<1
. Given that 𝑎0
 is integer it allows us to conclude that 𝑎0=⌊𝑟⌋
. Next numbers in the sequence may be found from:


1𝑟−𝑎0=[𝑎1,𝑎2,…,𝑎𝑛]
Since 𝑎0=⌊𝑝𝑞⌋
 we can write that 𝑟−𝑎0=𝑝−⌊𝑝/𝑞⌋⋅𝑞𝑞=𝑝mod𝑞𝑞
, so if 𝑝𝑞=[𝑎0,𝑎1,…,𝑎𝑛]
 then 𝑞𝑝mod𝑞=[𝑎1,𝑎2,…,𝑎𝑛]
. It provides us with important observation that if we have rational number defined by the pair (𝑝,𝑞)
 we may recursively reduce the computation of its nested fraction coefficients to the case (𝑞,𝑝mod𝑞)
, which is the same transition we do in Euclidean algorithm.

Convergents. Let's analyze the sequence 𝑟0,𝑟1,…,𝑟𝑛
 such that 𝑟𝑘=[𝑎0,𝑎1,…,𝑎𝑘]=𝑝𝑘𝑞𝑘
. Its elements are called convergents. Let's look at some explicit formulas for initial convergents, given that we know 𝑎0,𝑎1,…
:


𝑟0=𝑎01,𝑟1=𝑎0+1𝑎1=𝑎0𝑎1+1𝑎1,𝑟2=𝑎0+𝑎2𝑎1𝑎2+1=𝑎0𝑎1𝑎2+𝑎0+𝑎2𝑎1𝑎2+1.
We may see some patterns, for example 𝑟𝑘=𝑎0+1𝑠𝑘−1
 where 𝑠𝑘−1
 is the (𝑘−1)
-th convergent of [𝑎1,…,𝑎𝑛]
. Other important observation is that numerators and denominators of 𝑟𝑘
 are polynomials of 𝑎0,…,𝑎𝑘
. Let 𝑃𝑘(𝑥0,…,𝑥𝑘)
 be the polynomial corresponding to the numerator of 𝑟𝑘
-th numerator, for example:


𝑃0(𝑥0)=𝑥0,𝑃1(𝑥0,𝑥1)=𝑥0𝑥1+1,𝑃2(𝑥0,𝑥1,𝑥2)=𝑥0𝑥1𝑥2+𝑥0+𝑥2.
Since 𝑟𝑘=𝑎0+1𝑠𝑘−1
 we may conclude that denominator of 𝑟𝑘
 is the numerator of 𝑠𝑘−1
, therefore:


𝑟𝑘=𝑃𝑘(𝑎0,𝑎1,…,𝑎𝑘)𝑃𝑘−1(𝑎1,𝑎2,…,𝑎𝑘)
Substituting it into 𝑟𝑘=𝑎0+1𝑠𝑘−1
 we obtain the explicit recurrent formula:


𝑟𝑘=𝑎0+1𝑠𝑘−1=𝑎0+𝑃𝑘−2(𝑎2,𝑎3,…,𝑎𝑘)𝑃𝑘−1(𝑎1,𝑎2,…,𝑎𝑘)=𝑎0𝑃𝑘−1(𝑎1,𝑎2,…,𝑎𝑘)+𝑃𝑘−2(𝑎2,𝑎3,…,𝑎𝑘)𝑃𝑘−1(𝑎1,𝑎2,…,𝑎𝑘)
Thus, the final formula for 𝑃𝑘(𝑎0,𝑎1,…,𝑎𝑘)
 is as follows:


𝑃𝑘(𝑎0,𝑎1,…,𝑎𝑘)=𝑎0𝑃𝑘−1(𝑎1,𝑎2,…,𝑎𝑘)+𝑃𝑘−2(𝑎2,𝑎3,…,𝑎𝑘)
To make this formula work for 𝑘=0
 it's convenient to put 𝑃−1=1
 and 𝑃−2=0
, formally assuming 𝑟−1=10
. Though formula is not very convenient yet as we would rather like to use it to calculate numerators of 𝑟𝑘
 knowing numerators for 𝑟𝑘−1
, 𝑟𝑘−2
 and so on. To do this we should notice that 𝑃𝑘(𝑎0,𝑎1,…,𝑎𝑘)=𝑃𝑘(𝑎𝑘,𝑎𝑘−1,…,𝑎0)
. It may be proven by induction as it holds for 𝑃1
, 𝑃0
, 𝑃−1
 and 𝑃−2
.


Formal proof
Summary. Right now we obtained far more convenient formula for 𝑃𝑘(𝑎0,𝑎1,…,𝑎𝑘)
:


𝑃𝑘(𝑎0,𝑎1,…,𝑎𝑘)=𝑎𝑘𝑃𝑘−1(𝑎0,𝑎1,…,𝑎𝑘−1)+𝑃𝑘−2(𝑎0,𝑎1,…,𝑎𝑘−2)
Which means that consequent convergents may be derived from one another using simple recurrent formula:


𝑝−1𝑞−1=10,𝑝0𝑞0=𝑎01,𝑝𝑘𝑞𝑘=𝑎𝑘𝑝𝑘−1+𝑝𝑘−2𝑎𝑘𝑞𝑘−1+𝑞𝑘−2
These formulas were derived by Leonhard Euler, numerator polynomial 𝑃𝑘(𝑥0,𝑥1,…,𝑥𝑘)
 is known as the continuant and has some other peculiar properties on its own. Most notably, it can be explicitly written as the determinant of tridiagonal matrix:


𝑃𝑘(𝑥0,𝑥1,…,𝑥𝑘)=det⎡⎣⎢⎢⎢⎢⎢⎢𝑥0−10⋮01𝑥1−1⋮001𝑥2⋅………⋅⋱−100⋮1𝑥𝑘⎤⎦⎥⎥⎥⎥⎥⎥
Which makes immediate explanation for why it's invariant under the reverse of arguments.

To be continued... I feel like article is long enough already and it has an outline of some key results to begin with, so I decided to cut it on this point. In further parts I'd like to write more about connection between continued fractions, Euclidean algorithms and number theory as well as their importance for Diophantine approximations, so stay tuned and thanks for reading!

P.S. If you were interested in the topic, you may read about one of its applications right away in this article about how one may recover rational number from its remainder modulo some huge number using continued fractions. There is also another article on Pell's equation by, LieutenantLolicon which heavily utilizes continued fractions. Also, it would be great if anyone may suggest some other competitive programming problems utilizing continued fractions to highlight them in future articles.

Tags continued fraction, tutorial

On continued fractions. Part 2: Properties and interpretation

By adamant, history, 4 years ago, In English
Hi everyone!

Let's continue with learning continued fractions. We began with studying the case of finite continued fractions and now it's time to work a bit with an infinite case. It turns out that while rational numbers have unique representation as a finite continued fraction, any irrational number has unique representation as an infinite continued fraction.

Part 1: Introduction
Part 2: Properties and interpretation

Distance between convergents. In the first part we learned that for continued fraction 𝑟=[𝑎0,𝑎1,…,𝑎𝑛]
 elements of the sequence 𝑟0,𝑟1,…,𝑟𝑛
 where 𝑟𝑘=[𝑎0,𝑎1,…,𝑎𝑘]
 are called convergents. We also derived that subsequent convergents obey a simple formula:


𝑝−2𝑞−2=01,𝑝−1𝑞−1=10,𝑝𝑘𝑞𝑘=𝑎𝑘𝑝𝑘−1+𝑝𝑘−2𝑎𝑘𝑞𝑘−1+𝑞𝑘−2
Subsequent convergents seemingly should be closer to 𝑟
 with bigger 𝑘
, culminating in 𝑟𝑛=𝑟
. So, let's find some bound on how far away from 𝑟
 can 𝑟𝑘
 be. We may start with looking on the difference between 𝑟𝑘
 and 𝑟𝑘−1
:


𝑝𝑘𝑞𝑘−𝑝𝑘−1𝑞𝑘−1=𝑝𝑘𝑞𝑘−1−𝑝𝑘−1𝑞𝑘𝑞𝑘𝑞𝑘−1
Let's rewrite the numerator of this fraction:


𝑝𝑘𝑞𝑘−1−𝑝𝑘−1𝑞𝑘=(𝑎𝑘𝑝𝑘−1+𝑝𝑘−2)𝑞𝑘−1−𝑝𝑘−1(𝑎𝑘𝑞𝑘−1+𝑞𝑘−2)=𝑝𝑘−2𝑞𝑘−1−𝑝𝑘−1𝑞𝑘−2
Which means that numerator of 𝑟𝑘−𝑟𝑘−1
 is the opposite of numerator of 𝑟𝑘−1−𝑟𝑘−2
. The base case here is defined by 𝑝0𝑞−1−𝑝−1𝑞0
, which is equal to −1
. In this way we may conclude that 𝑝𝑘𝑞𝑘−1−𝑝𝑘−1𝑞𝑘=(−1)𝑘−1
, thus the whole difference is written as follows:


𝑟𝑘−𝑟𝑘−1=(−1)𝑘−1𝑞𝑘𝑞𝑘−1
Further we will also need to know the distance between 𝑟𝑘+1
 and 𝑟𝑘−1
:


𝑟𝑘+1−𝑟𝑘−1=(−1)𝑘𝑞𝑘+1𝑞𝑘+(−1)𝑘−1𝑞𝑘𝑞𝑘−1=(−1)𝑘−1(𝑞𝑘+1−𝑞𝑘−1)𝑞𝑘+1𝑞𝑘𝑞𝑘−1=(−1)𝑘−1𝑎𝑘+1𝑞𝑘+1𝑞𝑘−1
Approximation properties. Formulas above allow us to represent number 𝑟=𝑟𝑛
 as explicit telescopic sum (given that 𝑟0=𝑎0
):


𝑟=(𝑟𝑛−𝑟𝑛−1)+(𝑟𝑛−1−𝑟𝑛−2)+⋯+(𝑟1−𝑟0)+𝑟0=𝑎0+∑𝑘=1𝑛(−1)𝑘+1𝑞𝑘𝑞𝑘−1
Since 𝑞𝑘
 monotonically increases, we may say that 𝑞𝑘+1𝑞𝑘>𝑞𝑘𝑞𝑘−1
, thus we have an alternating sum which terms are decreasing by absolute value. This fact automatically implies several important properties, which may be illustrated by the picture below:





In our specific case difference between subsequent convergents quickly drops:


∣∣𝑟𝑘+1−𝑟𝑘𝑟𝑘−𝑟𝑘−1∣∣=𝑞𝑘−1𝑞𝑘+1≤𝑞𝑘−1𝑞𝑘+𝑞𝑘−1≤12
Consequent convergents maintain lower and upper bound on possible values of 𝑟
 and each summand makes one of bounds more accurate. Bound above means that segment of possible 𝑟
 is at least halved on each step. Few important corollaries follow from this:

Convergents with even indices are lower than 𝑟
 while convergents with odd indices are greater than 𝑟
,
If indices 𝑗
 and 𝑖
 have different parity then:
|𝑟𝑗−𝑟𝑖|=|𝑟𝑖−𝑟|+|𝑟𝑗−𝑟|
If indices 𝑗
 and 𝑖
 have same parity and 𝑗>𝑖
 then:
|𝑟𝑗−𝑟𝑖|=|𝑟𝑖−𝑟|−|𝑟𝑗−𝑟|
If 𝑗>𝑖
, then |𝑟𝑖−𝑟|≥|𝑟𝑗−𝑟|
.
These observations provide us convenient means of estimating how close is 𝑟𝑘
 to 𝑟
:


1(𝑞𝑘+1+𝑞𝑘)𝑞𝑘≤𝑎𝑘+1𝑞𝑘+2𝑞𝑘=|𝑟𝑘+2−𝑟𝑘|≤|𝑟𝑘−𝑟|≤|𝑟𝑘+1−𝑟𝑘|=1𝑞𝑘𝑞𝑘+1≤1𝑞2𝑘
So, the final "pretty" bounds on the distance between 𝑟𝑘
 and 𝑟
 may be written as follows:


∣∣𝑝𝑘𝑞𝑘−𝑟∣∣≤1𝑞2𝑘
Infinite fractions. Now that we had a proper introduction of finite continued fractions, we may introduce infinite fractions [𝑎0,𝑎1,𝑎2,…]
 as well. We say that 𝑟
 is represented by such infinite fraction if it's the limit of its convergents:


𝑟=lim𝑛→∞𝑟𝑛=𝑎0+∑𝑛=1∞(−1)𝑛−1𝑞𝑛𝑞𝑛−1
Properties above guarantee that this series is convergent, and there is an algorithm to calculate subsequent 𝑎𝑘
, which altogether means that there is one-to-one correspondence between irrational numbers and infinite continued fractions. Some well-known constants, like golden ration, have "pretty" continued fraction representations:


𝜑=[1,1,1,1,1,…],2‾√=[1,2,2,2,2,…],𝑒=[2,1,2,1,1,4,1,1,6,…,1,1,2𝑘,1,1,…]
It is also worth noting that continued fraction is periodic if and only if 𝑟
 is quadratic irrational, that is 𝑟
 is the solution of some quadratic algebraic equation. In terms of convergence infinite fractions also hold most of properties of finite fractions.

Geometric interpretation. Consider sequence 𝑠𝑘=(𝑞𝑘;𝑝𝑘)
 on 2-dimensional space. Each point 𝑠𝑘
 corresponds to convergent 𝑝𝑘𝑞𝑘
 which is the slope coefficient of the vector from (0;0)
 to (𝑞𝑘;𝑝𝑘)
. As was mentioned earlier, convergent coefficients are determined by formula:


𝑝𝑘𝑞𝑘=𝑎𝑘𝑝𝑘−1+𝑝𝑘−2𝑎𝑘𝑞𝑘−1+𝑞𝑘−2
Geometrically it means that 𝑠𝑘=𝑎𝑘𝑠𝑘−1+𝑠𝑘−2
. Let's recall how coefficients 𝑎𝑘
 are obtained. Let 𝑏0,𝑏1,…,𝑏𝑛
 be the states of 𝑟
 at each step. Thus, we initially have 𝑏0=𝑟
 and then for each 𝑏𝑘
 we calculate 𝑎𝑘=⌊𝑏𝑘⌋
 and say that 𝑏𝑘+1=1𝑏𝑘−𝑎𝑘
. So, for example:


𝑏0=𝑟1,𝑏1=1𝑟−𝑎0,𝑏2=11𝑟−𝑎0−𝑎1=𝑟−𝑎01+𝑎0𝑎1−𝑎1𝑟,𝑏3=1𝑟−𝑎01−𝑎1𝑟+𝑎0𝑎1−𝑎2=1+𝑎0𝑎1−𝑎1𝑟𝑟(1+𝑎1𝑎2)−(𝑎0+𝑎2+𝑎0𝑎1𝑎2)
If you look closely, you may see that the numerator of 𝑏𝑘
 is the denominator of 𝑏𝑘−1
 and the denominator of 𝑏𝑘
 is given by:


𝑄𝑘(𝑎0,𝑎1,…,𝑎𝑘−1,𝑟)=(−1)𝑘(𝑃𝑘−1(𝑎0,𝑎1,…,𝑎𝑘−1)−𝑟𝑃𝑘−2(𝑎1,𝑎2,…,𝑎𝑘−1))=(−1)𝑘(𝑝𝑘−1−𝑟𝑞𝑘−1)
Thus, the whole number 𝑏𝑘
 may be defined as follows:


𝑏𝑘=∣∣𝑝𝑘−2−𝑟𝑞𝑘−2𝑝𝑘−1−𝑟𝑞𝑘−1∣∣=𝑞𝑘−1𝑞𝑘−2∣∣𝑟𝑘−2−𝑟𝑟𝑘−1−𝑟∣∣
The meaning of this coefficient is as follows: Consider line 𝑦=𝑟𝑥
. As convergents approach 𝑟
, slope coefficient of (𝑞𝑘;𝑝𝑘)
 approaches 𝑟
 as well and thus points (𝑞𝑘;𝑝𝑘)
 lie closer and closer to this line. That being said, value |𝑝𝑘−𝑟𝑞𝑘|
 tells us how far is (𝑞𝑘;𝑝𝑘)
 from 𝑦=𝑟𝑥
 on the vertical line 𝑥=𝑞𝑘
. Note that 𝑠𝑘−1
 and 𝑠𝑘−2
 are always on opposite sides from line 𝑦=𝑟𝑥
, so adding 𝑠𝑘−1
 to 𝑠𝑘−2
 will decrease vertical distance of 𝑠𝑘−2
 by |𝑝𝑘−1−𝑟𝑞𝑘−1|
. In this way we can see that 𝑎𝑘=⌊𝑏𝑘⌋
 is nothing but the maximum amount of times we may add 𝑠𝑘−1
 to 𝑠𝑘−2
 without switching the sign of 𝑝𝑘−2−𝑟𝑞𝑘−2
. This gives geometric meaning to 𝑎𝑘
 and the algorithm of computing them:


Initially we have vectors 𝑠−2=(1;0)
 and 𝑠−1=(0;1)
 lying on different sides from 𝑦=𝑟𝑥
. On 𝑘
-th step we compute 𝑎𝑘
 to be the maximum amount of times we may add 𝑠𝑘−1
 to 𝑠𝑘−2
 without switching side. Then we say that 𝑠𝑘=𝑎𝑠𝑘−1+𝑠𝑘−2
 and, therefore:

𝑝𝑘𝑞𝑘=𝑎𝑘𝑝𝑘−1+𝑝𝑘−2𝑎𝑘𝑞𝑘−1+𝑞𝑘−2
Algorithm above was taught to Vladimir Arnold by Boris Delaunay who called it "the nose stretching algorithm". This algorithm was initially used to find all lattice points below the line 𝑦=𝑟𝑥
 but also turned out to provide simple geometric interpretation of continued fractions.

This connection between continued fractions and 2d-geometry established the notion of mediants. That is, for fractions 𝐴=𝑎𝑏
 and 𝐵=𝑐𝑑
 their mediant is 𝐶=𝑎+𝑐𝑏+𝑑
. If 𝐴
 and 𝐵
 are slope coefficients of two vectors, then 𝐶
 is the slope coefficient of the sum of these vectors.

That should be all for now, next time I hope to write in more details about how the nose stretching algorithm may be used to count lattice points under the given line without that much pain and to solve integer linear programming in 2D case. Also I'm going to write about how continued fractions provide best rational approximations and, hopefully, write more about competitive programming problems where this all may be used, so stay tuned!

Tags continued fraction, tutorial

Discrete Fourier Transform eigenvalues

By adamant, history, 3 years ago, In English
Hi everyone!

Long time no see. 3 years ago I announced a Telegram channel. Unfortunately, for the last ~1.5 years I had a total lack of inspiration for new blog posts. Well, now I have a glimpse of it once again, so I want to continue writing about interesting stuff. Here's some example:

Let :ℂ𝑛↦ℂ𝑛
 be the discrete Fourier transform:

(𝑎)𝑘=1𝑛√∑𝑗=0𝑛−1𝜔𝑗𝑘𝑛𝑎𝑗,
where 𝜔𝑛=𝑒2𝜋𝑖𝑛
 is the 𝑛
-th complex root of unity. Alse let :ℂ𝑛↦ℂ𝑛
 be the reverse operator:

(𝑎)𝑘=𝑎(𝑛−𝑘)mod𝑛.
It is a common fact that double discrete transform yields reverse: 2𝑎=𝑎
. Indeed,

(2𝑎)𝑘=1𝑛∑𝑗1=0𝑛−1𝜔𝑗1𝑘𝑛∑𝑗2=0𝑛−1𝜔𝑗2𝑗1𝑛𝑎𝑗2=1𝑛∑𝑗2=0𝑛−1𝑎𝑗2∑𝑗1=0𝑛−1𝜔𝑗1(𝑘+𝑗2)𝑛=∑𝑗2=0𝑛−1𝑎𝑗2𝛿𝑛(𝑘+𝑗2)=𝑎(𝑛−𝑘)mod𝑛,
where 𝛿𝑛(𝑘)
 is equal to 1
 if 𝑘≡0(mod𝑛)
 and to 0
 otherwise. It is obtained as a power sum:

𝑛𝛿𝑛(𝑘)=∑𝑗=0𝑛−1𝜔𝑗𝑘𝑛={𝑛1−𝜔𝑘𝑛𝑛1−𝜔𝑘𝑛=0, 𝑘≡0(mod𝑛), otherwise
This allows us to understand that 4=𝐼
 and eigenvalues of 
 are 1,𝑖,−1,−𝑖
. But what are their multiplicities? A common way to explore matrix's eigenvalues is to look on tr
, tr2
, …
, tr𝑛
:

tr𝑘=𝜆𝑘1+⋯+𝜆𝑘𝑛,
so the sequence tr,…,tr𝑛
 uniquely determines the eigenvalues 𝜆1,…,𝜆𝑛
. In our case (Mehta, 1986),

tr𝑘=1+∑𝑗=2𝑛(−𝑖)𝑘𝑗,
thus eigenvalues of 
 are 1
, (−𝑖)2
, (−𝑖)3
, …
, (−𝑖)𝑛
. Multiplicities of roots of unity here:

⌊𝑛+44⌋ for 1,⌊𝑛+24⌋ for −1,⌊𝑛+14⌋ for 𝑖,⌊𝑛−14⌋ for −𝑖.
 
As you could guess, this is a "translation" to Codeforces markup of the new post in aforementioned Telegram channel. When the channel was created, I was thinking that it would be for the better, as it better fits for some less related to competitive programming and/or less elaborate posts and I wanted to avoid spamming Codeforces' recent actions.

But well, it turns out I won't produce as many posts as I thought anyway, so posting all of them directly to Codeforces can be an option as well. So, I'll look on how such "example" post would be received here and will probably continue writing like this if people like it. As long, as I have some inspiration :)

P. S. Some time ago I wrote an article about Suffix automaton which was selected as featured article on Russian Wikipedia and was later translated to English Wikipedia as well. I'm really proud of this work and think that it is a very comprehensive reading about theoretical bases of this suffix structure. Feel free to enjoy the reading if, by any chance, the topic is as interesting to you as it is to me.

Tags fast fourier transform, linear algebra, a-blog-without-purpose, mindbun

Associativity and general identity testing

By adamant, history, 3 years ago, In English
Hi everyone!

Five days have passed since my previous post which was generally well received, so I'll continue doing posts like this for time being.

Abstract algebra is among my favorite subjects. One particular thing I find impressive is the associativity of binary operations. One of harder problems from my contests revolves around this property as you need to construct an operation with a given number of non-associative triples. This time I want to talk about how one can check this property for both groups and arbitrary magmas.

First part of my post is about Light's associativity test and how it can be used to deterministically check whether an operation defines a group in 𝑂(𝑛2log𝑛)
. Second part of the post is about Rajagopalan and Schulman probabilistic identity testing which allows to test associativity in 𝑂(𝑛2log𝑛log𝛿−1)
 where 𝛿
 is error tolerance. Finally, the third part of my post is dedicated to the proof of Rajagopalan–Schulman method and bears some insights into identity verification in general and higher-dimensional linear algebra.

For convenience, these three parts are separated by horizontal rule.

Consider a set 𝐺
 with |𝐺|=𝑛
 and an operation ⋅:𝐺×𝐺↦𝐺
 defined by its Cayley table (that is, (𝐺,⋅)
 is a finite magma).

You need to check if (𝑎⋅𝑏)⋅𝑐=𝑎⋅(𝑏⋅𝑐)
 for all 𝑎,𝑏,𝑐∈𝐺
 (that is, if it is a semigroup).

Until 1996 there were no known sub-𝑛3
 algorithms for it. In 1949 Alfred Clifford and Gordon Preston published Light's associativity test:

For 𝑔∈𝐺
 define operations 𝑥∘𝑔𝑦=(𝑥⋅𝑔)⋅𝑦
 and 𝑥⋆𝑔𝑦=𝑥⋅(𝑔⋅𝑦)
.

⋅
 is associative if and only if ∘𝑔∼⋆𝑔
 for every 𝑔∈𝐺
. Checking it for all 𝑔∈𝐺
 would take 𝑂(𝑛3)
 but if 𝐺
 has generating set 𝑆
 it is enough to test the equivalence only for 𝑔∈𝑆
, which follows from the fact that ∘𝑎=⋆𝑎
 and ∘𝑏=⋆𝑏
 implies ∘𝑎⋅𝑏=⋆𝑎⋅𝑏
:

𝑥⋅((𝑎⋅𝑏)⋅𝑦)=𝑥⋅(𝑎⋅(𝑏⋅𝑦))=(𝑥⋅𝑎)⋅(𝑏⋅𝑦)=((𝑥⋅𝑎)⋅𝑏)⋅𝑦=(𝑥⋅(𝑎⋅𝑏))⋅𝑦
Some magmas have 𝑛
 elements in the generating set, for example 𝑎⋅𝑏=max(𝑎,𝑏)
. But if for every 𝑎,𝑏∈𝐺
 there exists unique 𝑥∈𝐺
 such that 𝑥⋅𝑎=𝑏
 (such 𝐺
 are called quasigroups) then 𝐺
 has a generating set of at most ⌊log2𝑛⌋+1
 elements:

Let 𝑇⊂𝐺
 be such that ⟨𝑇⟩≠𝐺
 where ⟨𝑇⟩
 is the closure of 𝑇
 under ⋅
 and let 𝑏∈𝐺∖⟨𝑇⟩
. Due to quasigroup properties, all elements 𝑏⋅𝑎
 where 𝑎∈⟨𝑇⟩
 are unique and do not occur in ⟨𝑇⟩
, which means that |⟨𝑇∪
 {𝑏
}⟩|≥2|⟨𝑇⟩|
, thus adding new elements to non-empty 𝑇
 until ⟨𝑇⟩=𝐺
 is only possible at most log2𝑛
 times.

It is possible to test the magma for being a quasigroup in 𝑂(𝑛2)
, thus the whole associativity testing with Light's test can be done in 𝑂(𝑛2log𝑛)
 if both conditions need to be present. In particular, every group is also a quasigroup, thus it is possible to deterministically check if given magma is a group in 𝑂(𝑛2log𝑛)
.

What if we want to check associativity in arbitrary magma? Probabilistic solution to this was proposed by Sridhar Rajagopalan and Leonard Schulman in 1996. They consider group ring ℤ2[𝐺]
 which elements are formal sums ∑𝑔∈𝐺𝑎𝑔𝑔
 where 𝑎𝑔∈ℤ2
. In this ring:

𝐴+𝐵=∑𝑔∈𝐺(𝑎𝑔⊕𝑏𝑔)𝑔
 where 𝑎𝑔⊕𝑏𝑔
 is an xor of 𝑎𝑔
 and 𝑏𝑔
;
𝐴⋅𝐵=∑𝑔∈𝐺(⨁𝑥⋅𝑦=𝑔𝑎𝑥𝑏𝑦)𝑔
 where 𝑎𝑔𝑏𝑔
 is a conjunction of 𝑎𝑔
 and 𝑏𝑔
.
𝐴⋅(𝐵⋅𝐶)=(𝐴⋅𝐵)⋅𝐶
 holds for all 𝐴,𝐵,𝐶∈ℤ2[𝐺]
 if and only if 𝐺
 is a semigroup. Otherwise, probability of it being true for uniformly randomly picked 𝐴,𝐵,𝐶
 is at most 78
 (explained below), which allows adjusting error probability to 𝛿
 by repeating the same test log8/7𝛿−1
 times, making it a total time of 𝑂(𝑛2log𝛿−1)
 for the error tolerance 𝛿
.

Proof of it is in a way similar to Freivalds' algorithm which checks (𝐴𝐵−𝐶)𝑥=0
 for matrices to verify that 𝐴𝐵=𝐶
. In the associativity case, 𝐴⋅(𝐵⋅𝐶)−(𝐴⋅𝐵)⋅𝐶=0
 is checked and it may be rewritten in Einstein notation as
(𝐹𝑖𝑗𝑚𝐹𝑚𝑘𝑙−𝐹𝑖𝑚𝑙𝐹𝑗𝑘𝑚)𝑎𝑖𝑏𝑗𝑐𝑘=0
Where summations and products are done in ℤ2
 and 𝐹𝑖𝑗𝑘
 is a 3-dimensional array (tensor) defined as
𝐹𝑖𝑗𝑘={10if 𝑖⋅𝑗=𝑘,otherwise
For better understanding, the matrix identity from Freivalds' algorithm looks like this in Einstein notation
(𝐴𝑖𝑘𝐵𝑘𝑗−𝐶𝑖𝑗)𝑥𝑗=0
We will prove probability bounds for both tests in a similar generic manner. Let 𝐴𝑖1…𝑖𝑘+1
 be a rank 𝑘+1
 non-zero tensor. Then

𝐴𝑖1…𝑖𝑘+1𝑥(1)𝑖1…𝑥(𝑘)𝑖𝑘=0
where addition and multiplication is done in some ring 𝑅
 (in our case 𝑅=ℤ2
) can be checked by uniformly independently sampling vectors 𝑥(1),…,𝑥(𝑘)
 on 𝑅𝑛
 and calculating the identity with these vectors. Probability of obtaining non-zero result is at least 12𝑘
.

Since 𝐴𝑖1…𝑖𝑘+1
 is non-zero, there has to be vector 𝑦𝑖1
 such that
𝐴𝑖1…𝑖𝑘+1𝑦𝑖1≠0,
It means that for every 𝑥(1)𝑖1
 such that
𝐴𝑖1…𝑖𝑘+1𝑥(1)𝑖1=0,
there exists 𝑧𝑖1=𝑥(1)𝑖1+𝑦𝑖1
 such that
𝐴𝑖1…𝑖𝑘+1𝑧𝑖1=𝐴𝑖1…𝑖𝑘+1𝑦𝑖1≠0,
which means that there are at least as many vectors yielding non-zero result as vectors yielding zero. Thus the probability of obtaining non-zero result is at least 12
. It also proves base case 𝑘=1
. Now to make an induction step we should note that 𝐴𝑖1…𝑖𝑘+1𝑥(1)𝑖1
 can be perceived as a rank 𝑘
 tensor 𝐵𝑖2…𝑖𝑘+1=𝐴𝑖1…𝑖𝑘+1𝑥(1)𝑖1
, so we now deal with

𝐵𝑖2…𝑖𝑘+1𝑥(2)𝑖2…𝑥(𝑘)𝑖𝑘=0
For which, by induction, probability of producing non-zero is at least 12𝑘−1
. Since 𝑥(2),…,𝑥(𝑘)
 are chosen independently from 𝑥(1)
, probabilities of having non-zero can be multiplied directly leading to the 12𝑘
 probability of having non-zero result in the initial identity.

This result leads to the important conclusion that reduction to group rings can as well be used to test arbitrary read-once (such that each variable occurs at most once on both sides of identity) identities on operations defined by their Cayley table. In particular, if the composed operation depends on 𝑘
 variables, testing it requires 𝑂(𝑛2𝑘)
 time and yields at most 2𝑘−12𝑘
 error probability.

For Rajagopalan–Schulman test 𝑘=3
 and the error probability is at most 78
. For Freivalds' test 𝑘=1
 and the probability is at most 12
.

As a friendly reminder, this post is of my mindbun series. Once in a while I write something of interest to me, usually related to computer science and the mathematics it requires, and post it in the Telegram channel (English) and VK group (partially Russian). Hop in if you find this stuff interesting and/or want to get new posts first-hand!

Tags mindbun, algebra, linear algebra, groups, associativity, tensor

Subset convolution interpretation

By adamant, history, 2 years ago, In English
Hi everyone!

Recently aryanc403 brought up a topic of subset convolution and some operations related to it.

This inspired me to write this blog entry as existing explanations on how it works seemed unintuitive for me. I believe that having viable interpretations of how things work is of extreme importance as it greatly simplifies understanding and allows us to reproduce some results without lust learning them by heart.

Also this approach allows us to easily and intuitively generalize subset convolution to sum over 𝑖∪𝑗=𝑘
 and |𝑖∩𝑗|=𝑙
, while in competitive programming we usually only do it for |𝑖∩𝑗|=0
. Enjoy the reading!

Subset convolution 𝑎⋆𝑏
 is defined as follows, let 𝑎=(𝑎0,...,𝑎2𝑛−1)
 and 𝑏=(𝑏0,...,𝑏2𝑛−1)
. Then

(𝑎⋆𝑏)𝑘=∑𝑖⊂𝑘𝑎𝑖𝑏𝑘∖𝑖
That is, 𝑘
-th term of this convolution is a sum over all disjoint partitions of 𝑘
 into two submasks.

Generic approach to compute such kinds of sums is to make a transform which maps initial arrays into some space where convolution is done via pointwise (Hadamard) product (𝑎∘𝑏)𝑘=𝑎𝑘𝑏𝑘
. That is, to find some invertible transform 𝐹
 such that

𝐹(𝑎⋆𝑏)=𝐹(𝑎)∘𝐹(𝑏)
These kinds of convolutions can be interpreted as a product in groupoid ring. Let 𝑥
 and 𝑦
 be integers from 0
 to 2𝑛−1
. We define

𝑥⋆𝑦={𝑥∪𝑦,∅,if 𝑥∩𝑦=0,otherwise
where ∅
 is a special "sink" element indicating that this operation is invalid and shouldn't be counted.

Now if we consider formal sums 𝑎=∑𝑔=02𝑛−1𝑎𝑔𝑔
 and 𝑏=∑𝑔=02𝑛−1𝑏𝑔𝑔
, their product over groupoid ring is defined as

𝑎⋆𝑏=∑𝑖=02𝑛−1∑𝑗=02𝑛−1𝑎𝑖𝑏𝑗(𝑖⋆𝑗)
What would be extremely nice for us here is to introduce formal variable 𝑥
 such that 𝑥𝑖𝑥𝑗=𝑥𝑖⋆𝑗
. Then these convolutions can be interpreted as polynomial products where polynomials are taken over these weird variable. To compute this product, a common trick is to instead compute it in some specific values of 𝑥
, do the component-wise product and interpolate it back to polynomial to find the answer.

And since we work with a bitwise operation, we will use 𝑥𝑖=𝑥𝑖11𝑥𝑖22…𝑥𝑖𝑛𝑛
, where 𝑖𝑘
 is 0
 or 1
 depending on the 𝑘
-th bit of 𝑖
. Then

𝑥𝑖𝑥𝑗=(𝑥𝑖11𝑥𝑖22…𝑥𝑖𝑛𝑛)(𝑥𝑗11𝑥𝑗22…𝑥𝑗𝑛𝑛)=𝑥𝑖1⋆𝑗11𝑥𝑖2⋆𝑗22…𝑥𝑖𝑛⋆𝑗𝑛𝑛=𝑥𝑖⋆𝑗
Now before working with subset convolution, let's take a step back and recall what we're doing in "or" and "xor" convolutions. For "or" convolution we would need 𝑥0𝑥0=𝑥0
, 𝑥0𝑥1=𝑥1
 and 𝑥1𝑥1=𝑥1
. What are the actual numbers we can think of for which such identities hold? There are only two such numbers, they're 𝑥=0
 and 𝑥=1
. So, to compute the "or" convolution is equivalent to compute the values of the multivariate polynomial in all the points of {0,1
}𝑛
 boolean cube.

And what about "xor" convolution? For it we need 𝑥0𝑥0=𝑥0
, 𝑥0𝑥1=𝑥1
 and 𝑥1𝑥1=𝑥0
. What are numbers for which these identities hold? It is 𝑥=−1
 and 𝑥=1
. So, the "xor" convolution can be interpreted through computing values in {−1,1
}𝑛
 cube (and that's exactly what Walsh-Hadamard transform does as it is an 𝑛
-dimensional DFT of size 2
).

Now comes the "subset" convolution. We need some value such that 𝑥0𝑥0=𝑥0
, 𝑥0𝑥1=𝑥1
 and 𝑥1𝑥1=0
. Woah, there is only one such number, 𝑥=0
. One value is not enough for us to uniquely recover the polynomial. Now, what do we do if we desperately need an element with specific properties but we do not have one?

Correct, we formally introduce it and pretend that it actually exists! Remember 𝑖
 which was introduced with 𝑖2=−1
? Now here we formally introduce element 𝜀
 such that 𝜀2=0
 but 𝜀≠0
 and treat all elements of extended ring as 𝑎+𝑏𝜀
 with corresponding addition and multiplication. What was just introduced here is the concept of dual numbers.

It would almost work as it is possible to compute the values of a polynomial in {0,𝜀
}𝑛
. The problem begins when we need to inverse the transform as it requires division by 𝜀
 which is not doable as it doesn't have an inverse. Here comes another trick. What do we do if we desperately need to compute something modulo prime number 𝑝
, but in the process it can happen that we may need to divide by 𝑝
? What if we at the same time certainly know that such division will occur at most once?

We calculate everything modulo 𝑝2
 and when the division is needed, we just divide it directly! And if we need to divide at most 𝑘
 times, we may compute everything modulo 𝑝𝑘+1
. What we know here is that while computing values in 𝜀
, we will multiply by it every time there is a bit in the number, same goes for dividing by 𝜀
 during the inverse calculations: we store elements of kind 𝑎0+𝑎1𝜀+𝑎2𝜀2+⋯+𝑎𝑘𝜀𝑘
 to make sure that when time comes we are able to divide them by 𝜀
 at most 𝑘
 times (in our case, 𝑘
 is the number of bits we use, that is 𝑛
).

That being said, subset convolution also has a nice intuitive interpretation which is nothing more than computing the values of multivariate polynomial in {0,𝜀
}𝑛
 cube in the ring of polynomials over 𝜀
 as a formal variable.

And, as a sweet bonus, after the inverse convolution we will actually obtain some polynomials over 𝜀
 instead of simply numbers. Their "free" terms will correspond to (𝑎⋆𝑏)𝑘
, but what's the meaning of other terms? As it turns out,
[𝜀𝑙](𝑎⋆𝑏)𝑘=∑𝑖∪𝑗=𝑘,|𝑖∩𝑗|=𝑙𝑎𝑖𝑏𝑗
So after the inverse transform we obtain grouping not only by 𝑖∪𝑗
, but also by the size of 𝑖∩𝑗
. And it in fact makes much sense, as if 𝑥
 is either 0
 or 𝜀
 then 𝑥𝑖𝑥𝑗=𝜀𝑖∧𝑗𝑥𝑖∨𝑗
, which generalizes for 𝑥1,…,𝑥𝑛
 as:

(𝑥𝑖11𝑥𝑖22…𝑥𝑖𝑛𝑛)(𝑥𝑗11𝑥𝑗22…𝑥𝑗𝑛𝑛)=𝜀(𝑖1∧𝑗1)+⋯+(𝑖𝑛∧𝑗𝑛)𝑥𝑖1∨𝑗11𝑥𝑖2∨𝑗22…𝑥𝑖𝑛∨𝑗𝑛𝑛=𝜀|𝑖∩𝑗|𝑥𝑖∪𝑗
In this way, as promised earlier, we make a convolution over both 𝑖∪𝑗=𝑘
 and |𝑖∩𝑗|=𝑙
 and doing this we somewhat justify the additional 𝑛
 factor in the complexity.

UPD: note that you can also do arbitrary operations over such series by doing them on polynomial values.

For example, exponent of 𝐴
 is defined as

exp𝐴=1+𝐴+𝐴22+⋯+𝐴𝑘𝑘!+…
This exponent can be computed by taking the polynomial exponent from all polynomial values in {0,𝜀
}𝑛
, leading to

[𝜀𝑙𝑥𝑘]exp𝐴=𝑒𝑎0∑𝑖1∪𝑖2∪⋯∪𝑖𝑚=𝑘𝑓(𝑖1,𝑖2,…,𝑖𝑚)=𝑙0<𝑖1,𝑖2,…,𝑖𝑚𝑎𝑖1𝑎𝑖2…𝑎𝑖𝑚𝑚!,
where 𝑓(𝑖1,…,𝑖𝑚)
 is a function defined as

𝑓(𝑖1,…,𝑖𝑚)=|𝑖1|+|𝑖2|+⋯+|𝑖𝑚|−|𝑖1∪𝑖2∪⋯∪𝑖𝑚|
In particular,

[𝜀0𝑥𝑘]exp𝐴=𝑒𝑎0∑𝑖1⊔𝑖2⊔⋯⊔𝑖𝑚=𝑘0<𝑖1<𝑖2<⋯<𝑖𝑚𝑎𝑖1𝑎𝑖2…𝑎𝑖𝑚.
Which allows various computations across unordered disjoint partitions of 𝑘
.

UPD2: I also solved Lights out using this approach (23730542).

UPD3: Instead of {0,𝜀
}𝑛
 cube you may evaluate polynomial over {−𝑧,𝑧
}𝑛
 (see here), which is more similar to Walsh-Hadamard Transform which is used for "xor" rather than the transform used in "or" convolution. In this way, coefficient near 𝑧0
 will still correspond to subset convolution and coefficient near 𝑧𝑙𝑥𝑘
 would correspond to the sum of 𝑎𝑖𝑏𝑗
 over 𝑖Δ𝑗=𝑘
 and |𝑖∩𝑗|=𝑙2
 where Δ
 is the symmetric difference of 𝑖
 and 𝑗
, thus corresponding to the "xor" of such numbers.

UPD4: A more formal way to understand it is through generalized Chinese remainder theorem. For univariate polynomial it is known that
𝑝(𝑥)≡𝑝(𝑎)(mod𝑥−𝑎)
Thus, computing value of polynomial in different points 𝑎1,…,𝑎𝑛
 is equivalent to computing it modulo (𝑥−𝑎1)…(𝑥−𝑎𝑛)
. Thus if you multiply polynomials component-wise and interpolate them back, you'll get the result modulo (𝑥−𝑎1)…(𝑥−𝑎𝑛)
. In particular, for component-wise multiplication in DFT space one obtains result modulo (𝑥−1)(𝑥−𝜔𝑛)…(𝑥−𝜔𝑛−1𝑛)=𝑥𝑛−1
.

For multidimensional case computing 𝑝(𝑥1,…,𝑥𝑛)
 in the point (𝑎1,…,𝑎𝑛)
 yields
𝑝(𝑥1,…,𝑥𝑛)≡𝑝(𝑎1,…,𝑎𝑛)(mod𝑥1−𝑎1,…,𝑥𝑛−𝑎𝑛)
Meaning that 𝑝(𝑥1,…,𝑥𝑛)−𝑝(𝑎1,…,𝑎𝑛)
 lies in ⟨𝑥1−𝑎1,…,𝑥𝑛−𝑎𝑛⟩
 ideal. Due to generalized CRT, computing it in several points is equivalent to getting result in the quotient ring corresponding to the intersection of ideals formed by these points. In particular, computing values in {𝑎,𝑏
}𝑛
 is equivalent to computing it modulo ⟨(𝑥1−𝑎)(𝑥1−𝑏),…,(𝑥𝑛−𝑎)(𝑥𝑛−𝑏)⟩
 ideal.

In particular, we work in

𝑅[𝑥1,…,𝑥𝑛]/⟨𝑥21−1,…,𝑥2𝑛−1⟩
 quotient ring for "xor" convolution,
𝑅[𝑥1,…,𝑥𝑛]/⟨𝑥21−𝑥1,…,𝑥2𝑛−𝑥𝑛⟩
 for "or" convolution,
𝑅[𝑥1,…,𝑥𝑛]/⟨𝑥21,…,𝑥2𝑛⟩
 for "subset" convolution,
𝑅[𝜀][𝑥1,…,𝑥𝑛]/⟨𝑥21−𝜀𝑥1,…,𝑥2𝑛−𝜀𝑥𝑛⟩
 for point-wise multiplication in {0,𝜀
}𝑛
,
𝑅[𝑧][𝑥1,…,𝑥𝑛]/⟨𝑥21−𝑧2,…,𝑥2𝑛−𝑧2⟩
 for point-wise multiplication in {−𝑧,𝑧
}𝑛
.
Elegia's post and discussion with negiizhao in the comments helped me a lot in understanding this point of view on multivariate evaluation, and I hope that I got it correctly. I studied ring theory quite a long ago and I can't read original Chinese articles, so I just hope my vague understanding here is at least somewhat correct.

Tags tutorial, convolution, polynomial interpolation, chinese remainder theo., crt

Randomized general matching with Tutte matrix

By adamant, history, 2 years ago, In English
Hi everyone!

Some time ago Monogon wrote an article about Edmonds blossom algorithm to find the maximum matching in an arbitrary graph. Since the problem has a very nice algebraic approach, I wanted to share it as well. I'll start with something very short and elaborate later on.

Library Checker — Matching on General Graph. Given a simple undirected graph on 𝑁≤500
 vertices, find the maximum matching.

tl;dr. The Tutte matrix of the graph 𝐺=(𝑉,𝐸)
 is

𝑇(𝑥12,…,𝑥(𝑛−1)𝑛)=⎛⎝⎜⎜⎜⎜⎜⎜0−𝑥12𝑒12−𝑥13𝑒13⋮−𝑥1𝑛𝑒1𝑛𝑥12𝑒120−𝑥23𝑒23⋮−𝑥2𝑛𝑒2𝑛𝑥13𝑒13𝑥23𝑒230⋮−𝑥3𝑛𝑒3𝑛………⋱…𝑥1𝑛𝑒1𝑛𝑥2𝑛𝑒2𝑛𝑥3𝑛𝑒3𝑛⋮0⎞⎠⎟⎟⎟⎟⎟⎟
Here 𝑒𝑖𝑗=1
 if (𝑖,𝑗)∈𝐸
 and 𝑒𝑖𝑗=0
 otherwise, 𝑥𝑖𝑗
 are formal variables. Key facts:

Graph has a perfect matching if and only if det𝑇≠0
 when considered as polynomial of 𝑥𝑖𝑗
.
Rank of 𝑇
 is the number of vertices in the maximum matching.
Maximal linearly independent subset of rows corresponds to the subset of vertices on which it is a perfect matching.
If graph has a perfect matching, (𝑇−1)𝑖𝑗≠0
 iff there exists a perfect matching which includes the edge (𝑖,𝑗)
.
After such (𝑖,𝑗)
 is found, to fix it in the matching one can eliminate 𝑖
-th and 𝑗
-th rows and columns of 𝑇−1
 and find next edge.
Randomization comes when we substitute 𝑥𝑖𝑗
 with random values. It can be proven that conditions above still hold with high probability. This provides us with 𝑂(𝑛3)
 algorithm to find maximum matching in general graph. For details, dive below.

Let's start with bipartite case. For bipartite graph Edmonds matrix 𝐸
 is defined as 𝐸𝑖𝑗=1
 if there is an edge between 𝑖
-th vertex of the first part and 𝑗
-th vertex of the second part and 𝐸𝑖𝑗=0
 otherwise. Let 𝑆𝑛
 be a set of all permutations of 𝑛
 elements. Then

per 𝐸=∑𝜎∈𝑆𝑛∏𝑖=1𝑛𝐸𝑖𝜎𝑖,det𝐸=∑𝜎∈𝑆𝑛sgn 𝜎∏𝑖=1𝑛𝐸𝑖𝜎𝑖
Where per𝐸
 and det𝐸
 are the permanent and determinant of 𝐸
, and sgn 𝜎
 is a sign of permutation 𝜎
. By definition, per 𝐸
 is the number of perfect matchings in 𝐺
, so to check if there is a perfect matching is equivalent to check if this permanent is non-zero. Calculating permanent is #𝑃
-complete task, but calculating determinant is possible in 𝑂(𝑛3)
 with gaussian elimination.

To check if a permanent of some non-negative matrix is zero is far easier task than actually computing it, because it may be probabilistically reduced to the determinant computing. Instead of 𝐸
 consider matrix 𝐸(𝑝)
 such that 𝐸(𝑝)𝑖𝑗=𝐸𝑖𝑗𝑥𝑖𝑗
. For such matrix,
per 𝐸=0⟺det𝐸(𝑝)=0.
Indeed, the permanent of non-negative matrix is non-zero iff at least one summand is non-zero and every summand in det𝐸(𝑝)
 has a unique monomial associated with it, thus it is identical zero iff every summand is zero.

If we have a multivariate polynomial 𝑃(𝑥1,…,𝑥𝑘)
 of degree 𝑛
 and we check if it's identical zero by substituting each 𝑥𝑖
 with a random value chosen from a set 𝑆
, the probability of obtaining false positive is, by Schwartz–Zippel lemma, at most 𝑛|𝑆|
.

Let's move on to the general case. You may note that Tutte matrix of bipartite graph with 𝑛
-size parts looks like

𝑇=(0−𝐸𝑇𝐸0)
Thus, det𝑇=det2𝐸
 for such graph. Now, how should we interpret such determinant for generic skew-symmetric matrixes? To answer this question we should perceive permutation as a set of cycles. In this case, det𝑇
 is some alternating sum over all directed cycle covers of 𝐺
. Specifically, since 𝑇𝑖𝑖=0
, only derangements are considered. Note that 𝑇𝑖𝑗=−𝑇𝑗𝑖
, thus for any cycle (𝑖1,𝑖2,…,𝑖𝑘)
 it holds that
𝑇𝑖1𝑖2𝑇𝑖2𝑖3…𝑇𝑖𝑘𝑖1=(−1)𝑘𝑇𝑖1𝑖𝑘…𝑇𝑖3𝑖2𝑇𝑖2𝑖1
That means that if 𝜎
 contains a cycle of odd length, its summand will be cancelled out by the same permutation with this specific cycle reversed. Other permutations may be joined into classes of equivalence by comparing undirected sets of cycles corresponding to them. Equivalence class of a permutation that is comprised of even cycles, 𝑘
 of them having length greater than 2
, will contain 2𝑘
 permutations (for every cycle with length greater than 2
 its reversal will yield a new permutation) and products of 𝑇1𝜎1…𝑇𝑛𝜎𝑛
 will be the same for all such permutations due to aforementioned property.

In this way, det𝑇≠0
 if and only if there is a cycle cover of 𝐺
 comprised of even-length cycles. From any such cycle cover we may construct a perfect matching by taking alternating edges in each cycle, and in the same time perfect matching itself is a cycle cover of 2
-length cycles, thus it is equivalent to graph having a perfect matching.

A small digression about this determinant. One may note that for (𝑖1,𝑖2,…,𝑖2𝑘)
 it holds that

𝑇𝑖1𝑖2𝑇𝑖2𝑖3…𝑇𝑖2𝑘𝑖1=(𝑇𝑖1𝑖2𝑇𝑖3𝑖4…𝑇𝑖2𝑘−1𝑖2𝑘)(𝑇𝑖2𝑖3𝑇𝑖4𝑖5…𝑇𝑖2𝑘𝑖1)
Thus, any permutation comprised of even-length cycles can be represented as a combination of two perfect matchings. It highlights that det𝑇=Pf 2𝑇
 for even 𝑛
, where Pf 𝑇
 is the so-called Pfaffian of skew-symmetric matrix:

Pf 𝑇=12𝑛/2(𝑛2)!∑𝜎∈𝑆𝑛sgn 𝜎⋅𝑇𝜎1𝜎2𝑇𝜎3𝜎4…𝑇𝜎𝑛−1𝜎𝑛
Every perfect matching is counted 2𝑛/2(𝑛2)!
 times in the sum, as you may obtain the same matching by changing the order of multipliers or changing 𝑇𝑖𝑗
 with 𝑇𝑗𝑖
. This is canceled by dividing the sum with this amount. After two arbitrary summands of Pfaffian are multiplied, they may be perceived as a product over edges in some cycle cover. And every specific cycle cover may be obtained as a product of two perfect matchings in 2𝑘
 way, given that the cover has 𝑘
 cycles of length greater than 2
.

The determinant of Tutte matrix now allows us to check if a graph has a perfect matching. But what if there is no perfect matching and we want to recover the size of maximum matching? As was already mentioned, this size is equal to the rank rank 𝑇
 of Tutte matrix.

Let rank 𝑇=𝑚
 and 𝑎1,…,𝑎𝑚
 is the set of linearly independent rows of 𝑇
. By skew-symmetry, columns of 𝑇
 with these indices are linearly independent as well, from which we may conclude that submatrix comprised of these rows and columns is non-singular, thus has non-zero determinant.

Indeed, let's assume that it is not true. Then we may nullify the first row of this submatrix with Gaussian elimination. On the other hand, since 𝑚
 is the rank of matrix, and columns are linearly independent in 𝑇
, every other column can be obtained as their linear combination, which means that after elimination of submatrix's first row, the whole 𝑎1
 row would be nullified, which contradicts the assumption of rows being linearly independent.

On the other hand, if 𝑚
 is the size of maximum matching, corresponding submatrix will be non-singular, making rank of 𝑇
 at least 𝑚
. That being said, one can find a set of vertices on which maximum matching is achieved by finding a maximal set of linearly independent rows in Tutte matrix.

Finally, let's tackle the task of restoring perfect matching. It is known from linear algebra that

𝑇−1=adj 𝑇det𝑇
Where adj 𝑇
 is adjugate matrix, that is (adj 𝑇)𝑗𝑖
 is (−1)𝑖+𝑗
 times the determinant of 𝑇
 with 𝑖
-th row and 𝑗
-th column removed. In terms of sum over all cycle covers, deletion of 𝑖
-th rown and 𝑗
-th column is up to constant multiplier equivalent to turning all elements in 𝑖
-th row and 𝑗
-th column except for 𝑇𝑖𝑗
 into zero, effectively preventing 𝑖
-th vertex to be followed by anything but 𝑗
 in the cycle cover. Such sum would be non-zero if and only if there exists a cyclic cover of even-length cycles which passes through 𝑖→𝑗
 edge.

This immediately gives us 𝑂(𝑛4)
 algorithms to find the perfect matching as we may find any edge (𝑖,𝑗)
 that is guaranteed to be included in at least one perfect matching and then removing 𝑖
 and 𝑗
 vertices from the graph (correspondingly, 𝑖
-th and 𝑗
-th rows and columns from 𝑇
) and repeating the procedure of picking next such edge.

To recover maximum matching, one may firstly recover set of vertices on which it is a perfect matching by finding maximal linearly independent set of matrix rows and then find a perfect matching in a graph reduced to only these vertices.

Marcin Mucha and Piotr Sankowski from Warsaw university suggested 𝑂(𝑛3)
 improvement in 2004. They noticed that instead of calculating inverse matrix from scratch every time, one may simply do Gaussian elimination on corresponding rows and columns.

For simplicity, let's say that we want to remove 1
-st row and 1
-st column of matrix 𝐴
. Let

𝐴=(𝑎11𝑢𝑣𝑇𝐵)𝐴−1=(𝑎̂ 11𝑢̂ 𝑣̂ 𝑇𝐵̂ )
What we actually want to do here is to make transition from 𝐴−1
 to 𝐵−1
. For this we note

𝐴𝐴−1=(𝑎11𝑎̂ 11+𝑣𝑇𝑢̂ 𝑢𝑎̂ 11+𝐵𝑢̂ 𝑎11𝑣̂ 𝑇+𝑣𝑇𝐵̂ 𝑢𝑣̂ 𝑇+𝐵𝐵̂ )=(𝐼100𝐼𝑛−1)=𝐼𝑛
From these matrix equations we get

𝐵𝐵̂ =𝐼𝑛−1—𝑢𝑣̂ 𝑇,(𝑢𝑎̂ 11+𝐵𝑢̂ )𝑣̂ 𝑇=0
Second equation implies 𝐵𝑢̂ 𝑣̂ 𝑇=−𝑎̂ 11𝑢𝑣̂ 𝑇
, dividing it by −𝑎̂ 11
 and adding to the first one, we obtain

𝐵(𝐵̂ −𝑢̂ 𝑣̂ 𝑇𝑎̂ 11)=𝐼𝑛−1
Thus 𝐵−1=𝐵̂ −𝑢̂ 𝑣̂ 𝑇/𝑎̂ 11
. In more generic terms, this expression is called a Schur complement of 𝑎̂ 11
 and it can be obtained by Gaussian elimination of the first row and first column of 𝐴−1
. For erasure of 𝑖
-th row and 𝑗
-th column, one may eliminate these row and column with Gauss algorithm instead of computing inverse matrix from scratch. Ultimately, it results in the following algorithm:

Compute 𝐵=𝑇−1
.
Find any (𝑖,𝑗)
 such that 𝑇𝑖𝑗≠0
 and 𝐵𝑖𝑗≠0
 and add it to the current matching.
Eliminate 𝑖
-th and 𝑗
-th columns and rows of 𝐵
 with Gaussian elimination.
Repeat steps 2-3 until the matching is found.
Due to linear algebra properties, the process will converge for any non-singular matrix. Thus the failure probability is still at most 𝑛|𝑆|
 due to Schwartz–Zippel lemma and it may only occur if matrix rank turned out to be less than actual maximum matching size.

UPD: For possible implementation, please refer to this submission.

Tags tutorial, tutte matrix, maximum matching, probabilistic, linear algebra

Theoretical grounds of lambda optimization

By adamant, history, 2 years ago, In English
Hi everyone!

This time I'd like to write about what's widely known as "Aliens trick" (as it got popularized after 2016 IOI problem called Aliens). There are already some articles about it here and there, and I'd like to summarize them, while also adding insights into the connection between this trick and generic Lagrange multipliers and Lagrangian duality which often occurs in e.g. linear programming problems.

Familiarity with a previous blog about ternary search or, at the very least, definitions and propositions from it is expected.

Great thanks to mango_lassi and 300iq for useful discussions and some key insights on this.

Note that although explanation here might be quite verbose and hard to comprehend at first, the algorithm itself is stunningly simple.

Another point that I'd like to highlight for those already familiar with "Aliens trick" is that typical solutions using it require binary search on lambdas to reach specified constraint by minimizing its value for specific 𝜆
. However, this part is actually unnecessary and you don't even have to calculate the value of constraint function at all within your search.

It further simplifies the algorithm and extends applicability of aliens trick to the cases when it is hard to minimize constraint function while simultaneously minimizing target function for the given 𝜆
.

Tldr.
Problem. Let 𝑓:𝑋→ℝ
 and 𝑔:𝑋→ℝ𝑐
. You need to solve the constrained optimization problem

𝑓(𝑥)→min,𝑔(𝑥)=0.
Auxiliary function. Let 𝑡(𝜆)=inf𝑥[𝑓(𝑥)−𝜆⋅𝑔(𝑥)]
. Finding 𝑡(𝜆)
 is unconstrained problem and is usually much simpler.

Equivalently, 𝑡(𝜆)=inf𝑦[ℎ(𝑦)−𝜆⋅𝑦]
 where ℎ(𝑦)
 is the minimum possible 𝑓(𝑥)
 subject to 𝑔(𝑥)=𝑦
.

As a point-wise minimum of linear functions, 𝑡(𝜆)
 is concave, therefore its maximum can be found with ternary search.

Key observation. By definition, 𝑡(𝜆)≤ℎ(0)
 for any 𝜆
, thus max𝜆𝑡(𝜆)
 provides a lower bound for ℎ(0)
. When ℎ(𝑦)
 is convex, inequality turns into equation, that is max𝜆𝑡(𝜆)=ℎ(0)=𝑓(𝑥∗)
 where 𝑥∗
 is the solution to the minimization problem.

Solution. Assume that 𝑡(𝜆)
 is computable for any 𝜆
 and ℎ(𝑦)
 is convex. Then find max𝜆𝑡(𝜆)
 with the ternary search on 𝑡(𝜆)
 over possible values of 𝜆
. This maximum is equal to the minimum 𝑓(𝑥)
 subject to 𝑔(𝑥)=0
.

If 𝑔(𝑥)
 and 𝑓(𝑥)
 are integer functions, 𝜆𝑖
 corresponds to ℎ(𝑦𝑖)−ℎ(𝑦𝑖−1)
 and can be found among integers.

Boring and somewhat rigorous explanation is below, problem examples are belower.

Lagrange duality
Let 𝑓:𝑋→ℝ
 be the objective function and 𝑔:𝑋→ℝ𝑐
 be the constraint function. The constrained optimization problem

𝑓(𝑥)→min,𝑔(𝑥)=0
in some cases can be reduced to finding stationary points of the Lagrange function

𝐿(𝑥,𝜆)=𝑓(𝑥)−𝜆⋅𝑔(𝑥).
Here 𝜆⋅𝑔(𝑥)
 is a dot product of 𝑔(𝑥)
 and a variable vector 𝜆∈ℝ𝑐
.

Def. 1. A function 𝐿(𝑥,𝜆)
 defined above is called the Lagrange function, or the Lagrangian.

Def. 2. A vector 𝜆∈ℝ𝑐
 defined above is called a Lagrange multiplier. Its components are collectively called Lagrange multipliers.

Mathematical optimization focuses on finding stationary points of 𝐿(𝑥,𝜆)
. However, we're more interested in its infimal projection

𝑡(𝜆)=inf𝑥∈𝑋𝐿(𝑥,𝜆).
Def. 3. A function 𝑡(𝜆)
 defined above is called the Lagrange dual function.

Example. You're given an array 𝑎1,…,𝑎𝑛
. You need to find a sequence 1≤𝑥1<𝑥2<⋯<𝑥𝑘≤𝑛
 of exactly 𝑘
 indices such that 𝑎𝑥1+𝑎𝑥2+⋯+𝑎𝑥𝑘
 is minimum possible. This problem is solvable, for example, by sorting 𝑎1,…,𝑎𝑛
 and picking 𝑘
 smallest elements. However, let's see how it would be formulated in terms above to better understand this notion.

For this problem, 𝑋=2{1,2,…,𝑛}
 is a set of all such sequences. Let 𝑥={𝑥1,𝑥2,…,𝑥𝑚}∈𝑋
, then

𝑓(𝑥)𝑔(𝑥)𝑡(𝜆)=𝑎𝑥1+𝑎𝑥2+⋯+𝑎𝑥𝑚,=|𝑥|−𝑘=𝑚−𝑘,=inf𝑥[𝑓(𝑥)−𝜆(|𝑥|−𝑘)]=inf𝑥[(𝑎𝑥1−𝜆)+(𝑎𝑥2−𝜆)+⋯+(𝑎𝑥𝑚−𝜆)]+𝜆𝑘.
Note that finding 𝑡(𝜆)
 is unconstrained optimization problem and to compute its value it is enough to simply take in the set 𝑥
 every index 𝑖
 such that 𝑎𝑖−𝜆≤0
. We do not have to know |𝑥|
 beforehand, instead we equally distribute 𝜆|𝑥|
 between all elements of 𝑥
.

This is the most common example of how aliens trick is applied on practice: 𝑔(𝑥)
 is typically a size of some set (number of picked elements in a subsequence, number of selected subsegments, etc) and we treat subtracted 𝜆⋅𝑔(𝑥)
 as if each element of this set bears additional penalty 𝜆
 to the target function 𝑓(𝑥)
.

Another important observation to make here is that increasing 𝜆
 would increase the size of optimal 𝑥
 for 𝑡(𝜆)
 and decreasing it will make more elements positive, thus leading to a smaller size of 𝑥
. It means that it is possible to find 𝜆
 that will give us the correct size of 𝑥
 by the binary search.

Of course, there might be several 𝑎𝑖
 such that 𝑎𝑖=𝜆
 and it wouldn't matter if we include them in 𝑥
 for 𝑡(𝜆)
, so there might be several optimal 𝑥
 with different sizes for a given 𝜆
. To mitigate this, it is possible to compute a set of all possible |𝑥|
 for each 𝜆
 (it will be a contiguous sequence of integers) and make the binary search based on the smallest element of such set.

Note that 𝑡(𝜆)
, as a point-wise infimum of concave (linear in 𝜆
) functions, is always concave, no matter what 𝑋
 and 𝑓
 are.

If 𝑥∗
 is the solution to the original problem, then 𝑡(𝜆)≤𝐿(𝑥∗,𝜆)=𝑓(𝑥∗)
 for any 𝜆∈ℝ𝑐
. This allows us to introduce

Def. 4. The unconstrained optimization problem 𝑡(𝜆)→max
 is called the Lagrangian dual problem.

Def. 5. If 𝜆∗
 is the solution to the dual problem, the value 𝑓(𝑥∗)−𝑡(𝜆∗)
 is called the duality gap.

Def. 6. A condition when the duality gap is zero is called a strong duality.

Typical example here is Slater's condition, which says that strong duality holds if 𝑓(𝑥)
 is convex, 𝑔(𝑥)
 is affine and there exists 𝑥
 such that 𝑔(𝑥)=0
.

Change of domain
In competitive programming, the set 𝑋
 in definitions above is often weird and very difficult to analyze directly, so Slater's condition is not applicable. As a typical example, 𝑋
 could be a set of all possible partitions of {1,2,…,𝑛}
 into non-intersecting segments. Besides, instead of specific equation 𝑔(𝑥)=0
, you are often asked to minimize 𝑓(𝑥)
 subject to 𝑔(𝑥)=𝑦
 where 𝑦
 is a part of problem input.

To mitigate this, we define ℎ(𝑦)
 as the minimum value of 𝑓(𝑥)
 subject to 𝑔(𝑥)=𝑦
. In this notion, the dual function is written as

𝑡(𝜆)=inf𝑦∈𝑌[ℎ(𝑦)−𝜆⋅𝑦],
where 𝑌={𝑔(𝑥):𝑥∈𝑋}
. The set 𝑌
 is usually much more regular than 𝑋
, as just by definition it is already a subset of ℝ𝑐
. The strong duality condition is also very clear in this terms: it holds if and only if 0∈𝑌
 and there is a 𝜆
 for which 𝑦=0
 delivers infimum.

Def. 7. The epigraph of a function 𝑓:𝑌→ℝ
 is a set epi 𝑓={(𝑦,𝑧):𝑧≥𝑓(𝑦)}⊂𝑌×ℝ
.

Def. 8. A supporting hyperplane of a set 𝑋⊂ℝ𝑑
 with a normal vector 𝜆∈ℝ𝑑
 is a surface defined as 𝜆⋅𝑥=𝑐
, where 𝑐
 is the largest possible number such that 𝜆⋅𝑥≥𝑐
 for all 𝑥∈𝑋
. Equivalently, 𝑐
 is the infimum of 𝜆⋅𝑥
 among all 𝑥∈𝑋
.

Geometrically, 𝑡(𝜆)
 defines a level at which the epigraph of ℎ(𝑦)
 has a supporting hyperplane with the normal vector (−𝜆,1)
. Indeed, the half-space bounded by such hyperplane on the level 𝑐
 is defined as {(𝑦,𝑧):𝑧−𝜆⋅𝑦≥𝑐}
.

All the points (𝑦,ℎ(𝑦))
 at which the hyperplane touches the epigraph minimize the 𝑡(𝜆)
. Please, refer to the picture below. Lower 𝑐
 would move the hyperplane lower, while higher 𝑐
 would move it higher. With 𝑐=𝑡(𝜆)
, the hyperplane is lowest possible while still intersecting the epigraph of the function in the point (𝑦∗,ℎ(𝑦∗))
 where 𝑦∗
 delivers the minimum of ℎ(𝑦)−𝜆⋅𝑦
.


For strong duality to hold for all inputs, all 𝑦∈𝑌
 should have a supporting hyperplane that touches the epigraph at (𝑦,ℎ(𝑦))
. This condition is essentially equivalent to ℎ(𝑦)
 being convex-extensible, that is, there should exist a convex function on ℝ𝑐
 such that its restriction to 𝑌
 yields ℎ(𝑦)
.

Returning to our example above, ℎ(𝑦)
 equals to the minimum 𝑓(𝑥)
 subject to 𝑔(𝑥)=|𝑥|−𝑘=𝑦
, which means that ℎ(𝑦)
 corresponds to the same problem, but with number of elements in 𝑥
 changed from 𝑘
 to 𝑘+𝑦
.

As we already noted, optimal way to solve this problem is to sort all elements and pick 𝑘+𝑦
 smallest ones. Since elements are increasing, sum of first 𝑘+𝑦
 elements is convex as a function of 𝑦
.

This means that for the example problem strong duality holds and instead of doing binary search on 𝜆
 to match the needed |𝑥|
 we could do a ternary search to find the largest 𝑡(𝜆)
, as the maximum value of 𝑡(𝜆)
 would correspond to the minimum 𝑓(𝑥)
 subject to 𝑔(𝑥)=0
.

1
-dimensional case
For now, let's assume that the problem has a single constraint, thus only one dimension to deal with.

Due to general properties of convex functions, larger 𝑦
 would require larger 𝜆
 for the supporting line in the point 𝑦
 and vice versa — larger 𝜆
 would be able to define a supporting line on larger 𝑦
 only. This monotonicity means that we can find optimal 𝜆
.

Algorithm for finding 𝜆
 that delivers optimal 𝑡(𝜆)
:

Do the binary search on 𝜆
. Assume that you work in [𝜆𝑙,𝜆𝑟)
 and test 𝜆𝑚
 with 𝑚≈𝑙+𝑟2
.
Compute optimal 𝑥𝜆
 for 𝑓(𝑥)−𝜆𝑚𝑔(𝑥)
 function and 𝑦𝜆=𝑔(𝑥𝜆)
 corresponding to it.
When there are several possible 𝑥𝜆
, choose the one that delivers minimum 𝑦𝜆
.
If 𝑦𝜆>0
, you should reduce working range to [𝜆𝑙,𝜆𝑚)
, otherwise reduce it to [𝜆𝑚,𝜆𝑟)
.
Third step is essential, as 𝜆𝑚
 can correspond to several consequent 𝑦
 such that the points (𝑦,ℎ(𝑦))
 lie on the same line and, therefore, have a common supporting line. However, if we look on the smallest 𝑦
 for every 𝜆𝑚
, we will guarantee that the values we find are non-decreasing as a function of 𝜆𝑚
. If finding minimal 𝑦𝜆
 is cumbersome, one might use ternary search instead to solve the dual problem directly.

Note: This approach doesn't guarantee that we find 𝑥𝜆
 such that 𝑔(𝑥𝜆)=0
, however we will find 𝜆
 that corresponds to the optimum, which would mean that (𝑦𝜆,𝑓(𝑥𝜆))
 and (𝑦∗,𝑓(𝑥∗))
 lie on the same line with a slope coefficient 𝜆
, thus you can get the answer as

𝑓(𝑥∗)=𝑓(𝑥𝜆)+(𝑦∗−𝑦𝜆)⋅𝜆=𝑓(𝑥𝜆)−𝑦𝜆⋅𝜆=𝑡(𝜆).
Integer search
What are the possible 𝜆
 for specific 𝑦
? When ℎ(𝑦)
 is continuously differentiable, it essentially means that 𝜆
 corresponds to 𝑦
 such that 𝜆=ℎ′(𝑦)
. On the other hand, when 𝑌
 is the set of integers, 𝑦
 optimizes 𝑡(𝜆)
 for all 𝜆
 such that

ℎ(𝑦)−ℎ(𝑦−1)≤𝜆≤ℎ(𝑦+1)−ℎ(𝑦).
So, if ℎ(𝑦)
 is an integer function, we may do the integer search of 𝜆
 on possible values of ℎ(𝑘)−ℎ(𝑘−1)
 only.

In our example with the smallest-sum subset of 𝑘
 elements, only those 𝜆
 that are equal to some of 𝑎𝑖
 are the points at which possible optimal 𝑥
 for 𝑡(𝜆)
 change. It corresponds to the transition from ℎ(𝑦)
 to ℎ(𝑦+1)
, as the difference between them is equal to the next 𝑎𝑖
 in a sorted list.

In a very generic case, when the function is not continuously differentiable and 𝑦𝑖
 are not necessarily integer, the set of possible 𝜆𝑖
 for a given 𝑦𝑖
 is defined as ∂ℎ(𝑦𝑖)
, the so-called sub-differential of ℎ
 in 𝑦𝑖
, formally defined as [𝑎,𝑏]
 where

𝑎=sup𝑦′𝑖<𝑦𝑖𝑓(𝑦𝑖)−𝑓(𝑦′𝑖)𝑦𝑖−𝑦′𝑖,  𝑏=inf𝑦𝑖<𝑦′𝑖𝑓(𝑦′𝑖)−𝑓(𝑦𝑖)𝑦′𝑖−𝑦𝑖.
The concept of sub-derivatives and sub-differentials can be generalized to multi-dimensional case as well with sub-gradients.

2
-dimensional case
When 𝑐>1
, the original problem might be reduced to a sequence of nested single-dimensional problems. Let's start with 𝑐=2
.

inf𝑔1(𝑥)=0𝑔2(𝑥)=0𝑓(𝑥)=max𝜆1inf𝑔2(𝑥)=0[𝑓(𝑥)−𝜆1𝑔1(𝑥)]=max𝜆1𝑡(𝜆1),
The function 𝑡(𝜆1)
 can also be rewritten as a maximum of the Lagrangian dual:

𝑡(𝜆1)=max𝜆2inf𝑥[𝑓𝜆1(𝑥)−𝜆2𝑔2(𝑥)]=max𝜆2𝑡𝜆1(𝜆2),𝑓𝜆1(𝑥)=𝑓(𝑥)−𝜆1𝑔1(𝑥).
Thus, rather than solving joint problem on 𝜆1,𝜆2
, we need to solve two Lagrangian dual problems:

𝑡(𝜆1)𝑡𝜆1(𝜆2)=inf𝑔2(𝑥)=0[𝑓(𝑥)−𝜆1𝑔1(𝑥)]→max,=inf𝑥[𝑓𝜆1(𝑥)−𝜆2𝑔2(𝑥)]→max.
Rewriting them in ℎ
-terms, we obtain

𝑡(𝜆1)𝑡𝜆1(𝜆2)=inf𝑦1[ℎ(𝑦1)−𝜆1𝑦1],=inf𝑦2[ℎ𝜆1(𝑦2)−𝜆2𝑦2],
where ℎ
-functions are defined in the following way:

ℎ(𝑦1)ℎ𝜆1(𝑦2)=ℎ(𝑦1,0),=inf𝑦1[ℎ(𝑦1,𝑦2)−𝜆1𝑦1].
For both problems to have strong duality, both ℎ(𝑦1)=ℎ(𝑦1,0)
 and ℎ𝜆1(𝑦1)
 must be convex.

Multidimensional case
Let's for the sake of clarity write down the full sequence of nested 1
-dimensional optimization problems in 𝑐
-dimensional case.

We need to optimize 𝑓(𝑥)
 w.r.t 𝑔1(𝑥)=𝑔2(𝑥)=⋯=𝑔𝑛(𝑥)=0
. We introduce 𝜆1,…,𝜆𝑛
. and solve the optimization problem

max𝜆1…𝜆𝑛𝑡(𝜆1,…,𝜆𝑛)=max𝜆1…max𝜆𝑛inf𝑥[𝑓(𝑥)−𝜆1⋅𝑔1(𝑥)−⋯−𝜆𝑛⋅𝑔𝑛(𝑥)].
We decompose it in a sequence of nested problems in the following way:

max𝜆𝑡(𝜆)=max𝜆1𝑡(𝜆1)=max𝜆1max𝜆2𝑡𝜆1(𝜆2)=max𝜆1max𝜆2max𝜆3𝑡𝜆1𝜆2(𝜆3)=…,
where

𝑡𝜆1…𝜆𝑘−1(𝜆𝑘)=inf𝑔𝑘+1(𝑥)=0…𝑔𝑛(𝑥)=0[𝑓𝜆1…𝜆𝑘−1(𝑥)−𝜆𝑘⋅𝑔𝑘(𝑥)],𝑓𝜆1…𝜆𝑘−1(𝑥)=𝑓(𝑥)−𝜆1𝑔1(𝑥)−⋯−𝜆𝑘−1𝑔𝑘−1(𝑥).
With the help of ℎ(𝑦1,…,𝑦𝑛)
, the same could be rewritten as

𝑡𝜆1…𝜆𝑘−1(𝜆𝑘)=inf𝑦𝑘[ℎ𝜆1…𝜆𝑘−1(𝑦𝑘)−𝜆𝑘⋅𝑦𝑘],ℎ𝜆1…𝜆𝑘−1(𝑦𝑘)=inf𝑦1…𝑦𝑘−1[ℎ(𝑦1,…,𝑦𝑘−1,𝑦𝑘,0,…,0)−𝜆1𝑦1−⋯−𝜆𝑘−1𝑦𝑘−1].
For nested binary search on 𝜆
 components to work, every function ℎ𝜆1…𝜆𝑘−1(𝑦𝑘)
 must be convex, including ℎ(𝑦1)=ℎ(𝑦1,0,…,0)
.

The whole procedure could roughly look as follows:

void adjust(double *lmb, int i, int c) {
    if(i == c) {
        return;
    }
    double l = -inf, r = +inf; // some numbers that are large enough
    while(r - l > eps) { // Might be unsafe on large numbers, consider fixed number of iterations
        double m = (l + r) / 2;
        lmb[i] = m;
        adjust(lmb, i + 1, c);
        auto [xl, yl] = solve(lmb, i); // returns (x_lambda, y_lambda) with the minimum y_lambda[i]
        if(yl[k] > 0) {
            r = m;
        } else {
            l = m;
        }
    }
    lmb[i] = (l + r) / 2;
}
Alternatively, one might consider nested ternary search to solve the joint dual problem directly.

Testing convexity
Differential criteria. When 𝑌
 is continuous set, convexity may be tested by local criteria. For one-dimensional set, ℎ(𝑦)
 is convex if and only if ℎ′(𝑦)
 is non-decreasing. If it has a second derivative, we might also say that the function is convex if and only if ℎ″(𝑦)≥0
 for all 𝑦
.

In multidimensional case, the local criterion is that the Hessian matrix ∂ℎ∂𝑦𝑖∂𝑦𝑗
 is a positive semi-definite.

Finite differences. In discrete case, derivatives can be substituted with finite differences. A function ℎ(𝑦):ℤ→ℝ
 is convex if and only if

Δ[ℎ](𝑦)=ℎ(𝑦+1)−ℎ(𝑦)
is non-decreasing or, which is equivalent,

Δ2[ℎ](𝑦)=ℎ(𝑦+2)−2ℎ(𝑦+1)+ℎ(𝑦)≥0.
Reduction to mincost flow. Another possible way to prove that 1
-dimensional ℎ(𝑦)
 is convex in relevant problems is to reduce it to finding min-cost flow of size 𝑦
 in some network. The common algorithm to find such flow pushes a flow of size 1
 through the shortest path in the residual network. The shortest path becomes larger with each push, guaranteeing that mincost flow is convex in 𝑦
.

Similarly, reduction to maxcost 𝑘
-flow would prove the concavity of ℎ(𝑘)
. This method was revealed to me by 300iq.

Testing convexity in 1
-dimensional reductions. In multidimensional discrete case testing convexity might be challenging. Instead of this, one may try to directly prove that every single-dimensional ℎ
-function defined above is convex. Specifically, for 2
-dimensional case one would need to prove the convexity of

ℎ(𝑦1)ℎ𝜆1(𝑦2)=ℎ(𝑦1,0),=inf𝑦1[ℎ(𝑦1,𝑦2)−𝜆1𝑦1].
Problem examples
Sometimes the problem is stated with max
 rather than min
. In this case, 𝑡(𝜆)
 and ℎ(𝑦)
 are defined as supremum and maximum rather than infimum and minimum. Correspondingly, ℎ(𝑦)
 needs to be concave rather than convex to allow the usage of the trick.

Gosha is hunting. You're given 𝑎
, 𝑏
, 𝑝1,…,𝑝𝑛
 and 𝑞1,…,𝑞𝑛
. You need to pick two subsets 𝐴
 and 𝐵
 of {1,2,…,𝑛}
 of size 𝑎
 and 𝑏
 correspondingly in such a way that the following sum is maximized:

𝑓(𝐴,𝐵)=∑𝑖∈𝐴𝑝𝑖+∑𝑗∈𝐵𝑞𝑗−∑𝑘∈𝐴∩𝐵𝑝𝑘𝑞𝑘.
Formulating it as a constrained optimization problem, we obtain

𝑓(𝐴,𝐵)→max,|𝐴|−𝑎=0,|𝐵|−𝑏=0.
Lagrangian dual here is

𝑡(𝜆1,𝜆2)=max𝐴,𝐵[𝑓(𝐴,𝐵)−𝜆1|𝐴|−𝜆2|𝐵|]+𝜆1𝑎+𝜆2𝑏.
Let ℎ(𝛼,𝛽)=max𝑓(𝐴,𝐵)
 subject to |𝐴|=𝛼
 and |𝐵|=𝛽
, then

𝑡(𝜆1,𝜆2)=max𝛼,𝛽[ℎ(𝛼,𝛽)−𝜆1(𝛼−𝑎)−𝜆2(𝛽−𝑏)].
As it is 2
-dimensional problem, we would need to prove the concavity of the following functions:

ℎ(𝛼)ℎ𝜆1(𝛽)=ℎ(𝛼,0),=max𝛼[ℎ(𝛼,𝛽)−𝜆1𝛼]+𝜆1𝑎.
To prove the concavity of ℎ𝜆1
, we can consider the alternative formulation of this problem

ℎ𝜆1(𝛽)=max|𝐵|=𝛽[𝑓(𝐴,𝐵)−𝜆1|𝐴|]+𝜆1𝑎,
which is the primal problem for ℎ𝜆1
. We can interpret it as if we're penalized by 𝜆1
 for every element from the set 𝐴
, but we have no constraint on its size. It means, that we might by default take into the answer all elements of 𝐴
 having 𝑝𝑖≥𝜆1
.

Then we might sort all potential elements of 𝐵
 by their potential contribution to 𝑓(𝐴,𝐵)
 if we pick this element and pick top 𝛽
 elements in the sorted list. Since we sorted the elements by their contribution before picking them, increasing 𝛽
 will lead to smaller increases in 𝑓(𝐴,𝐵)
 proving that ℎ𝜆1
 is concave. Here's an example solution with nested ternary search on 𝑡(𝜆1,𝜆2)
:

Code
Note that we do not have to minimize |𝐴|
 or |𝐵|
 when we use ternary search on 𝑡(𝜆1,𝜆2)
 instead of binary search on 𝜆
 to match it with |𝐴|=𝑎
 and |𝐵|=𝑏
. In this way, we do not need to care about |𝐴|
 and |𝐵|
 and might as well not store it at all throughout the code.

Minkowski addition on epigraphs. Let 𝑓1:[0,𝑁]→ℝ
 and 𝑓2:[0,𝑀]→ℝ
 be convex functions computable in 𝑂(1)
. You need to quickly compute 𝑓(𝑥)
 for any given point 𝑥
 where 𝑓:[0,𝑁+𝑀]→ℝ
 is a convex function defined as:

epi 𝑓=epi 𝑓1+epi 𝑓2.
From this definition follows an alternative formula for 𝑓(𝑥)
:

𝑓(𝑥)=min𝑥1+𝑥2=𝑥[𝑓1(𝑥1)+𝑓2(𝑥2)],
thus we can say that 𝑓
 is a convolution of 𝑓1
 and 𝑓2
 with min
-operation.

To solve this problem, we introduce

𝑡(𝜆)=inf𝑥1,𝑥2[𝑓1(𝑥1)+𝑓2(𝑥2)−𝜆𝑥1−𝜆𝑥2]+𝜆𝑥.
Formally, constraint function here is 𝑔(𝑥1,𝑥2)=𝑥1+𝑥2−𝑥
 and corresponding ℎ
-function is defined as

ℎ(𝑦)=min𝑥1+𝑥2−𝑥=𝑦[𝑓1(𝑥1)+𝑓2(𝑥2)]=𝑓(𝑥+𝑦),
therefore ℎ(𝑦)
 is convex as well and strong duality holds. To compute 𝑡(𝜆)
, we can separate variables as

𝑡(𝜆)=inf𝑥1[𝑓1(𝑥1)−𝜆𝑥1]+inf𝑥2[𝑓2(𝑥2)−𝜆𝑥2]+𝜆𝑥.
Since 𝑓1
 and 𝑓2
 are convex, corresponding infimums are computable in 𝑂(log𝐶)
 with the ternary search, therefore 𝑓(𝑥)
 as a whole is computable in 𝑂(log2𝐶)
.

Another noteworthy fact here is that optimal (𝑥1,𝑥2)
 always define points such that both 𝑓1
 and 𝑓2
 have a supporting line with the slope 𝜆
 in 𝑥1
 and 𝑥2
 correspondingly.

It means that if 𝑓1
 and 𝑓2
 are defined in [0,𝑁]∩ℤ
 and [0,𝑀]∩ℤ
 instead of [0,𝑁]
 and [0,𝑀]
, it is possible to compute values of 𝑓
 in [0,𝑁+𝑀]∩ℤ
 in 𝑂(𝑁+𝑀)
 by merging two arrays as in merge-sort, but comparing 𝑎𝑖+1−𝑎𝑖
 and 𝑏𝑖+1−𝑏𝑖
 instead of 𝑎𝑖
 and 𝑏𝑖
:

// a and b are convex, compute c[i+j] = min(a[i] + b[j])
vector<int> min_conv(vector<int> a, vector<int> b) {
    adjacent_difference(begin(a), end(a), begin(a));
    adjacent_difference(begin(b), end(b), begin(b));
    vector<int> c = {a[0] + b[0]};
    merge(begin(a) + 1, end(a), begin(b) + 1, end(b), back_inserter(c));
    partial_sum(begin(c), end(c), begin(c));
    return c;
}
Honorable Mention. You're given an array 𝑎1,…,𝑎𝑛
 and 𝑞
 queries. Each query is a triple 𝑙,𝑟,𝑘
 and you need to compute the maximum sum on 𝑘
 non-empty non-intersecting subsegments of [𝑙,𝑟]
.

This is an example of a problem where concavity may be proven via the reduction to maxcost 𝑘
-flow on the following network:

 

Any 𝑘
-flow on this network corresponds to a set of 𝑘
 non-empty non-intersecting subsegments, thus ℎ(𝑘)
 equals to the cost of the maximum 𝑘
-flow in the network and thus ℎ(𝑘)
 is concave.

To solve the problem efficiently, we should be able to pick a set of subsegments of [𝑙,𝑟]
 such that each segment is penalized by 𝜆
 and their sum is maximized. As is often the case with problems that deal with segments, we're going to use a segment tree.

Let 𝑑𝑣(𝑘,𝑝,𝑠)
 be the the largest number we can obtain on a segment [𝑙,𝑟)
 corresponding to the vertex 𝑣
 of the segment tree, such that exactly 𝑘
 subsegments are used. Here 𝑝,𝑠∈{0,1}
 are prefix and suffix indicators. If 𝑝=1
 it means that one of 𝑘
 subsegments must be a prefix of [𝑙,𝑟)
 and if 𝑠=1
, it means that one of 𝑘
 subsegments must be a suffix of [𝑙,𝑟)
. In this notion,

𝑑𝑣(𝑘,𝑝,𝑠)=max⎧⎩⎨⎪⎪max𝑖+𝑗=𝑘[𝑑𝑙(𝑣)(𝑖,𝑝,0)+𝑑𝑟(𝑣)(𝑗,0,𝑠)],max𝑖+𝑗=𝑘+1[𝑑𝑙(𝑣)(𝑖,𝑝,1)+𝑑𝑟(𝑣)(𝑗,1,𝑠)]⎫⎭⎬⎪⎪.
Where 𝑙(𝑣)
 and 𝑟(𝑣)
 are left and right children of 𝑣
 in the segment tree correspondingly. These expressions are (max,+)
 convolutions of concave functions, thus if all values of 𝑑𝑣
 are stored in each vertex for every valid triple (𝑘,𝑝,𝑠)
, it is possible to calculate 𝑑𝑣
 from 𝑑𝑙(𝑣)
 and 𝑑𝑟(𝑣)
 with the help of the previous example problem.

Now that each segment hold all possible values of 𝑑𝑣
, let's deal with the triple 𝑙,𝑟,𝑘
. Let's say that [𝑙,𝑟]
 is decomposed into vertices 𝑣1,…,𝑣𝑘
. Instead of solving for specific 𝑘
, we will penalize penalize each segment by 𝜆
 and do ternary search on 𝑡(𝜆)
.

Let 𝑟𝑖(𝑠)
 be the optimal answer on 𝑣1,…,𝑣𝑖
 where 𝑠
 is a suffix-indicator, then

𝑟𝑖(𝑠)=max⎧⎩⎨⎪⎪max𝑘[𝑟𝑖−1(0)+𝑑𝑣𝑖(𝑘,0,𝑠)−𝜆𝑘],max𝑘[𝑟𝑖−1(1)+𝑑𝑣𝑖(𝑘,1,𝑠)−𝜆(𝑘−1)]⎫⎭⎬⎪⎪.
Since 𝑑𝑣𝑖(𝑘,…)
 is concave, so is 𝑑𝑣𝑖(𝑘,…)−𝜆𝑘
, thus the optimal value of this function might be found with another ternary search.

Or binary search on 𝑑𝑣𝑖(𝑘+1,…)−𝑑𝑣𝑖(𝑘,…)
, as the optimal value corresponds to the smallest 𝑘
 such that this difference is not greater than 𝜆
. One of possible implementations for the solution is below:

code
The solution here has the complexity of 𝑂(𝑛log3𝑛)
, however it is possible to reduce it to 𝑂(𝑛log2𝑛)
 with parallel binary search.

References
Duality (optimization) — English Wikipedia
The Trick From Aliens — Serbanology
My Take on Aliens' Trick — Mamnoon Siam's Blog
Incredibly beautiful DP optimization from N^3 to N log^2 N
Comment on Codeforces by _h_
Part of the article was once revealed to me in a dream
Tags lambda, aliens, tutorial, lagrange, duality

Lagrange interpolation and partial fraction decomposition

By adamant, history, 2 years ago, In English
Hi everyone!

Today I'd like to write yet another blog about polynomials. Specifically, I will cover the relationship between polynomial interpolation and Chinese remainder theorem, and I will also highlight how it is useful when one needs an explicit meaningful solution for partial fraction decomposition.

Lagrange interpolation
It's quite well-known that the system

⎧⎩⎨⎪⎪𝑃(𝑥0)=𝑦0,𝑃(𝑥1)=𝑦1,…𝑃(𝑥𝑛)=𝑦𝑛
has a unique solution 𝑃(𝑥)
 among polynomials of degree at most 𝑛
. A direct way to prove that 𝑃(𝑥)
 exists is through Lagrange's interpolation. To have a better grasp of it, let's recall that 𝑃(𝑥)≡𝑃(𝑥0)(mod𝑥−𝑥0)
, thus system becomes

⎧⎩⎨⎪⎪𝑃(𝑥)≡𝑦0(mod𝑥−𝑥0),𝑃(𝑥)≡𝑦1(mod𝑥−𝑥1),…𝑃(𝑥)≡𝑦𝑛(mod𝑥−𝑥𝑛).
From the Chinese remainder theorem follows that 𝑃(𝑥)
 is unique modulo 𝑄(𝑥)=(𝑥−𝑥0)…(𝑥−𝑥𝑛)
 and is explicitly given as

𝑃(𝑥)=∑𝑖=0𝑛𝑦𝑖𝑄𝑖(𝑥)𝑄𝑖(𝑥𝑖),
where 𝑄𝑖(𝑥)=𝑄(𝑥)𝑥−𝑥𝑖
. Noteworthy, 𝑄𝑖(𝑥𝑖)=𝑄′(𝑥𝑖)
, as 𝑄′(𝑥)=𝑄0(𝑥)+⋯+𝑄𝑛(𝑥)
 and 𝑄𝑗(𝑥𝑖)=0
 when 𝑖≠𝑗
.

Partial fraction decomposition
The other well-known fact is that for deg𝑃<deg𝑄
, rational function

𝑃(𝑥)𝑄(𝑥)=𝑃(𝑥)(𝑥−𝑥0)𝑑0…(𝑥−𝑥𝑛)𝑑𝑛
can be represented as the sum

𝑃(𝑥)𝑄(𝑥)=∑𝑖=0𝑛С𝑖(𝑥)(𝑥−𝑥𝑖)𝑑𝑖
where deg𝐶𝑖<𝑑𝑖
. A bit less well-known are the explicit formulas to compute 𝐶𝑖(𝑥)
 and their connection to Lagrange interpolation. To begin with, let's look on this expression in the case 𝑑0=⋯=𝑑𝑛=1
 and multiply both sides with 𝑄(𝑥)
. What we obtain is

𝑃(𝑥)=∑𝑖=0𝑛𝑐𝑖𝑄(𝑥)𝑥−𝑥𝑖=∑𝑖=0𝑛𝑐𝑖𝑄𝑖(𝑥).
It is strikingly similar to the Lagrange interpolation expression, from which we may derive that

𝑐𝑖=𝑃(𝑥𝑖)𝑄𝑖(𝑥𝑖)=𝑃(𝑥𝑖)𝑄′(𝑥𝑖).
Thus, for monic square-free polynomial 𝑄(𝑥)
 with deg𝑃<deg𝑄
 it holds that

𝑃(𝑥)𝑄(𝑥)=∑𝑖=0𝑛𝑃(𝑥𝑖)𝑄′(𝑥𝑖)1𝑥−𝑥𝑖.
Multiplicities
Let's now understand what's going on when 𝑄(𝑥)
 have multiple roots. The corresponding system of equations would look like this:

⎧⎩⎨⎪⎪𝑃(𝑥)≡𝑌0(𝑥)(mod(𝑥−𝑥0)𝑑0),𝑃(𝑥)≡𝑌1(𝑥)(mod(𝑥−𝑥1)𝑑1),…𝑃(𝑥)≡𝑌𝑛(𝑥)(mod(𝑥−𝑥𝑛)𝑑𝑛).
Utilizing Chinese remainder theorem again, the solution to the whole system is given as

𝑃(𝑥)=∑𝑖=0𝑛𝑄𝑖(𝑥)⋅[𝑌𝑖(𝑥)⋅𝑄−1𝑖(𝑥)mod(𝑥−𝑥𝑖)𝑑𝑖],
where 𝑄𝑖(𝑥)=𝑄(𝑥)(𝑥−𝑥𝑖)𝑑𝑖
. Let's get back to partial fraction decomposition. If both parts are multiplied by 𝑄(𝑥)
, we get

𝑃(𝑥)=∑𝑖=0𝑛𝐶𝑖(𝑥)𝑄𝑖(𝑥),
thus 𝐶𝑖(𝑥)=[𝑃(𝑥)⋅𝑄−1𝑖(𝑥)mod(𝑥−𝑥𝑖)𝑑𝑖]
 and the final formula is as follows:

𝑃(𝑥)𝑄(𝑥)=∑𝑖=0𝑛[𝑃(𝑥)⋅𝑄−1𝑖(𝑥)mod(𝑥−𝑥𝑖)𝑑𝑖](𝑥−𝑥𝑖)𝑑𝑖.
Shifted remainders
For 𝑑𝑖=1
 we knew that 𝑄−1(𝑥)
 modulo 𝑥−𝑥𝑖
 is equial to the inverse of 𝑄(𝑥𝑖)
. To compute 𝑄−1(𝑥)mod(𝑥−𝑥𝑖)𝑑𝑖
, we should change the basis from 1,𝑥,𝑥2,…
 to 1,𝑥−𝑥𝑖,(𝑥−𝑥𝑖)2,…
, so that 𝑄(𝑥)=𝑄𝑥𝑖(𝑥−𝑥𝑖)
. Now, computing 𝑄−1(𝑥)
 modulo (𝑥−𝑥𝑖)𝑑𝑖
 is the same as computing 𝑄−1𝑥𝑖(𝑥)
 modulo 𝑥𝑑𝑖
 and changing the basis back to 1,𝑥,𝑥2,…
, which is done as follows:

∑𝑖=0𝑛𝑝𝑖(𝑥+𝑎)𝑖=∑𝑖=0𝑛∑𝑗=0𝑖(𝑖𝑗)𝑝𝑖𝑥𝑗𝑎𝑖−𝑗==∑𝑗=0𝑛𝑥𝑗𝑗!∑𝑖=𝑗𝑛𝑝𝑖𝑖!⋅𝑎𝑖−𝑗(𝑖−𝑗)!.
The latter sum can be computed for all 𝑗
 as a convolution of the sequences 𝑝𝑖𝑖!
 and 𝑎𝑖−𝑗(𝑖−𝑗)!
 in 𝑂(𝑛log𝑛)
.

Then, the whole partial fraction decomposition can be computed in 𝑂(𝑛log2𝑛)
 with divide and conquer technique similar to the polynomial evaluation. In fact, when 𝑄(𝑥)
 is square-free it exactly is a polynomial evaluation, as it is enough to compute 𝑃(𝑥𝑖)
 and 𝑄′(𝑥𝑖)
 for all 𝑥𝑖
 to compute the partial fraction decomposition in this case.

Noteworthy, 𝑂(𝑛log2𝑛)
 algorithm to compute partial fraction decomposition was proposed in 1976 by H. T. Kung, who is also known as one of the authors of 𝑂(𝑛log𝑛)
 algorithm to compute inverse series of polynomial with Newton method (Sieveking-Kung algorithm).

Interpreting equivalence with multiplicities
We may reckon the Taylor expansion formula

𝑃(𝑥)≡𝑃(𝑥𝑖)+𝑃′(𝑥𝑖)(𝑥−𝑥𝑖)+⋯+𝑃(𝑘)(𝑥𝑖)(𝑥−𝑥𝑖)𝑘𝑘!+…
Thus, we have the following equivalence:

𝑃(𝑥)≡𝑌𝑖(𝑥)(mod(𝑥−𝑥𝑖)𝑑𝑖)⟺⎧⎩⎨⎪⎪⎪⎪𝑃(𝑥𝑖)=𝑌𝑖(𝑥𝑖),𝑃′(𝑥𝑖)=𝑌′𝑖(𝑥𝑖),…𝑃(𝑑𝑖−1)(𝑥𝑖)=𝑌(𝑑𝑖−1)𝑖(𝑥𝑖)
So, equivalence modulo the polynomial which have multiple roots is, essentially, still equivalent to equalities on values in the roots of this polynomial, but not only of 𝑃(𝑥)
, but also its derivatives up to the multiplicity of the root.

Tags polynomial interpolation, lagrange-interpolation, chinese remainder theo., crt, polynomials, partial fraction
Vote: I like it+171Vote: I do not like it
Author adamantPublication date 2 years agoComments 2

Multi-dimensional ternary search

By adamant, history, 2 years ago, In English
Hi everyone!

I'm currently trying to write an article about 𝜆
-optimization in dynamic programming, commonly known as "aliens trick". While writing it, I stumbled upon a fact which, I believe, is a somewhat common knowledge, but is rarely written out and proved explicitly. This fact is that we sometimes can repeatedly use ternary search when we need to optimize a multi-dimensional function.

Thanks to

mango_lassi for a useful discussion on this and for counter-example on integer ternary search!
Neodym for useful comments and remarks about the article structure.
1
-dimensional search
Def. 1. A function 𝑓:𝑋→ℝ
, where 𝑋⊂ℝ
, is called unimodal if there is a segment [𝐿,𝑅]
 such that 𝑓(𝑥)
 strictly decreases on (−∞,𝐿]∩𝑋
, strictly increases on 𝑥∈[𝑅,+∞)∩𝑋
 and is equal to its global infimum on 𝑥∈[𝐿,𝑅]∩𝑋
.

Ternary search is a family of similar algorithms that allow to estimate the infimum value of a unimodal function 𝑓:𝑋→ℝ
 on a continuous segment of arguments by splitting it into 3
 roughly equal parts and reducing the search to at most 2
 of these parts.

The algorithm takes a segment [𝑙,𝑟]
 as an input and chooses two points 𝑥1,𝑥2∈𝑋
 such that 𝑙≤𝑥1<𝑥2≤𝑟
. If 𝑓(𝑥1)<𝑓(𝑥2)
, the segment is reduced to [𝑙,𝑥2]
 and otherwise it's reduced to [𝑥1,𝑟]
.

The algorithm stops when a certain condition is met, typically when 𝑟−𝑙≤𝜀
 or when a certain number of iterations is met.

Multi-dimensional unimodality
Def. 2. A set 𝑋⊂ℝ𝑐
 is called convex if for all 𝑥1,𝑥2∈𝑋
 and all 𝜆∈[0,1]
 it contains 𝑥1+𝜆[𝑥2−𝑥1]
.

Let 𝑋⊂ℝ𝑐
 be a convex set. There are two most common ways to generalize unimodality on higher dimensions.

Def. 3. A function 𝑓:𝑋→ℝ
 is called convex if for any 𝑥1,𝑥2∈𝑋
 and any 𝜆∈[0,1]
, it holds that

𝑓(𝑥1+𝜆[𝑥2−𝑥1])≤𝑓(𝑥1)+𝜆[𝑓(𝑥2)−𝑓(𝑥1)].
Def. 4. A function 𝑓:𝑋→ℝ
 is called strictly quasi-convex if for any 𝑥1,𝑥2∈𝑋
 and any 𝜆∈(0,1)
, it holds that

𝑓(𝑥1+𝜆[𝑥2−𝑥1])<max{𝑓(𝑥1),𝑓(𝑥2)}.
It is possible to prove that a 1
-dimensional convex or strictly quasi-convex function is also unimodal. Moreover, all unimodal functions for which 𝐿=𝑅
 (infimal segment has a zero length) are also strictly quasi-convex, so these classes are almost equivalent on ℝ
.

However, convexity and strict quasi-convexity do not imply one another. For example, max(1,|𝑥|)
 is convex, but not strictly quasi-convex. On the other hand, |𝑥|‾‾‾√
 is strictly quasi-convex, but not convex. Ternary search would work on both of these functions.

Note that 𝑓(𝑥)
 must be strictly decreasing until it reaches minimum value and after the minimum value segment it must be strictly increasing for this to work. Otherwise it would be impossible to choose a correct segment reduction for 𝑓(𝑥1)=𝑓(𝑥2)
. This also means that non-strict quasi-convexity is not enough, as floor(|𝑥|)
 is quasi-convex and has segments of equal values other than 𝑓(𝑥∗)
.

Multi-dimensional search
Consider the problem of estimating the infimum of 𝑓:𝑋×𝑌→ℝ
. We can notice that

inf𝑥,𝑦𝑓(𝑥,𝑦)=inf𝑥inf𝑦𝑓(𝑥,𝑦)=inf𝑥ℎ(𝑥),
where

ℎ(𝑥)=inf𝑦𝑓(𝑥,𝑦)=inf𝑦𝑓𝑥(𝑦).
Def. 5. Function ℎ(𝑥)
 defined above is called an infimal projection of 𝑓(𝑥,𝑦)
 on 𝑋
.

For 𝑋⊂ℝ
 and 𝑌⊂ℝ𝑐−1
 it allows to reduce the minimization of 𝑐
-dimensional 𝑓(𝑥,𝑦)
 to the minimization of 1
-dimensional function ℎ(𝑥)
, to compute which one needs to solve the minimization of (𝑐−1)
-dimensional function 𝑓𝑥(𝑦)
. The similar reduction can be applied to 𝑓𝑥(𝑦)
 to obtain a nested sequence of 1
-dimensional minimization problems.

For 𝑐
-dimensional function 𝑓(𝑥1,…,𝑥𝑛)
 this reduction would look as follows:

inf𝑥1…𝑥𝑛𝑓(𝑥1,…,𝑥𝑛)=inf𝑥1ℎ(𝑥1)=inf𝑥1inf𝑥2ℎ𝑥1(𝑥2)=inf𝑥1inf𝑥2inf𝑥3ℎ𝑥1𝑥2(𝑥3)=…,
where

ℎ𝑥1…𝑥𝑘−1(𝑥𝑘)={inf𝑥𝑘+1ℎ𝑥1…𝑥𝑘(𝑥𝑘+1)𝑓(𝑥1,…,𝑥𝑛),𝑘<𝑛,,𝑘=𝑛.
Possible approach to this problem would be to apply a ternary search on each reduction level, so that the ternary search on level 𝑘
minimizes 𝑓(𝑥1,…,𝑥𝑛)
 directly with fixed 𝑥1,…,𝑥𝑛−1
 that are given from the levels above when 𝑘=𝑛
,
passes 𝑥1,…,𝑥𝑘−1
 and 𝑥𝑘
 to the ternary search on the level below and uses its result to compute ℎ𝑥1,…,𝑥𝑘−1(𝑥𝑘)
 when 𝑘<𝑛
.
Sometimes, it is possible to prove directly that all functions ℎ𝑥1…𝑥𝑘(𝑥𝑘+1)
 are unimodal.

Another possible way when the domain of 𝑓
 is a Cartesian product of convex sets is to prove that 𝑓
 as a whole is convex or strictly quasi-convex, which would imply that all one-dimensional functions in the reduction preserve this property.

We will focus on the very first transition from 𝑓(𝑥,𝑦)
 to ℎ(𝑥)
 and 𝑓𝑥(𝑦)
 where 𝑋⊂ℝ
 and 𝑌⊂ℝ𝑐−1
. We will assume that 𝑓(𝑥,𝑦)
 is strictly quasi-convex and will prove that ℎ(𝑥)
 and 𝑓𝑥(𝑦)
 are also strictly quasi-convex, the rest will follow from mathematical induction.

Proposition 1. Let 𝑓:𝑋×𝑌→ℝ
 be a strictly quasi-convex function. Then 𝑓𝑥(𝑦)=𝑓(𝑥,𝑦)
 and ℎ(𝑥)=inf𝑦𝑓(𝑥,𝑦)
 are strictly quasi-convex as well.

Proof. 𝑓𝑥
 is quasi-convex by definition, as it is equal to 𝑓(𝑥,𝑦)
 on the interval between (𝑥,inf𝑌)
 and (𝑥,sup𝑌)
.

Let 𝑦1
 and 𝑦2
 to be optimal 𝑦
 for ℎ(𝑥1)
 and ℎ(𝑥2)
. The interval between (𝑥1,𝑦1)
 and (𝑥2,𝑦2)
 contains a point (𝑥,𝑦)
 such that

ℎ(𝑥)≤𝑓(𝑥,𝑦)<max{𝑓(𝑥1,𝑦1),𝑓(𝑥2,𝑦2)}=max{ℎ(𝑥1),ℎ(𝑥2)}.
In a very similar way it is possible to prove that ℎ(𝑥)
 and 𝑓𝑥(𝑦)
 are convex when 𝑓(𝑥,𝑦)
 is.

Proposition 2. Let 𝑓:𝑋×𝑌→ℝ
 be a convex function. Then 𝑓𝑥(𝑦)=𝑓(𝑥,𝑦)
 and ℎ(𝑥)=inf𝑦𝑓(𝑥,𝑦)
 are convex as well.

Proof. 𝑓𝑥
 is convex by definition. Let 𝑥1,𝑥2∈𝑋
 and let 𝑦1
 and 𝑦2
 to be optimal 𝑦
 for ℎ(𝑥1)
 and ℎ(𝑥2)
.

The interval between (𝑥1,𝑦1)
 and (𝑥2,𝑦2)
 contains a point (𝑥,𝑦)=(𝑥1,𝑦1)+𝜆[(𝑥2,𝑦2)−(𝑥1,𝑦1)]
 such that

ℎ(𝑥)≤𝑓(𝑥,𝑦)≤𝑓(𝑥1,𝑦1)+𝜆[𝑓(𝑥2,𝑦2)−𝑓(𝑥1,𝑦1)]=ℎ(𝑥1)+𝜆[ℎ(𝑥2)−ℎ(𝑥1)].
Perhaps, more intuitive way to see this is to notice that the epigraph of ℎ(𝑥)
 is a projection on the 𝑥𝑂𝑧
 plane of the epigraph of 𝑓(𝑥,𝑦)
.

Below is the illustration. For 𝑓(𝑥,𝑦)=(𝑥−5)2+(𝑦−5)24+1
, its infimal projection is ℎ(𝑥)=(𝑥−5)2+1
.



Another illustration with a non-convex function: 𝑓(𝑥,𝑦)=|𝑥|+|𝑦−5|+cos(𝑥+𝑦−5)
, ℎ(𝑥)=|𝑥|+cos𝑥
.



Caveat
Convexity and strict quasi-convexity of 𝑓
 on ℝ2
 don't necessarily imply that ternary search would work on ℤ2
. Let

𝑓(𝑥,𝑦)=(𝑦−𝑥2)2+𝑥2+𝑦2109.
The function 𝑓
 is convex and strictly quasi-convex on ℝ2
. However, in small integers ℎ(2𝑘)≈0
 and ℎ(2𝑘+1)≈14
, thus the function is neither convex nor uni-modal. It doesn't mean that ternary search is never applicable to integers, however you would need to be extra careful and prove directly that ℎ
 and 𝑓𝑥
 are uni-modal to use it.

Open questions
Are there any problems that are solvable with nested ternary search over integers or any other discrete set?
Are there any better criterion for ternary search to be applicable in a discrete case, other than proving uni-modality of ℎ
 and 𝑓𝑥
?
What would be the most meaningful generalization of binary search on multi-dimensional case? Quasi-linear functions?
Tags ternary search, optimization, tutorial

Moment-generating functions, inversions and q-analogs

By adamant, history, 23 months ago, In English
Hi everyone!

Today I want to write about the inversions in permutations. The blog is mostly inspired by the problem С from Day 3 of 2022 winter Petrozavodsk programming camp. I will also try to shed some light on the relation between inversions and 𝑞
-analogs.

Key results
Let 𝐹(𝑥)=𝑎0+𝑎1𝑥+𝑎2𝑥2+…
, then 𝐹(𝑒𝑥)
 is the exponential generating function of

𝑏𝑖=∑𝑘=0∞𝑎𝑘𝑘𝑖.
In other words, it is a moment-generating function of the parameter by which 𝐹(𝑥)
 enumerates objects of class 𝐹
.

Motivational example:

The generating function of permutations of size 𝑛
, enumerated by the number of inversions is

𝐹𝑛(𝑥)=∏𝑘=1𝑛1−𝑥𝑘1−𝑥.
The moment-generating function for the number of inversions in a permutation of size 𝑛
 is

𝐺𝑛(𝑥)=∏𝑘=1𝑛1−𝑒𝑘𝑥1−𝑒𝑥.


Model problem
Let 𝑖𝑛𝑣(𝑝)
 be the number of inversions in permutation 𝑝
. You're given 𝑛
 and 𝑘
, calculate

𝑑𝑛(𝑘)=∑𝑝∈𝑆𝑛𝑖𝑛𝑣(𝑝)𝑘.
Direct solution
First thing one should notice to solve it is that

∑𝑝∈𝑆𝑛+1𝑖𝑛𝑣(𝑝)𝑘=∑𝑖=0𝑛∑𝑞∈𝑆𝑛(𝑖𝑛𝑣(𝑞)+𝑖)𝑘.
This is due to the fact that when you insert (𝑛+1)
 in the permutation 𝑞
 of 𝑛
, the number of inversions increases by 𝑛−𝑖
, where 𝑖
 is the index of (𝑛+1)
 in the new permutation. Expanding this expression, we get

∑𝑝∈𝑆𝑛+1𝑖𝑛𝑣(𝑝)𝑘=∑𝑖=0𝑛∑𝑗=0𝑘(𝑘𝑗)𝑖𝑘−𝑗∑𝑞∈𝑆𝑛𝑖𝑛𝑣(𝑞)𝑗,
which rewrites in 𝑑
-terms as

𝑑𝑛+1(𝑘)𝑘!=∑𝑗=0𝑘(𝑑𝑛(𝑗)𝑗!)(∑𝑖=0𝑛𝑖𝑘−𝑗(𝑘−𝑗)!).
Let's denote the moment-generating function of 𝑖𝑛𝑣(𝑞)
 over 𝑆𝑛
 as

𝐺𝑛(𝑥)=∑𝑡=0∞𝑥𝑡∑𝑞∈𝑆𝑛𝑖𝑛𝑣(𝑞)𝑡𝑡!=∑𝑡=0∞𝑥𝑡𝑑𝑛(𝑡)𝑡!,
then the equation above rewrites as

𝐺𝑛+1(𝑥)=𝐺𝑛(𝑥)𝐴𝑛(𝑥),
with the base case 𝐺1(𝑥)=1
, where

𝐴𝑛(𝑥)=∑𝑡=0∞𝑥𝑡∑𝑖=0𝑛𝑖𝑡𝑡!=∑𝑖=0𝑛𝑒𝑖𝑥=1−𝑒(𝑛+1)𝑥1−𝑒𝑥.
So, the explicit expression for 𝐺𝑛(𝑥)
 is

𝐺𝑛(𝑥)=∏𝑡=1𝑛1−𝑒𝑡𝑥1−𝑒𝑥.
This formula right away allows to calculate 𝑑𝑛(𝑘)
 for all 𝑛≤𝑁
 and 𝑘≤𝐾
 in 𝑂(𝑁𝐾log𝐾)
.

𝑞
-analogs
General idea of 𝑞
-analogs is to generalize some expression with the new parameter 𝑞
 such that it is identical to the initial expression as 𝑞→1
. One noteworthy example of 𝑞
-analogs that you may be familiar with from competitive programming is the subset convolution:

We want to multiply polynomials in 𝑅[𝑥1,…,𝑥𝑛]/⟨𝑥21,…,𝑥2𝑛⟩
. As this is non-trivial, we instead multiply them in

𝑅[𝑞][𝑥1,…,𝑥𝑛]/⟨𝑥21−𝑞2,…,𝑥2𝑛−𝑞2⟩
or

𝑅[𝑞][𝑥1,…,𝑥𝑛]/⟨𝑥21−𝑥1𝑞,…,𝑥2𝑛−𝑥𝑛𝑞⟩.
With 𝑞→1
 we may see that the first ring gives the 𝑞
-analog of xor-convolution (Walsh-Hadamard transform), while the second expression is the 𝑞
-analog of or-convolution (Möbius transform). And 𝑞→0
 gives us the subset convolution in both cases.

𝑞
-factorials
𝑞
-analog of a positive integer number 𝑛
 is typically defined as

[𝑛]𝑞=1+𝑞+⋯+𝑞𝑛−1=1−𝑞𝑛1−𝑞.
From this expression, we may derive the so-called 𝑞
-factorial:

[𝑛]𝑞!=[1]𝑞[2]𝑞…[𝑛]𝑞=∏𝑘=1𝑛1−𝑞𝑘1−𝑞.
This expression should already be familiar to us, as with 𝑞→𝑒𝑥
 it is the moment-generating function for the number of inversions. On the other hand, with 𝑞→1
 this expression is simply equal to 𝑛!
, thus it's natural to assume that [𝑛]𝑞!
 enumerates permutations in some way.

Consider the generating function for the permutations of length 𝑛
 enumerated by the number of inversions:

𝐹𝑛+1(𝑥)=∑𝑝∈𝑆𝑛+1𝑥𝑖𝑛𝑣(𝑝)=∑𝑖=0𝑛∑𝑝∈𝑆𝑛𝑥𝑖𝑛𝑣(𝑝)+𝑖=∑𝑖=0𝑛𝑥𝑖𝐹𝑛(𝑥)=1−𝑥𝑛+11−𝑥𝐹𝑛(𝑥)=[𝑛+1]𝑥!
It means that [𝑛]𝑞!
 in fact, enumerates permutations of 𝑆𝑛
 grouped by the number of inversions. In other words,

𝐹𝑛(𝑞)=∏𝑘=1𝑛1−𝑞𝑘1−𝑞=[𝑛]𝑞!
Moment-generating function
Let 𝐹(𝑥)=𝑎0+𝑎1𝑥+𝑎2𝑥2+…
 be the ordinary generating function for the sequence 𝑎0,𝑎1,…
, then

𝐹(𝑒𝑥)=∑𝑘=0∞𝑎𝑘𝑒𝑘𝑥=∑𝑘=0∞𝑎𝑘∑𝑖=0∞𝑘𝑖𝑥𝑖𝑖!=∑𝑖=0∞𝑥𝑖𝑖!∑𝑘=0∞𝑎𝑘𝑘𝑖.
In other words, 𝐹(𝑒𝑥)
 is the EGF for

𝑏𝑖=∑𝑘=0∞𝑎𝑘𝑘𝑖,
which is the 𝑖
-th moment of the number by which 𝐹(𝑥)
 enumerates objects of class 𝐹
. For 𝐹𝑛(𝑥)
, all permutations 𝑝∈𝑆𝑛
 are enumerated by 𝑖𝑛𝑣(𝑝)
, thus 𝐺𝑛(𝑥)=𝐹𝑛(𝑒𝑥)
 is the moment-generating function for the number of inversions of 𝑝∈𝑆𝑛
.

𝑞
-Pochhammer symbol
𝑞
-Pochhammer symbol is defined as

(𝑎;𝑞)𝑛=(1−𝑎)(1−𝑎𝑞)(1−𝑎𝑞2)…(1−𝑎𝑞𝑛−1),
as (almost) the 𝑞
-analog of the regular Pochhammer symbol

(𝑎)𝑛=𝑎(𝑎−1)…(𝑎−(𝑛−1)).
With 𝑞
-Pochhammer symbol, expressions for 𝐹𝑛(𝑥)
 and 𝐺𝑛(𝑥)
 can be simplified to

𝐹𝑛(𝑥)=(𝑥;𝑥)𝑛(1−𝑥)𝑛,
and

𝐺𝑛(𝑥)=(𝑒𝑥;𝑒𝑥)𝑛(1−𝑒𝑥)𝑛.
Sorry, I can't provide any explanation on why it's useful, but it looks pretty.

Bonus:

Assume that you need to calculate the expected value of 𝑖𝑛𝑣(𝑝)𝑘
 over |𝑝|=𝑛
 with a relatively small 𝑘
 and a large 𝑛
. Then you can do it in 𝑂(𝑘2log𝑘)
 pre-processing and 𝑂(𝑘)
 for every 𝑛
 with a fixed 𝑘
. Deriving specific solution is left to the curious reader as an exercise.

Questions to the audience:

Can anyone get rid of this nasty log𝑘
? Or do the pre-processing without FFT?
Is there a meaningful expression for OGF or EGF of 𝑑𝑛(𝑘)
 where powers of 𝑥
 traverse through 𝑛
 rather than 𝑘
?
Tags generating function, inversions, q-analog, polynomials, tutorial, power series

OGFs, EGFs, differentiation and Taylor shifts

By adamant, history, 23 months ago, In English
Hi everyone!

Today I'd like to write another blog about polynomials. Consider the following problem:

You're given 𝑃(𝑥)=𝑎0+𝑎1𝑥+⋯+𝑎𝑛−1𝑥𝑛−1
, you need to compute 𝑃(𝑥+𝑎)=𝑏0+𝑏1𝑥+⋯+𝑏𝑛−1𝑥𝑛−1
.

There is a well-known solution to this, which involves some direct manipulation with coefficients. However, I usually prefer approach that is more similar to synthetic geometry when instead of low-level coordinate work, we work on a higher level of abstraction. Of course, we can't get rid of direct coefficient manipulation completely, as we still need to do e.g. polynomial multiplications.

But we can restrict direct manipulation with coefficients to some minimal number of black-boxed operations and then strive to only use these operations in our work. With this goal in mind, we will develop an appropriate framework for it.

Thanks to clyring for inspiring me to write about it with this comment. You can check it for another nice application of calculating 𝑔(𝐷)𝑓(𝑥)
 for a specific series 𝑔(𝐷)
 over the differentiation operator:

While this article mostly works with 𝑒𝑎𝐷𝑓(𝑥)
 to find 𝑓(𝑥+𝑎)
, there you have to calculate

(𝐷1−𝑒−𝐷)𝑝(𝑥)
to find a polynomial 𝑓(𝑥)
 such that 𝑓(𝑥)=𝑝(0)+𝑝(1)+⋯+𝑝(𝑥)
 for a given polynomial 𝑝(𝑥)
.

Key results
Let [⋅]
 and {⋅}
 be a linear operators in the space of formal power series such that [𝑥𝑘]=𝑥𝑘𝑘!
 and {𝑥𝑘}=𝑘!𝑥𝑘
.

The transforms [⋅]
 and {⋅}
 are called the Borel transform and the Laplace transform correspondingly.

As we also work with negative coefficients here, we define 1𝑘!=0
 for 𝑘<0
, hence [𝑥𝑘]=0
 for such 𝑘
.

In this notion,

𝑓(𝑥+𝑎)=𝑒𝑎𝐷𝑓(𝑥)=[𝑒𝑎𝑥−1{𝑓(𝑥)}],
where 𝐷=𝑑𝑑𝑥
 is the differentiation operator. Thus, {𝑓(𝑥+𝑎)}
 is a part with non-negative coefficients of the cross-correlation of 𝑒𝑎𝑥
 and {𝑓(𝑥)}
 as formal power series. More generally, for arbitrary formal power series 𝑔(𝐷)
, it holds that

𝑔(𝐷)𝑓(𝑥)=[𝑔(𝑥−1){𝑓(𝑥)}],
that is {𝑔(𝐷)𝑓(𝑥)}
 is exactly the non-negative part of the cross-correlation of 𝑔(𝑥)
 and {𝑓(𝑥)}
.

Detailed explanation is below.

OGF and EGF
Let 𝑎0,𝑎1,…
 be a sequence of numbers. In analytic combinatorics, there are two ways to represent it with a generating function:

𝐹(𝑥)=𝑎0+𝑎1𝑥+𝑎2𝑥2+⋯+𝑎𝑘𝑥𝑘+…𝐺(𝑥)=𝑎0+𝑎1𝑥+𝑎2𝑥22+⋯+𝑎𝑘𝑥𝑘𝑘!+…
Functions 𝐹
 and 𝐺
 are called ordinary generating function and exponential generating function correspondingly.

Example. OGF for the sequence 1,1,…
 is 1+𝑥+𝑥2+⋯=11−𝑥
, while its EGF is 1+𝑥+𝑥22!+⋯=𝑒𝑥
.

Differentiation operator
The differentiation operator 𝐷=𝑑𝑑𝑥
 is formally defined as a linear operator such that 𝐷𝑥𝑘=𝑘𝑥𝑘−1
. In other words,

𝐷𝐹=𝑎1+2𝑎2𝑥+⋯+𝑘𝑎𝑘𝑥𝑘−1+…𝐷𝐺=𝑎1+𝑎2𝑥+𝑎3𝑥22+⋯+𝑎𝑘𝑥𝑘−1(𝑘−1)!+…
Thus, if looked through underlying sequence perspective, 𝐷
 on EGF corresponds to a simple shift of 𝑎0,𝑎1,…
.

Operator exponent
Returning to our original problem, a single power of 𝑥+𝑎
 is given as

(𝑥+𝑎)𝑘=∑𝑖=0𝑘(𝑘𝑖)𝑎𝑖𝑥𝑘−𝑖=𝑘!∑𝑖=0𝑘𝑎𝑖𝑖!𝑥𝑘−𝑖(𝑘−𝑖)!,
or in a symmetric form

(𝑥+𝑎)𝑘𝑘!=∑𝑖+𝑗=𝑘𝑎𝑖𝑖!𝑥𝑗𝑗!.
Knowing that 𝑑𝑑𝑥𝑥𝑘𝑘!=𝑥𝑘−1(𝑘−1)!
, we may rewrite it as

(𝑥+𝑎)𝑘𝑘!=∑𝑖=0𝑘𝑎𝑖𝑖!(𝐷𝑖𝑥𝑘𝑘!)=(∑𝑖=0∞𝑎𝑖𝐷𝑖𝑖!)𝑥𝑘𝑘!=𝑒𝑎𝐷𝑥𝑘𝑘!.
Here 𝑒𝑎𝐷
 is an exponent of operator 𝐷
, formally defined as

𝑒𝑎𝐷=∑𝑖=0∞(𝑎𝐷)𝑖𝑖!.
When 𝑎
 is constant, 𝑎
 and 𝐷
 commute, thus (𝑎𝐷)𝑖=𝑎𝑖𝐷𝑖
. It also means that we can get rid of 𝑘!
 and obtain (𝑥+𝑎)𝑘=𝑒𝑎𝐷𝑥𝑘
 which, in turn, generalizes to 𝑒𝑎𝐷𝐹(𝑥)=𝐹(𝑥+𝑎)
 for an arbitrary formal power series 𝐹
.

Transitions
With the definitions above in mind, let's introduce operations [⋅]
 and {⋅}
 such that 𝐺=[𝐹]
 and 𝐹={𝐺}
. The operations can be computed for any power series by multiplying or dividing its coefficients with 𝑘!
 correspondingly.

There are two essential observations we should make that hold for any formal power series 𝐹
.

First of all, {[𝐹]}=[{𝐹}]=𝐹
 for any formal power series 𝐹
.

The second important observation is

𝐷𝑖[𝑥𝑗]=𝑥𝑗−𝑖(𝑗−𝑖)!=[𝑥𝑗−𝑖],
which can be written generically as

𝐷𝑖[𝑓(𝑥)]=[𝑥−𝑖𝑓(𝑥)]
for arbitrary formal power series 𝑓(𝑥)
. Here we define (𝑗−𝑖)!=∞
 for 𝑖>𝑗
, thus [𝑥𝑘]
 is 0
 for negative 𝑘
.

The second property, due to linearity of [⋅]
 is further generalized as

𝑔(𝐷)[𝑓(𝑥)]=[𝑔(𝑥−1)𝑓(𝑥)]
for the arbitrary formal power series 𝑔(𝐷)
. Combining it with the first property, we get

𝑔(𝐷)𝑓(𝑥)=𝑔(𝐷)[{𝑓(𝑥)}]=[𝑔(𝑥−1){𝑓(𝑥)}].
In particular, for 𝑔(𝐷)=𝑒𝑎𝐷
, we obtain

𝑓(𝑥+𝑎)=𝑒𝑎𝐷𝑓(𝑥)=[𝑒𝑎𝑥−1{𝑓(𝑥)}],
thus {𝑓(𝑥+𝑎)}
 is the non-negative part of the cross-correlation of 𝑒𝑎𝑥
 and {𝑓(𝑥)}
.

Check your understanding
You can implement and check your solution for it on this Library Checker problem.

Tags generating function, power series

Some rotational invariants in geometry

By adamant, history, 22 months ago, In English
Hi everyone!

Today I'd like to write about some polynomials which are invariant under the rotation and relabeling in euclidean spaces. Model problems work with points in the 3D space, however both ideas, to some extent, might be generalized for higher number of dimensions. They might be useful to solve some geometric problems under the right conditions. I used some ideas around them in two problems that I set earlier.

Congruence check in random points
You're given two set of lines in 3D space. The second set of lines was obtained from the first one by the rotation and relabeling. You're guaranteed that the first set of lines was generated uniformly at random on the sphere, find the corresponding label permutation.

Actual problem: 102354F - Cosmic Crossroads.

Solution
Let 𝑃4(𝑥,𝑦,𝑧)=∑𝑙=1𝑛((𝑥−𝑥𝑙)2+(𝑦−𝑦𝑙)2+(𝑧−𝑧𝑙)2)2
. It is a fourth degree polynomial, which geometric meaning is the sum of distances from (𝑥,𝑦,𝑧)
 to all points in the set, each distance raised to power 4
. Distance is preserved under rotation, hence this expression is invariant under rotation transform. On the other hand it may be rewritten as

𝑃4(𝑥,𝑦,𝑧)=∑𝑖=04∑𝑗=04∑𝑘=04𝐴𝑖𝑗𝑘𝑥𝑖𝑦𝑗𝑧𝑘,
where 𝐴𝑖𝑗𝑘
 is obtained as the sum over all points (𝑥𝑙,𝑦𝑙,𝑧𝑙)
 from the initial set. To find the permutation, it is enough to calculate 𝑃4
 for all points in both sets and them match points with the same index after they were sorted by the corresponding 𝑃4
 value.

It is tempting to try the same trick with 𝑃2(𝑥,𝑦,𝑧)
, but it is the same for all the points in the set for this specific problem:

𝑃2(𝑥,𝑦,𝑧)===∑𝑙=1𝑛[(𝑥−𝑥𝑙)2+(𝑦−𝑦𝑙)2+(𝑧−𝑧𝑙)2]𝑛⋅(𝑥2+𝑦2+𝑧2)−2𝑥∑𝑙=1𝑛𝑥𝑙−2𝑦∑𝑙=1𝑛𝑦𝑙−2𝑧∑𝑙=1𝑛𝑧𝑙+∑𝑙=1𝑛[𝑥2𝑙+𝑦2𝑙+𝑧2𝑙]𝑛[(𝑥−𝑥¯)2+(𝑦−𝑦¯)2+(𝑧−𝑧¯)2]−𝑛(𝑥¯2+𝑦¯2+𝑧¯2)+∑𝑙=1𝑛(𝑥2𝑙+𝑦2𝑙+𝑧2𝑙),
where 𝑥¯
, 𝑦¯
 and 𝑧¯
 are the mean values of 𝑥𝑙
, 𝑦𝑙
 and 𝑧𝑙
 correspondingly. As you can see, non-constant part here is simply the squared distance from (𝑥,𝑦,𝑧)
 to the center of mass of the points in the set. Thus, 𝑃2(𝑥,𝑦,𝑧)
 is the same for all points having the same distance from the center of mass, so it is of no use in 102354F - Cosmic Crossroads, as all the points have this distance equal to 1
 in the input.

Burunduk1 taught me this trick after the Petrozavodsk camp round which featured the model problem.

Sum of squared distances to the axis passing through the origin
You're given a set of points 𝑟𝑘=(𝑥𝑘,𝑦𝑘,𝑧𝑘)
. A torque needed to rotate the system of points around the axis 𝑟=(𝑥,𝑦,𝑧)
 is proportional to the sum of squared distances to the axis across all points. You need to find the minimum amount of points that have to be added to the set, so that the torque needed to rotate it around any axis passing through the origin is exactly the same.

Actual problem: Hackerrank — The Axis of Awesome

Solution
The squared distance from the point 𝑟𝑘
 to the axis 𝑟
 is expressed as

|𝑟𝑘×𝑟|2𝑟⋅𝑟=(𝑦𝑘𝑧−𝑧𝑘𝑦)2+(𝑥𝑘𝑧−𝑧𝑘𝑥)2+(𝑥𝑘𝑦−𝑦𝑘𝑥)2𝑥2+𝑦2+𝑧2.
The numerator here is a quadratic form, hence can be rewritten as

|𝑟𝑘×𝑟|2=(𝑥𝑦𝑧)⎛⎝⎜⎜⎜𝑦2𝑘+𝑧2𝑘−𝑥𝑘𝑦𝑘−𝑥𝑘𝑧𝑘−𝑥𝑘𝑦𝑘𝑥2𝑘+𝑧2𝑘−𝑦𝑘𝑧𝑘−𝑥𝑘𝑧𝑘−𝑦𝑘𝑧𝑘𝑥2𝑘+𝑦2𝑘⎞⎠⎟⎟⎟⎛⎝⎜⎜𝑥𝑦𝑧⎞⎠⎟⎟.
Correspondingly, the sum of squared distances for 𝑘=1..𝑛
 is defined by the quadratic form

𝐼=∑𝑘=1𝑛⎛⎝⎜⎜⎜𝑦2𝑘+𝑧2𝑘−𝑥𝑘𝑦𝑘−𝑥𝑘𝑧𝑘−𝑥𝑘𝑦𝑘𝑥2𝑘+𝑧2𝑘−𝑦𝑘𝑧𝑘−𝑥𝑘𝑧𝑘−𝑦𝑘𝑧𝑘𝑥2𝑘+𝑦2𝑘⎞⎠⎟⎟⎟,
known in analytic mechanics as the inertia tensor. As any other tensor, its coordinate form is invariant under rotation.

Inertia tensor is a positive semidefinite quadratic form, hence there is an orthonormal basis in which it is diagonal:

𝐼=⎛⎝⎜⎜𝐼1000𝐼2000𝐼3⎞⎠⎟⎟.
Here 𝐼1
, 𝐼2
 and 𝐼3
 are the eigenvalues of 𝐼
, also called the principal moments of inertia (corresponding eigenvectors are called the principal axes of inertia). From this representation we deduce that the condition from the statement is held if and only if 𝐼1=𝐼2=𝐼3
.

Adding a single point on a principal axis would only increase principal moments on the other axes. For example, adding (𝑥,0,0)
 would increase 𝐼2
 and 𝐼3
 by 𝑥2
. Knowing this, one can prove that the answer to the problem is exactly 3−𝑚
 where 𝑚
 is the multiplicity of the smallest eigenvalue of 𝐼
.

Applying it to the first problem
Now, another interesting observation about inertia tensor is that both principal inertia moments and principal inertia axes would be preserved under rotation. It means that in the first problem, another possible way to find the corresponding rotation and the permutation of points is to find principal inertia axes for both sets of points and then find a rotation that matches corresponding principal inertia axes in the first and the second sets of points.

Unfortunately, this method still requires that principal inertia moments are all distinct (which generally holds for random sets of points), otherwise there would be an infinite amount of eigendecompositions of 𝐼
.

Tags tutorial, geometry

On linear recurrences and the math behind them

By adamant, history, 22 months ago, In English
Hi everyone!

There are already dozens of blogs on linear recurrences, why not make another one? In this article, my main goal is to highlight the possible approaches to solving linear recurrence relations, their applications and implications. I will try to derive the results with different approaches independently from each other, but will also highlight similarities between them after they're derived.

Definitions
Def. 1. An order 𝑑
 homogeneous linear recurrence with constant coefficients (or linear recurrence) is an equation of the form

𝐹𝑛=∑𝑘=1𝑑𝑎𝑘𝐹𝑛−𝑘.
Def. 2. In the equation above, the coefficients 𝑎1,…,𝑎𝑑∈𝑅
 are called the recurrence parameters,

Def. 3. and a sequence 𝐹0,𝐹1,⋯∈𝑅
 is called an order 𝑑
 linear recurrence sequence.

The most common task with linear recurrences is, given initial coefficients 𝐹0,𝐹1,…,𝐹𝑑−1
, to find the value of 𝐹𝑛
.
Example 1. A famous Fibonacci sequence 𝐹𝑛=𝐹𝑛−1+𝐹𝑛−2
 is an order 2 linear recurrence sequence.
Example 2. Let 𝐹𝑛=𝑛2
. One can prove that 𝐹𝑛=3𝐹𝑛−1−3𝐹𝑛−2+𝐹𝑛−3
.

Example 3. Moreover, for 𝐹𝑛=𝑃(𝑛)
, where 𝑃(𝑛)
 is a degree 𝑑
 polynomial, it holds that

𝐹𝑛=∑𝑘=1𝑑+1(−1)𝑘+1(𝑑+1𝑘)𝐹𝑛−𝑘.
If this fact is not obvious to you, do not worry as it will be explained further below.

Finally, before proceeding to next sections, we'll need one more definition.
Def. 4. A polynomial
𝐴(𝑥)=𝑥𝑑−∑𝑘=1𝑑𝑎𝑘𝑥𝑑−𝑘
is called the characteristic polynomial of the linear recurrence defined by 𝑎1,…,𝑎𝑑
.

Example 4. For Fibonacci sequence, the characteristic polynomial is 𝐴(𝑥)=𝑥2−𝑥−1
.

Matrix approach
For linear recurrence sequences it holds that

𝐅𝑛=⎡⎣⎢⎢⎢⎢𝐹𝑛+𝑑−1𝐹𝑛+𝑑−2…𝐹𝑛⎤⎦⎥⎥⎥⎥=⎡⎣⎢⎢⎢⎢𝑎11⋮0𝑎20⋮0……⋱…𝑎𝑑−10⋮1𝑎𝑑0⋮0⎤⎦⎥⎥⎥⎥⎡⎣⎢⎢⎢⎢𝐹𝑛+𝑑−2𝐹𝑛+𝑑−3…𝐹𝑛−1⎤⎦⎥⎥⎥⎥=⎛⎝⎜⎜⎜⎜⎡⎣⎢⎢⎢⎢𝑎11⋮0𝑎20⋮0……⋱…𝑎𝑑−10⋮1𝑎𝑑0⋮0⎤⎦⎥⎥⎥⎥⎞⎠⎟⎟⎟⎟𝑛⎡⎣⎢⎢⎢⎢𝐹𝑑−1𝐹𝑑−2…𝐹0⎤⎦⎥⎥⎥⎥=𝐴𝑛𝐅0.
This representation allows to find 𝐹𝑛
 in 𝑂(𝑑3log𝑛)
 with binary matrix exponentiation.

Improving the complexity of this solution is possible with the following theorem:

Theorem 1 (Cayley–Hamilton theorem). Every square matrix over a commutative ring satisfies its own characteristic polynomial.
For the matrix 𝐴
 used above, it can be proven that its characteristic polynomial is (−1)𝑑𝐴(𝑥)
, thus
𝐴𝑑=∑𝑘=1𝑑𝑎𝑘𝐴𝑑−𝑘.
For the purpose of calculating 𝐹𝑛
 it means that we may find 𝑏0,𝑏1,…,𝑏𝑑−1
 such that

𝑥𝑛≡𝑏0+𝑏1𝑥+⋯+𝑏𝑑−1𝑥𝑑−1mod𝐴(𝑥),
and this equivalence would also hold if we substitute 𝑥
 with 𝐴
. This also implies that

𝐹𝑛=(𝐅𝑛)𝑑=(𝐴𝑛𝐅0)𝑑=∑𝑘=0𝑑−1𝑏𝑘(𝐴𝑘𝐅0)𝑑=∑𝑘=0𝑑−1𝑏𝑘𝐹𝑘,
thus allowing to compute 𝐹𝑛
 in 𝑂(𝑑log𝑑log𝑛)
 using the binary lifting modulo 𝐴(𝑥)
 (see on CP-Algorithms).

Approach above is intuitive and good if you want just to find the 𝑛
-th element of the recurrence. However it might be hard to analyze sequences with it (e.g. estimate the asymptotic behavior or find a recurrence for a sum of sequences).
Problem example: CodeChef — Random Number Generator. Just find 𝐹𝑛
, where 𝐹𝑘
 is a given linear recurrence.

Generating function approach
Consider the generating function 𝐺(𝑥)=𝐹0+𝐹1𝑥+𝐹2𝑥2+…
 of the sequence. For 𝐺(𝑥)
 it holds that

𝐺(𝑥)=𝑃(𝑥)+∑𝑘=1𝑑𝑎𝑘𝑥𝑘𝐺(𝑥),
where 𝑃(𝑥)
 is some polynomial of degree less than 𝑑
 used to calibrate the fact that 𝐹0,…,𝐹𝑑−1
 are pretty much arbitrary and might not adhere to the recurrence with lower terms. From this equation we see that

𝐺(𝑥)=𝑃(𝑥)𝑄(𝑥),
where 𝑄(𝑥)
 is a polynomial obtained by reversing the coefficients of 𝐴(𝑥)
, that is

𝑄(𝑥)=1−∑𝑘=1𝑑𝑎𝑘𝑥𝑘=𝑥𝑑𝐴(𝑥−1).
Side note: If you know 𝐹0,…,𝐹𝑑−1
 and 𝑎1,…,𝑎𝑑
, you can find 𝑃(𝑥)
 from

𝑃(𝑥)≡𝐺(𝑥)𝑄(𝑥)(mod𝑥𝑑),
in other words,

𝐺(𝑥)=𝐺(𝑥)𝑄(𝑥)mod𝑥𝑑𝑄(𝑥).
Now, if we allow negative powers near 𝑥
, the following equation will hold:

𝐹𝑛=[𝑥0]𝑥𝑛𝐺(𝑥−1),
where [𝑥𝑘]𝑃(𝑥)
 is the coefficient near 𝑥𝑘
 in the series 𝑃(𝑥)
. We should notice that

𝐴(𝑥)𝐺(𝑥−1)=𝑥𝑑𝑃(𝑥−1)
has only positive coefficients, so is 𝑥𝑘𝐴(𝑥)𝐺(𝑥−1)
 and so is all their linear combinations. In other words,

[𝑥0]𝑋(𝑥)𝐺(𝑥−1)=[𝑥0]𝑌(𝑥)𝐺(𝑥−1)
whenever 𝑋(𝑥)−𝑌(𝑥)
 is divisible by 𝐴(𝑥)
. From this, we conclude that

𝐹𝑛=[𝑥0](𝑥𝑛mod𝐴)𝐺(𝑥−1)=∑𝑘=0𝑑−1𝑏𝑘[𝑥0]𝑥𝑘𝐺(𝑥−1)=∑𝑘=0𝑑−1𝑏𝑘𝐹𝑘,
which is essentially the same result as in the matrix approach.

Graeffe's method
This approach was described by Golovanov399 in this comment.

Alternatively to finding 𝑥𝑛mod𝐴(𝑥)
, you may consider the following transform:

𝐹𝑛=[𝑥𝑛]𝑃(𝑥)𝑄(𝑥)=[𝑥𝑛]𝑃(𝑥)𝑄(−𝑥)𝑄(𝑥)𝑄(−𝑥)=[𝑥𝑛]𝑆0(𝑥2)+𝑥𝑆1(𝑥2)𝑇(𝑥2).
For even 𝑛
, it is equal to [𝑥𝑛/2]𝑆0(𝑥)𝑇(𝑥)
 and for odd 𝑘
 it is equal to [𝑥(𝑛−1)/2]𝑆1(𝑥)𝑇(𝑥)
.

It works with the same complexity, but has a better constant.

Side note: Same approach could be used to find the polynomial inverse in 𝑂(𝑛log𝑛)
:

1𝑄(𝑥)=𝑄(−𝑥)𝑄(𝑥)𝑄(−𝑥)=𝑄(−𝑥)𝑇(𝑥2).
Closed-form solution
While it could be hard to analyze the asymptotic behavior of 𝐹𝑛
 with matrices (you probably can, but you'd need to work with eigenvalues of 𝐴
), it is way simpler with the generating function representation, thanks to the existence of the partial fraction decomposition:

𝑃(𝑥)𝑄(𝑥)=∑𝑖=1𝑚𝐶𝑖(𝑥)(1−𝜆𝑖𝑥)𝑑𝑖,
where 𝜆1,…,𝜆𝑚
 are the roots of 𝐴(𝑥)
 with multiplicities 𝑑1,…,𝑑𝑚
, and 𝐶𝑖(𝑥)
 is a polynomial of degree less than 𝑑𝑖
. In other words,

𝑄(𝑥)=∏𝑖=1𝑚(1−𝜆𝑖𝑥)𝑑𝑖,𝐴(𝑥)=∏𝑖=1𝑚(𝑥−𝜆𝑖)𝑑𝑖.
It is generally known that

1(1−𝜆𝑥)𝑑=∑𝑘=0∞(𝑘+𝑑−1𝑑−1)𝜆𝑘𝑥𝑘,
from which we may conclude that the general solution to the linear recurrence is given by

𝐹𝑛=∑𝑖=1𝑚𝑝𝑖(𝑛)𝜆𝑛𝑖,
where 𝑝𝑖(𝑛)
 is a polynomial of degree less than 𝑑𝑖
, determined from 𝐹0,…,𝐹𝑑−1
.

Example 3 (follow-up): in particular, the formula above means that generating function for 𝑝(0),𝑝(1),…
, where 𝑝(𝑥)
 is a polynomial of degree 𝑑
, has a denominator (1−𝑥)𝑑+1
. This proves the formula from the example 3.

Problem example: Timus — 2125. Continue the Sequence. You're given a sequence 𝑎1,…,𝑎𝑛
 and a number 𝑚
. Let 𝑝(𝑥)
 be a minimum-degree polynomial such that 𝑝(1)=𝑎1,…,𝑝(𝑛)=𝑎𝑛
. Find 𝑝(𝑛+1),…,𝑝(𝑛+𝑚)
 in 𝑂(𝑛log𝑛)
.

Umbral calculus approach
Thanks to ftiasch for letting me know about umbral calculus! This approach makes so much more sense to me now.

Let's do something esoteric. We're given the recurrence

𝐹𝑛=∑𝑘=1𝑑𝑎𝑘𝐹𝑛−𝑘.
Consider formal variable 𝑏
 such that

𝑏𝑛=∑𝑘=1𝑑𝑎𝑘𝑏𝑛−𝑘.
Using the formula above we may express any 𝑏𝑛
 as a linear combination of 𝑏0,…,𝑏𝑑−1
 by calculating it modulo 𝐴(𝑏)
. If, after that, we do a back-substitution of 𝐹𝑘
 instead of 𝑏𝑘
, we will have a formula that is true for any sequence 𝐹
 adhering to the recurrence.

What we did is known as the umbral calculus. Formal theory behind it is as follows.

Consider a linear functional 𝐿:𝑅[𝑏]→𝑅
 such that

𝐿(𝑏𝑛)=𝐹𝑛.
By the definition of the functional,

𝐿(𝑏𝑛−𝑑𝐴(𝑏))=𝐹𝑛−∑𝑘=1𝑑𝑎𝑘𝐹𝑛−𝑘=0.
As it is a linear functional, all possible linear combinations of 𝑏𝑘𝐴(𝑏)
 would also give zero. In other words,

𝐿(𝑋)−𝐿(𝑌)=0
when 𝑋(𝑏)−𝑌(𝑏)
 is divisible by 𝐴(𝑏)
. In other words, ⟨𝐴(𝑏)⟩⊂ker𝐿
, where ⟨𝐴(𝑏)⟩
 is the ideal generated by 𝐴(𝑏)
.

In particular, it is true for 𝑋(𝑏)=𝑏𝑛
 and 𝑌(𝑏)=𝑏𝑛mod𝐴(𝑏)
, leading us to the 𝑏𝑛mod𝐴(𝑏)
 solution again.

Side note: Comparing this approach with the genfunc one, you may notice that 𝐿(𝑃)=[𝑥0]𝑃(𝑥)𝐺(𝑥−1)
.

This is the approach you could've seen earlier in TLE's blog. Special thanks to islingr for a very clear explanation.

I wonder if it can be used to derive closed-form solution...

Shift operator approach
It is of little use in competitive programming, but I like it because of its genericity.

Let Δ:𝑅ℕ→𝑅ℕ
 be a linear operator defined on sequences over 𝑅
, such that (Δ𝑎)𝑛=𝑎𝑛+1
.

With this operator, the recurrence rewrites as

Δ𝑑𝐹=∑𝑘=1𝑑𝑎𝑘Δ𝑑−𝑘𝐹,
or, which is equivalent, as

𝐴(Δ)𝐹=(Δ𝑑−∑𝑘=1𝑑𝑎𝑘Δ𝑑−𝑘)𝐹=0.
Finding all possible 𝐹
 is equivalent to parametrizing the elements of ker𝐴(Δ)
. The following result holds:

Theorem 2. Let 𝐴:𝑋→𝑋
 be a linear operator defined on a complex vector space 𝑋
. Let 𝜆1,…,𝜆𝑚
 be different complex numbers and 𝑑1,…,𝑑𝑚
 be positive integer numbers. Then the following holds:

ker∏𝑖=1𝑚(𝐴−𝜆𝑖)𝑑𝑖=∑𝑖=1𝑚ker(𝐴−𝜆𝑖)𝑑𝑖.
Here ker𝐴={𝑥:𝐴𝑥=0}
 is the kernel of the linear operator 𝐴
 and Σ
 denotes the Minkowski sum of corresponding subspaces.

Proof sketch: If 𝑋
 is finite-dimensional space, one can prove this by considering the Jordan normal form of the corresponding matrix. For infinite-dimension case, it is clear that right-hand side is a subset of left-hand side. For any vector 𝑥
 from the left-hand side, the span of 𝑥,𝐴𝑥,𝐴2𝑥,…
 (known as a Krylov subspace) is finite-dimensional and you can prove that 𝑥
 is also in the right-hand side by reducing all operators to this finite-dimensional subspace (it's invariant for all operators in the equation).

Thanks to tranquility for kindly proving this result to me!

What this means for us is that we may solve the problem independently for all (Δ−𝜆)𝑑
, where 𝜆
 are roots of 𝐴(𝑥)
 and 𝑑
 are their multiplicities, and the final solution will be a linear combination of these basic solutions.

For 𝑑=1
, the equation here is of form

Δ𝐹=𝜆𝐹,
or, equivalently,

𝐹𝑛+1=𝜆𝐹𝑛.
The solution to this is simply 𝐹𝑛=𝐶⋅𝜆𝑛
, where 𝐶
 is arbitrary constant.

As for higher-dimensional case, it can be reduced to the lower-dimensional as

(Δ−𝜆)𝑑𝐹=0⟺(Δ−𝜆)𝑑−1[(Δ−𝜆)𝐹]=0.
In other words, let's denote by 𝐹(𝑘)
 the generic solution to (Δ−𝜆)𝑘𝐹(𝑘)=0
. Then the equation above rewrites as

(Δ−𝜆)𝐹(𝑘)=𝐹(𝑘−1)
with the base case 𝐹(1)𝑛=𝐶1⋅𝜆𝑛
. For example,

(Δ−𝜆)𝐹(2)=𝐹(1)=𝐶1𝜆𝑛,
which has a generic solution of form 𝐹(2)𝑛=(𝑎+𝑏𝑛)𝜆𝑛
.

For this series of equations it can be proven that 𝐹(𝑘)𝑛=𝐶𝑘(𝑛)⋅𝜆𝑛
, where 𝐶𝑘(𝑛)
 is a polynomial of degree less than 𝑘
. This essentially proves the closed-form solution to linear recurrences.

If you have some intuition about Jordan normal form and generalized eigenvectors, you can compare them to the construction above!

As you may see, 𝐹(𝑘)
 is the generalized eigenvector of Δ
 of rank 𝑘
, and the construction itself is very similar with Jordan chains.

In a very similar manner and using the same theorem you may prove that the solution to the linear differential equation

𝑓(𝑑)(𝑥)=∑𝑘=1𝑑𝑎𝑘𝑓(𝑑−𝑘)(𝑥)
is given as

𝑓(𝑥)=∑𝑖=1𝑚𝑝𝑖(𝑥)𝑒𝜆𝑖𝑥,
where 𝜆𝑖
 are the roots of differential equation's characteristic polynomial and 𝑝𝑖(𝑥)
 are polynomials of degree less than 𝑑𝑖
.

To get it, one should consider operator 𝐴=∂∂𝑥
, hence the equation is

(∂𝑛∂𝑥𝑛−∑𝑘=1𝑑𝑎𝑘∂𝑑−𝑘∂𝑥𝑑−𝑘)𝑓(𝑥)=0.
Tags tutorial, linear recurrence, umbral calculus, generating function, matrix, matrix exponentiation

On continued fractions. Part 3: In competitive programming

By adamant, history, 21 month(s) ago, In English
Hi everyone!

It's been quite some time since I wrote two previous articles in the cycle:

Part 1: Introduction
Part 2: Properties and interpretation
Part 3: In competitive programming

This time I finally decided to publish something on how one can actually use continued fractions in competitive programming problems.

Few months ago, I joined CP-Algorithms as a collaborator. The website also underwent a major design update recently, so I decided it would be great to use this opportunity and publish my new article there, so here it is:

CP-Algorithms — Continued fractions
It took me quite a while to write and I made sure to not only describe common competitive programming challenges related to continued fractions, but also to describe the whole concept from scratch. That being said, article is supposed to be self-contained.

Main covered topics:

Notion of continued fractions, convergents, semiconvergents, complete quotients.
Recurrence to compute convergents, notion of continuant.
Connection of continued fractions with the Stern-Brocot tree and the Calkin-Wilf tree.
Convergence rate with continued fractions.
Linear fractional transformations, quadratic irrationalities.
Geometric interpretation of continued fractions and convergents.
I really hope that I managed to simplify the general story-telling compared to previous 2 articles.

Here are the major problems that are dealt with in the article:

Given 𝑎1,…,𝑎𝑛
, quickly compute [𝑎𝑙;𝑎𝑙+1,…,𝑎𝑟]
 in queries.
Which number of 𝐴=[𝑎0;𝑎1,…,𝑎𝑛]
 and 𝐵=[𝑏0;𝑏1,…,𝑏𝑚]
 is smaller? How to emulate 𝐴−𝜀
 and 𝐴+𝜀
?
Given 𝐴=[𝑎0;𝑎1,…,𝑎𝑛]
 and 𝐵=[𝑏0;𝑏1,…,𝑏𝑚]
, compute the continued fraction representations of 𝐴+𝐵
 and 𝐴⋅𝐵
.
Given 01≤𝑝0𝑞0<𝑝1𝑞1≤10
, find 𝑝𝑞
 such that (𝑞,𝑝)
 is lexicographically smallest and 𝑝0𝑞0<𝑝𝑞<𝑝1𝑞1
.
Given 𝑥
 and 𝑘
, 𝑥
 is not a perfect square. Let 𝑥‾‾√=[𝑎0;𝑎1,…]
, find 𝑝𝑘𝑞𝑘=[𝑎0;𝑎1,…,𝑎𝑘]
 for 0≤𝑘≤109
.
Given 𝑟
 and 𝑚
, find the minimum value of 𝑞𝑟(mod𝑚)
 on 1≤𝑞≤𝑛
.
Given 𝑟
 and 𝑚
, find 𝑝𝑞
 such that 𝑝,𝑞≤𝑚‾‾√
 and 𝑝𝑞−1≡𝑟(mod𝑚)
.
Given 𝑝
, 𝑞
 and 𝑏
, construct the convex hull of lattice points below the line 𝑦=𝑝𝑥+𝑏𝑞
 on 0≤𝑥≤𝑛
.
Given 𝐴
, 𝐵
 and 𝐶
, find the maximum value of 𝐴𝑥+𝐵𝑦
 on 𝑥,𝑦≥0
 and 𝐴𝑥+𝐵𝑦≤𝐶
.
Given 𝑝
, 𝑞
 and 𝑏
, compute the following sum:
∑𝑥=1𝑛⌊𝑝𝑥+𝑏𝑞⌋.
So far, here is the list of problems that are explained in the article:

DMOPC '19 Contest 7 P4 — Bob and Continued Fractions
Tavrida NU Akai Contest — Continued Fraction
Timus — Crime and Punishment
June Challenge 2017 — Euler Sum
NAIPC 2019 — It's a Mod, Mod, Mod, Mod World
Library Checker — Sum of Floor of Linear
102354I - От модулей к рациональным
GCJ 2019, Round 2 — New Elements: Part 2
And an additional list of practice problems where continued fractions could be useful:

UVa OJ — Continued Fractions
ProjectEuler+ #64: Odd period square roots
305B - Цепные дроби
346E - Doodle Jump
585C - Алиса, Боб, Апельсины и Яблоки
POJ Founder Monthly Contest 2008.03.16 — A Modular Arithmetic Challenge
2019 Multi-University Training Contest 5 — fraction
SnackDown 2019 Elimination Round — Election Bait
There are likely much more problems where continued fractions are used, please mention them in the comments if you know any!

Finally, since CP-Algorithms is supposed to be a wiki-like project (that is, to grow and get better as time goes by), please feel free to comment on any issues that you might find while reading the article, ask questions or suggest any improvements. You can do so in the comments below or in the issues section of the CP-Algorithms GitHub repo. You can also suggest changes via pull request functionality.

Tags tutorial, cp-algorithms, continued fraction, #continued fractions, euclidean algorithm, number theory

Basis change in linear recurrences

By adamant, history, 21 month(s) ago, In English
Hi everyone!

Today I'd like to write about Fibonacci numbers. Ever heard of them? Fibonacci sequence is defined as 𝐹𝑛=𝐹𝑛−1+𝐹𝑛−2
.

It got me interested, what would the recurrence be like if it looked like 𝐹𝑛=𝛼𝐹𝑛−𝑝+𝛽𝐹𝑛−𝑞
 for 𝑝≠𝑞
?

Timus — Fibonacci Sequence. The sequence 𝐹
 satisfies the condition 𝐹𝑛=𝐹𝑛−1+𝐹𝑛−2
. You're given 𝐹𝑖
 and 𝐹𝑗
, compute 𝐹𝑛
.

Using 𝐿(𝑥𝑛)=𝐹𝑛
 functional, we can say that we essentially need to solve the following system of equations:

1≡𝛼𝑥−𝑝+𝛽𝑥−𝑞(mod𝑥2−𝑥−1).
To get the actual solution from it, we should first understand what exactly is the remainder of 𝑥𝑛
 modulo 𝑥2−𝑥−1
. The remainder of 𝑃(𝑥)
 modulo (𝑥−𝑎)(𝑥−𝑏)
 is generally determined by 𝑃(𝑎)
 and 𝑃(𝑏)
:

𝑃(𝑥)≡𝑟mod(𝑥−𝑎)(𝑥−𝑏)⟺{𝑃(𝑎)=𝑟,𝑃(𝑏)=𝑟.
Therefore, our equation above is, equivalent to the following:

{𝛼𝑎−𝑝+𝛽𝑎−𝑞=1,𝛼𝑏−𝑝+𝛽𝑏−𝑞=1.
The determinant of this system of equations is 𝑎−𝑝𝑏−𝑞−𝑎−𝑞𝑏−𝑝
. Solving the system, we get the solution

𝛼=𝑏−𝑞−𝑎−𝑞𝑎−𝑝𝑏−𝑞−𝑎−𝑞𝑏−𝑝,𝛽=𝑎−𝑝−𝑏−𝑝𝑎−𝑝𝑏−𝑞−𝑎−𝑞𝑏−𝑝.
Multiplying numerators and denominators by 𝑎𝑞𝑏𝑞
 for 𝛼
 and 𝑎𝑝𝑏𝑝
 for 𝛽
, we get a nicer form:

𝛼=𝑎𝑞−𝑏𝑞𝑎𝑞−𝑝−𝑏𝑞−𝑝,𝛽=𝑎𝑝−𝑏𝑝𝑎𝑝−𝑞−𝑏𝑝−𝑞.
This is a solution for a second degree recurrence with the characteristic polynomial (𝑥−𝑎)(𝑥−𝑏)
.

Note that for Fibonacci numbers in particular, due to Binet's formula, it holds that

𝐹𝑛=𝑎𝑛−𝑏𝑛𝑎−𝑏.
Substituting it back into 𝛼
 and 𝛽
, we get

𝐹𝑛=𝐹𝑞𝐹𝑞−𝑝𝐹𝑛−𝑝+𝐹𝑝𝐹𝑝−𝑞𝐹𝑛−𝑞
which is a neat symmetric formula.

P. S. you can also derive it from Fibonacci matrix representation, but this way is much more fun, right?

UPD: I further simplified the explanation, should be much easier to follow it now.

Note that the generic solution only covers the case of (𝑥−𝑎)(𝑥−𝑏)
 when 𝑎≠𝑏
. When the characteristic polynomial is (𝑥−𝑎)2
, the remainder of 𝑃(𝑥)
 modulo (𝑥−𝑎)2
 is determined by 𝑃(𝑎)
 and 𝑃′(𝑎)
:

𝑃(𝑥)≡𝑟mod(𝑥−𝑎)2⟺{𝑃(𝑎)=𝑟,𝑃′(𝑎)=0.
Therefore, we have a system of equations

{𝛼𝑎−𝑝𝛼𝑝𝑎−𝑝−1++𝛽𝑎−𝑞𝛽𝑞𝑎−𝑞−1==1,0.
For this system, the determinant is 𝑞−𝑝𝑎𝑝+𝑞+1
 and the solution is

𝛼=𝑞𝑎𝑝𝑞−𝑝,𝛽=𝑝𝑎𝑞𝑝−𝑞
Another interesting way to get this solution is via L'Hôpital's rule:

lim𝑥→0𝑎𝑞−(𝑎+𝑥)𝑞𝑎𝑞−𝑏−(𝑎+𝑥)𝑞−𝑝=lim𝑥→0𝑞(𝑎+𝑥)𝑞−1(𝑞−𝑝)(𝑎+𝑥)𝑞−𝑝−1=𝑞𝑎𝑝𝑞−𝑝.
Let's consider the more generic case of the characteristic polynomial (𝑥−𝜆1)(𝑥−𝜆2)…(𝑥−𝜆𝑘)
.

102129D - Basis Change. The sequence 𝐹
 satisfies 𝐹𝑛=∑𝑖=1𝑘𝑎𝑖𝐹𝑛−𝑖
. Find 𝑐1,…,𝑐𝑛
 such that 𝐹𝑛=∑𝑖=1𝑘𝑐𝑖𝐹𝑛−𝑏𝑖
.

We need to find 𝛼1,…,𝛼𝑘
 such that 𝐹𝑛=𝛼1𝐹𝑛−𝑐1+⋯+𝛼𝑘𝐹𝑛−𝑐𝑘
. It boils down to the system of equations

⎧⎩⎨⎪⎪⎪⎪𝛼1𝜆−𝑐11+⋯+𝛼𝑘𝜆−𝑐11=1,𝛼1𝜆−𝑐22+⋯+𝛼𝑘𝜆−𝑐𝑘2=1,…𝛼1𝜆−𝑐𝑘𝑘+⋯+𝛼𝑘𝜆−𝑐𝑘𝑘=1.
This system of equations has a following matrix:

𝐴=⎡⎣⎢⎢⎢⎢⎢𝜆−𝑐11𝜆−𝑐12⋮𝜆−𝑐1𝑘𝜆−𝑐21𝜆−𝑐22⋮𝜆−𝑐2𝑘……⋱…𝜆−𝑐𝑘1𝜆−𝑐𝑘2⋮𝜆−𝑐𝑘𝑘⎤⎦⎥⎥⎥⎥⎥
Matrices of this kind are called alternant matrices. Let's denote its determinant as 𝐷𝜆1,…,𝜆𝑘(𝑐1,…,𝑐𝑘)
, then the solution is

𝛼𝑖=𝐷𝜆1,…,𝜆𝑘(𝑐1,…,𝑐𝑖−1,0,𝑐𝑖+1,…,𝑐𝑘)𝐷𝜆1,…,𝜆𝑘(𝑐1,…,𝑐𝑖−1,𝑐𝑖,𝑐𝑖+1,…,𝑐𝑘).
Unfortunately, on practice in makes more sense to find 𝛼𝑖
 with the Gaussian elimination rather than with these direct formulas.

Tags math, fibonacci, linear recurrence

Recovering a linear recurrence with the extended Euclidean algorithm

By adamant, history, 21 month(s) ago, In English
Hi everyone!

The task of finding the minimum linear recurrence for the given starting sequence is typically solved with the Berlekamp-Massey algorithm. In this article I would like to highlight another possible approach, with the use of the extended Euclidean algorithm.

Great thanks to nor for the proofreading and all the useful comments to make the article more accessible and rigorous.

Tl'dr.
The procedure below is essentially a formalization of the extended Euclidean algorithm done on 𝐹(𝑥)
 and 𝑥𝑚+1
.

If you need to find the minimum linear recurrence for a given sequence 𝐹0,𝐹1,…,𝐹𝑚
, do the following:

Let 𝐹(𝑥)=𝐹𝑚+𝐹𝑚−1𝑥+⋯+𝐹0𝑥𝑚
 be the generating function of the reversed 𝐹
.

Compute the sequence of remainders 𝑟−2,𝑟−1,𝑟0,…,𝑟𝑘
 such that 𝑟−2=𝐹(𝑥)
, 𝑟−1=𝑥𝑚+1
 and

𝑟𝑘=𝑟𝑘−2mod𝑟𝑘−1.
Let 𝑎𝑘(𝑥)
 be a polynomial such that 𝑟𝑘=𝑟𝑘−2−𝑎𝑘𝑟𝑘−1
.

Compute the auxiliary sequence 𝑞−2,𝑞−1,𝑞0,…,𝑞𝑘
 such that 𝑞−2=1
, 𝑞−1=0
 and

𝑞𝑘=𝑞𝑘−2+𝑎𝑘𝑞𝑘−1.
Pick 𝑘
 to be the first index such that deg𝑟𝑘<deg𝑞𝑘
. Let 𝑞𝑘(𝑥)=𝑎0𝑥𝑑−∑𝑘=1𝑑𝑎𝑘𝑥𝑑−𝑘
, then it also holds that

𝐹𝑛=∑𝑘=1𝑑𝑎𝑘𝑎0𝐹𝑛−𝑘
for any 𝑛≥𝑑
 and 𝑑
 is the minimum possible. Thus, 𝑞𝑘(𝑥)
 divided by 𝑎0
 is the characteristic polynomial of the minimum linear for 𝐹
.

More generally, one can say for such 𝑘
 that

𝐹(𝑥)≡(−1)𝑘𝑟𝑘(𝑥)𝑞𝑘(𝑥)(mod𝑥𝑚+1).


Linear recurrence interpolation
In the previous article on linear recurrences we derived that the generating function of the linear recurrence always looks like

𝐺(𝑥)=𝑃(𝑥)𝑄(𝑥),
where 𝑃(𝑥)
 and 𝑄(𝑥)
 are polynomials and deg𝑃<deg𝑄
. In this representation, 𝑄(𝑥)=1−∑𝑘=1𝑑𝑎𝑘𝑥𝑘
 corresponds to

𝐹𝑛=∑𝑘=1𝑑𝑎𝑘𝐹𝑛−𝑘.
Typical competitive programming task of recovering linear recurrence is formulated as follows:

Library Checker — Find Linear Recurrence. You're given 𝐹0,…,𝐹𝑚
. Find 𝑎1,…,𝑎𝑑
 with minimum 𝑑
 such that
𝐹𝑛=∑𝑘=1𝑑𝑎𝑘𝐹𝑛−𝑘.(∀𝑛≥𝑑)
In formal power series terms it means that we're given 𝐹(𝑥)=𝐹0+𝐹1𝑥+⋯+𝐹𝑚𝑥𝑚
 and we need to find 𝑃(𝑥)𝑄(𝑥)
 such that

𝐹(𝑥)≡𝑃(𝑥)𝑄(𝑥)(mod𝑥𝑚+1),
deg𝑃<𝑑
, deg𝑄≤𝑑
 and 𝑑
 is the minimum possible. Note that it is not required for 𝑎𝑑
 to be non-zero.

In this terms, as we will see later, what the problem asks us to find is essentially one of the Padé approximants of 𝐹(𝑥)
.

Padé approximants
Given a formal power series 𝑓(𝑥)=∑𝑘=0∞𝑓𝑘𝑥𝑘
, its Padé approximant of order [𝑝/𝑞]
 is a pair of polynomials (𝑃,𝑄)
 such that the degree of 𝑃
 is at most 𝑝
, the degree of 𝑄
 is at most 𝑞
, the polynomial 𝑄
 is non-zero and

𝑓(𝑥)𝑄(𝑥)≡𝑃(𝑥)(mod𝑥𝑚+1)
where 𝑚=𝑝+𝑞
. Padé approximant is denoted [𝑝/𝑞]𝑓
.

Padé approximant is unique in the following sense. Let (𝑃0,𝑄0)
 and (𝑃1,𝑄1)
 both satisfy the definition of [𝑝/𝑞]𝑓
, then

{𝑓(𝑥)𝑄0(𝑥)≡𝑃0(𝑥)(mod𝑥𝑚+1),𝑓(𝑥)𝑄1(𝑥)≡𝑃1(𝑥)(mod𝑥𝑚+1).
This, in turn, implies

𝑃0(𝑥)𝑄1(𝑥)≡𝑓(𝑥)𝑄0(𝑥)𝑄1(𝑥)≡𝑃1(𝑥)𝑄0(𝑥)(mod𝑥𝑚+1),
meaning that 𝑃0(𝑥)𝑄1(𝑥)−𝑃1(𝑥)𝑄0(𝑥)
 is divisible by 𝑥𝑚+1
. But the degree of the difference is at most 𝑥𝑚
, thus

𝑃0(𝑥)𝑄1(𝑥)=𝑃1(𝑥)𝑄0(𝑥),
even disregarding the mod𝑥𝑚+1
 part. Note that if (𝑃0,𝑄0)
 is an approximant and (𝑃1,𝑄1)
 satisfies the identity above, it does not imply that (𝑃1,𝑄1)
 is also an approximant, meaning that we generally can't just require gcd(𝑃,𝑄)=1
 normalization.

However, we may search for the representative with minimum possible degree of 𝑄
, as the difference deg𝑄−deg𝑃
 is the same for all representations, assuming deg0=−∞
.

Note: In scholarly articles, this is often bypassed by requiring gcd(𝑃,𝑄)=1
 and using the normalization 𝑄(0)=1
, thus requiring 𝑃(𝑥)𝑄(𝑥)
 to be a meaningful rational function that coincides with 𝑓(𝑥)
 in terms up to 𝑥𝑚
.

Online Judge — Rational Approximation. Given 𝑓(𝑥)
, compute 𝑝(𝑥)
 and 𝑞(𝑥)
 of degrees at most 𝑚−1
 and 𝑛−1
 such that

𝑓(𝑥)𝑞(𝑥)−𝑝(𝑥)≡0(mod𝑥𝑚+𝑛).
Extended Euclidean algorithm
Let's look again on the condition

𝐹(𝑥)𝑄(𝑥)≡𝑃(𝑥)(mod𝑥𝑚+1).
It translates into the following Bézout's identity:

𝐹(𝑥)𝑄(𝑥)=𝑃(𝑥)+𝑥𝑚+1𝐾(𝑥),
where 𝐾(𝑥)
 is a formal power series. When 𝑃(𝑥)
 is divisible by gcd(𝐹(𝑥),𝑥𝑚+1)
 the solution to this equation can be found with the extended Euclidean algorithm. Turns out, the extended Euclidean algorithm can also be used to enumerate all [𝑝/𝑞]𝐹
 with 𝑝+𝑞=𝑚
.

Formalizing the algorithm
Let's formalize the extended Euclidean algorithm of 𝐴(𝑥)
 and 𝐵(𝑥)
. Starting with 𝑟−2=𝐴
 and 𝑟−1=𝐵
, we compute the sequence

𝑟𝑘=𝑟𝑘−2mod𝑟𝑘−1=𝑟𝑘−2−𝑎𝑘𝑟𝑘−1.
If you're familiar with continued fractions, you could recognize, that this sequence corresponds to the representation

𝐴(𝑥)𝐵(𝑥)=𝑎0(𝑥)+1𝑎1(𝑥)+1𝑎2(𝑥)+1…=[𝑎0(𝑥);𝑎1(𝑥),𝑎2(𝑥),…].
Same as with rational numbers, for such sequence it is possible to define the sequence of convergents

𝑝𝑘(𝑥)𝑞𝑘(𝑥)=[𝑎0(𝑥);𝑎1(𝑥),…,𝑎𝑘(𝑥)]=𝑎𝑘(𝑥)𝑝𝑘−1(𝑥)+𝑝𝑘−2(𝑥)𝑎𝑘(𝑥)𝑞𝑘−1(𝑥)+𝑞𝑘−2(𝑥),
starting with

𝑝−2(𝑥)𝑞−2(𝑥)=01,𝑝−1(𝑥)𝑞−1(𝑥)=10,𝑝0(𝑥)𝑞0(𝑥)=𝑎0(𝑥)1,…
Now, one can prove the following identity:

(−1)𝑘𝑟𝑘(𝑥)=𝑞𝑘(𝑥)𝐴(𝑥)−𝑝𝑘(𝑥)𝐵(𝑥).
Explanation
In other words, we get three sequences 𝑟𝑘
, 𝑝𝑘
 and 𝑞𝑘
 that define a family of solutions to the Bézout's identity.

Using 𝐴(𝑥)=𝐹(𝑥)
 and 𝐵(𝑥)=𝑥𝑚+1
, we conclude that

𝐹(𝑥)𝑞𝑘(𝑥)≡(−1)𝑘𝑟𝑘(𝑥)(mod𝑥𝑚+1).
Enumerating approximants
Let's estimate the degrees of 𝑟𝑘
 and 𝑞𝑘
 to understand how they relate with Padé approximants.

It follows from the recurrence that deg𝑞𝑘−deg𝑞𝑘−1=deg𝑎𝑘=deg𝑟𝑘−2−deg𝑟𝑘−1
, therefore

deg𝑞𝑘=deg𝑎0+⋯+deg𝑎𝑘
and

deg𝑟𝑘=(𝑚+1)−deg𝑎0−⋯−deg𝑎𝑘−deg𝑎𝑘+1=(𝑚−deg𝑞𝑘)−(deg𝑎𝑘+1−1).
This means that (−1)𝑘𝑟𝑘𝑞𝑘
 is the Padé approximants [(𝑚−𝑞)/𝑞]𝐹
 for all 𝑞
 from deg𝑞𝑘
 to deg𝑞𝑘+deg𝑎𝑘+1−1=deg𝑞𝑘+1−1
.

Joining it together for all 𝑞𝑘
 we see that all [(𝑚−𝑞)/𝑞]𝐹
 for 𝑞
 from 0
 to 𝑚
 are covered.

Thus, if you find 𝑘
 such that deg𝑞𝑘≤𝑞
 and deg𝑞𝑘+1>𝑞
, assuming 𝑝+𝑞=𝑚
, it will hold that

[𝑝/𝑞]𝐹=(−1)𝑘𝑟𝑘𝑞𝑘.
Finding the linear recurrence
Let 𝐹(𝑥)𝑄(𝑥)≡𝑃(𝑥)(mod𝑥𝑚+1)
 for some 𝑃(𝑥)
 and 𝑄(𝑥)
 such that deg𝑃(𝑥)<deg𝑄(𝑥)
.

If deg𝑃≤𝑚−deg𝑄
 then (𝑃,𝑄)
 satisfies the definition of [(𝑚−deg𝑄)/deg𝑄]𝐹
. Otherwise, [(𝑚−deg𝑄)/deg𝑄]𝐹
 would provide another pair of 𝑃(𝑥)
 and 𝑄(𝑥)
 with possibly smaller degree of 𝑄
 and certainly smaller degree of 𝑃
.

To avoid dealing with special cases around 𝑄(0)=0
, and also around possible necessity to find needed 𝑑
, while iterating over approximants, you may deem useful to reverse the sequence, for which you search for the recurrence. In this way, rather than fixing 𝑄(0)=1
 for the linear recurrence, you'd rather look for highest term of 𝑄(𝑥)
 being equal to 1
.

The approaches are equivalent in a sense that when 𝑄(𝑥)=1−𝑏1𝑥−⋯−𝑏𝑞𝑥𝑞
 defines the linear recurrence

𝐹𝑛=∑𝑘=1𝑑𝑏𝑘𝐹𝑛−𝑘,
the reversed polynomial 𝑥𝑑𝑄(𝑥−1)=𝑥𝑑−𝑏1𝑥𝑑−1−⋯−𝑏𝑑
 defines the recurrence

𝐹𝑛=∑𝑘=1𝑑𝑏𝑘𝐹𝑛+𝑘.
Formal proof
Thus to find the first kind recurrence for 𝐹0,𝐹1,…,𝐹𝑚
, we could find the second kind recurrence for 𝐹𝑚,𝐹𝑚−1,…,𝐹0
 instead.

In this notion, the minimum recurrence is defined by the first 𝑘
 such that deg𝑟𝑘<deg𝑞𝑘
 and has a characteristic polynomial 𝑞𝑘
.

I have implemented the 𝑂(𝑛2)
 algorithm as a part of my polynomial algorithms library:

// Returns the characteristic polynomial
// of the minimum linear recurrence for
// the first d+1 elements of the sequence
poly min_rec(int d = deg()) const {
    // R1 = reversed (Q(x) mod x^{d+1}), R2 = x^{d+1}
    auto R1 = mod_xk(d + 1).reverse(d + 1), R2 = xk(d + 1);
    auto Q1 = poly(T(1)), Q2 = poly(T(0));
    while(!R2.is_zero()) {
        auto [a, nR] = R1.divmod(R2); // R1 = a*R2 + nR, deg nR < deg R2
        tie(R1, R2) = make_tuple(R2, nR);
        tie(Q1, Q2) = make_tuple(Q2, Q1 + a * Q2);
        if(R2.deg() < Q2.deg()) {
            return Q2 / Q2.lead(); // guarantee that the highest coefficient is 1
        }
    }
    assert(0);
}
You can see this Library Judge submission for further details.

Also here is a library-free version, if you prefer it.

Half-GCD algorithm
The notion above provides the basis to construct the 𝑂(𝑛log2𝑛)
 divide and conquer algorithm of computing gcd(𝑃,𝑄)
 in polynomials and finding the minimum linear recurrence. I have a truly marvelous demonstration of this proposition that this margin is, unfortunately, too narrow to contain. I hope, I will be able to formalize the process for good and write an article about it sometime...

As a teaser, here's an example of the problem, that (probably) requires Half-GCD:

Library Checker — Inv of Polynomials. You're given 𝑓(𝑥)
 and ℎ(𝑥)
. Compute 𝑓−1(𝑥)
 modulo ℎ(𝑥)
.

Let 𝑓(𝑥)ℎ(𝑥)=[𝑎0;𝑎1,…,𝑎𝑘]
 and 𝑝𝑘−1𝑞𝑘−1=[𝑎0;𝑎1,…,𝑎𝑘−1]
, then 𝑓𝑞𝑘−1−ℎ𝑝𝑘−1=(−1)𝑘−2𝑟𝑘=(−1)𝑘−2gcd(𝑓,𝑝)
.

Therefore, if 𝑟𝑘=gcd(𝑓,ℎ)
 is non-constant, the inverse doesn't exist. Otherwise, the inverse is (−1)𝑘−2𝑞𝑘−1(𝑥)
.

In the problem, 𝑛≤5⋅104
, therefore you need to do something better than 𝑂(𝑛2)
 Euclidean algorithm.

Tags berlekamp-massey, tutorial, linear recurrence, continued fraction

Half-GCD algorithm

By adamant, history, 20 months ago, In English
Hi everyone!

Today I'd like to finally talk about an algorithm to solve the following tasks in 𝑂(𝑛log2𝑛)
:

Compute the greatest common divisor of two polynomials 𝑃(𝑥)
 and 𝑄(𝑥)
;
Given 𝑓(𝑥)
 and ℎ(𝑥)
 find the multiplicative inverse of 𝑓(𝑥)
 modulo ℎ(𝑥)
;
Given 𝐹0,𝐹1,…,𝐹𝑚
, recover the minimum linear recurrence 𝐹𝑛=𝑎1𝐹𝑛−1+⋯+𝑎𝑑𝐹𝑛−𝑑
;
Given 𝑃(𝑥)
 and 𝑄(𝑥)
, find 𝐴(𝑥)
 and 𝐵(𝑥)
 such that 𝑃(𝑥)𝐴(𝑥)+𝑄(𝑥)𝐵(𝑥)=gcd(𝑃,𝑄)
;
Given 𝑃(𝑥)=(𝑥−𝜆1)…(𝑥−𝜆𝑛)
 and 𝑄(𝑥)=(𝑥−𝜇1)…(𝑥−𝜇𝑚)
 compute their resultant.
More specifically, this allows to solve in 𝑂(𝑛log2𝑛)
 the following problems:

Library Checker — Find Linear Recurrence. You're given 𝐹0,…,𝐹𝑚
. Find 𝑎1,…,𝑎𝑑
 with minimum 𝑑
 such that

𝐹𝑛=∑𝑘=1𝑑𝑎𝑘𝐹𝑛−𝑘.
Library Checker — Inv of Polynomials. You're given 𝑓(𝑥)
 and ℎ(𝑥)
. Compute 𝑓−1(𝑥)
 modulo ℎ(𝑥)
.

All tasks here are connected with the extended Euclidean algorithm and the procedure we're going to talk about is a way to compute it quickly. I recommend reading article on recovering minimum linear recurrence first, as it introduces some useful results and concepts. It is also highly recommended to familiarize yourself with the concept of continued fractions.

Euclidean algorithm and continued fractions
Assume that we want to compute the greatest common divisor of 𝐴(𝑥)
 and 𝐵(𝑥)
 for deg𝐴≥deg𝐵
. For this purpose, we compute

𝑟𝑖=𝑟𝑖−2mod𝑟𝑖−1=𝑟𝑖−2−𝑎𝑖𝑟𝑖−1,
starting with 𝑟−2=𝐴(𝑥)
 and 𝑟−1=𝐵(𝑥)
. The last element in the sequence is 𝑟𝑘=0
 for some 𝑘
.

This sequence corresponds to the continued fraction

𝐴(𝑥)𝐵(𝑥)=𝑎0(𝑥)+1𝑎1(𝑥)+1𝑎2(𝑥)+…=[𝑎0(𝑥);𝑎1(𝑥),𝑎2(𝑥),…]
With this representation, the sequence of convergents 𝑝𝑖(𝑥)𝑞𝑖(𝑥)
 is defined, such that

𝑝𝑖(𝑥)𝑞𝑖(𝑥)=[𝑎0;𝑎1,…,𝑎𝑖]=𝑎0+1⋯+1𝑎𝑖.
The sequence of convergents adheres to the following recurrence:

𝑝𝑖(𝑥)𝑞𝑖(𝑥)=𝑎𝑖(𝑥)𝑝𝑖−1(𝑥)+𝑝𝑖−2(𝑥)𝑎𝑖(𝑥)𝑞𝑖−1(𝑥)+𝑞𝑖−2(𝑥),
starting with 𝑝−2𝑞−2=01
 and 𝑝−1𝑞−1=10
. Note that the following recurrence stands connecting the degrees of 𝑞𝑖
, 𝑝𝑖
 and 𝑟𝑖
:

deg𝑞𝑖−deg𝑞𝑖−1=deg𝑝𝑖−deg𝑝𝑖−1=deg𝑟𝑖−2−deg𝑟𝑖−1=𝑎𝑖.
From this fact it follows that

⎧⎩⎨⎪⎪deg𝑝𝑖=deg𝑎0+deg𝑎1+⋯+deg𝑎𝑖,deg𝑞𝑖=deg𝑎1+⋯+deg𝑎𝑖,deg𝑟𝑖=deg𝐴−deg𝑎0−⋯−deg𝑎𝑖+1=deg𝐴−deg𝑝𝑖+1.
For 𝐴(𝑥)𝐵(𝑥)=[𝑎0;𝑎1,…,𝑎𝑘]
, the sequence of convergents ends with 𝑝𝑘(𝑥)𝑞𝑘(𝑥)=𝐴(𝑥)𝐵(𝑥)
, such that 𝑝𝑘
 and 𝑞𝑘
 are coprime. Therefore,

deg𝑝𝑘=deg𝐴gcd(𝐴,𝐵)=deg𝑎0+⋯+deg𝑎𝑘.
From this follows that the sequence 𝑎0(𝑥),𝑎1(𝑥),…,𝑎𝑘(𝑥)
 provides another linear-size representation of 𝐴(𝑥)𝐵(𝑥)
.

It can be proven (see the previous article) that

𝐴(𝑥)𝑞𝑖(𝑥)−𝐵(𝑥)𝑝𝑖(𝑥)=(−1)𝑖𝑟𝑖(𝑥),
in particular it means that 𝐴(𝑥)𝑞𝑖−1(𝑥)−𝐵(𝑥)𝑝𝑖−1(𝑥)=(−1)𝑖−1gcd(𝐴,𝐵)
.

The key to execute the extended Euclidean algorithm in 𝑂(𝑛log2𝑛)
 is to be able to switch between the two representations.

Conversion of [𝑎0(𝑥);𝑎1(𝑥),…,𝑎𝑘(𝑥)]
 to 𝑝𝑘
, 𝑞𝑘
 and 𝑟𝑘
The recurrence 𝑝𝑖=𝑝𝑖−2+𝑎𝑖𝑝𝑖−1
 can be written in matrix form as

(𝑎𝑖110)(𝑝𝑖−1𝑝𝑖−2)=(𝑝𝑖𝑝𝑖−1),
therefore, the following formula stands:

(𝑎𝑖110)…(𝑎1110)(𝑎0110)(10)=(𝑝𝑖𝑝𝑖−1).
The product of 𝑖+1
 matrices can be computed in 𝑂(𝑛log2𝑛)
 with divide and conquer algorithm for 𝑛=deg𝑎0+⋯+deg𝑎𝑖
.

Using (01)
 starting vector, one will get (𝑞𝑖𝑞𝑖−1)
, thus the following joint formula is also true:

(𝑎𝑖110)…(𝑎1110)(𝑎0110)=(𝑝𝑖𝑝𝑖−1𝑞𝑖𝑞𝑖−1).
Knowing 𝑝𝑖
 and 𝑞𝑖
, one can compute 𝑟𝑖
 from the formula above:

(−1)𝑖𝑟𝑖=𝐴(𝑥)𝑞𝑖(𝑥)−𝐵(𝑥)𝑝𝑖(𝑥).
Note that transposing left-hand side and right-hand side will also give us the following identity:

(𝑎0110)(𝑎1110)…(𝑎𝑖110)=(𝑝𝑖𝑞𝑖𝑝𝑖−1𝑞𝑖−1).
Conversion of 𝐴(𝑥)𝐵(𝑥)
 to [𝑎0(𝑥);𝑎1(𝑥),…,𝑎𝑘(𝑥)]
Connection between convergents and remainders
From the continued fraction properties it follows that

𝐴(𝑥)𝐵(𝑥)=[𝑎0;𝑎1,…,𝑎𝑖−1,𝑠𝑖],
where 𝑠𝑖=𝑟𝑖−2𝑟𝑖−1
 is the so-called 𝑖
-th complete quotient of 𝐴𝐵
. In the matrix form this is denoted as

(𝑝𝑖−1𝑞𝑖−1𝑝𝑖−2𝑞𝑖−2)(𝑟𝑖−2𝑟𝑖−1)=(𝐴𝐵).
From continued fraction properties it also follows that 𝑝𝑖−1𝑞𝑖−2−𝑞𝑖−1𝑝𝑖−2=(−1)𝑖
, therefore

(𝑟𝑖−2𝑟𝑖−1)=(−1)𝑖(𝑞𝑖−2−𝑞𝑖−1−𝑝𝑖−2𝑝𝑖−1)(𝐴𝐵).
Knowing 𝑝𝑖−1
, 𝑝𝑖−2
, 𝑞𝑖−1
, 𝑞𝑖−2
, it is possible to to recover the remaining partial quotients as the partial expansion of 𝑟𝑖−2𝑟𝑖−1
.

In the algorithm below we'll learn how to find a transform that significantly advances 𝐴𝐵
 to 𝑠𝑖
.

Half-GCD
Denoting 𝐴(𝑥)𝐵(𝑥)=𝐴0(𝑥)+𝑥𝑡𝐴1(𝑥)𝐵0(𝑥)+𝑥𝑡𝐵1(𝑥)
, let's look on what we can derive from the continued fraction expansion of 𝐴1(𝑥)𝐵1(𝑥)
. Let

𝐴1𝑞′𝑖−𝐵1𝑝′𝑖=(−1)𝑖𝑟′𝑖,
then

𝐴𝑞′𝑖—𝐵𝑝′𝑖=(𝐴0+𝑥𝑡𝐴1)𝑞′𝑖—(𝐵0+𝑥𝑡𝐵1)𝑝′𝑖=(𝐴0𝑞′𝑖—𝐵0𝑝′𝑖)+𝑥𝑡(−1)𝑖𝑟′𝑖.
Thus if the degree of 𝑥𝑡(−1)𝑟′𝑖
 is greater than of 𝐴0𝑞′𝑖−𝐵0𝑝′𝑖
, the fraction 𝑝′𝑖/𝑞′𝑖
 is also the 𝑖
-th convergent of 𝐴(𝑥)/𝐵(𝑥)
 itself and not only 𝐴1(𝑥)/𝐵1(𝑥)
. With this in mind, we can design a divide and conquer algorithm that will safely advance the computation of the continued fraction representation of 𝐴(𝑥)/𝐵(𝑥)
 by 𝑖
 steps using the continued fraction representation of 𝐴1(𝑥)/𝐵1(𝑥)
.

Let 𝑛=deg𝐴
 and 𝑡=⌈𝑛2⌉
. We will demand an algorithm to find the transform

(𝑝𝑖𝑞𝑖𝑝𝑖−1𝑞𝑖−1)(𝑟𝑖−1𝑟𝑖)=(𝐴𝐵)
such that deg𝑟𝑖−1≥𝑡>deg𝑟𝑖
. To do this, we decompose 𝐴𝐵=𝐴0+𝑥𝑡𝐴1𝐵0+𝑥𝑡𝐵1
 and solve the problem recursively for 𝐴1𝐵1
.

Let 𝑅
 be the transform found by the recursive algorithm, now we should apply 𝑅−1
 to (𝐴𝐵)
 to get 𝑟𝑘−1𝑟𝑘
.

If after that the condition above holds, we return 𝑅
. Otherwise, we advance one step in continued fraction computation, from 𝑟𝑘−1𝑟𝑘
 to

𝑟𝑘𝑟𝑘+1=𝑟𝑘𝑟𝑘−1mod𝑟𝑘=𝑟𝑘𝑟𝑘−1−𝑎𝑘+1𝑟𝑘.
Now we have a fraction 𝑟𝑘𝑟𝑘+1
 and we need to continue the process till we get to 𝑟𝑖−1𝑟𝑖
.

We need to further reduce the denominator degree from deg𝑟𝑘+1
 to less than 𝑡
. Thus, we need to reduce this value by deg𝑟𝑘+1−𝑡
.

For this purpose, we represent 𝑟𝑘𝑟𝑘+1
 as 𝐴′0+𝑥𝑚𝐴′1𝐵′0+𝑥𝑚𝐵′1
 and compute half-GCD again, but for 𝐴′1𝐵′1
.

For this to reduce deg𝑟𝑘
 to 𝑡
, it must hold that

deg𝐴′1−⌈deg𝐴′12⌉=deg𝑟𝑘−𝑡=(deg𝑟𝑘−𝑚)−⌈deg𝑟𝑘−𝑚2⌉,
Thus we should pick 𝑚=2𝑡−deg𝑟𝑘
.

If the transform returned by the second half-GCD call is 𝑆
, the overall transform is of form

𝑅⋅(𝑎𝑘+1110)⋅𝑆.
The algorithm above makes 2
 recursive calls on halved sizes and needs 𝑂(𝑛log𝑛)
 time before the second recursive call and before returning the answer. Therefore, the overall running time is

𝑇(𝑛)=2𝑇(𝑛2)+𝑂(𝑛log𝑛)=𝑂(𝑛log2𝑛).
Additionally to the linear transform matrix you can maintain a sequence [𝑎0;𝑎1,…,𝑎𝑘]
 itself. Then, once half_GCD is implemented, you can repeatedly apply it to 𝐴𝐵
 advancing its computation. Each advance will reduce the current size of polynomials by at least a factor of 2
, hence the overall complexity will still be 𝑂(𝑛log2𝑛)
.

Implementation
I've implemented this algorithm for my polynomial library. For 𝐴𝐵=[𝑎0;𝑎1,…,𝑎𝑘]
, the function full_gcd return a pair of vector that contains polynomials 𝑎0,…,𝑎𝑘
 and of the transform

(𝑝𝑘𝑞𝑘𝑝𝑘−1𝑞𝑘−1).
A code that implements the algorithm in the terms of my polynomial library looks like this:

Source code
From the formulas above, it holds that

𝐴𝑞𝑘−1−𝐵𝑝𝑘−1=(−1)𝑘−1gcd(𝐴,𝐵).
This formula allows to find gcd(𝐴,𝐵)
 and a multiplicative inverse of 𝐴(𝑥)
 modulo 𝐵(𝑥)
 when deggcd(𝐴,𝐵)=0
 (submission).

Finally, to find the minimum linear recurrence in 𝑂(𝑛log2𝑛)
, you should compute 𝑥𝑛+1𝐹(𝑥)=[𝑎0;𝑎1,…]
 representation explicitly and find 𝑘
 for which deg𝑟𝑘<deg𝑝𝑘
, as described in the previous article. Then, compute and output [𝑎0;𝑎1,…,𝑎𝑘]
 (submission).

Tags tutorial, berlekamp-massey, continued fraction, polynomials

Polynomials and roots of unity

By adamant, history, 20 months ago, In English
Hi everyone!

Today I'd like to write a bit on the amazing things you can get out of a power series by putting roots of unity into its arguments. It will be another short story without any particular application in competitive programming (at least I don't know of them yet, but there could be). But I find the facts below amazing, hence I wanted to share them.

You're expected to know some basic stuff about the discrete Fourier transform and a bit of linear algebra to understand the article.

Bisections
To begin with, let's start with the most simple non-trivial system of roots of unity. That is, with {1,−1}
.

Let 𝑓(𝑥)
 be a formal power series of 𝑥
. If you represent it as 𝑓(𝑥)=𝑓0(𝑥2)+𝑥𝑓1(𝑥2)
, you might note that

𝑓0(𝑥2)=𝑓(𝑥)+𝑓(−𝑥)2,𝑓1(𝑥2)=𝑓(𝑥)−𝑓(−𝑥)2𝑥.
The functions 𝑓0(𝑥2)
 and 𝑥𝑓1(𝑥2)
 are called the bisections of 𝑓(𝑥)
. As you see, they're closely related to 𝑓(𝑥)
 and 𝑓(−𝑥)
.

Another interesting function you could get is by multiplying 𝑓(𝑥)
 and 𝑓(−𝑥)
:

𝑓(𝑥)𝑓(−𝑥)=𝑓20(𝑥2)−𝑥2𝑓21(𝑥2).
As you see, the result only depends on 𝑥2
, hence it is an even function.

Multisections
A multisection of a formal power series

𝑓(𝑥)=∑𝑘=0∞𝑎𝑘𝑥𝑘
is a formal power series 𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)
, defined as

𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)=∑𝑘=0∞𝑎𝑘𝑚+𝑟𝑥𝑘𝑚+𝑟.
In other words, a multisection is obtained by extracting equally spaced terms from the power series. From its definition it follows that

𝑓(𝑥)=∑𝑟=0𝑚−1𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)
which is an equivalent way to define a multisection.

Discrete Fourier transform
As we saw, for 𝑚=2
 it is possible to construct multisections as linear combinations of 𝑓(𝑥)
 and 𝑓(−𝑥)
. A natural question one might ask is if it is possible to construct a closed-form expression for the multisections knowing only 𝑓(𝑥)
. And it turns out, it is possible to do so.

Let 𝜔
 be the 𝑚
-th root of unity, that is 𝜔𝑚=1
 and 𝜔𝑘≠1
 for 𝑘<𝑚
. Then it holds that

𝑓(𝜔𝑥)=∑𝑟=0𝑚−1𝜔𝑟𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚𝜔𝑚)=∑𝑟=0𝑚−1𝜔𝑟𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚).
Likewise, for 𝑓(𝜔𝑘𝑟)
 we will get roughly the same formula except for 𝜔𝑟𝑘
 instead of 𝜔𝑟
. It essentially means that if we let

𝐹(𝑦)=∑𝑟=0𝑚−1𝑦𝑟𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚),
it would hold that 𝑓(𝜔𝑘𝑥)=𝐹(𝜔𝑘)
, where 𝐹(𝑦)
 is, in fact, a polynomial in 𝑦
. Let 𝐹𝑟(𝑥)=𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)
, then

𝐹(𝜔𝑘)=∑𝑟=0𝑚−1𝐹𝑟(𝑥)(𝜔𝑘)𝑟=𝑓(𝜔𝑘𝑥).
In other words, 𝐹(1),𝐹(𝜔),…,𝐹(𝜔𝑚−1)
 is the Fourier transform of the sequence 𝐹0,𝐹1,…,𝐹𝑚−1
.

From this follows that multisections 𝐹0(𝑥),𝐹1(𝑥),…,𝐹𝑚−1(𝑥)
 could be recovered with the inverse Fourier transform:

𝐹𝑟(𝑥)=1𝑚∑𝑘=0𝑚−1𝜔−𝑘𝑟𝐹(𝜔𝑘).
Hence, the multisections are determined as

𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)=1𝑚∑𝑘=0𝑚−1𝜔−𝑘𝑟𝑓(𝜔𝑘𝑥)
For example, with 𝑚=2
 we will get the same formulas for bisections that were derived above and for 𝑚=3
 it will hold that

𝑓0,3(𝑥3)=𝑓(𝑥)+𝑓(𝜔𝑥)+𝑓(𝜔2𝑥)3,𝑓1,3(𝑥3)=𝑓(𝑥)+𝜔2𝑓(𝜔𝑥)+𝜔1𝑓(𝜔2𝑥)3𝑥,𝑓2,3(𝑥3)=𝑓(𝑥)+𝜔1𝑓(𝜔𝑥)+𝜔2𝑓(𝜔2𝑥)3𝑥2.
Kronecker delta
Another interesting way to explain this result is with the notion of Kronecker delta over ℤ𝑚
:

𝛿𝑚(𝑘)={1,0,𝑘≡0(mod𝑚), otherwise.
In this notion,

𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)=∑𝑘=0∞𝛿𝑚(𝑘−𝑟)𝑎𝑘𝑥𝑘.
But at the same time, 𝛿𝑚(𝑥)
 can be expressed in a following way:

𝛿𝑚(𝑥)=1𝑚∑𝑘=0𝑚−1𝜔𝑘𝑥.
Indeed, when 𝑥≡0(mod𝑚)
, every summand is 1
, and otherwise the sum equates to 1−𝜔𝑥𝑚1−𝜔𝑥=0
.

Putting it in the expression above, we get

𝑥𝑟𝑓𝑟,𝑚(𝑥𝑚)=1𝑚∑𝑘=0∞∑𝑖=0𝑚−1𝜔𝑖(𝑘−𝑟)𝑎𝑘𝑥𝑘=1𝑚∑𝑖=0𝑚−1𝜔−𝑖𝑟∑𝑘=0∞𝜔𝑖𝑘𝑎𝑘𝑥𝑘=1𝑚∑𝑖=0𝑚−1𝜔−𝑖𝑟𝑓(𝜔𝑖𝑥),
which is the same formula as above.

Sums and products
I promise that the following problem is somewhat related to what's going on:

102129G - Permutant. Each row of the matrix 𝐴
 is the same permutation of the previous row. Find det𝐴
.

From the formulas above it follows that

∑𝑘=0𝑚−1𝑓(𝜔𝑘𝑥)=𝑚𝑓0,𝑚(𝑥𝑚).
It's even more interesting that the product of such functions is also a function of 𝑥𝑚
, same as 𝑓(𝑥)𝑓(−𝑥)
 is a function of 𝑥2
. Let

𝑇(𝑥)=∏𝑘=0𝑚−1𝑓(𝜔𝑘𝑥)=∏𝑘=0𝑚−1𝐹(𝜔𝑘).
For such 𝑇(𝑥)
 it holds that 𝑇(𝜔𝑥)=𝑇(𝑥)
, as it just reorders the quotients. It, in turn, implies that whenever there is an 𝑎𝑘𝑥𝑘
 summand it must hold that 𝑎𝑘=𝑎𝑘𝜔𝑘
, which only happens when 𝑚
 divides 𝑘
. By definition, it is equivalent to 𝑇(𝑥)
 being a function of 𝑥𝑚
.

Is there any way to define this function explicitly in terms of the multisections 𝑥𝑟𝑓𝑟,𝑚(𝑥𝑟)
, as it was done with 𝑓(𝑥)𝑓(−𝑥)
? Yes, there is! Turns out, 𝑇(𝑥)
 can be represented as the determinant of the following circulant matrix:

𝑇(𝑥)=det⎛⎝⎜⎜⎜⎜⎜⎜𝐹0𝐹𝑚−1⋮𝐹2𝐹1𝐹1𝐹0𝐹𝑚−1𝐹2⋯𝐹1𝐹0⋱⋯𝐹𝑚−2⋱⋱𝐹𝑚−1𝐹𝑚−1𝐹𝑚−2⋮𝐹1𝐹0⎞⎠⎟⎟⎟⎟⎟⎟
This is due to the fact that 𝐹(1),𝐹(𝜔),…,𝐹(𝜔𝑚−1)
 are the eigenvalues of such matrix (follows from generic circulant matrix properties) and thus its determinant is the product of its eigenvalues. So, in particular

𝑓(𝑥)𝑓(−𝑥)=det(𝑓0,2𝑥𝑓1,2𝑥𝑓1,2𝑓0,2)=𝑓20(𝑥2)−𝑥2𝑓21(𝑥2).
and for 𝑚=3
 it holds that

𝑓(𝑥)𝑓(𝜔𝑥)𝑓(𝜔2𝑥)=det⎛⎝⎜⎜⎜𝑓0,3𝑥2𝑓2,3𝑥𝑓1,3𝑥𝑓1,3𝑓0,3𝑥2𝑓2,3𝑥2𝑓2,3𝑥𝑓1,3𝑓0,3⎞⎠⎟⎟⎟=𝑓30,3(𝑥3)+𝑥3𝑓31,3(𝑥3)+𝑥6𝑓32,3(𝑥3)−3𝑥3𝑓0,3(𝑥3)𝑓1,3(𝑥3)𝑓2,3(𝑥3).
From this follows that the product of 𝑓(𝜔𝑘𝑥)
 is not only the function of 𝑥𝑚
, but also preserves the ring of coefficients of the original series. In other words, if 𝑓(𝑥)
 is, for example, a power series with integer coefficients, so will be the product of 𝑓(𝜔𝑘𝑥)
.

Another convenient and compact way to denote the product of 𝑓(𝜔𝑘𝑥)=𝐹(𝜔𝑘)
 is as the resultant:

∏𝑘=0𝑚−1𝐹(𝜔𝑘)=Res(𝐹(𝑦),𝑦𝑚−1).
Tags fast fourier transform, power series, polynomials

Fast polynomial composition

By adamant, history, 20 months ago, In English
Hi everyone!

Today I want to describe an efficient solution of the following problem:

Composition of Formal Power Series. Given 𝐴(𝑥)
 and 𝐵(𝑥)
 of degree 𝑛−1
 such that 𝐵(0)=0
, compute 𝐴(𝐵(𝑥))(mod𝑥𝑛)
.

The condition 𝐵(0)=0
 doesn't decrease the generality of the problem, as 𝐴(𝐵(𝑥))=𝑃(𝑄(𝑥))
, where 𝑃(𝑥)=𝐴(𝑥+𝐵(0))
 and 𝑄(𝑥)=𝐵(𝑥)−𝐵(0)
. Hence you could replace 𝐴(𝑥)
 and 𝐵(𝑥)
 with 𝑃(𝑥)
 and 𝑄(𝑥)
 when the condition is not satisfied.

Solutions that I'm going to describe were published in Joris van der Hoeven's article about operations on formal power series. The article also describes a lot of other common algorithms on polynomials. It is worth noting that Joris van der Hoeven and David Harvey are the inventors of the breakthrough 𝑂(𝑛log𝑛)
 integer multiplication algorithm in the multitape Turing machine model.

Naive methods
An obvious solution would be to evaluate 𝐵(𝑥)
 in 𝑛2
 points, then evaluate 𝐴(𝐵(𝑥𝑖))
 in these points and interpolate (𝐴∘𝐵)(𝑥)
. This solution would require 𝑂(𝑛2log2𝑛)
 operations with an enormous constant. Not the best idea for 𝑛≤8000
.

Another possible approach is to go from the definition of the polynomial composition:

𝐴(𝐵(𝑥))=∑𝑖=0𝑛−1𝑎𝑖𝐵𝑖(𝑥).
You could compute 𝐵2(𝑥),𝐵3(𝑥),…,𝐵𝑛−1(𝑥)
 modulo 𝑥𝑛
 one by one and sum them up, which would take 𝑂(𝑛2log𝑛)
 operations.

Divide and conquer
If we write 𝐴(𝑥)=𝐴0(𝑥)+𝑥𝑡𝐴1(𝑥)
, then

𝐴(𝐵(𝑥))=𝐴0(𝐵(𝑥))+𝐵𝑡(𝑥)𝐴1(𝐵(𝑥)).
With this decomposition, you can reduce the computation of 𝐴(𝐵(𝑥))
 to 2
 recursive calls to 𝐴0(𝐵(𝑥))
 and 𝐴1(𝐵(𝑥))
 with 𝑂(𝑛log𝑛)
 overhead for the multiplication. If you use 𝑡=⌊𝑛2⌋
, the degree of 𝐴0
 will be at most ⌊𝑛2⌋
 and the degree of 𝐴1
 will be ⌈𝑛2⌉
.

Considering possible values of 𝑡
 over all recursion levels they will be at most 𝑂(log𝑛)
 different values, hence you can compute necessary powers of 𝐵(𝑥)
 in advance, which would require 𝑂(𝑛log2𝑛)
 cumulative time for the pre-processing.

The overall time complexity for this algorithm is still 𝑂(𝑛2log𝑛)
, but it allows significant constant-time optimizations, which is enough to pass the Library Checker problem. For this one need to carefully consider how many terms do you really need on each level of recursion.

In a more generic situation when deg𝐴=𝑝
 and deg𝐵=𝑞
, it is possible to prove the alternative complexity estimation of 𝑂(𝑝𝑞log2𝑛)
, which is a bit better when 𝑝
 or 𝑞
 are significantly smaller than 𝑛
.

Square root trick
In 1978, Brent and Kung suggested a faster algorithm to compute 𝐴∘𝐵
 assuming that formal power series are taken over the division ring. Instead of decomposing 𝐴(𝑥)
, it is possible to decompose 𝐵(𝑥)=𝐵0(𝑥)+𝑥𝑡𝐵1(𝑥)
 and use the Taylor expansion:

𝐴(𝐵(𝑥))=∑𝑘=0⌊𝑛/𝑡⌋𝑥𝑘𝑡𝐵𝑘1(𝑥)𝐴(𝑘)(𝐵0(𝑥))𝑘!+𝑂(𝑥𝑛).
For this expression, it generally holds that

∂∂𝑥𝐴(𝑘)(𝐵(𝑥))=𝐵′(𝑥)𝐴(𝑘+1)(𝐵(𝑥)),
which allows to compute 𝐴(1)(𝐵0(𝑥)),𝐴(2)(𝐵0(𝑥)),…
 in 𝑂(𝑛2𝑡log𝑛)
 by successively multiplying them with 𝐵′0(𝑥)
 and integrating, starting with 𝐴(⌊𝑛/𝑡⌋)(𝐵0(𝑥))
, which is computed with the divide and conquer algorithm above with the complexity 𝑂(𝑛𝑡log2𝑛)
.

Optimal value of 𝑡
 to minimize both complexities is 𝑡=𝑛log𝑛‾‾‾‾√
, with which the final complexity is

𝑂(𝑛𝑛log𝑛‾‾‾‾‾‾√log𝑛).
In a more generic setting, when you need to compute 𝐴(𝐵(𝑥))
 modulo 𝑥𝑛
 and deg𝐴=𝑝
, deg𝐵=𝑞
, the corresponding complexities are 𝑂(𝑛2𝑡log𝑛)
 and 𝑂(𝑝𝑡log2𝑛)
, hence the optimal value of 𝑡
 is 𝑡=𝑛2𝑝log𝑛‾‾‾‾‾‾√
 and the total complexity is

𝑂(𝑛𝑝log3𝑛‾‾‾‾‾‾‾√).
I've implemented the algorithm in CP-Algorithm polynomial library, on the actual problem it works in around 1280 ms for 𝑛≈8000
.

Thanks to Benq for pointing out a mistake in my initial calculation with arbitrary 𝑝
 and 𝑞
!

Another square root trick
Alternatively, we can represent 𝐴(𝐵(𝑥))
 in the following manner:

𝐴(𝐵(𝑥))=(𝑎0+𝑎1𝐵+⋯+𝑎𝑞−1𝐵𝑞−1)+𝐵𝑞(𝑎𝑞+𝑎𝑞+1𝐵+⋯+𝑎2𝑞−1𝐵𝑞−1)+𝐵2𝑞(𝑎2𝑞+𝑎2𝑞+1𝐵+⋯+𝑎3𝑞−1𝐵𝑞−1)+…
If you compute all 𝐵,𝐵2,…,𝐵𝑞−1
, which is doable in 𝑂(𝑛𝑞log𝑛)
, you can compute the right-hand side of each summand in 𝑂(𝑛𝑞)
 per block for 𝑛𝑞
 blocks, making a total of 𝑂(𝑛2)
 for the summation. Then you compute 𝐵𝑞,𝐵2𝑞,…
 in 𝑂(𝑛2𝑞log𝑛)
 and multiply them with the values computed on the previous step. Then the answer is the sum of these values, which is computed in 𝑂(𝑛2𝑞log𝑛)
.

If you choose 𝑞=𝑛√
, overall complexity would be 𝑂(𝑛2+𝑛𝑛√log𝑛)=𝑂(𝑛2)
.

Although this gives a worse complexity, it is likely to perform better on practice for deg𝐴=deg𝐵=𝑛
 due to better constant. However, it loses advantage over the algorithm above when 𝑛
 is significantly larger than deg𝐴
 and deg𝐵
 (for example, when you want to compute the full composition of small polynomials).

I've also implemented this algorithm, it runs for around 800 ms on the Library Checker problem.

Thanks to box for telling me about this simpler approach!

A bit more on square roots with polynomials
This algorithm is not the only application of square root techniques to polynomials.

Large factorial. You're given two numbers 𝑛
 and 𝑚
. Compute 𝑛!mod𝑚
 for 𝑛≤109
.

For a fixed 𝑚
, the solution would be to precompute 𝑞!,(2𝑞)!,…
 for 𝑞≈𝑚‾‾√
. It's not possible when 𝑚
 is a part of input. Instead, you can evaluate the polynomial 𝑥(𝑥−1)…(𝑥−𝑞+1)
 in 𝑞,2𝑞,…
 and multiply the values to get the answer.

Tags tutorial, polynomials, power series

p-adic exponent or why there is a primitive root modulo powers of prime numbers (except 2)

By adamant, history, 19 months ago, In English
Hi everyone!

You probably know that the primitive root modulo 𝑚
 exists if and only if one of the following is true:

𝑚=2
 or 𝑚=4
;
𝑚=𝑝𝑘
 is a power of an odd prime number 𝑝
;
𝑚=2𝑝𝑘
 is twice a power of an odd prime number 𝑝
.
Today I'd like to write about an interesting rationale about it through 𝑝
-adic numbers.

Hopefully, this will allow us to develop a deeper understanding of the multiplicative group modulo 𝑝𝑘
.

Tl;dr.
For a prime number 𝑝>2
 and 𝑟≡0(mod𝑝)
 one can uniquely define

exp𝑟=∑𝑘=0∞𝑟𝑘𝑘!(mod𝑝𝑛).
In this notion, if 𝑔
 is a primitive root of remainders modulo 𝑝
 lifted to have order 𝑝−1
 modulo 𝑝𝑛
 as well, then 𝑔exp𝑝
 is a primitive root of remainders modulo 𝑝𝑛
.

Finally, for 𝑝=2
 and 𝑛>2
 the multiplicative group is generated by two numbers, namely −1
 and exp4
.

𝑝
-adic numbers
We define a 𝑝
-adic number 𝑟
 as a formal power series 𝑟=∑𝑖=𝑘∞𝑟𝑖𝑝𝑖
, where 0≤𝑟𝑖<𝑝
 are integer coefficients.

Essentially, as formal power series of 𝑥
 extend polynomials of 𝑥
, 𝑝
-adic numbers extend regular numbers base 𝑝
, using the same rules for the summation and multiplication of the base 𝑝
 numbers. The number 𝑘
 in the definition above is called the 𝑝
-adic valuation of 𝑟
.

Generally, any rational number can be uniquely represented as 𝑝𝑘𝑛𝑑
, where 𝑛
 and 𝑑
 are co-prime integers, also co-prime with 𝑝
 and 𝑑
 is positive. In this representation, the possibly negative number 𝑘
 corresponds to the number 𝑘
 in the definition above.

This number 𝑘
 is called the 𝑝
-adic valuation of 𝑟
. The numbers with non-negative valuation are called 𝑝
-adic integers and we will focus on such numbers in this article, because then we can say

𝑟≡∑𝑖=0𝑘−1𝑟𝑖𝑝𝑖(mod𝑝𝑘)
and treat the 𝑝
-adic number as nested set of remainders modulo 𝑝𝑘
 for increasing 𝑘
, which is essentially the same as with regular polynomials of 𝑥
 that are much more known in competitive programming.

𝑝
-adic logarithms and exponents
One of the things that we regularly do with formal power series is taking logarithms and exponents of it. They have some combinatorics meaning in terms of generating functions, but also e.g. provide an efficient way to compute several first coefficients of 𝑃𝑘(𝑥)
 as

𝑃𝑘=exp(𝑘log𝑃).
Similar thing could be done for 𝑝
-adic numbers. But in this case it would be done to investigate multiplicative group of remainders modulo 𝑝𝑘
 in additive terms. Let's recall the power series definitions of log and exp:

log(1−𝑥)=−∑𝑘=1∞𝑥𝑘𝑘,exp𝑥=∑𝑘=0∞𝑥𝑘𝑘!.
The former can be rewritten for log𝑥
 as

log𝑥=−∑𝑘=1∞(1−𝑥)𝑘𝑘.
For these expressions to be meaningful we would need them to converge. That is, for each coefficient near 𝑝𝑘
 we should be able to eventually determine the exact value of such coefficient. Since we have 𝑥𝑘
 in the summation, a natural thing to demand here is 𝑥≡0(mod𝑝)
 for exp𝑥
 and 𝑥≡1(mod𝑝)
 for log𝑥
.

In this case, 𝑥𝑘
 for exp
 and (1−𝑥)𝑘
 for log
 would be divisible by 𝑝𝑘
, generally leading for increasing valuation of each summand.

Note that dividing by 𝑘
 for log
 and by 𝑘!
 would drag the valuation of the 𝑘
-th summand down.

For log
 it would generally be decreased by at most log𝑝𝑘
 and for exp
 by at most 𝑘𝑝+𝑘𝑝2+⋯≈𝑘𝑝−1
.

Therefore the valuation of 𝑘
-th summand for log
 is at least 𝑘−log𝑝𝑘
, which is good, as it limits to infinity. But for exp
 it is nearly 𝑘(𝑝−2)𝑝−1
 which is also fine for most 𝑝
. For most 𝑝
 except 2
, for which we will consistently get valuation of exactly 1
 when 𝑘
 is a power of 2
.

So, for odd prime numbers, exp𝑥
 converges for all 𝑥
 that are divisible by 𝑝
.

But for 𝑝=2
, we need 𝑥
 to be divisible by at least 4=𝑝2
 for exp
 to converge.

Why does it matter for primitive roots?
Let's take for granted that primitive root 𝑔
 does exist for every prime number 𝑝>2
.

Any invertible remainder modulo 𝑝𝑘
 can be represented as 𝑟≡𝑔𝑙𝑡(mod𝑝𝑘)
, where 𝑡≡1(mod𝑝)
.

It is possible to prove that explog𝑥=𝑥
 when both log𝑥
 and explog𝑥
 converge. That being said, any 𝑝
-adic number 𝑥
 such that 𝑥≡1(mod𝑝)
 can be uniquely represented as 𝑥=exp𝛼=explog𝑥
.

Note that there are 𝑝𝑘−1
 numbers 𝑥
 (taken modulo 𝑝𝑘
) such that 𝑥≡1(mod𝑝)
, as well as 𝑝𝑘−1
 numbers 𝛼
 (modulo 𝑝𝑘
) such that 𝛼≡0(mod𝑝)
, making log
-exp
 define a one-to-one correspondence between numbers having remainder 0
 and 1
.

Therefore, the remainder 𝑟
 can be represented alternatively as

𝑟≡𝑔𝑙(𝑟)exp𝛼(𝑟)(mod𝑝𝑘).
Here 𝑙(𝑟)
 is a number such that 𝑟≡𝑔𝑙(𝑟)(mod𝑝)
 and 𝛼(𝑟)
 is a number such that the equation above is true.

To make sure that what's written next is correct, 𝑔
 should be lifted (see the comment below) to have order 𝑝−1
 in the ring of 𝑝
-adic numbers as well.

There are 𝑝−1
 different meaningful values of 𝑙(𝑟)
 and 𝑝𝑘−1
 different meaningful values of 𝛼(𝑟)
, making up for (𝑝−1)𝑝𝑘−1
 different invertible numbers modulo 𝑝𝑘
. If we were to multiply to numbers modulo 𝑝𝑘
, the following would stand:

𝑥𝑦≡𝑔𝑙(𝑥)+𝑙(𝑦)exp(𝛼(𝑥)+𝛼(𝑦))(mod𝑝𝑘).
So, any number in the multiplicative group can be uniquely represented by the pair (𝑙,𝛼)
, where 𝑙
 goes from 0
 to 𝑝−1
 and 𝛼
 goes from 0
 to (𝑝−1)(𝑝+𝑝2+⋯+𝑝𝑘−1)=𝑝𝑘−𝑝
, traversing all remainder modulo 𝑝𝑘
 that are divisible by 𝑝
.

In this notion, all meaningful values of 𝑙
 are generated by the multiples of number 1
, granted that they're taken modulo 𝑝−1
 and all meaningful values of 𝛼
 are generated by the multiples of number 𝑝
, taken modulo 𝑝𝑘
.

First number has a period of 𝑝−1
, second number has a period of 𝑝𝑘−1
. These numbers are co-prime, hence the joint period of (1,𝑝)
 is (𝑝−1)𝑝𝑘−1
, which means that (1,𝑝)
 generates all possible pairs of (𝑙,𝛼)
.

This, in turn, means that all remainders modulo 𝑝𝑘
 are generated by the 𝑝
-adic number 𝑔exp𝑝
.

Ok, and what about 𝑝=2
?
As we mentioned earlier, exp𝑟
 is still well-defined for 𝑟≡0(mod4)
, so modulo 2𝑘
 for 𝑘>2
, exp
-log
 pair define a bijection between numbers that have remainder 1
 modulo 4
 and numbers that have remainder 0
 modulo 4
.

As such, it means that powers of log4
 generate all the numbers that are equal to 1
 modulo 4
. This fact highlights that in remainders modulo 2𝑘
, there is an element of degree 2𝑘−2
 that generates a multiplicative subgroup of numbers that are equal to 1
 modulo 4
.

Therefore the whole multiplicative group of remainders modulo 2𝑘
 is generated by −1
 and exp4
.

Tags number theory, remainder, primitive root, p-adic numbers

Hadamard product and binomial convolution of linear recurrences

By adamant, history, 19 months ago, In English
Hi everyone!

Let 𝑅
 be a ring, 𝑑0,𝑑1,𝑑2,⋯∈𝑅
 and 𝑒0,𝑒1,𝑒2,⋯∈𝑅
 be linear recurrence sequences, such that

𝑑𝑚=∑𝑖=1𝑘𝑎𝑖𝑑𝑚−𝑖 for 𝑚≥𝑘,𝑒𝑚=∑𝑖=1𝑙𝑏𝑖𝑒𝑚−𝑖 for 𝑚≥𝑙.
In some applications, the following two sequences arise:

𝑓𝑘𝑓𝑘==𝑑𝑘𝑒𝑘∑𝑖+𝑗=𝑘(𝑘𝑖)𝑑𝑖𝑒𝑗(Hadamard product),(binomial convolution).
Today I'd like to write about the framework that allows to prove that both the sequences defined above are also linear recurrences. It would also allow to compute their characteristic polynomials in 𝑂(𝑘𝑙log𝑘𝑙)
, which is optimal as their degrees are 𝑂(𝑘𝑙)
 in both cases.

Umbral calculus
Generally, a linear recurrence 𝑓𝑘
 can be described and analyzed with the help of the linear functional 𝑇:𝑅[𝑓]↦𝑅
 such that

𝑇(𝑓𝑘)=𝑓𝑘.
For such functional, 𝑇(𝑃(𝑓))=0
 when 𝑃(𝑓)
 is a multiple of the characteristic polynomial of 𝑓𝑘
. The existence of the characteristic polynomial is the criterion of 𝑓𝑘
 being a linear recurrence. So, we need to prove that there is such a polynomial for 𝑓𝑘
 defined above.

Joint umbral calculus
To analyze joint properties of 𝑑𝑘
 and 𝑒𝑘
, we define a linear functional 𝑇:𝑅[𝑑,𝑒]→𝑅
 such that

𝑇(𝑑𝑖𝑒𝑗)=𝑑𝑖𝑒𝑗.
Similarly to the case of a single recurrence, 𝑇(𝑓(𝑑,𝑒))=0
 whenever 𝑓(𝑑,𝑒)
 is a linear combination of 𝑎(𝑑)
 and 𝑏(𝑒)
, where

𝑎(𝑑)𝑏(𝑒)==𝑑𝑘−∑𝑖=1𝑘𝑎𝑖𝑑𝑘−𝑖,𝑒𝑙−∑𝑗=1𝑙𝑏𝑗𝑒𝑙−𝑗.
are the characteristic polynomials of 𝑑𝑖
 and 𝑒𝑗
. In other words, 𝑇(𝑓(𝑑,𝑒))=0
 whenever 𝑓(𝑑,𝑒)
 lies in the ideal ⟨𝑎(𝑑),𝑏(𝑒)⟩
.

Composed sum
For the binomial convolution let 𝑓=𝑑+𝑒
, then

𝑇(𝑓𝑘)=𝑇((𝑑+𝑒)𝑘)=𝑇(∑𝑖=0𝑘(𝑘𝑖)𝑑𝑖𝑒𝑘−𝑖)=∑𝑖=0𝑘(𝑘𝑖)𝑇(𝑑𝑖𝑒𝑘−𝑖)=𝑓𝑘
To show that 𝑓𝑘
 is a linear recurrence obeying to the rule

𝑓𝑚=∑𝑖=1𝑡𝑐𝑖𝑓𝑚−𝑖 for 𝑚≥𝑡,
it is sufficient to show that there is a characteristics polynomial 𝑐(𝑓)
 such that 𝑐(𝑓)∈⟨𝑎(𝑑),𝑏(𝑒)⟩
.

Assume that 𝑅
 is an integral domain. Then the polynomial exists and can be defined explicitly as

𝑐(𝑑+𝑒)=∏𝑖=1𝑘∏𝑗=1𝑙((𝑑+𝑒)−(𝜆𝑖+𝜇𝑗)),
where

𝑎(𝑑)𝑏(𝑒)==∏𝑖=1𝑘(𝑑−𝜆𝑖),∏𝑗=1𝑙(𝑒−𝜇𝑗).
The fact that 𝑐(𝑑+𝑒)∈⟨𝑎(𝑑),𝑏(𝑒)⟩
 is proven as follows:

𝑐(𝑑+𝑒)==∑𝑟𝑖𝑗∈{0,1}∏𝑖=1𝑘∏𝑗=1𝑙((𝑒−𝜇𝑗)+(𝑑−𝜆𝑖))=∏𝑖=1𝑘∏𝑗=1𝑙(𝑒−𝜇𝑗)𝑟𝑖𝑗(𝑑−𝜆𝑖)1−𝑟𝑖𝑗.
In the sum above, there are 2𝑘𝑙
 summands, each of them is divisible by either 𝑎(𝑑)
 or 𝑏(𝑒)
, so 𝑐(𝑑+𝑒)∈⟨𝑎(𝑑),𝑏(𝑒)⟩
.

The polynomial 𝑐(𝑓)
 defined above is called the composed sum of 𝑎(𝑑)
 and 𝑏(𝑒)
.

Composed product
Now the question is, how to prove that the Hadamard product 𝑓𝑘=𝑑𝑘𝑒𝑘
 is a linear recurrence?

Using similar logic as above, one would define 𝑓=𝑑𝑒
 and then look for 𝑐(𝑓)∈⟨𝑎(𝑑),𝑏(𝑒)⟩
. Let

𝑐(𝑑𝑒)=∏𝑖=1𝑘∏𝑗=1𝑙(𝑑𝑒−𝜆𝑖𝜇𝑗).
This one is a bit trickier to prove. Let's start with 𝑘=𝑙=1
:

𝑐(𝑑𝑒)=𝑑𝑒−𝜆𝜇=𝑑(𝑒−𝜇)+(𝑑−𝜆)𝜇.
Rewriting it in the same way for arbitrary 𝑘
 and 𝑙
, we get

𝑐(𝑑𝑒)==∑𝑟𝑖𝑗∈{0,1}∏𝑖=1𝑘∏𝑗=1𝑙(𝑑(𝑒−𝜇𝑗)+(𝑑−𝜆𝑖)𝜇𝑗)=∏𝑖=1𝑘∏𝑗=1𝑙𝑑𝑟𝑖𝑗𝜇1−𝑟𝑖𝑗𝑗(𝑒−𝜇𝑗)𝑟𝑖𝑗(𝑑−𝜆𝑖)1−𝑟𝑖𝑗.
Then the same logic applies as to 𝑐(𝑑+𝑒)
 in the binomial convolution case.

The polynomial 𝑐(𝑓)=𝑐(𝑑𝑒)
 defined above is called the composed product of 𝑎(𝑑)
 and 𝑏(𝑒)
.

Computing composed products and sums
Let 𝑠𝑖
 be the sum of 𝑖
-th powers of all 𝜆
 and 𝑡𝑗
 be the sum of 𝑗
-th powers of all 𝜇
, that is

𝑠𝑖=𝜆𝑖1+⋯+𝜆𝑖𝑘,𝑡𝑗=𝜇𝑗1+⋯+𝜇𝑗𝑙.
The roots of the composed sum are 𝜆𝑖+𝜇𝑗
 for all 𝑖
 and 𝑗
 and of the composed product are 𝜆𝑖𝜇𝑗
, from which we can see that

∑𝑖=1𝑘∑𝑗=1𝑙(𝜆𝑖𝜇𝑗)𝑧∑𝑖=1𝑘∑𝑗=1𝑙(𝜆𝑖+𝜇𝑗)𝑧==∑𝑖=1𝑘𝜆𝑧𝑖∑𝑗=1𝑙𝜇𝑧𝑗∑𝑥+𝑦=𝑧(𝑧𝑥)∑𝑖=1𝑘𝜆𝑥𝑖∑𝑗=1𝑙𝜇𝑦𝑗==𝑠𝑧𝑡𝑧,∑𝑥+𝑦=𝑧(𝑧𝑥)𝑠𝑥𝑡𝑦.
So, if we're able to transform from 𝑎(𝑑)
 to 𝑠𝑖
, then from 𝑏(𝑒)
 to 𝑡𝑗
, compute the transforms above on them and then recover the characteristics polynomials from the result, it would solve the problem.

Next thing we should note is that the generating function of 𝑠𝑖
 is

∑𝑖=0∞𝑠𝑖𝑥𝑖=∑𝑖=0∞∑𝑗=1𝑘𝜆𝑖𝑗𝑥𝑖=∑𝑗=1𝑘11−𝜆𝑖𝑥.
It can be further expanded as

∑𝑗=1𝑘11−𝜆𝑖𝑥=𝑘+∑𝑗=1𝑘𝜆𝑖𝑥1−𝜆𝑖𝑥=𝑘−𝑥𝐴′(𝑥)𝐴(𝑥).
where

𝐴(𝑥)=1−∑𝑖=1𝑘𝑎𝑖𝑥𝑖=∏𝑖=1𝑘(1−𝜆𝑖𝑥)
is the reversed characteristic polynomial of 𝑑𝑘
. Its log-derivative is indeed

𝐴′(𝑥)𝐴(𝑥)=∑𝑖=1𝑘−𝜆𝑖1−𝜆𝑖𝑥.
Finally, to inverse this transform, we could make use of the fact that 𝐴′𝐴=(log𝐴)′
, hence for

𝐵(𝑥)=𝑘−𝑥𝐴′(𝑥)𝐴(𝑥)
it holds that

𝐴(𝑥)=exp∫𝑘−𝐵(𝑥)𝑥𝑑𝑥.
The resulting 𝑓(𝑥)
 has degree 𝑘𝑙
, so only 𝑘𝑙
 terms of 𝐴′𝐴
 and exp
 are needed and they may be computed in 𝑂(𝑘𝑙log𝑘𝑙)
.

Tags tutorial, linear recurrence, composed sum, composed product, polynomials, power series

Nimbers and Sprague-Grundy theorem

By adamant, history, 18 months ago, In English
Hi everyone!

Today I'd like to write about the so-called Grundy numbers, or nimbers. I will start by providing a formal recap on the Sprague-Grundy theorem and then will advance to the topic that is rarely covered in competitive programming resources, that is I will write about nimber product and its meaning to the game theory.

I was asked to add the following to the blog: THANK SIR MASTER Adam_GS FOR GREAT AND VALUABLE REVIEW SIR

Prerequisites
Familiarity with the following concepts:

Basic set theory notion: union, cartesian product and symmetric difference of sets, set of all subsets denoted as 2𝐴
, etc;
Groups and fields: definitions, basic facts, most well-known examples (e.g. ℝ
, ℤ𝑝
);
Familiarity with the xor operation and its properties;
Notion of equivalence relations.
Familiarity with formal computational models (e.g. deterministic finite automata) is not required, but would be helpful.

Although the nimbers are typically constructed on top of ordinal numbers and transfinite nim, I will try to stick with natural numbers and classic nim. You may familiarize yourself with transfinite nim in e.g. this article by emorgan5289.

Basic definitions
Def. 1. An impartial game is a game that can be represented as a tuple 𝐺=(𝑉,𝛿,𝑠)
, such that

𝑉
 is a set of game states;
𝛿:𝑉↦2𝑉
 is a move function, meaning that the player can move from 𝑣
 to 𝑢
 if and only if 𝑢∈𝛿(𝑣)
;
𝑠∈𝑉
 is the starting state of the game;
Starting in 𝑠
, two players take turns alternatingly, changing the current state 𝑣
 to 𝑢∈𝛿(𝑣)
;
( normal play convention ) Player that can't make a move loses or
( misère play convention ) Player that can't make a move wins.
Informally, an impartial game takes place in a directed graph 𝐺
 with a starting vertex 𝑠
, while the players always move along the arcs of the graph. The key word impartial here means that the set of possible moves 𝛿(𝑣)
 from the vertex 𝑣
 is same for both players.

Correspondingly, a partisan game would imply that there are two move functions 𝛿1(𝑣)
 and 𝛿2(𝑣)
, defining possible moves for the first and the second players correspondingly. We're not concerned about such games in this article.

Def 2. An impartial game 𝐺
 is called winning if the first player has a winning strategy under the used play convention. Correspondingly, the game is called losing if the second player has a winning strategy. In a similar manner, states of the game are classified into losing and winning ones, e.g. is the state 𝑣
 is winning if the game (𝑉,𝛿,𝑣)
 is winning and vice versa.

Generally, the game might be neither winning, nor losing if the game graph has cycles.


A simple game represented as a directed graph. Losing states are red, winning states are blue.
Claim 1. The state 𝑣
 is losing if and only if 𝛿(𝑣)
 is empty or all its elements are winning states. The state 𝑣
 is winning if and only if there is a losing state in 𝛿(𝑣)
.

The result above allows to classify all game states on arbitrary graphs into either winning, losing or drawing (when the game started in the state would go indefinitely) with some kind of reverse breadth-first search.

Def. 3. An impartial game is progressively bounded if there is an upper bound for every state 𝑠
 on the number of turns that can be made starting in 𝑠
, until a state 𝑣
 having 𝛿(𝑣)=∅
 is reached.

Claim 2. Every state in the progressively bounded game is either winning or losing.

Game sum
Def. 4. Let 𝐺1=(𝑉1,𝛿1,𝑠1)
 and 𝐺2=(𝑉2,𝛿2,𝑠2)
 be impartial games. The sum of games 𝐺=𝐺1+𝐺2
 is defined as

𝐺1+𝐺2=(𝑉1×𝑉2,𝛿,(𝑠1,𝑠2)),
where 𝛿:𝑉1×𝑉2↦2𝑉1×𝑉2
 is a joint move function defined as 𝛿(𝑣1,𝑣2)=𝛿(𝑣1)×{𝑣2}∪{𝑣1}×𝛿(𝑣2)
.

Here 𝐴×𝐵={(𝑎,𝑏):𝑎∈𝐴,𝑏∈𝐵}
 is the Cartesian product of 𝐴
 and 𝐵
.

Informally, 𝐺1+𝐺2
 is a game in which players have both 𝐺1
 and 𝐺2
 on the table, and in one turn player could do a valid turn either in 𝐺1
, or in 𝐺2
. Correspondingly, player which can't make a move in both games loses.

Def. 5. Nim is a game in which two players take turns removing objects from distinct heaps. On each turn, a player must remove at least one object, and may remove any number of objects provided they all come from the same heap. The player that can't make a move loses.

Let 𝑁𝑎
 be a game of nim consisting of the only heap of size 𝑎
. Then the game of nim on heaps of sizes 𝑎1,𝑎2,…,𝑎𝑛
 is represented as

𝑁=𝑁𝑎1+𝑁𝑎2+⋯+𝑁𝑎𝑛.

Graphic representation of 𝑁2
, 𝑁3
 and 𝑁2+𝑁3
.
Claim 3. If 𝐴
 and 𝐵
 are impartial progressively bounded games, then so is 𝐴+𝐵
.

Claim 4. If 𝐴
 and 𝐵
 are losing, then so is 𝐴+𝐵
.

Proof. Second player can just respond to any turn with the corresponding second player winning strategy in that game. ◻
Sprague-Grundy theorem
Def 6. Impartial progressively bounded games 𝐴
 and 𝐵
 are equivalent (denoted 𝐴∼𝐵
) if for any impartial progressively bounded game 𝐶
, 𝐴+𝐶
 has the same outcome as 𝐵+𝐶
.

Claim 5. If 𝐵
 is losing, then 𝐴∼𝐴+𝐵
.

Proof. We have to prove that 𝐴+𝐶
 has the same outcome as (𝐴+𝐶)+𝐵
 when 𝐵
 is losing. If 𝐴+𝐶
 is losing, it follows from claim 4. Otherwise, first player makes a move in 𝐴+𝐶
 into losing position (𝐴+𝐶)′
. Now both (𝐴+𝐶)′
 and 𝐵
 are losing, hence (𝐴+𝐶)′+𝐵
 is losing for the second player. ◻
Claim 6. For any game 𝐴
, its sum with itself 𝐴+𝐴
 is losing.

Proof. Second player can symmetrically repeat the moves of the first one in the second game. ◻
Claim 7. ( Equivalence criterion ) 𝐴∼𝐵
 if and only if 𝐴+𝐵
 is losing.

Proof. Let 𝐴∼𝐵
. Then 𝐴+𝐵
 has the same outcome as 𝐵+𝐵
, so it is losing. Now assume that 𝐴+𝐵
 is losing. From claim 5 it follows that (𝐴+𝐵)+𝐵∼𝐵
. On the other hand, 𝐵+𝐵
 is losing, so 𝐴+(𝐵+𝐵)∼𝐴
 and 𝐴∼𝐵
. ◻
Claim 8. ( Sprague-Grundy theorem ) Every impartial progressively bounded game is equivalent to a one-heap game of nim.

Proof. The game in which 𝛿(𝑠)=∅
 is equivalent to a game of nim on the heap of size 0
. On the other hand, in progressively bounded games we will always reach state 𝑣
 such that 𝛿(𝑣)=∅
 after finite amount of turns, so we can prove the result with a backwards induction on the distance to the furthest blocked state.

Essentially, in each move we change our game from (𝑉,𝛿,𝑣)
 to (𝑉,𝛿,𝑢)
, where 𝑢∈𝛿(𝑣)
. All such vertices 𝑢
 must have a lower distance to the furthest losing state than 𝑣
, hence by induction (𝑉,𝛿,𝑢)∼𝑁𝑎𝑢
, where 𝑎𝑢
 is the size of the one-heap nim for 𝑢
.

What we need to prove now is that there is 𝑎𝑣
 such that (𝑉,𝛿,𝑣)∼𝑁𝑎𝑣
. Core result discovered by Sprague and Grundy is that

𝑎𝑣=mex({𝑎𝑢:𝑢∈𝛿(𝑣)}),
where mex(𝑆)
 ( minimum excluded ) is the smallest non-negative integer that does not belong to 𝑆
.

Let's prove that 𝑁𝑎𝑣+𝐺𝑣
, where 𝐺𝑣=(𝑉,𝛿,𝑣)
 is, indeed, a losing game.

If the first player moves from 𝑁𝑎𝑣
 to 𝑁𝑥
 with 𝑥<𝑎𝑣
, the second one can move into 𝑁𝑥
 in 𝐺𝑣
, as there is 𝑢
 such that 𝑎𝑢=𝑥
. If the first player moves from 𝐺𝑣
 into 𝑁𝑎𝑢
, we end up with the game 𝑁𝑎𝑣+𝑁𝑎𝑢
, where 𝑎𝑢≠𝑎𝑣
, hence the second player may match the sizes of the heaps. In both cases after second player moves, the resulting game looks like 𝑁𝑥+𝑁𝑥
 and it is a losing game. ◻
Def. 7. A nimber 𝑥
 of the game 𝐺
 is the size of its one-heap equivalent, that is 𝐺∼𝑁𝑥
.

From the definition it follows that the nimber of 𝑁𝑎
 is 𝑎
.

Nimber sum
You can use this Lenstra's publication as a further reference for the stuff mentioned below

Def. 8. The nimber sum 𝑎⊕𝑏
 is a nimber of 𝑁𝑎+𝑁𝑏
, that is 𝑁𝑎⊕𝑏∼𝑁𝑎+𝑁𝑏
.

Note that from 𝑁𝑣
 it is possible to move to any 𝑁𝑢
 such that 0≤𝑢<𝑣
, thus from Sprague-Grundy theorem it follows that

𝑎⊕𝑏=mex({𝑎′⊕𝑏:𝑎′<𝑎}∪{𝑎⊕𝑏′:𝑏′<𝑏}).
Claim. 9. 𝑎⊕𝑏
 is the xor of 𝑎
 and 𝑏
.

Proof. We have to prove that 𝑁𝑎xor𝑏+𝑁𝑎+𝑁𝑏
 is a losing game. After the first move we will have three heaps 𝑁𝑥
, 𝑁𝑦
 and 𝑁𝑧
 such that the xor of 𝑥
, 𝑦
 and 𝑧
 will not be equal to 0
. It is always possible to reduce one of the heaps, so that their xor becomes 0
 again, which yields a winning strategy for the second player. ◻
Algebraic rationale
One can check that nimbers with the nimber sum operation form up a group. Conway pointed out that, in a sense, it is the simplest group that can be defined on the set of non-negative integers.

Indeed, for any group operation it must hold that

𝑎′⊕𝑏≠𝑎⊕𝑏 and 𝑎⊕𝑏′≠𝑎⊕𝑏
for 𝑎≠𝑎′
 and 𝑏≠𝑏′
. Otherwise, the operation wouldn't form a group. Now 𝑎⊕𝑏
 is the smallest value not violating these rules.

Nimber product
Using the algebraic rationale above, it is also possible to define a "simplest" operation ⋅
 such that nimbers with operations ⊕
 and ⋅
 form up a field. Specifically, there must not be any non-trivial divisors of zero in a field, hence

(𝑎−𝑎′)⊗(𝑏−𝑏′)≠0⟺𝑎𝑏≠𝑎′𝑏+𝑎𝑏′−𝑎′𝑏′
for 𝑎′≠𝑎
 and 𝑏′≠𝑏
. Again, taking the smallest possible value of 𝑎𝑏
 that wouldn't violate the rule above, we arrive to the following definition of the nimber product:

Def. 9. The nimber product 𝑎𝑏
 is defined recursively as

𝑎𝑏=mex({𝑎′𝑏⊕𝑎𝑏′⊕𝑎′𝑏′:𝑎′<𝑎,𝑏′<𝑏}).
Conway proved that nimbers with ⊕
 and ⋅
 operations form a field of characteristic 2
 that is algebraically closed if nimbers are constructed on top of the proper class of ordinal numbers (this part goes a bit beyond the scope of the article).

Which is nice, but we'd also like to see some game theoretic implications of the nimber product, right?

Def. 10. The diminishing rectangles game is described as follows:

Let's represent a nimbers product 𝑛𝑚
 as a stone in the point (𝑛,𝑚)
 on 2
-dimensional plane.

Then the definition of the nimber product corresponds to the following set of moves:

pick a point (𝑥,𝑦)
 such that 𝑥,𝑦≥1
 and there is a stone on (𝑥,𝑦)
;
pick a pair of non-negative integers 𝑥′
 and 𝑦′
 such that 0≤𝑥′<𝑥
 and 0≤𝑦′<𝑦
;
remove a stone from the point (𝑥,𝑦)
 and place one stone in each of points (𝑥′,𝑦)
, (𝑥,𝑦′)
 and (𝑥′,𝑦′)
.
The game continues on until there is no stone with positive coordinates left.


A move in the diminishing rectangles game.
Note that 𝐴⊕𝐴=0
, hence, we may take the number of stones modulo 2
, which results in the following game:

Def. 11. The coin turning game is described as follows:

There is an (𝑛+1)×(𝑚+1)
 sized rectangular table. In each table cell there is a coin. All coins are initially tails up, except for the coin in the cell (𝑛,𝑚)
, which is heads up. In a turn, the player can do the following:

pick a cell (𝑥,𝑦)
 such that 𝑥,𝑦≥1
 and the coin in (𝑥,𝑦)
 is heads up;
pick a pair 0≤𝑥′<𝑥
 and 0≤𝑦′<𝑦
;
flip all the coins in the rectangle formed by (𝑥′,𝑦′)
, (𝑥′,𝑦)
, (𝑥,𝑦′)
 and (𝑥,𝑦)
.
The game continues on until all cells with 𝑥,𝑦≥1
 have only coins facing tails up.

Game product
Now, having a bit more of intuition on the product of nim game, we may define a product of arbitrary games:

Def. 12. Let 𝐺1=(𝑉1,𝛿1,𝑠1)
 and 𝐺2=(𝑉2,𝛿2,𝑠2)
 be impartial games. The product of games 𝐺1⋅𝐺2
 is defined as

𝐺1⋅𝐺2=(2𝑉1×𝑉2,𝛿,{(𝑠1,𝑠2)}),
where 𝛿:2𝑉1×𝑉2↦22𝑉1×𝑉2
 is defined as

𝛿(𝑆)={𝑆△{(𝑢1,𝑢2),(𝑢1,𝑣2),(𝑣1,𝑢2),(𝑣1,𝑣2)}:(𝑣1,𝑣2)∈𝑆,𝑢1∈𝛿1(𝑣1),𝑢2∈𝛿2(𝑣2)},
where 𝐴△𝐵
 is the symmetric difference of two sets.

Informally, there is nothing I can say really to explain what's going on here.

Computation
Library Checker — Nim product. Given 𝑎1,…,𝑎𝑛
 and 𝑏1,…,𝑏𝑛
, compute 𝑎𝑖⋅𝑏𝑖
 for each 𝑖
, where 𝑎𝑖,𝑏𝑖<264
 and 𝑛≤106
.

Any non-negative integer 𝑛
 can be decomposed as 𝑛=2𝑎1⊕2𝑎2⊕…
, where 0≤𝑎1<𝑎2<…
 are integers. Let 𝑚=2𝑏1⊕2𝑏2⊕…
, where 0≤𝑏1<𝑏2<…
, then using the distributivity of ⊕
 and ⋅
, we may rewrite the nimber product 𝑛𝑚
 as

𝑛⋅𝑚=⨁𝑖,𝑗2𝑎𝑖⋅2𝑏𝑗.
In other words, knowing how to compute the nim product 2𝑎⋅2𝑏
 would allow us to compute the nim product for arbitrary 𝑛
 and 𝑚
.

Claim 10. Let 𝑎=22𝛼
 and 𝑏=22𝛽
, where 𝛼≠𝛽
. Then 𝑎⋅𝑏=22𝛼+2𝛽
 and 𝑎⋅𝑎=3222𝛼
.

Now, let 𝑎=2𝛼1+2𝛼2+…
 and 𝑏=2𝛽1+2𝛽2+…
, where 0≤𝛼1<𝛼2<…
 and 0≤𝛽1<𝛽2<…
 then

2𝑎⋅2𝑏=(22𝛼1⋅22𝛼2⋅…)(22𝛽1⋅22𝛽2⋅…)=(22𝛼1⋅22𝛽1)⋅(2𝑎−2𝛼1⋅2𝑏−2𝛽1).
This reduction allows to compute 2𝑎⋅2𝑏
 as 𝑥⋅𝑦
, where 𝑥≤2𝑎−1
 and 𝑦≤2𝑏−1
, so it will sooner or later reach 𝑥=1
 or 𝑦=1
.

When 𝑎,𝑏<22𝑛
, it also holds that 𝑎⋅𝑏<22𝑛
, so ⊕
 and ⋅
 define a field on the set of non-negative integers less than 22𝑛
 for any 𝑛
. It also holds for 264=226
, so if all 2𝑎⋅2𝑏
 are precomputed, one could find the nim product of any 64
-bit numbers in 642
 operations:

Code
It is also possible to significantly improve the running time of the nimber multiplication using Karatsuba-like scheme:

(𝑎0⊕22𝑘⋅𝑎1)⋅(𝑏0⊕22𝑘⋅𝑏1)=(𝑎0⋅𝑏0)⊕(𝑎0⋅𝑏1⊕𝑎1⋅𝑏0)⋅22𝑘⊕(𝑎1⋅𝑏1)⋅3222𝑘,
because

𝑎0⋅𝑏1⊕𝑎1⋅𝑏0=(𝑎0⊕𝑎1)⋅(𝑏0⊕𝑏1)⊕𝑎0⋅𝑏0⊕𝑎1⋅𝑏1
and 3222𝑘=22𝑘⊕22𝑘−1
. Here's my implementation that takes 1.4s on the Library Judge.

UPD: With exploitation of nimber product properties (see the comments below), it's possible to reduce the running time further to ~410 ms.

Problem examples
Left to the reader as an exercise.

1310F - Bad Cryptography. Given 𝑎1,…,𝑎𝑛
 and 𝑏1,…,𝑏𝑛
, solve 𝑎𝑥𝑖=𝑏𝑖
 for each 𝑖
, where 𝑎𝑖,𝑏𝑖<264
 and 𝑛≤100
.

1338C - Perfect Triples. It is related to nimber product somehow, right?

Tags nimber, sprague-grundy, game theory, tutorial

Submask range queries

By adamant, history, 18 months ago, In English
Hi everyone!

There was once a problem named "Easy When You Know How". Unfortunately, I can't remember the contest it originated from.

You're given an array 𝑎0,𝑎1,…,𝑎2𝑛−1
. Answer 𝑞
 queries that are:

Given 𝑚
, compute the sum over all 𝑎𝑖
 such that 𝑖
 is a submask of 𝑚
;
Given 𝑚
 and 𝑐
, add 𝑐
 to all 𝑎𝑚
.
Constraints were along the lines of 2𝑛,𝑞≤105
.

I tried to find anything about this technique on Codeforces, but failed, so I thought it'd be nice to write a brief blog about this cute problem.

The problem is indeed simple if you know how to solve it. The solution utilizes square root decomposition and looks as follows:

Let 𝑘=⌊𝑛2⌋
 and 𝑏0,𝑏1,…,𝑏2𝑛−1
 be an array such that 𝑏𝑥+2𝑘𝑦
 is a sum of 𝑎𝑥+2𝑘𝑦′
 over all possible submasks 𝑦′
 of 𝑦
.

Now, to answer queries, let 𝑚=𝑥+2𝑘𝑦
.

The answer to the first query is a sum of 𝑏𝑥′+2𝑘𝑦
 over all submasks 𝑥′
 of 𝑥
.
For the second query one has to add 𝑐
 to 𝑏𝑥+2𝑘𝑦′
 for all supermasks 𝑦′
 of 𝑦
 (i. e. all 𝑦′
 s.t. 𝑦
 is a submask of 𝑦′
).
This allows to solve the problem in 𝑂(2⌈𝑛/2⌉𝑞)
. I wonder if there are any better ways to solve it?

Tags submasks, range query, tutorial

Combinatorial species: An intuition behind generating functions

By adamant, history, 18 months ago, In English
Hi everyone!

The usage of generating functions is quite common in competitive programming these days. But it seems to happen so, that they're mostly presented as a way to simplify formal manipulation with polynomial coefficients, rather than something meaningful. But there actually is a meaning to all of this, and today I'm going to shed a light on it.

Alongside the blog post we'll uncover the combinatorial meaning of

The addition 𝐹(𝑥)+𝐺(𝑥)
,
The multiplication 𝐹(𝑥)𝐺(𝑥)
,
The exponent exp𝐹(𝑥)
,
The logarithm log𝐹(𝑥)
,
The sum of the infinite geometric progression 11−𝐹(𝑥)=1+𝐹(𝑥)+𝐹2(𝑥)+…
,
The general composition 𝐹(𝐺(𝑥))
for the generating functions 𝐹(𝑥)
 and 𝐺(𝑥)
 and the underlying structures they represent.

Prerequisites
Basic notion of set theory (cardinality of sets, mappings, bijections, cartesian product, disjoint union, etc);
Polynomials and formal power series (representation, convolution formula, power series definitions of exp
 and log
);
Elementary combinatorics (combinations, partitions, etc).
Although combinatorial species are defined with category theory concepts, I will try to avoid them for simplicity. However, knowledge of some category theory concepts would greatly simplify the reading experience.

Some of definitions use pretty dense mathematical notation, I will follow up with informal explanation right away when it is the case.

Commutative diagrams
I will also use commutative diagrams to illustrate complicated identities. You may use the explanation below as a reference.

Explanation
Every diagram is also provided with a more verbose text description, so don't be afraid if the concept seems complicated.

Definitions
A mapping 𝐹
 that maps elements of 𝐴
 to elements of 𝐵
 is denoted as 𝐹:𝐴→𝐵
.

Def. 1. The identity map Id𝐴:𝐴→𝐴
 is defined as Id𝐴(𝑎)=𝑎
 for 𝑎∈𝐴
.

Def. 2. Let 𝐹:𝐴→𝐵
 and 𝐺:𝐵→𝐶
. Then the composition 𝐺∘𝐹
 is defined as (𝐺∘𝐹)(𝑎)=𝐺(𝐹(𝑎))
 for 𝑎∈𝐴
.

In the definition below we use a notion of a class. Informally, it is a collection of sets joined by some property, which does not have to be a set of its own, but for which we may still consider some mappings (functions) on top of it. It is also possible to formalize the concept or to bypass the need for it to not be a set, see the comment below.

Alternatively, if you're really uncomfortable working with proper classes (which requires the addition of new axioms to the Zermelo-Fraenkel set theory), you may assume that the work is done in the set of hereditarily finite sets rather than the class of finite sets.

Def. 3. Let 𝑈
 be a class of finite sets. A combinatorial species is a mapping 𝐹:𝑈→𝑈
 and a family of bijections 𝐹𝜎:𝐹(𝐴)→𝐹(𝐵)
 defined for every bijection 𝜎:𝐴→𝐵
 between 𝐴,𝐵∈𝑈
 in such way that

If 𝜎:𝐴→𝐵
 and 𝜏:𝐵→𝐶
 are bijections, then 𝐹𝜏∘𝜎=𝐹𝜏∘𝐹𝜎
;
For 𝐵=𝐴
 and Id𝐴:𝐴→𝐵
 it holds that 𝐹Id𝐴=Id𝐹(𝐴)
.
The elements of 𝐹(𝐴)
 are called the 𝐹
-structures and the bijection 𝐹𝜎
 is called the transport of 𝜎
 along 𝐹
-structures.


In category theory terms, a combinatorial species is a functor on the category of finite sets.
Informally, 𝐹(𝐴)
 is a set of combinatorial structures (graphs, trees, sequences, etc) that are defined by the species 𝐹
 and labeled by the elemenets of 𝐴
. The bijections 𝐹𝜎
 define the rules in which structures change after re-labeling. In this terms, the rules are read as

Re-labeling from 𝐴
 to 𝐵
 using 𝜎
 and then from 𝐵
 to 𝐶
 using 𝜏
 should be same as re-labeling directly from 𝐴
 to 𝐶
 using 𝜏∘𝜎
;
Re-labeling from 𝐴
 to 𝐴
 using the identity map should not change 𝐹(𝐴)
.
For example, let 𝐹
 be a species of labeled trees. Then, 𝐹(𝐴)
 should map 𝐴
 to a set of all labeled trees that can be build on |𝐴|
 vertices, such that each vertex is labeled by an element from 𝐴
. On the picture below, you see 𝐹(𝐴2)
, 𝐹(𝐴3)
 and 𝐹(𝐴4)
:


Labeled trees on sets 𝐴2={∙,∙}
, 𝐴3={∙,∙,∙}
 and 𝐴4={∙,∙,∙,∙}
.
The image by Júlio Reis is distributed under CC BY-SA 3.0 license.
Correspondingly, if you were to change labels from 𝐴4={∙,∙,∙,∙}
 to 𝑋4={1,2,3,4}
 with a mapping 𝜎:𝐴4→𝑋4
, the bijection 𝐹𝜎:𝐹(𝐴4)→𝐹(𝑋4)
 would map each tree from 𝐹(𝐴4)
 to 𝐹(𝑋4)
 according to the way labels are changed from 𝐴4
 to 𝑋4
 by 𝜎
.

Further examples
Set species is defined as 𝐸(𝐴)={𝐴}
, combinatorial structure is a set of |𝐴|
 elements labeled by 𝐴
;
𝑛
-set species is defined as 𝐸𝑛(𝐴)={𝐴}
 if |𝐴|=𝑛
 and 𝐸𝑛(𝐴)=∅
 otherwise;
Singleton species 𝑋
 is defined as 𝑋(𝐴)=𝐸1(𝐴)
;
Subset species is defined as 𝐹(𝐴)=2𝐴
, combinatorial structure is a subset of 𝐴
;
Permutation species 𝑆
 is defined as 𝑆(𝐴)=𝑆𝐴
, combinatorial structures are permutations (bijections to itself) of 𝐴
;
Graph species is defined as 𝐹(𝐴)={(𝐴,𝐸):𝐸⊂𝐴×𝐴}
, combinatorial structures are graphs with 𝐴
 as vertices;
Cycle species 𝐶
 corresponds to combinatorial structures being cycles of |𝐴|
 elements labeled by 𝐴
;
Partition species Par
 corresponds to combinatorial structures being partitions of 𝐴
;
Linear order species 𝐿
 corresponds to combinatorial structures being ordered |𝐴|
-tuples of distinct elements from 𝐴
.
Species isomorphism
Def. 4. Let 𝐹
 and 𝐺
 be two species. Species isomorphism 𝛼
 is a family of bijections 𝛼𝐴:𝐹(𝐴)→𝐺(𝐴)
 for 𝐴∈𝑋
, such that for every bijection 𝜎:𝐴→𝐵
, and every 𝑓∈𝐹(𝐴)
 it holds that 𝐺𝜎(𝛼𝐴(𝑓))=𝛼𝐵(𝐹𝜎(𝑓))
.


In category theory terms, the mapping 𝛼
 is a natural transformation.
Informally, 𝛼𝐴
 tells us how to transform any combinatorial structure of the species 𝐹(𝐴)
 into the one of 𝐺(𝐴)
, so that the transform is consistent with any re-labeling from 𝐴
 to 𝐵
.

Further on, when writing 𝐹=𝐺
 for species 𝐹
 and 𝐺
 we will mean that the species are isomorphic.

Examples
Let 𝑋𝑛={1,2,…,𝑛}
.

Species of subsets is isomorphic to the species of ordered possibly empty partitions into two blocks.
E.g. the subset {1,3,4}
 of 𝑋5
 corresponds to the partition ({1,3,4},{2,5})
.
Species of permutations is isomorphic to the species of sets of cycles.
E.g. the permutation 𝜎=(1 2 3 42 1 4 3)
 corresponds to the set of cycles {(1→2),(3→4)}
.
Note that the species of linear orders 𝐺
 (orderings of 𝐴
) are not isomorphic to the species of permutations 𝐹
, even though |𝐹(𝑋𝑛)|=|𝐺(𝑋𝑛)|=𝑛!
. Consider 𝐹(𝑋2)={(1 21 2),(1 22 1)}
 and 𝐺(𝑋2)={(1,2),(2,1)}
. If we relabel with 𝜎(1)=2
 and 𝜎(2)=1
, elements of 𝐹(𝑋2)
 will not change, while elements of 𝐺(𝑋2)
 will get swapped. Hence, there is no 𝛼
 consistent with such 𝜎
.

Operations on species
The operations defined below are the core ones that are related to generating functions.

This section uses the concept of the disjoint union of two sets. For sets 𝐴
 and 𝐵
, their disjoint union 𝐴⊔𝐵
 is constructed in such way that every element that belongs to both 𝐴
 and 𝐵
 appears twice: once as an element of 𝐴
 and once as an element of 𝐵
. This is unlike the regular union in which elements from the intersection appear once. You can define the disjoint union formally as

𝐴⊔𝐵=𝐴×{∘}∪𝐵×{∙},
where ∘
 marks the elements from 𝐴
 and ∙
 marks the elements from 𝐵
.


The disjoint union of 𝐴
 and 𝐵
 is formed by the union of elements labeled by the set they came from.
The image by Kismalac is distributed under CC BY-SA 3.0 license.
Def. 5. The addition 𝐹+𝐺
 of species 𝐹
 and 𝐺
 is defined as their disjoint union:

(𝐹+𝐺)(𝐴)=𝐹(𝐴)⊔𝐺(𝐴)
Informally, (𝐹+𝐺)
-structure is either an 𝐹
-structure or a 𝐺
-structure.

Def. 6. The cartesian product 𝐹×𝐺
 of species 𝐹
 and 𝐺
 is defined as

(𝐹×𝐺)(𝐴)=𝐹(𝐴)×𝐺(𝐴)
Informally, (𝐹×𝐺)
-structure is a pair of an 𝐹
-structure and a 𝐺
-structure, both constructed on the set 𝐴
.

Def. 7. The ordinary product 𝐹⋅𝐺
 of species 𝐹
 and 𝐺
 is defined as

(𝐹⋅𝐺)(𝐴)=⨆𝐵∪𝐶=𝐴𝐵∩𝐶=∅𝐹(𝐵)×𝐺(𝐶)
Informally, (𝐹⋅𝐺)
-structure on 𝐴
 is a pair of an 𝐹
-structure on 𝐵
 and a 𝐺
-structure on 𝐶
 such that (𝐵,𝐶)
 is an ordered partition of 𝐴
.


𝐴
 is partitioned in 𝐴=𝐵⊔𝐶
, then an 𝐹
-structure is constructed on top of 𝐵
 and a 𝐺
-structure is constructed on top of 𝐶
.
The image by Brent Yorgey is distributed under CC BY-SA 3.0 license.
Def. 8. Let 𝜋
 be an unordered partition of 𝐴
. The composition 𝐹∘𝐺
 of species 𝐹
 and 𝐺
 is defined as

(𝐹∘𝐺)(𝐴)=⨆𝜋(𝐹(𝜋)×∏𝐵∈𝜋𝐺(𝐵))
Here ∏
 denotes the cartesian product over all elements of 𝜋
.

Informally, (𝐹∘𝐺)
-structure on 𝐴
 is an 𝐹
-structure built on top of 𝐺
-structures that are constructed on each element of the partition.


𝐴
 is partitioned by 𝜋
, a 𝐺
-structure is formed on top of each element of 𝜋
, then an 𝐹
-structure is formed on top of 𝜋
.
The image by Brent Yorgey is distributed under CC BY-SA 3.0 license.
Examples
Species of sets and sequences as a sum of simpler species.

The set species can be represented as a sum of all 𝑛
-set species:
𝐸=∑𝑛=0∞𝐸𝑛.
The species 𝐿𝑛
 of linear orders of 𝑛
-sets is isomorphic to 𝑋𝑛=𝐸𝑛1
 and
𝐿=∑𝑛=0∞𝑋𝑛.
The non-empty sets species 𝐸+
 is represented as
𝐸+=∑𝑛=1∞𝐸𝑛.
Neutral elements for operations.

Empty species 0=𝐺
 such that 𝐺(𝐴)=∅
 for any 𝐴
 is the neutral element of the species addition. That is, 0+𝐺=𝐺+0=𝐺
.
Empty set species 1=𝐸0
 is the neutral element of the species ordinary product. That is, 1⋅𝐹=𝐹⋅1=𝐹
.
Singleton species 𝑋=𝐸1
 is the neutral element of the species composition. That is, 𝑋∘𝐹=𝐹∘𝑋=𝐹
.
Species as compositions of simpler species.

𝐸∘𝐺
 is a species of sets of 𝐺
-structures, 𝐸𝑛∘𝐺
 is a species of 𝑛
-sets of 𝐺
-structures;
𝑆=𝐸∘𝐶
: a species of permutations is isomorphic to the species of sets of cycles;
Par=𝐸∘𝐸+
: a species of partitions is a species of sets of non-empty sets;

Function species is a set of cycles of rooted trees
An illustration from Combinatorial Species article by François Bergeron
𝑓=𝑆∘𝑇
: a species 𝑓
 of functions from 𝐴
 to 𝐴
 can be represented as a permutation (set of cycles) of rooted tree species 𝑇
.
Recursively defined species.


An ordered tree may be represented as a root, followed by a sequence of ordered trees.
An illustration from Binomial species and combinatorial exponentiation article by Gilbert Labelle.
A species △
 of ordered trees is described as a tree, in which the children of each node are ordered. In other words, ordered tree is a pair (𝑥,𝐿)
, where 𝑥
 is a node of the tree and 𝐿
 is a possibly empty ordered tuple of its subtrees. Therefore, in terms of species operations,

△=𝑋⋅(1+△+△2+△3+…)=𝑋⋅(𝐿∘△),
where 𝑋=𝐸1
 is a node of the tree and 1=𝐸0
 is the empty sequence.

In the picture above, the set 𝐴={𝑎,𝑏,𝑐,𝑑,𝑒,𝑓,𝑔,ℎ,𝑖,𝑗,𝑘,0,1,2,3,4,5,6,7,8,9}
 is first split in a pair

(𝑋,𝐿(𝑇))=({𝑒},{𝑎,𝑏,𝑐,𝑑,𝑓,𝑔,ℎ,𝑖,𝑗,𝑘,0,1,2,3,4,5,6,7,8,9}),
which corresponds to the ordinary product of singleton species and the sequence of ordered trees. The label 𝑒
 from the first set is given to the root and the set of remaining labels then is given to the "list of trees". Then the later set is partitioned with

𝜋={{𝑎,1,9,5,𝑔,𝑗,𝑐,7},{𝑓,𝑑,0},{𝑏,4,3,2,𝑖,ℎ,6,8,𝑘}}.
At last, another tree species is constructed on each set from 𝜋
 and then the sequence species constructed on 𝜋
 itself to make it ordered, which corresponds to the composition of the sequence species and ordered tree species.

Cardinalities of species operations
Note that the cardinality of 𝐹(𝐴)
 is determined by the cardinality of 𝐴
 (otherwise 𝐹𝜎
 wouldn't be a bijection).

Let 𝑓𝑖=|𝐹(𝐴)|
 for |𝐴|=𝑖
 and 𝑔𝑗=|𝐺(𝐴)|
 for |𝐴|=𝑗
. What can we say about |(𝐹+𝐺)(𝐴)|
 and |(𝐹⋅𝐺)(𝐴)
 for |𝐴|=𝑘
?

For disjoint union, the formula is simple:
|(𝐹+𝐺)(𝐴)|=|𝐹(𝐴)⊔𝐺(𝐴)|=|𝐹(𝐴)|+|𝐺(𝐴)|=𝑓𝑘+𝑔𝑘.
Correspondingly, for the cartesian product:
|(𝐹×𝐺)(𝐴)|=|𝐹(𝐴)×𝐺(𝐴)|=|𝐹(𝐴)|⋅|𝐺(𝐴)|=𝑓𝑘⋅𝑔𝑘.
For the ordinary product:
|(𝐹⋅𝐺)(𝐴)|=∣∣∣∣∣⨆𝐵∪𝐶=𝐴𝐵∩𝐶=∅𝐹(𝐵)×𝐺(𝐶)∣∣∣∣∣=∑𝐵∪𝐶=𝐴𝐵∩𝐶=∅|𝐹(𝐵)|⋅|𝐺(𝐶)|=∑𝑖+𝑗=𝑘(𝑘𝑖)𝑓𝑖𝑔𝑗.
And for the composition (assuming 𝑔0=0
):
|(𝐹∘𝐺)(𝐴)|=∑𝜋|𝐹(𝜋)|∏𝐵∈𝜋|𝐺(𝐵)|=∑𝑖=0∞𝑓𝑖𝑖!∑𝑗1+⋯+𝑗𝑖=𝑘(𝑘𝑗1 𝑗2 … 𝑗𝑖)𝑔𝑗1…𝑔𝑗𝑖.
In the last two formulas we have used the fact that there are (𝑘𝑖)
 ways to partition a set 𝐴
 of size 𝑘
 into sets 𝐵
 and 𝐶
 of sizes 𝑖
 and 𝑘−𝑖
. Similarly for the composition, there are

(𝑘𝑗1 𝑗2 … 𝑗𝑖)=𝑘!𝑗1!…𝑗𝑖!
ways to partition a set into 𝑖
 sets of sizes 𝑗1,…,𝑗𝑖
. The division by 𝑖!
 here is needed to account for partitions being unordered, as every unordered partitions is counted for every permutation of (𝑗1,…,𝑗𝑖)
 and there are 𝑖!
 such permutations.

Generating functions
The idea behind generating functions is to define a class of objects with operations of sum, product and composition that is consistent with a way these operations are defined above for species. The formulas above for the ordinary product and the composition could be rewritten in the following way:

For ℎ𝑘=|(𝐹⋅𝐺)(𝐴)|
 with |𝐴|=𝑘
:
ℎ𝑘𝑘!=∑𝑖+𝑗=𝑘𝑓𝑖𝑖!𝑔𝑗𝑗!.
For ℎ𝑘=|(𝐹∘𝐺)(𝐴)|
 with |𝐴|=𝑘
:
ℎ𝑘𝑘!=∑𝑖=0∞𝑓𝑖𝑖!∑𝑗1+⋯+𝑗𝑖=𝑘𝑔𝑗1𝑗1!…𝑔𝑗𝑖𝑗𝑖!.
These formulas are consistent with the following definition:

Def. 9. The exponential generating function (EGF) of a species 𝐹
 is defined as a formal power series

𝐹(𝑥)=∑𝑘=0∞𝑓𝑘𝑘!𝑥𝑘,
where 𝑓𝑘=|𝐹(𝑋𝑘)|
 is the number of 𝐹
-structures you could build on a set of 𝑘
 elements.

With this definition, we can show that

The addition of species yields the addition of the generating functions:
𝐹(𝑥)+𝐺(𝑥)=∑𝑘=0∞𝑓𝑘+𝑔𝑘𝑘!𝑥𝑘=(𝐹+𝐺)(𝑥).
The cartesian product of species yields something similar to the Hadamard product of the generating functions:
𝐹(𝑥)⋆𝐺(𝑥)=∑𝑘=0∞𝑓𝑘𝑔𝑘𝑘!𝑥𝑘=(𝐹×𝐺)(𝑥).
The ordinary product of species yields the product of the generating functions:
𝐹(𝑥)𝐺(𝑥)=∑𝑘=0∞𝑥𝑘∑𝑖+𝑗=𝑘𝑓𝑖𝑖!𝑔𝑗𝑗!=∑𝑘=0∞ℎ𝑘𝑘!𝑥𝑘=(𝐹⋅𝐺)(𝑥).
The composition of species yields the composition of the generating functions:
𝐹(𝐺(𝑥))=∑𝑖=0∞𝑓𝑖𝑖!𝐺𝑖(𝑥)=∑𝑘=0∞𝑥𝑘∑𝑖=0∞𝑓𝑖𝑖![𝑥𝑘]𝐺𝑖(𝑥)=∑𝑘=0∞𝑥𝑘∑𝑖=0∞𝑓𝑖𝑖!∑𝑗1+⋯+𝑗𝑖=𝑘𝑔𝑗1𝑗1!…𝑔𝑗𝑖𝑗𝑖!=(𝐹∘𝐺)(𝑥).
The last formula is only valid for 𝐺(0)=𝑔0=0
, that is 𝐺(∅)=∅
, as all sets in the partition are non-empty.

Examples
EGF of sets. The EGF of sets species is

𝐸(𝑥)=∑𝑘=0∞𝑥𝑘𝑘!=𝑒𝑥.
The expression above provides the combinatorial meaning to exp𝐹(𝑥)
 and log𝐹(𝑥)
 operations on the generating functions. Specifically, exp𝐹(𝑥)
 is the generating function for the disjoint sets of 𝐹
 structure, while log𝐹(𝑥)
 makes the inverse transform from disjoint sets of 𝐺
 to 𝐺
 itself. The result more commonly known as the exponential formula.

The EGF of 𝑛
-sets species is 𝐸𝑛(𝑥)=𝑥𝑛𝑛!
. In particular, the EGF of a singleton species is 𝑋(𝑥)=𝐸1(𝑥)=𝑥
.

EGF of permutations. The EGF of permutations species is

𝑆(𝑥)=∑𝑘=0∞𝑘!𝑘!𝑥𝑘=∑𝑘=0∞𝑥𝑘=11−𝑥.
Permutations species is isomorphic to sets of cycles, that is 𝑆=𝐸∘𝐶
. It means that

11−𝑥=𝑆(𝑥)=𝐸(𝐶(𝑥))=𝑒𝐶(𝑥),
where 𝐶(𝑥)
 is the generating function of cycles. Therefore,

𝐶(𝑥)=log𝑆(𝑥)=log11−𝑥=∑𝑘=1∞𝑥𝑘𝑘=∑𝑘=1∞(𝑘−1)!𝑘!𝑥𝑘.
Indeed, there are (𝑘−1)!
 ways to make a cycle out of 𝑘
 distinct elements.

EGF of sequences. Species of linear orders is an ordered tuple of 𝑘
 singletons for some 𝑘
, hence

𝐿(𝑥)=∑𝑘=0∞𝐸𝑘1(𝑥)=∑𝑘=0∞𝑥𝑘=11−𝑥.
Another way to get it is to note that 𝐿=𝐸0+𝑋⋅𝐿
, that is any sequence is either empty (corresponds to 𝐸0
) or can be represented as a pair of the first element (corresponds to 𝑋
) and the sequence constructed from remaining elements (corresponds to 𝐿
 in 𝑋⋅𝐿
). This yields an equation 𝐿(𝑥)=1+𝑥𝐿(𝑥)
.

EGF of partitions. The partition is a set of non-empty sets: Par=𝐸∘𝐸+
. Therefore, the EGF of the partition species is

Par(𝑥)=𝑒𝐸+(𝑥)=𝑒𝑒𝑥−1.
EGFs related to Catalan numbers. In the examples below we work with labeled versions of balanced bracket sequences and trees. Informally, it means that each vertex of the tree has a label (e.g. a number from 1
 to 𝑛
) and each pair of brackets in the bracket sequence is also associated with some label.

Changing labels on the same tree/bracket sequence generally leads to a different object with the same underlying structure. For "unlabeled" case see the next section.

Balanced bracket sequence species 𝐵
 is a (possibly empty) sequence (linear order) of balanced bracket sequences, each of them wrapped in a pair of brackets. The generating function for the balanced bracket sequence wrapped in a pair of brackets is 𝑋⋅𝐵
 (as it can be represented as a pair of the outer brackets, perceived as a singleton, and the underlying sequence). Correspondingly, a sequence of wrapped bracket sequences is 𝐿∘(𝑋⋅𝐵)
 and it has a generating function 𝐿(𝑥𝐵(𝑥))
. Therefore:

𝐵(𝑥)=11−𝑥𝐵(𝑥).
Solving the equation above for 𝐵(𝑥)
 yields 𝑥𝐵2(𝑥)−𝐵(𝑥)+1=0
, hence

𝐵(𝑥)=1±1−4𝑥√2𝑥,
which corresponds to the genfunc of the Catalan numbers.

Ordered tree species △
 adheres to the identity △=𝑋⋅(𝐿∘△)
. Thus,

△(𝑥)=𝑥⋅𝐿(△(𝑥))=𝑥⋅11−△(𝑥).
Solving the equation for △(𝑥)
 yields △2(𝑥)−△(𝑥)+𝑥=0
, hence

△(𝑥)=1±1−4𝑥√2,
which is also the generating function for the Catalan numbers.

Functions and rooted trees. Recall the species of functions 𝑓
 and rooted trees 𝑇
 that were introduced in the previous section. Counting rooted trees is not a trivial task, but we know that there are 𝑛𝑛
 functions from a set of size 𝑛
 onto itself. That means that

𝑓(𝑥)=∑𝑘=0∞𝑘𝑘𝑘!𝑥𝑘,
but on the other hand 𝑓=𝑆∘𝑇
, so

𝑓(𝑥)=11−𝑇(𝑥),
from which we get

𝑇(𝑥)=1−1𝑓(𝑥).
Unlabeled species
The techniques and concepts developped above also can be used to deal with unlabeled objects.

Def. 10. Two structures 𝛼,𝛽∈𝐹(𝐴)
 are equivalent if there is a bijection 𝜎:𝐴→𝐴
 such that 𝐹𝜎(𝛼)=𝛽
.

Informally, the structures are equivalent if one is obtainable from another via re-labeling to the same set 𝐴
.

Def. 11. An unlabeled structure is an equivalence class on 𝐹(𝐴)
 under the equivalence relation defined above.

Informally, unlabeled structure is what we get if we "erase" labels in 𝐹(𝐴)
 (e.g. erase labels of vertices in graphs, etc).


On the left and on the right are equivalent trees. In the middle is the representation of their unlabeled structure.
Def. 12. The ordinary generating function (OGF) of a species 𝐹
 is defined as a formal power series

𝐹˜(𝑥)=∑𝑘=0∞𝑓̃ 𝑘𝑥𝑘,
where 𝑓̃ 𝑘
 is the number of unlabeled 𝐹
-structures on 𝑋𝑘
.

One can prove that (𝐹⋅𝐺)˜(𝑥)=𝐹˜(𝑥)𝐺˜(𝑥)
 and (𝐹+𝐺)˜(𝑥)=𝐹˜(𝑥)+𝐺˜(𝑥)
. The ordinary product formula stands, as you do not have to multiply by (𝑘𝑖)
 to account for the distribution of labels between the first and the second sets of the partition.

At the same time, (𝐹∘𝐺)˜(𝑥)
 generally can't be computed just from 𝐹˜(𝑥)
 and 𝐺˜(𝑥)
.

Examples
The OGF of 𝐿
 species is

𝐿˜(𝑥)=∑𝑘=0∞𝑥𝑘=11−𝑥,
as all ordered 𝑛
-tuples are equivalent under re-labeling.

The OGF of 𝑆
 species is

𝑆˜(𝑥)=∏𝑘=1∞11−𝑥𝑘,
which is the generating function of partitions, as permutations are equivalent under re-labeling if and only if they have the same cycle type, and the number of distinct cycle types of permutations of size 𝑛
 amounts for the number of partitions of 𝑛
.

As you see, although 𝐿(𝑥)=𝑆(𝑥)
, at the same time 𝐿˜(𝑥)≠𝑆˜(𝑥)
, which highlights the fact that 𝑆
 and 𝐿
 are not isomorphic species.

Exercises
Composition formula for sequences. Show that (𝐿∘𝐹)˜(𝑥)=𝐿˜(𝐹˜(𝑥))=11−𝐹˜(𝑥)
 for any species 𝐹
.

Counter-example to general composition formula. Find species 𝐴
 and 𝐵
 such that (𝐴∘𝐵)˜(𝑥)≠𝐴˜(𝐵˜(𝑥))
.

General formula for multisets of species. Show that (𝐸∘𝐹)˜(𝑥)=exp(𝐹˜(𝑥)+𝐹˜(𝑥2)2+𝐹˜(𝑥3)3+…)
 for any species 𝐹
.

General formula for sets of species. Note that erasing labels would possibly allow repeated structures in sets. To get a generating functions for unlabeled sets, in which repeated structures are disallowed, one may use the following formula:

PSET𝐹˜(𝑥)=exp(𝐹˜(𝑥)−𝐹˜(𝑥2)2+𝐹˜(𝑥3)3−…).
Here, PSET𝐹˜(𝑥)
 denotes the generating function that enumerates sets of unlabeled 𝐹
-structures, in which structures are not allowed to repeat. In this terms, 𝐸∘𝐹˜
 may also be represented as MSET𝐹˜(𝑥)
, which is computed as

MSET𝐹˜(𝑥)=exp(𝐹˜(𝑥)+𝐹˜(𝑥2)2+𝐹˜(𝑥3)3+…).
Unlike MSET
, it is hard to express PSET
 only in species terms, hence this approach is more commonly used in the symbolic method.

OGF for ordered trees and balanced bracket sequences. Show that 𝐵(𝑥)=𝐵˜(𝑥)
 and △(𝑥)=△˜(𝑥)
 for the ordered trees and the balanced bracket sequences.


All unlabeled ordered trees on 𝑛∈{1,2,3,4}
 vertices.
The structure of the composition on unlabeled structures is a topic of interest in itself and can be found in a more or less general form with the so-called cycle index series, which in turn leads to the Pólya enumeration theorem (which generalizes Burnside's lemma). But to avoid math overload we'll leave this topic for another time.

Further reading
Combinatorial species — English Wikipedia
An Introduction to Combinatorial Species — Ira M. Gessel
Polya enumeration theorem from scratch — Sergey Dovgal
Tags tutorial, combinatorics, generating function

Lagrange inversion theorem

By adamant, history, 18 months ago, In English
Hi everyone!

There is a concept that is sometimes mentioned here and there. The Lagrange inversion theorem.

Let 𝑥=𝑓(𝑦)
. We want to solve this equation for 𝑦
. In other words, we want to find a function 𝑔
 such that 𝑦=𝑔(𝑥)
.

The Lagrange inversion theorem gives an explicit formula for the coefficients of 𝑔(𝑥)
 as a formal power series over 𝕂
:

𝑘[𝑥𝑘]𝑔(𝑥)=[𝑥−1]𝑓−𝑘
In a special case 𝑦=𝑥𝜙(𝑦)
, that is 𝑓(𝑦)=𝑦𝜙(𝑦)
, which is common in combinatorics, it can also be formulated as

𝑘[𝑥𝑘]𝑔(𝑥)=[𝑥𝑘−1]𝜙𝑘
Finally, to avoid division by 𝑘
 one may use (see the comment by Elegia) the following formula:

[𝑥𝑘]𝑔(𝑥)=[𝑥−2]𝑓′𝑓−(𝑘+1),
which may be formulated for 𝑦=𝑥𝜙(𝑦)
 as

[𝑥𝑘]𝑔(𝑥)=[𝑥𝑘−1](𝜙−𝑥𝜙′)𝜙𝑘−1.
Prerequisites
Familiarity with the following:

It is required to have knowledge in the following:

Polynomials, formal power series and generating functions;
Basic notion of analytic functions (e.g. Taylor series);
Basic concepts of graph theory (graphs, trees, etc);
Basic concepts of set theory (describing graphs, trees, etc as sets, tuples, etc).
I mention the concept of fields, but you're not required to know them to understand the article. If you're not familiar with the notion of fields, assume that we're working in real numbers, that is, 𝕂=ℝ
.

It is recommended (but not required) to be familiar with combinatorial species, as they provide lots of intuition to how the equations on generating functions below are constructed.

Motivational examples
In my practice, most obvious use cases of the inversion theorem are related to the enumeration of trees.

Ordered trees
An ordered tree is a rooted tree in which an ordering of children is specified for each vertex.


All ordered trees on 𝑛∈{1,2,3,4}
 vertices.
The task is, given 𝑘
, to find the number of ordered trees on 𝑛
 vertices such that every vertex is either a leaf or has at least 𝑘
 children.

Ordered tree can be described recursively in the following manner:

Tree=(Root,Tree1,Tree2,…,Tree𝑘).
In other words, as a set-theoretic object, an ordered tree can be represented as a (𝑘+1)
-tuple, such that the first element is its root and the next 𝑘
 elements are its subtrees, which are also objects of the ordered tree class.

We can enumerate ordered 𝑛
-tuples of arbitrary objects from class 𝐴
 as 𝐴𝑛(𝑥)
. And to enumerate them for every 𝑛≥𝑘
, we would need to sum up the functions, obtaining the generating function for the ordered sets of ordered trees:

𝐴𝑘(𝑥)+𝐴𝑘+1(𝑥)+𝐴𝑘+2(𝑥)+⋯=𝐴𝑘(𝑥)1−𝐴(𝑥).
Let 𝑇(𝑥)
 be the generating function of ordered trees. Then it might be represented as

𝑇(𝑥)=𝑥⋅(1+𝑇𝑘1−𝑇).
For 𝑘=1
, the solution is generated by

𝑇(𝑥)=1−1−4𝑥√2,
and for 𝑘=2
, the solution is generated by

𝑇(𝑥)=12[1−1−3𝑥1+𝑥‾‾‾‾‾√].
But as 𝑘
 grows, the explicit formula for 𝑇(𝑥)
 gets more and more complicated (see the solution Wolfram found for 𝑘=4
).

Alternate approach is to express 𝑥=𝑓(𝑇)
. Then the Lagrange inversion theorem would still allow us to compute coefficients.

𝑘
-ary trees
Another task of interest is to enumerate the 𝑘
-ary trees.


A ternary tree
A 𝑘
-ary tree is a tree in which every vertex is either a leaf or has exactly 𝑘
 ordered children.

The generating function for 𝑘
-ary trees adheres to the identity 𝑇(𝑥)=𝑥⋅(1+𝑇𝑘)
.

Labeled trees
Yet another task is to enumerate labeled rooted trees. While an ordered tree is represented as a pair of the root vertex and the list of subtrees, unordered tree may be represented as a pair of the root vertex and the set of subtrees. In other words, the generating function of the rooted trees adheres to the equation 𝑇(𝑥)=𝑥⋅𝑒𝑇(𝑥)
.

The later follows from the exponential formula which states that the generating function for the class of sets of 𝑇
 is 𝑒𝑇(𝑥)
.

Note that this equation only works for labeled trees, and their exponential generating function:

𝑇(𝑥)=∑𝑘=0∞𝑎𝑘𝑘!𝑥𝑘,
where 𝑎𝑘
 is the number of labeled trees.

Inversion theorem
Let 𝑥=𝑓(𝑦)
. We want to solve this equation for 𝑦
. In other words, we want to find a function 𝑔
 such that 𝑦=𝑔(𝑥)
.

This means that 𝑥=𝑓(𝑔(𝑥))
 and 𝑦=𝑔(𝑓(𝑦))
, so what we really want to find here is the functional inverse of 𝑓
.

As it stands with generating functions, we're more interested in finding some expression for 𝑔(𝑥)
 as a formal power series, rather than closed elementary form (which may not exist). In doing so, we may use the Lagrange inversion theorem.

Formulation
Let 𝑓,𝑔
 be formal power series, such that 𝑓(𝑔(𝑥))=𝑥
 and 𝑓(0)=𝑔(0)=0
, then

𝑘[𝑥𝑘]𝑔𝑛(𝑥)=𝑛[𝑥−𝑛]𝑓−𝑘(𝑥)
where [𝑥𝑘]𝑓(𝑥)
 denotes the coefficient near 𝑥𝑘
 in 𝑓(𝑥)
. In particular the 𝑔(𝑥)
 itself is found as

𝑘[𝑥𝑘]𝑔(𝑥)=[𝑥−1]𝑓−𝑘(𝑥)
𝑥−𝑛
 in the formulas above expects negative power of 𝑥
 and 𝑓−𝑘
 expects the existence of the multiplicative inverse for arbitrary formal power series 𝑓(𝑥)
, which is problematic. To mitigate this, we expand the domain of functions we're working with to formal Laurent series.

Def. 1. The set of formal Laurent series 𝕂((𝑥))
 over 𝕂
 is defined as

𝕂((𝑥))={∑𝑘=−𝑁∞𝑎𝑘𝑥𝑘∣∣∣𝑁∈ℤ,𝑎𝑘∈𝕂}.
In other words, formal Laurent series is a formal power series, which is also allowed to have finitely many terms 𝑥𝑘
 with 𝑘<0
.

When 𝕂
 is a field, 𝕂((𝑥))
 also form up a field. In particular, it means that for a formal Laurent series 𝐴(𝑥)≠0
 there always is a multiplicative inverse 𝐴−1(𝑥)
 such that 𝐴(𝑥)𝐴−1(𝑥)=1
.

Explanation
Now that the formula is properly formulated, you may find the detailed proof below:

Proof
Note. Using the same reasoning, we can deduce it in even more general form for any analytic function ℎ(𝑥)
:

𝑘[𝑥𝑘]ℎ(𝑔(𝑥))=[𝑥−1]ℎ′(𝑥)𝑓𝑘(𝑥)
In combinatorics
Quite often the equation that you deal with in combinatorics is formulated as 𝑦=𝑥𝜙(𝑦)
.

It is possible to get a simpler expression for such cases.

Equivalently, 𝑦=𝑥𝜙(𝑦)
 rewrites as

𝑥=𝑓(𝑦)=𝑦𝜙(𝑦).
Since

[𝑥−1]𝑓(𝑥)=[𝑥𝑘−1]𝜙𝑘(𝑥),
we have a formula for 𝑦=𝑔(𝑥)
:

𝑘[𝑥𝑘]𝑔(𝑥)=[𝑥𝑘−1]𝜙𝑘(𝑥)
It is especially convenient due to the fact that we don't need to actually deal with Laurent series.

More general result for this case is known as the Lagrange–Bürmann formula:

𝑘[𝑥𝑘]ℎ(𝑔(𝑥))=[𝑥𝑘−1]ℎ′(𝑥)𝜙𝑘(𝑥)
for any analytic function ℎ(𝑥)
.

Cracking up the examples
Ordered trees
The equation is

𝑇=𝑥⋅(1+𝑇𝑘1−𝑇).
It means that 𝑛[𝑥𝑛]𝑇(𝑥)=[𝑇𝑛−1]𝜙𝑛(𝑇)
, where 𝜙(𝑇)=1+𝑇𝑘1−𝑇
. To compute it, we use the binomial formula:

[𝑇𝑛−1](1+𝑇𝑘1−𝑇)𝑛=∑𝑖=0𝑛(𝑛𝑖)[𝑇𝑛−1]𝑇𝑘𝑖(1−𝑇)𝑖.
Note that [𝑇𝑎]𝑇𝑏𝑓(𝑇)=[𝑇𝑎−𝑏]𝑓(𝑇)
. And, knowing the expansion

1(1−𝑥)𝑖=1(𝑖−1)!∂𝑖−1∂𝑥𝑖−1(1−𝑥)−1=∑𝑘=0∞(𝑘+1)…(𝑘+𝑖−1)(𝑖−1)!𝑥𝑘=∑𝑘=0∞(𝑘+𝑖−1𝑖−1)𝑥𝑘,
we may rewrite it as

[𝑥𝑛]𝑇=1𝑛[𝑇𝑛−1]𝜙𝑛(𝑇)=1𝑛∑𝑖=0𝑛(𝑛𝑖)[𝑇𝑛−𝑘𝑖−1]1(1−𝑇)𝑖=1𝑛∑𝑖=0𝑛(𝑛𝑖)(𝑛−(𝑘−1)𝑖−2𝑖−1).
Indeed, computing first few terms for different 𝑘
 we see that

𝑘=1
 yields Catalan numbers,
𝑘=2
 yields Riordan numbers,
𝑘=3
 yields the sequence A114997,
𝑘=4
 yields the sequence A215341.
Further 𝑘
 are seemingly not present on OEIS.

𝑘
-ary trees
The equation is 𝑇=𝑥⋅(1+𝑇𝑘)
. So, for this case 𝜙(𝑇)=1+𝑇𝑘
 and we need to find

[𝑇𝑛−1]𝜙𝑛(𝑇)=[𝑇𝑛−1](1+𝑇𝑘)𝑛=[𝑇𝑛−1]∑𝑖=0𝑛(𝑛𝑖)𝑇𝑘𝑖
The expression on the right only has coefficient near 𝑇𝑛−1
 when 𝑘
 divides 𝑛−1
, in which cases the number of trees is

[𝑥𝑘𝑡+1]𝑇(𝑥)=1𝑘𝑡+1(𝑘𝑡+1𝑡)
for 𝑛=𝑘𝑡+1
. Let's look on the generated sequences for different 𝑘
:

𝑘=1
 yields 1𝑡+1(𝑡+1𝑡)=1
. Indeed, there is only one 1
-ary tree (bamboo) for each possible number of vertices;
𝑘=2
 yields 12𝑡+1(2𝑡+1𝑡)
. This is actually the 𝑡
-th Catalan number and another neat formula for them!
𝑘=3
 yields the number of ternary trees (A001764);
𝑘=4
 yields the number of quartic trees (A002293).
Labeled trees
The equation is 𝑇=𝑥𝑒𝑇
. Let's find the coefficients with 𝜙(𝑇)=𝑒𝑇
:

[𝑇𝑛−1]𝜙𝑛(𝑇)=[𝑇𝑛−1]𝑒𝑛𝑇=[𝑇𝑛−1]∑𝑘=0∞𝑛𝑘𝑇𝑘𝑘!.
From this follows that

[𝑥𝑛]𝑇(𝑥)=1𝑛𝑛𝑛−1(𝑛−1)!=𝑛𝑛−1𝑛!.
Note that we were specifically considering the exponential generating functions, thus the actual number of trees is 𝑛![𝑥𝑛]𝑇(𝑥)=𝑛𝑛−1
.

The function 𝑇(𝑥)
 can be expressed as 𝑇(𝑥)=−𝑊(−𝑥)
, where 𝑊(𝑥)
 is the Lambert W function (see e.g. here).

Tags tutorial, generating function, lagrange inversion

Partitions and pentagonal number theorem

By adamant, history, 18 months ago, In English
Hi everyone!

In today's blog I'd like to write about the theorem that allows to efficiently solve the following problems:

What is the number of partitions of a non-negative number 𝑛
 for 𝑛≤105
?
What is the number of partitions of a non-negative number 𝑛
 for 𝑛≤105
, so that no summand repeats 𝑑
 or more times?
What is the number of partitions of a non-negative number 𝑛
 for 𝑛≤105
, so that no summand is divisible by 𝑑
?
HackerEarth — Perfect Permutations: What is the number of permutations of length 𝑛
 with 𝑘
 inversions for 𝑘≤𝑛≤105
?
102354E - Decimal Expansion: Let 𝜙=910⋅99100⋅9991000⋅…
, what is the 𝑘
-th digit of 𝜙
 for 𝑘≤1018
?
SNSS-2018 Round 5 — Investment Portfolio: You have 2𝑛
 dollars and there are 𝑛
 types of products in the market. There are 𝑎𝑖
 instances of the 𝑖
-th product, costing 𝑖
 dollars each, such that 𝑎𝑖>𝑎𝑗
 when 𝑖>𝑗
. Besides that, there are also 𝑚
 individual offers, 𝑗
-th of them costing 𝑏𝑗
. How many ways are there to spend all 2𝑛
 dollars, while taking exactly one individual offer for 𝑛,𝑚≤105
?
The core fact that allows us to solve these problems efficiently is the pentagonal number theorem:

∏𝑘=1∞(1−𝑥𝑘)=1+∑𝑘=1∞(−1)𝑘(𝑥𝑘(3𝑘+1)2+𝑥𝑘(3𝑘−1)2)
Prerequisites
Familiarity with generating functions, formal power series and operations on them.
Basic notion of combinatorics.
Basic notion of number theory.
Motivational examples
General partitions
Let 𝑛
 be a non-negative number. A partition is a way of writing it as a sum of positive integers. For example,

4=4=3+1=2+2=2+1+1=1+1+1+1.
Let 𝑝(𝑛)
 be the partition function meaning that 𝑝(𝑛)
 is the number of partitions of 𝑛
. Let 𝑃(𝑥)
 be such that

𝑃(𝑥)=∑𝑛=0∞𝑝(𝑛)𝑥𝑛,
that is 𝑃(𝑥)
 is the ordinary generating function of the partition sequence. It can be expressed as an infinite product

𝑃(𝑥)=(1+𝑥+𝑥2+𝑥3+…)(1+𝑥2+𝑥4+𝑥6+…)(1+𝑥3+𝑥6+𝑥9+…)…
Indeed, after expanding the expression into a sum of monomials, 𝑘
-th multiplier will contribute with 𝑥𝑘𝑡
 for some 𝑡≥0
, which may be interpreted as 𝑡
 summands in the partition that are equal to 𝑘
. Than the contributions are summed up to

1⋅𝑡1+2⋅𝑡2+3⋅𝑡3+⋯=𝑛,
making one of possible partitions of the number 𝑛
.

On the other hand, 1+𝑥𝑘+𝑥2𝑘+𝑥3𝑘+⋯=11−𝑥𝑘
 is the sum of geometric progression, so 𝑃(𝑥)
 may be expressed as

𝑃(𝑥)=11−𝑥11−𝑥211−𝑥3⋯=∏𝑘=1∞11−𝑥𝑘.
Conveniently, if we only need to compute the number of partitions of numbers up to 𝑛
, we may consider only first 𝑛
 multipliers discarding the remaining part of the infinite product. Now, to compute it, we may find the multiplicative inverse of the denominator after computing it. But how to compute the denominator? There are several approaches.

Square root decomposition
The solution below is borrowed from ABalobanov's comment

Solution
Exp and Log of power series
Solution
Although this approach is a bit more complicated than the one with the pentagonal number theorem, it generalizes to compute

(1−𝑥)(1−𝑥2)…(1−𝑥𝑛)(mod𝑥𝑘),
or even

(1−𝑥𝑎1)(1−𝑥𝑎2)…(1−𝑥𝑎𝑛)(mod𝑥𝑘)
for arbitrary 𝑘
 in 𝑂(𝑘log𝑘)
, including the case 𝑘>𝑛
, for which pentagonal number theorem fails if applied directly.

Pentagonal number theorem
Solution
Pentagonal number theorem allows to compute the number of partitions faster than 𝑂(𝑛𝑛√)
, while also not involving complicated operations like computing polynomial logarithms and exponents.

Note that the pentagonal number theorem also allows for a simpler 𝑂(𝑛𝑛√)
 solution, using the recurrence

𝑝(𝑛)=∑𝑘=1…(−1)𝑘−1[𝑝(𝑛−𝑘(3𝑘+1)2)+𝑝(𝑛−𝑘(3𝑘−1)2)],
as only first 𝑂(𝑛√)
 values of 𝑘
 are of interest for it.

Distinct partitions, odd partitions
What if we additionally ask the partition to only have distinct summands? Or only odd summands?

Nore generally, that each summand appears less than 𝑑
 times? That each summand is not divisible by 𝑑
?

Solution
The result shown in the solution is known as Glaisher's theorem.

102354E - Decimal Expansion
Find the 𝑘
-th (𝑘≤1018
) digit of the decimal expansion of the constant

𝜙=910991009991000⋯=∏𝑘=1∞(1−10−𝑘)
Solution
Enumerating permutations by inversions
What is the number of permutations of length 𝑛
 with 𝑘
 inversions for 𝑘≤min(𝑛,105)
 and 𝑛≤109
?

Solution
SNSS-2018 Round 5 — Investment Portfolio
You have 2𝑛
 dollars and there are 𝑛
 types of products in the market. There are 𝑎𝑖
 instances of the 𝑖
-th product, costing 𝑖
 dollars each, such that 𝑎𝑖>𝑎𝑗
 when 𝑖>𝑗
. Besides that, there are also 𝑚
 individual offers, 𝑗
-th of them costing 𝑏𝑗
. How many ways are there to spend all 2𝑛
 dollars, while taking exactly one individual offer for 𝑛,𝑚≤105
?

Solution
Explanation of the theorem
The expansion of (1−𝑥)(1−𝑥2)(1−𝑥3)…
 was first discovered by Euler. He derived the formula

𝑃(𝑥)=∏𝑘=1∞11−𝑥𝑘
and then tried to expand the denominator. As a result, he saw that

(1−𝑥)(1−𝑥2)(1−𝑥3)⋯=1−𝑥−𝑥2+𝑥5+𝑥7−𝑥12−𝑥15+𝑥22+𝑥26−𝑥35−𝑥40+…
First mention of the theorem was in Euler's letter to Bernoulli in 1741. Although he mentioned the formula several more times in his correspondence, only in 1750 Euler derived a complete proof to it, as follows from his letter to Goldbach.

Franklin's proof
In modern time, most common proof to be taught is due to Hungarian born American mathematician Fabian Franklin, first published in 1881. The proof inteprets the series (1−𝑥)(1−𝑥2)(1−𝑥3)…
 as a generating function for strict partitions (i.e. partitions in which all summands are distinct), such that every partition is weighted by (−1)𝑘
, where 𝑘
 is the number of partition elements. In other words,

(1−𝑥)(1−𝑥2)(1−𝑥3)⋯=∑𝑘=0∞[𝑑𝑒(𝑘)−𝑑𝑜(𝑘)]𝑥𝑘,
where 𝑑𝑒(𝑘)
 and 𝑑𝑜(𝑘)
 are the numbers of partitions of 𝑘
 into distinct summands such that the amount of summands is even for 𝑑𝑒(𝑘)
 and odd for 𝑑𝑜(𝑘)
. As it turns out, 𝑑𝑒(𝑘)=𝑑𝑜(𝑘)
 for vast majority of numbers 𝑘
.

Franklin discovered that in most times it is possible to pair up partitions with odd number of summands and partitions with even number of summands. To understand how the pairing is obtained, we may represent a partition as a pattern of dots (Ferrers diagram).

∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙⟺∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙
Franklin's idea was, considering the last row and the last diagonal, to either move the last diagonal below the last row, or to move the last row after the last diagonal. In this way, 7+6+4+3
 is paired with 6+5+4+3+2
.

The pairing works most of the times. Except when last row and last diagonal share a dot and the size of the last row is either same as the size of the last diagonal or larger than it by exactly 1
:

∙∙∙∙∙∙∙∙∙∙∙∙ ∙∙∙∙∙∙∙∙∙∙∙∙∙∙∙
In the first case, moving the diagonal or the row would make an invalid diagram, as the last row would be of larger size than the one above it. In the second case, moving diagonal would make a valid partition, but last two elements of the partition would both be equal (in the picture above to the number 3
), meaning that the partition is not strict, as needed for the bijection.

Let 𝑘
 be the number of summands in the partition. The first case yields a sum of

𝑘2+𝑘(𝑘−1)2=𝑘(3𝑘−1)2,
and the second case yields a sum of

𝑘2+𝑘(𝑘+1)2=𝑘(3𝑘+1)2,
which proves the theorem:

∏𝑘=1∞(1−𝑥𝑘)=1+∑𝑘=1∞(−1)𝑘(𝑥𝑘(3𝑘+1)2+𝑥𝑘(3𝑘−1)2)
Why pentagonal?
Because the numbers appear in a construction of dotted pentagons, which is similar to triangular numbers and square numbers.

Further reading
English Wikipedia — Pentagonal number theorem
Dick Koch — The Pentagonal Number Theorem and All That
Tags tutorial, generating function, pentagonal numbers, euler, partitions

A bit more of general ideas

By adamant, history, 18 months ago, In English
Hi everyone!

Here's another collection of little tricks and general ideas that might make your life better (or maybe worse).

Most of them are somewhat well-known, but perhaps would still be useful to someone.

1. Evaluating polynomial modulo small prime 𝑝
. Given a polynomial 𝑞(𝑥)
, it may be evaluated in all possible 𝑎∈ℤ𝑝
 in 𝑂(𝑝log𝑝)
. To do this, compute 𝑞(0)
 separately and use chirp Z-transform to compute 𝑞(𝑔0),𝑞(𝑔1),…,𝑞(𝑔𝑝−2)
, where 𝑔
 is a primitive root modulo 𝑝
.

This method can be used to solve 1054H - Epic Convolution.

2. Generalized Euler theorem. Let 𝑎
 be a number, not necessarily co-prime with 𝑚
, and 𝑘>log2𝑚
. Then

𝑎𝑘≡𝑎𝜙(𝑚)+𝑘mod𝜙(𝑚)(mod𝑚),
where 𝜙(𝑚)
 is Euler's totient. This follows from the Chinese remainder theorem, as it trivially holds for 𝑚=𝑝𝑑
.

This fact can be used in 906D - Power Tower.

3. Range add/range sum in 2D. Fenwick tree, generally, allows for range sum/point add queries.

Let 𝑠𝑥𝑦
 be a sum on [1,𝑥]×[1,𝑦]
. If we add 𝑐
 on [𝑎,+∞)×[𝑏,+∞)
, the sum 𝑠𝑥𝑦
 would change as

𝑠𝑥𝑦↦𝑠𝑥𝑦+(𝑥−𝑎+1)(𝑦−𝑏+1)𝑐,
for 𝑥≥𝑎
 and 𝑦≥𝑏
. To track these changes, we may represent 𝑠𝑥𝑦
 as

𝑠𝑥𝑦=𝑠(0)𝑥𝑦+𝑥⋅𝑠(𝑥)𝑥𝑦+𝑦⋅𝑠(𝑦)𝑥𝑦+𝑥𝑦⋅𝑠(𝑥𝑦)𝑥𝑦,
which allows us to split the addition of 𝑐
 on [𝑎,+∞)×[𝑏,+∞)
 into additions in (𝑎;𝑏)
:

𝑠(0)𝑥𝑦𝑠(𝑥)𝑥𝑦𝑠(𝑦)𝑥𝑦𝑠(𝑥𝑦)𝑥𝑦↦𝑠(0)𝑥𝑦+(𝑎−1)(𝑏−1)𝑐,↦𝑠(𝑥)𝑥𝑦−(𝑏−1)𝑐,↦𝑠(𝑦)𝑥𝑦−(𝑎−1)𝑐,↦𝑠(𝑥𝑦)𝑥𝑦+𝑐.
code
The solution generalizes 1-dimensional Fenwick tree range updates idea from Petr blog from 2013.

You can check your implementation on eolymp — Чипполино.

4. DP on convex subsets. You want to compute something related to convex subsets of a given set of points in 2D space.


You sort points over bottom-left point 𝑂
, then over point 𝐵
 and go through all pairs (𝐴,𝐶)
 with two pointers
This can be done with dynamic programming, which generally goes as follows:

Iterate over possible bottom left point 𝑂
 of the convex subset;
Ignore points below it and sort points above it by angle that they form with 𝑂
;
Iterate over possible point 𝐵
 to be the "last" in the convex subset. It may only be preceded by a point that was sorted before it and succeeded by a points that was sorted after it when the points were sorted around 𝑂
;
Sort considered points around 𝐵
, separately in "yellow" and "green" areas (see picture);
Iterate over possible point 𝐶
 which will succeed 𝐵
 in the convex subset;
Set of points that may precede 𝐵
 with a next point 𝐶
 form a contiguous prefix of points before 𝐵
;
The second pointer 𝐴
 to the end of the prefix is maintained;
Eventually, for every 𝐵
, all valid pairs of 𝐴
 and 𝐶
 are iterated with two pointers.
This allows to consider in 𝑂(𝑛3)
 all the convex subsets of a given set of points, assuming that sorting around every point 𝐵
 was computed beforehand in 𝑂(𝑛2log𝑛)
 and is now used to avoid actual second sorting of points around 𝐵
.

The method may probably be used to solve AtCoder — ConvexScore.

5. Subset sum on segments. Given 𝑎1,…,𝑎𝑛
, answer 𝑞
 queries. Each query is whether 𝑎𝑙,𝑎𝑙+1,…,𝑎𝑟
 has a subset of sum 𝑤
. This can be done with dynamic programming 𝐿[𝑟][𝑤]
 being the right-most 𝑙
 such that 𝑎𝑙,…,𝑎𝑟
 has a subset with sum 𝑤
:

𝐿[𝑟][𝑤]=max(𝐿[𝑟−1][𝑤],𝐿[𝑟−1][𝑤−𝑎𝑟]).
This allows to solve the problem in 𝑂(𝑛𝑤+𝑞)
.

Unfortunately, I forgot the original problem on which I saw this approach.

6. Data structure with co-primality info. There is a structure that supports following queries:

Add/remove element 𝑥
 from the set, all prime divisors of 𝑥
 are known;
Count the number of elements in the structure that are co-prime with 𝑥
.
Without loss of generality, we may assume that the numbers are square-free.

Let 𝑤(𝑥)
 be the number of distinct prime divisors of 𝑥
 and 𝑁𝑥
 be the amount of numbers divisible by 𝑥
 in the structure. When 𝑥
 is added or removed from the structure, you need to update 2𝑤(𝑥)
 values of 𝑁𝑥
. Now, having 𝑁𝑥
, how to count numbers co-prime with 𝑥
?

𝐴𝑥=∑𝑑|𝑥(−1)𝑤(𝑑)𝑁𝑑=∑𝑑|𝑥𝜇(𝑑)𝑁𝑑,
where 𝜇(𝑑)
 is the Möbius function. This formula essentially uses inclusion-exclusion principle, as 𝑁𝑑
 counts numbers divisible by 𝑑
 and we need to count numbers that are not divisible by any divisor of 𝑥
.

The method was used in 102354B - Yet Another Convolution.

7. Generalized inclusion-exclusion. Let 𝐴1,…,𝐴𝑛
 be some subsets of a larger set 𝑆
. Let 𝐴𝑖⎯⎯⎯⎯⎯⎯=𝑆∖𝐴𝑖
.

With the inclusion-exclusion principle, we count the number of points from 𝑆
 that lie in neither of 𝐴𝑖
:

∣∣∣∣⋂𝑖=1𝑛𝐴𝑖⎯⎯⎯⎯⎯⎯∣∣∣∣=∑𝑚=0𝑛(−1)𝑚∑|𝑋|=𝑚∣∣∣∣⋂𝑖∈𝑋𝐴𝑖∣∣∣∣,
assuming the empty intersection to be the full set 𝑆
. We may split the formula half-way as

∣∣∣∣⋃|𝑌|=𝑟(⋂𝑖∈𝑌𝐴𝑖⋂𝑗∈𝑌𝐴𝑗⎯⎯⎯⎯⎯⎯)∣∣∣∣=∑𝑚=𝑟𝑛(−1)𝑚−𝑟(𝑚𝑟)∑|𝑋|=𝑚∣∣∣∣⋂𝑖∈𝑋𝐴𝑖∣∣∣∣.
This way, we're able to count the number of points from 𝑆
 that lie in exactly 𝑟
 set among 𝐴1,…,𝐴𝑛
.

Explanation lies in the fact that for a fixed 𝑌
, we may use PIE directly:

∣∣∣∣⋂𝑖∈𝑌𝐴𝑖⋂𝑗∈𝑌𝐴𝑗⎯⎯⎯⎯⎯⎯∣∣∣∣=∑𝑚=𝑟𝑛(−1)𝑚−𝑟∑|𝑋|=𝑚𝑌⊂𝑋∣∣∣∣⋂𝑖∈𝑋𝐴𝑖∣∣∣∣,
then if summing up over all possible 𝑌
, each set 𝑋
 will always have (−1)𝑚−𝑟
 coefficient and will occur for (𝑚𝑟)
 sets 𝑌
.

8. Finding roots of polynomials over ℤ𝑝
. You're given 𝑞(𝑥)
. You want to find all 𝑎∈ℤ𝑝
, such that 𝑞(𝑎)=0
.

This is done in two steps. First, you compute

ℎ(𝑥)=gcd(𝑞(𝑥),𝑥𝑝−𝑥)
to get rid of non-linear or repeated linear factors of 𝑞(𝑥)
, as

𝑥𝑝−𝑥≡∏𝑎=0𝑝−1(𝑥−𝑎)(mod𝑝).
Second, you pick random 𝑎
 and compute

gcd(ℎ(𝑥),(𝑥+𝑎)𝑝−12−1).
This will filter roots of ℎ(𝑥)
 by whether they're quadratic residues if 𝑎
 is added to them or not.

Quadratic residues make up 𝑝−12
 of numbers in ℤ𝑝
 and are distributed uniformly, so you'll have at least 12
 chance to get non-trivial divisor of ℎ(𝑥)
. This is particularly useful when you want to solve e.g. 𝑥2≡𝑎(mod𝑝)
, which can be done in 𝑂(log𝑝)
 with this algorithm:

code
Generally, the probability of getting a divisor of ℎ(𝑥)
 of degree 𝑘
 for degℎ=𝑛
 can be expressed as 2−𝑛(𝑛𝑘)
, thus on average this method nearly halves the degree of ℎ(𝑥)
 in a single iteration. From this follows that the expected complexity of the algorithm is 𝑂(𝑛2log𝑝)
 if naive multiplication is used or 𝑂(𝑛log2𝑛log𝑛𝑝)
 if one uses FFT-based multiplication and half-GCD.

The method is called Berlekamp–Rabin algorithm and can be generalized to find all factors of 𝑞(𝑥)
 over ℤ𝑝
 (see this comment).

You can check your implementation on Library Judge — Sqrt Mod.

9. Matching divisible by 𝑚
. You're given a weighted bipartite graph and you need to check if there exists a perfect matching that sums up to the number that is divisible by 𝑚
. In other words, whether there exists a permutation 𝜎1,…,𝜎𝑛
 such that

𝐴1𝜎1+⋯+𝐴𝑛𝜎𝑛≡0(mod𝑚).
For this, we introduce matrices 𝑅(0),…,𝑅(𝑚−1)
 such that

𝑅(𝑘)𝑖𝑗=𝑥𝑖𝑗𝜔𝑘𝐴𝑖𝑗,
where 𝐴𝑖𝑗
 is weight between 𝑖
 in the first part and 𝑗
 in the second part, 𝑥𝑖𝑗
 is a random number when there is an edge between 𝑖
 and 𝑗
 or 0
 otherwise, and 𝜔
 is a root of unity of degree 𝑚
. The determinants of such matrices is then

det𝑅(𝑘)=∑𝜎∈𝑆𝑛((−1)𝑁(𝜎)∏𝑖=1𝑛𝑥𝑖𝜎𝑖)(𝜔𝑘)∑𝑖=1𝑛𝐴𝑖𝜎𝑖,
where 𝑁(𝜎)
 is a parity of 𝜎
. If you sum them up, you get

∑𝑘=0𝑚−1det𝑅(𝑘)=∑𝜎∈𝑆𝑛((−1)𝑁(𝜎)∏𝑖=1𝑛𝑥𝑖𝜎𝑖)∑𝑘=0𝑚−1(𝜔𝑘)∑𝑖=1𝑛𝐴𝑖𝜎𝑖.
But at the same time,

∑𝑘=0𝑚−1𝜔𝑘𝑥={𝑚0,𝑥≡0(mod𝑚),,𝑥≢0(mod𝑚).
Thus, a summand near 𝜎1,…,𝜎𝑛
 will be non-zero only if 𝐴1𝜎1+⋯+𝐴𝑛𝜎𝑛
 sums up to the number divisible by 𝑚
.

Therefore, the property can be checked in 𝑂(𝑚𝑛3)
.

The method was used in CSAcademy — Divisible Matching.

10. Eigenvectors of circulant matrix. Let 𝐴
 be a matrix such that each of its rows is a cyclic shift of the previous one (see circulant matrix). Let the first column be 𝑎0,…,𝑎𝑛−1
 and 𝐴(𝑥)=𝑎0+𝑎1𝑥+⋯+𝑎𝑛−1𝑥𝑛−1
. Then the eigenvalues of 𝐴
 are

𝐴(1),𝐴(𝜔),…,𝐴(𝜔𝑛−1),
where 𝜔
 is an 𝑛
-th root of unity. Correspondingly, 𝑘
-th eigenvector is of form

(1𝜔𝑘𝜔2𝑘…𝜔𝑘(𝑛−1))⊤.
In particular it means that the determinant of such matrix is

det𝐴=𝐴(1)𝐴(𝜔)…𝐴(𝜔𝑛−1)
and multiplication by its inverse may be found with pointwise division after DFT of degree 𝑛
.

These facts may be used to solve 102129G - Permutant and 901E - Cyclic Cipher.

11. Knapsack with repetitions. You have 𝑛
 item types, there are 𝑎𝑖
 items of type 𝑖
, having weight 𝑏𝑖
 and cost 𝑐𝑖
. What is the maximum cost you may get with having total weight at most 𝑤
? This is solvable in 𝑂(𝑛𝑤)
. The transition formula looks like

𝑑[𝑖][𝑤]=max𝑡=0𝑎𝑖(𝑑[𝑖−1][𝑤−𝑡⋅𝑏𝑖]+𝑐𝑖⋅𝑡)
To compute it quickly enough, you should divide 𝑑[𝑖−1]
 into groups having the same remainder modulo 𝑏𝑖
, after which the maximum is taken on contiguous segments of the same width rather than with steps of 𝑏𝑖
, and can be computed with monotonic queue.

12. Reverses and palindromes. Given strings 𝑆
 and 𝑇
, is it possible to reverse some non-intersecting substrings of 𝑆
 to obtain 𝑇
?

In other words, we need to check if 𝑆
 may be represented as

𝑆=𝐴0𝐵1𝐴1𝐵2𝐴2…𝐵𝑛𝐴𝑛,
such that

𝑇=𝐴0𝐵⊤1𝐴1𝐵⊤2…𝐵⊤𝑛𝐴𝑛,
where 𝐵⊤
 is reversed string 𝐵
. To check this, one may use operation mix(𝑆,𝑇)
 such that

mix(𝑠1𝑠2…𝑠𝑛,𝑡1𝑡2…𝑡𝑛)=𝑠1𝑡1𝑠2𝑡2…𝑠𝑛𝑡𝑛.
Key fact here is that mix(𝐴,𝐴⊤)
 gives a palindrome of even length and is invertible operation.

Correspondingly, mix(𝐴,𝐴)
 may be perceived as a concatenation of |𝐴|
 palindromes of length 2
.

That being said, checking that 𝑇
 is obtained from 𝑆
 by reversing some of its substrings is equivalent to checking whether mix(𝑆,𝑇)
 can be split in palindromes of even length, which is doable in 𝑂(𝑛log𝑛)
 with palindromic tree.

This method was used in 906E - Reverses.

Tags tutorial, i love tags

Duality in linear programming. Part 1 — definition and construction

By adamant, history, 17 months ago, In English
Hi everyone!

Previously I wrote about theoretical grounds of "aliens trick". Specifically, I introduced the concept of the Lagrangian duality and explained its connection with the trick. Today I want to elaborate a bit more on the concept of dual problems and their applications to linear programming as well as to common problems in competitive programming.

I initially wanted to also write about some applications in competitive programming problems, but given that the introduction is already quite lengthy, I decided to leave it for another blog, while using most common and well-known theoretical examples here, focusing more on how to construct and interpret dual problems to begin with, rather than how to use it in contests.

I think, it is a crucial first step towards using the duality in actual programming challenges.

Prerequisites
It is highly recommended to have some general understanding of basic mathematical optimization concepts (e.g. convexity, Lagrange multipliers), as well as basic understanding of flow algorithms and most well-known results around them, such as cycle-path decomposition of flows, maximum flow / minimum cut duality, minimum cost flows (ideally, the algorithm that computes it using Dijkstra with potentials). The article should hopefully shed some further light on these topics.

General Lagrangian duality
I wrote a long and elaborate story about general Lagrangian duality. It should provide you with additional insights and interpretation on what's going on if you, like me, like to start learning new concepts from abstract nonsense rather than specific examples. However according to some feedback such approach would rather scare anyone else, so I put the scary stuff under the spoiler tag.

Long story about general duality
Among all the stuff above, I'll just briefly mention the most important things. For an optimization problem

min𝑥∈𝑋𝑠.𝑡.𝑓(𝑥)𝑔(𝑥)≥0,
where 𝑓:𝑋↦ℝ
 is the objective function and 𝑔:𝑋↦ℝ𝑐
 is the constraint function, the dual problem is

max𝜆∈ℝ𝑐𝑠.𝑡.𝑡(𝜆)𝜆≥0,
where 𝑡(𝜆)=min𝑥∈𝑋[𝑓(𝑥)−𝜆⊤𝑔(𝑥)]
 is the Lagrangian dual function and 𝜆⊤𝑔(𝑥)
 denotes the dot product of 𝜆
 and 𝑔(𝑥)
.

Correspondingly, for the maximization problem

max𝑥∈𝑋𝑠.𝑡.𝑓(𝑥)𝑔(𝑥)≥0,
its dual is

min𝜆∈ℝ𝑐𝑠.𝑡.𝑡(𝜆)𝜆≥0,
where 𝑡(𝜆)=max𝑥∈𝑋[𝑓(𝑥)+𝜆⊤𝑔(𝑥)]
. Note that the sign has changed.

Let 𝑥∗
 be the solution for the primal problem and 𝜆∗
 be the solution for the dual problem. Then for the minimization problem it holds that 𝑡(𝜆∗)≤𝑓(𝑥∗)
 and for the maximization problem it holds that 𝑡(𝜆∗)≥𝑓(𝑥∗)
.

This connection between the solution to the primal and dual problem is called the weak duality. In linear programming specifically, as we will see, in a significant number of cases the strong duality 𝑡(𝜆∗)=𝑓(𝑥∗)
 holds.

In linear programming
Def. 12. Let 𝑐∈ℝ𝑑
, 𝑏∈ℝ𝑐
 and 𝐴∈ℝ𝑐×𝑑
. The primal linear programming problem is defined in canonical form as

max𝑥∈ℝ𝑑𝑠.𝑡.𝑐⊤𝑥,𝐴𝑥≤𝑏,𝑥≥0.
In other words, the objective function and the constraint functions in a linear programming problem are all linear functionals.

If we follow the approach above, we'd need to introduce the Lagrange multipliers 𝜆1∈ℝ𝑐
 and 𝜆2∈ℝ𝑑
. Then the Lagrangian is

𝐿(𝑥,𝜆1,𝜆2)=𝑐⊤𝑥+𝜆⊤1(𝑏−𝐴𝑥)+𝜆⊤2𝑥
and the Lagrangian dual is

𝑡(𝜆1,𝜆2)=max𝑥∈ℝ𝑑[𝑐⊤𝑥+𝜆⊤1(𝑏−𝐴𝑥)+𝜆⊤2𝑥]=max𝑥∈ℝ𝑑[𝜆⊤1𝑏+(𝑐−𝐴⊤𝜆1+𝜆2)⊤𝑥].
When 𝐴⊤𝜆1−𝑐≠𝜆2
, this value can be arbitrarily large. Otherwise 𝑡(𝜆1,𝜆2)=𝜆⊤1𝑏
. To make a problem a bit more regular and avoid dealing with +∞
, we place the 𝐴⊤𝜆1−𝑐=𝜆2
 equality into constraints section.

After that 𝑡(𝜆1,𝜆2)
 does not depend on 𝜆2
 and we can completely remove it from the dual problem formulation replacing 𝐴⊤𝜆1−𝑐=𝜆2
 equality with 𝐴⊤𝜆1−𝑐≥0
 inequality. Then, we can formulate the dual problem.

Def. 13. The dual linear programming problem is defined as

min𝜆∈ℝ𝑐𝑠.𝑡.𝑏⊤𝜆,𝐴⊤𝜆≥𝑐,𝜆≥0.
In a similar way dual problems can be constructed for minimization problems and with different kinds of constraints. There is an alternative way to interpret the duality for LP problems specifically. If we multiply both parts of 𝐴𝑥≤𝑏
 with 𝜆⊤
, we get

𝜆⊤𝐴𝑥≤𝜆⊤𝑏.
At the same time 𝜆⊤𝐴=(𝐴⊤𝜆)⊤
, hence for 𝐴⊤𝜆≥𝑐
 it holds that

𝑐⊤𝑥≤𝜆⊤𝐴𝑥≤𝜆⊤𝑏.
This immediately shows that 𝜆⊤𝑏
 provides an upper bound for 𝑐⊤𝑥
 and by minimizing 𝜆⊤𝑏
, we make the bound as tight as possible.

Mnemonics to construct dual problems
On practice, it is not always possible to easily write LP problem in the needed form.

To mitigate this, one should keep in mind that there is generally a correspondence between:

Primal variables and dual constraints;
Primal constraints and dual variables.
This correspondence can be summed up in a table:

Maximization	Minimization
Inequality constraint ≤
Nonnegative variable ≥
Inequality constraint ≥
Nonpositive variable ≤
Equality constraint =
Free variable
Nonnegative variable ≥
Inequality constraint ≥
Nonpositive variable ≤
Inequality constraint ≤
Free variable	Equality constraint =
With the standard definition of LP duality it looks like

max𝑥𝑠.𝑡.𝑐⊤𝑥𝐴𝑥≤𝑏𝑥≥0(maximization)(constraint ≤)(variable ≥)⟺dualmin𝜆𝑠.𝑡.𝑏⊤𝜆𝜆≥0𝐴⊤𝜆≥𝑐(minimization)(variable ≥)(constraint≥)
Here's an example that highlights all possible variations (source):

max𝑥,𝑦,𝑧𝑠.𝑡.𝑐⊤𝑥+𝑑⊤𝑦+𝑓⊤𝑧𝐴𝑥+𝐵𝑦+𝐶𝑧≤𝑝𝐷𝑥+𝐸𝑦+𝐹𝑧≥𝑞𝐺𝑥+𝐻𝑦+𝐽𝑧=𝑟𝑥≥0𝑦≤0𝑧 free⟺dualmin𝜆,𝜂,𝜇𝑠.𝑡.𝑝⊤𝜆+𝑞⊤𝜂+𝑟⊤𝜇𝜆≥0𝜂≤0𝜇 free𝐴⊤𝜆+𝐷⊤𝜂+𝐺⊤𝜇≥𝑐𝐵⊤𝜆+𝐸⊤𝜂+𝐻⊤𝜇≤𝑑𝐶⊤𝜆+𝐹⊤𝜂+𝐽⊤𝜇=𝑓
Finally, if the primal problem has explicit constraint on the variable (e.g. 𝑥=0
), it doesn't produce a constraint in the dual problem.

Weirdly indexed variables and constraints
It often happens so that e.g. variables are indexed by two numbers (e.g. row and column) rather than just one, which might make the construction of the dual somewhat puzzling. What is the actual meaning behind 𝐴𝑥≤𝑏
 and 𝐴⊤𝜆≥𝑐
?

The 𝐴𝑥≤𝑏
 inequality says that there are 𝑛
 variables 𝑥1,…,𝑥𝑛
 and 𝑚
 constraints defined by rows of 𝐴
 and 𝑏1,…,𝑏𝑚
. In such terms, 𝑖
-th constraints is a linear combination of variables which should be less or equal than 𝑏𝑖
. Let the coefficient near 𝑗
-th variable in 𝑖
-th constraint be 𝑎𝑖𝑗
.

Now, 𝐴⊤𝜆≥𝑐
 inequality says that there are 𝑚
 dual variable 𝜆1,…,𝜆𝑚
 and 𝑛
 constraints defined by columns of 𝐴
 and 𝑐1,…,𝑐𝑛
. Specifically, 𝑗
-th constraint collects 𝑎𝑖𝑗
 and places it near 𝜆𝑖
. In other words, the 𝑗
-th constraint in the dual problem collects together all the coefficients that were near 𝑗
-th variable in each of primal constraints.

Let's practice it on some concrete examples.

Shortest paths as linear programming
As a simpler example, we start off with the shortest path problem. Given a weighted directed graph, one needs to find the length of the shortest path from 𝑠
 to all the other vertices. We assume that there are 𝑛
 vertices and the weight of the edge from 𝑖
 to 𝑗
 is denoted by 𝑐𝑖𝑗
. If there is no such edge, we set 𝑐𝑖𝑗=+∞
.

Let 𝑑𝑖
 symbolize the length of the shortest path from 𝑠
 to 𝑖
. Then the primal problem could be formulated as:

max𝑑1,…,𝑑𝑛∈ℝ𝑠.𝑡.𝑑1+⋯+𝑑𝑛,𝑑𝑗≤𝑑𝑖+𝑐𝑖𝑗,𝑑𝑠=0.(∀𝑖,𝑗)
On one hand, each of 𝑑𝑖
 can't be greater than the length of the shortest path from 𝑠
 to 𝑖
. Indeed, let 𝑠=𝑖0→𝑖1→𝑖2→⋯→𝑖𝑘=𝑖
 be the shortest path. Then the inequalities imply that 𝑑𝑠≥𝑑𝑖−𝑐𝑖0𝑖1−⋯−𝑐𝑖𝑘−1𝑖𝑘
. If 𝑑𝑖
 is greater than the sum which is subtracted from it, it would mean that 𝑑𝑠>0
 and would violate 𝑑𝑠=0
 constraint.

On the other hand, setting all 𝑑𝑖
 to the actual lengths of the shortest path would satisfy all constraints, hence deliver the maximum sum.

Constructing dual
For the dual problem we need to introduce dual variables 𝑥𝑖𝑗
 corresponding to the 𝑑𝑗−𝑑𝑖≤𝑐𝑖𝑗
 constraint.

In this way, the target function would be

𝜆⊤𝑐=∑𝑖,𝑗𝑥𝑖𝑗𝑐𝑖𝑗.
The coefficient near 𝑥𝑖𝑗
 in the 𝑘
-th constraint would be equal to the coefficient near 𝑑𝑘
 in the 𝑑𝑗−𝑑𝑖≤𝑐𝑖𝑗
 constraint in the primal problem. That being said, we take the coefficient 1
 near 𝑥𝑖𝑘
 for every 𝑑𝑘−𝑑𝑖≤𝑐𝑖𝑘
 constraint and we take the coefficient −1
 near 𝑥𝑘𝑗
 for every 𝑑𝑗−𝑑𝑘≤𝑐𝑘𝑗
 constraint, and the coefficient 0
 from all other constraints. This results into the following dual problem:

min𝑥𝑖𝑗∈ℝ𝑠.𝑡.∑𝑖,𝑗𝑥𝑖𝑗𝑐𝑖𝑗,∑𝑖=1𝑛𝑥𝑖𝑘−∑𝑗=1𝑛𝑥𝑘𝑗=1𝑥𝑖𝑗≥0.(∀𝑘≠𝑠)(∀𝑖,𝑗)
Note that, because of 𝑑𝑠=0
 constraint in the primal problem, we only add constraints for 𝑘≠𝑠
 in the dual problem. At the same time, because all the other primal variables were free, all the constraints in the dual problem are equality-like.

To interpret this problem, we could say that 𝑥𝑖𝑗
 denotes the number of times the edge 𝑖→𝑗
 was picked into a network formed by the combination of all shortest paths from 𝑠
 to all vertices in the graph. The =1
 constraints mean that every vertex (except for 𝑠
) must have one more incoming edge than outgoing ones.

In terms of flows it would mean that we used 𝑠
 as a source vertex and every other vertex is connected to the sink with an edge that has zero cost and capacity 1
 while original graph edges have a cost of 𝑐𝑖𝑗
 for traversing them, but infinite capacities. Then, 𝑥𝑖𝑗
 is the minimum cost flow in such network.

Maximum flow as linear programming
Let's recall the definition of a flow. Let (𝑉,𝐸)
 be a network with 𝑠,𝑡∈𝑉
 being the source and the sink. Each edge 𝑖→𝑗
 has a capacity 𝑞𝑖𝑗≥0
. Then the flow is a function on the edges 𝑓𝑖𝑗
 such that:

The flow on the edge is non-negative and doesn't exceed its capacity (capacity constraint): 0≤𝑓𝑖𝑗≤𝑞𝑖𝑗
;
The sum of incoming flow is equal to the sum of outgoing flow for any vertex except 𝑠
 and 𝑡
 (flow conservation):
∑𝑖=1𝑛𝑓𝑖𝑘=∑𝑖=1𝑛𝑓𝑘𝑖.
In this terms, we can define a net flow 𝑏𝑘
 on the vertex 𝑘
 as the difference between outgoing and incoming flows in this vertex. The definition above essentially says that the net flow of all vertices except 𝑠
 and 𝑡
 should be 0
. Let 𝑓
 be the net flow in the vertex 𝑠
, from the flow conservation it follows that the net flow of 𝑡
 is −𝑓
. In this terms the maximum flow problem is

max𝑓𝑖𝑗,𝑏𝑖∈ℝ𝑠.𝑡.𝑏𝑠,0≤𝑓𝑖𝑗≤𝑞𝑖𝑗,∑𝑗=1𝑛𝑓𝑘𝑗−∑𝑖=1𝑛𝑓𝑖𝑘=𝑏𝑘,𝑏𝑘=0,𝑏𝑡=−𝑏𝑠.(∀𝑖,𝑗)(∀𝑘)(∀𝑘∉{𝑠,𝑡})
Constructing dual
Each constraint in the primal problem will correspond to a variable in the dual problem. There are three types of variables, let 𝜆𝑖𝑗
 be a variable corresponding to 𝑓𝑖𝑗≤𝑞𝑖𝑗
 constraint, 𝜇𝑘
 be the variables corresponding to flow conservation constraints and let 𝛼
 be the variable corresponding to 𝑏𝑠=−𝑏𝑡
 constraints.

Then, the target function is

∑𝑖,𝑗𝜆𝑖𝑗𝑞𝑖𝑗.
As for the dual constraints, the constraint corresponding to the primal variable 𝑓𝑖𝑗
 would look like

𝜆𝑖𝑗+𝜇𝑖−𝜇𝑗≥0,
getting 𝜆𝑖𝑗
 from 𝑓𝑖𝑗≤𝑞𝑖𝑗
 and 𝜇𝑖,−𝜇𝑗
 from flow conservation constraints with 𝑘=𝑖
 and 𝑘=𝑗
.

As for the primal variable 𝑏𝑘
, it will only generate dual constraints for 𝑘=𝑠
 and 𝑘=𝑡
, specifically:

{𝛼−𝜇𝑠=1,𝛼−𝜇𝑡=0.
Here, 𝛼
 comes with coefficients from 𝑏𝑡=−𝑏𝑠
 and 𝜇𝑠,𝜇𝑡
 come with coefficients from flow conservation constraints. Finally, 1
 from the first equality comes from the target function 𝑏𝑠
. Note that 𝛼
 and 𝜇𝑘
 are free variables, as they come from equality constraints, while 𝜆𝑖𝑗
 must be non-negative. This allows to get rid of 𝛼
 altogether substituting two constraints above with 𝜇𝑡=𝜇𝑠+1
.

Getting it all together, the dual to the maximum flow looks like

min𝜆𝑖𝑗,𝜇𝑖∈ℝ𝑠.𝑡.∑𝑖,𝑗𝜆𝑖𝑗𝑞𝑖𝑗,𝜆𝑖𝑗≥max(0,𝜇𝑗−𝜇𝑖),𝜇𝑡=𝜇𝑠+1.(∀𝑖,𝑗)
Note that increasing all 𝜇𝑘
 by the same value will not change anything, so we may stick to 𝜇𝑠=0
 and 𝜇𝑡=1
. Another useful observation is that, since 𝑞𝑖𝑗
 are by definition non-negative it is always good to make 𝜆𝑖𝑗
 as low as possible, either saturating the inequality 𝜆𝑖𝑗≥𝜇𝑗−𝜇𝑖
 or making 𝜆𝑖𝑗=0
. With this observations, we may further simplify the dual problem and rewrite it as

min𝜇𝑖∈ℝ∑𝑖,𝑗𝑞𝑖𝑗max(0,𝜇𝑗−𝜇𝑖),𝜇𝑠=0,𝜇𝑡=1,
Thus getting rid of 𝜆𝑖𝑗
 altogether. Then, it can be proven that in this special case, the optimal solution only have 𝜇𝑘∈{0,1}
, forming a cut between sets 𝑆={𝑘:𝜇𝑘=0}
 and 𝑇={𝑘:𝜇𝑘=1}
. Since 𝑠∈𝑆
 and 𝑡∈𝑇
, the dual problem indeed looks for the minimum cut between 𝑠
 and 𝑡
.

Min-cost flow
Now assume that alongside the capacity 𝑞𝑖𝑗
, each edge also has a cost 𝑐𝑖𝑗
 for one unit of flow on the edge. Then, you want to find a flow of value 𝑏
, while maintaing its minimum cost. The primal problem for such case is formulated as

min𝑓𝑖𝑗∈ℝ𝑠.𝑡.∑𝑖,𝑗𝑓𝑖𝑗𝑐𝑖𝑗,0≤𝑓𝑖𝑗≤𝑞𝑖𝑗,∑𝑗=1𝑛𝑓𝑘𝑗−∑𝑖=1𝑛𝑓𝑖𝑘=𝑏𝑘,𝑏𝑠=𝑏,𝑏𝑡=−𝑏,𝑏𝑘=0.(∀𝑖,𝑗)(∀𝑘)(∀𝑘∉{𝑠,𝑡})
Constructing dual
We will need two types of dual variables. Specifically, 𝜆𝑖𝑗
 for each 𝑓𝑖𝑗≤𝑞𝑖𝑗
 constraint and 𝜋𝑘
 for each flow conservation constraint.

The target function then will be

∑𝑖,𝑗𝑞𝑖𝑗𝜆𝑖𝑗+∑𝑘𝜋𝑘𝑏𝑘.
Variables 𝑏𝑘
 are, in fact, constant, hence will not generate any constraints. Each variable 𝑓𝑖𝑗
 will generate a single constraint:

𝜆𝑖𝑗+𝜋𝑖−𝜋𝑗≤𝑐𝑖𝑗.
That being said, the dual problem is

max𝜆𝑖𝑗,𝜋𝑘∈ℝ𝑠.𝑡.∑𝑖,𝑗𝜆𝑖𝑗𝑞𝑖𝑗−𝑏(𝜋𝑡−𝜋𝑠),𝜆𝑖𝑗≤min(0,𝑐𝑖𝑗−𝜋𝑖+𝜋𝑗).(∀𝑖,𝑗)
Note that here 𝜆𝑖𝑗≤0
, as we're considering dual to the minimization problem and it is generated by ≤
-type constraint.

Since 𝑞𝑖𝑗≥0
 and we want to maximize target function, we would like to make 𝜆𝑖𝑗
 as large as possible. Thus, it is always optimal to take

𝜆𝑖𝑗=min(0,𝑐𝑖𝑗−𝜋𝑖+𝜋𝑗),
which allows us to get rid of 𝜆𝑖𝑗
 altogether, leaving us with the simplest version:

max𝜋𝑘∈ℝ∑𝑖,𝑗min(0,𝑐𝑖𝑗−𝜋𝑖+𝜋𝑗)𝑞𝑖𝑗−𝑏(𝜋𝑡−𝜋𝑠)
Interpreting dual, its connection to Johnson's potentials
Let 𝑐𝜋𝑖𝑗=𝑐𝑖𝑗−𝜋𝑖+𝜋𝑗
 be the adjusted cost of the edge 𝑖→𝑗
. Consider a path

𝑖0→𝑖1→⋯→𝑖𝑘
and sum up the adjusted weights on the path:

𝑐𝜋𝑖0𝑖1+⋯+𝑐𝜋𝑖𝑘−1𝑖𝑘=(𝑐𝑖0𝑖1+⋯+𝑐𝑖𝑘−1𝑖𝑘)+𝜋𝑖𝑘−𝜋𝑖0.
In particular, for 𝑖0=𝑖𝑘
, when the path is a cycle, the total adjusted cost is same as total initial cost.

From this and the cycle-path decomposition theorem follows that the cost of any 𝑏
-flow on adjusted costs would exceed the cost of same 𝑏
-flow on initial costs by exactly 𝑏(𝜋𝑡−𝜋𝑠)
, so we get rid of this excess by subtracting this value from the target function.

As for the first summand, it is comprised as if all negative-cost edges are taken into the flow, bounding the minimum-cost flow from below, so maximizing it would take it as close to the actual minimum-cost flow as possible.

On the other hand, for any particular minimum-cost flow, we may define 𝜋𝑘
 to be the shortest path to 𝑡
 from 𝑘
 in the residual network of the 𝑏
-flow. Since the flow is minimum, residual network does not have negative cycles and 𝜋𝑘
 are all well-defined. Moreover, defined this way, for any edge 𝑖→𝑗
 the following important properties hold:

If 𝑓𝑖𝑗<𝑞𝑖𝑗
 (i.e. the edge is not saturated), then 𝑐𝜋𝑖𝑗≥0
;
If 𝑓𝑖𝑗>0
 (i.e. the edge has non-zero flow), then 𝑐𝜋𝑖𝑗≤0
;
If 0<𝑓𝑖𝑗<𝑞𝑖𝑗
 (i.e. the edge is not saturated, but has non-zero flow), then 𝑐𝜋𝑖𝑗=0
.
The first property is derived from the fact that 𝑖→𝑗
 may participate in augmenting paths, hence 𝜋𝑖≤𝜋𝑗+𝑐𝑖𝑗
. The second property is derived from the fact that 𝑗→𝑖
 may participate in augmenting paths, hence 𝜋𝑗≤𝜋𝑖−𝑐𝑖𝑗
. Combined, they yield the third property.

Thus, among edges that participate in the minimum cost flow, only saturated edges will possibly have (negative) non-zero adjusted cost and only they would contribute to the target function, meaning that the target function, indeed, equates to the minimum cost of the flow.

That being said, 𝜋𝑘
 is the Johnson potential computed in the residual network of the minimum flow, but taken as a shortest path to 𝑡
, rather than shortest path from 𝑠
. Note that the same reasoning applies to potentials defined as negated lengths of shortest path from 𝑠
 to 𝑘
.

Another important caveat to mention is that the reasoning above works when all vertices are reachable from 𝑠
 (or 𝑡
 is reachable from all vertices). If some vertices are not reachable, it is still possible to compute potentials satisfying the properties above by adding edges of infinite cost and non-zero capacity between all vertices and the starting vertex.

Practical example
605C - Freelancer's Dreams. Statement as it is boils down to the following LP problem, given 𝑎1,…,𝑎𝑛,𝑏1,…,𝑏𝑛,𝑝,𝑞
:

min𝑡1,…,𝑡𝑛∈ℝ𝑠.𝑡.𝑡1+⋯+𝑡𝑛,𝑡1𝑎1+⋯+𝑡𝑛𝑎𝑛≥𝑝,𝑡1𝑏1+⋯+𝑡𝑛𝑏𝑛≥𝑞,𝑡𝑖≥0.
Its dual is

max𝜆,𝜇∈ℝ𝑠.𝑡.𝜆𝑝+𝜇𝑞,𝜆𝑎𝑖+𝜇𝑏𝑖≤1,𝜆,𝜇≥0.(∀𝑖)
The optimal solution to the dual problem can be found with nested binary search, as there are only two variables involved.

This approach was shared by amiya in his comment to the contest editorial.

Further reading
Laurent Lessard — Dual flows and algorithms
Mikhail Lavrov — The Max-Flow Min-Cut Theorem
Tags tutorial, duality, linear programming, max-flow min-cut, flows, maxflow

Kőnig's and Hall's theorems through minimum cut in bipartite graphs

By adamant, history, 17 months ago, In English
Hi everyone!

There is a well-known result about bipartite graphs which is formulated as follows:

In any bipartite graph, the number of edges in a maximum matching equals the number of vertices in a minimum vertex cover.

Knowing it you can easily find the size of a minimum vertex cover in a given bipartite graph. But what about actually recovering such cover? Well... There is an algorithm to it, but it's lengthy and not very motivated, so I constantly forget its details. However, there is a simple alternative, which is easily reproduced from scratch, even if you forget specific details.

Besides, there is another well-known result which is formulated as follows:

A bipartite graph has a perfect matching if and only if every subset 𝛼
 of one part has a neighborhood 𝛽
 in the other part such that |𝛽|≥|𝛼|
.

This result can be proven with a similar approach. It is likely known to those familiar with the topic, but still might be of interest to someone.

Kőnig's theorem
Recall that given a flow network 𝐺=(𝑉,𝐸)
, finding minimum cut between 𝑠
 and 𝑡
 is done as follows:

Find a maximum flow 𝑓
 from 𝑠
 to 𝑡
 in the network;
Find 𝑋
, the set of all vertices reachable from 𝑠
 in the residual network;
Minimum cut is formed by 𝑆=𝑋
 and 𝑇=𝑉∖𝑋
, that is the minimum cut is formed by the edges going from 𝑆
 to 𝑇
.
Now, recall that bipartite matching between sets of vertices 𝐴
 and 𝐵
 may be found as maximum flow in the following network:

There is an edge of capacity 1
 going from 𝑠
 to every vertex of 𝐴
;
There is an edge of capacity 1
 going from every vertex of 𝐵
 to 𝑡
;
There is an edge of capacity +∞
 going between some vertices of 𝐴
 and 𝐵
, as defined by the bipartite graph.
Now... Is there anything special about minimum cut in such network?


Minimum cut (𝑆,𝑇)
 in a flow network induced by a bipartite graph
Generally, some vertices from 𝐴
 and some vertices from 𝐵
 will be with the source 𝑠
 in the cut, while other will be with the sink 𝑡
.

Let 𝐴=𝐴𝑆∪𝐴𝑇
 and 𝐵=𝐵𝑆∪𝐵𝑇
, such that 𝐴𝑆,𝐵𝑆⊂𝑆
 and 𝐴𝑇,𝐵𝑇⊂𝑇
. Then the following is evidently true:

There are no edges from 𝐴𝑆
 to 𝐵𝑇
 (otherwise the cut would be infinite);
Thus, every edge in the bipartite graph is incident to some vertex from 𝐴𝑇
 or 𝐵𝑆
 (or both);
Minimum cut is formed only by edges going from 𝑆
 to 𝐴𝑇
 and from 𝐵𝑆
 to 𝑇
 and thus its size is |𝐴𝑇|+|𝐵𝑆|
.
Putting it all together, the minimum vertex cover is 𝐴𝑇∪𝐵𝑆
, and it can be easily found from the minimum cut:

Find a minimum cut (𝑆,𝑇)
 in the flow network of the maximum matching on bipartite graph with parts 𝐴
 and 𝐵
;
A minimum vertex cover is comprised of the sets 𝐴𝑇=(𝐴∩𝑇)
 and 𝐵𝑆=(𝐵∩𝑆)
.
(Generalized) Hall's theorem
The approach above is useful in some other cases as well. Consider the following problem:

A bipartite graph on parts 𝐴
 and 𝐵
 is a 𝑘
-expander if the neighborhood (set of adjacent vertices) 𝛽⊂𝐵
 of any 𝛼⊂𝐴
 is such that

|𝛽|≥𝑘|𝛼|.
Given a bipartite graph, you need to check whether it is a 𝑘
-expander.

Note that for 𝑘=1
 and |𝐴|=|𝐵|
, the condition above is, by Hall's theorem, equivalent to graph having a perfect matching.

Let's construct a similar flow network, but the edges from 𝑠
 to 𝐴
 will have capacity 𝑘
, while the edges from 𝐵
 to 𝑡
 have capacity 1
, and the edges between 𝐴
 and 𝐵
 still have capacity +∞
. One has to check whether such graph has a maximum flow of size 𝑘|𝐴|
.

If the graph is not a 𝑘
-expander, there can't be such flow, as there is a subset 𝛼⊂𝐴
 through which you can't push flow of size 𝑘|𝛼|
 no matter what. But how to prove that the graph is a 𝑘
-expander when there is such flow?

Well, we rather prove that if the minimum cut in the graph has a capacity less than 𝑘|𝐴|
, the graph is not a 𝑘
-expander.

Let's split the minimum cut into 𝐴𝑆∪𝐵𝑆
 and 𝐴𝑇∪𝐵𝑇
, as above. There still must be no edges from 𝐴𝑆
 to 𝐵𝑇
, as they have infinite capacities. So, there is a cut of size 𝑘|𝐴𝑇|+|𝐵𝑆|<𝑘|𝐴|
, which translates into |𝐵𝑆|<𝑘(|𝐴|−|𝐴𝑇|)=𝑘|𝐴𝑆|
.

On the other hand, edges from 𝐴𝑆
 are only directed in 𝐵𝑆
, thus 𝛼=𝐴𝑆
 is a subset of 𝐴
 for which |𝛽|≥𝑘|𝛼|
 does not hold.

That being said, the algorithm to check whether the graph is a 𝑘
-expander is as follows:

Construct a network, in which edges 𝑠→𝐴
 have capacity 𝑘
, edges 𝐵→𝑡
 have capacity 1
 and edges 𝐴→𝐵
 have capacity +∞
;
Check that the maximum flow (minimum cut) in the network is 𝑘|𝐴|
. If it is, the graph is a 𝑘
-expander, otherwise it's not.
Tags tutorial, konigs theorem, bipartite matching

Duality in linear programming. Part 2 — in competitive programming

By adamant, history, 17 months ago, In English
Hi everyone!

Previously, I wrote a general introduction to linear programming duality. In this blog, I would like to write about several problems that could be solved with this technique. Familiarity with the first blog, or general knowledge of dual problems and how to construct them is generally expected to navigate in this one.

Thanks to brunovsky and Golovanov399 for problem suggestions!

And particularly special thanks to WeakestTopology for problem suggestions and all insightful discussions on the topic!

Dual construction mnemonics
To simplify the construction of dual problems, let's recall the correspondence between constraints/variables in primal and dual problems.

LP duality mnemonics
Swapping variables and constraints
Let's continue from where we left in the previous article:

605C - Freelancer's Dreams. There are 𝑛
 jobs. The 𝑖
-th job gets you 𝑎𝑖
 experience and 𝑏𝑖
 dollars per second. You want to gain at least 𝑝
 experience and at least 𝑞
 money overall, while spending as little time overall as possible. How much time would it take?

Primal formulation
Dual formulation
Solution
So, the first nice property of LP duality in competitive programming is that it allows to swap variables and constraints, effectively reducing the dimensions of the problem when there are very little constraints.

Dual of minimum cost flow
Library Checker — Minimum cost b-flow. Given a flow network, find a minimum cost 𝐛
-flow 𝐟
 and its dual 𝜋
.

Primal formulation
Dual formulation
Solution
In this way, dual solution may be found even if you didn't use Dijkstra with potentials and e.g. used SPFA instead.

Inverse MST
acmsguru — 206. Roads. Given a weighted graph and its spanning tree 𝑇
, you need to adjust weights of graph edges from 𝑐𝑖
 to 𝑑𝑖
 in such way that the sum of |𝑐𝑖−𝑑𝑖|
 is minimum possible and 𝑇
 is a minimum spanning tree of the graph with new edges.

Primal formulation
Dual formulation
Solution
Duality... On segments?
1696G - Fishingprince Plays With Array Again. You may do the following operations with the array 𝑎1,…,𝑎𝑛
:

Pick 1≤𝑖<𝑛
, then decrease 𝑎𝑖
 by 𝑥
 per second and decrease 𝑏𝑖
 by 𝑦
 per second;
Pick 1≤𝑖<𝑛
, then decrease 𝑎𝑖
 by 𝑦
 per second and decrease 𝑏𝑖
 by 𝑥
 per second.
Let 𝑓(𝑎1,…,𝑎𝑛)
 be the minimum time needed to make all 𝑎𝑘
 less or equal than zero. Process 𝑞
 queries of the following kind:

Change 𝑎𝑘
 to 𝑣
;
Given 𝑙
 and 𝑟
, print 𝑓(𝑎𝑙,…,𝑎𝑟)
.
In this problem, it always holds that 𝑎𝑘≥1
 for all 𝑘
.

Primal formulation
Dual formulation
Solution
Duality in bipartite graphs
ARC 125 — Snack. There are 𝑚
 kids and 𝑛
 kinds of snacks. Distribute maximum amount of snack among children in such way that

Total distributed amount of 𝑗
-th snack is at most 𝐴𝑗
;
Total amount of each snack type distributed to 𝑖
-th child is at most 𝐵𝑖
;
Total amount of snacks distributed to 𝑖
-th child is at most 𝐶𝑖
.
Primal formulation
Dual formulation
Solution
The problem could as well be solved with minimum cut directly, but perhaps duality makes it more evident.

Besides, it sheds some light on how flow LP looks like in bipartite graphs, which is particularly useful in the following task.

Duality in bipartite graphs 2
ABC 224 — Security Camera 2. You have a bipartite graph (𝐿,𝑅)
. You may pay 𝐴𝑖
 to put 1
 camera at vertex 𝑖∈𝐿
 or pay 𝐵𝑗
 to put 1
 camera at vertex 𝑗∈𝑅
. For every pair (𝑖,𝑗)
 you want the total number of cameras installed in 𝑖∈𝐿
 and 𝑗∈𝑅
 to be at least 𝐶𝑖𝑗
. What is the least amount you need to pay to satisfy this condition?

Primal formulation
Dual formulation
Solution
Bonus: How to recover the answer (amount of cameras in each vertex)?

Is this aliens trick?
XIX Open Cup, Grand Prix of Korea — Utilitarianism. Given a weighted tree, find a maximum-weight matching of exactly 𝑘
 edges.

Primal formulation
Dual formulation
Solution
Yes, this is essentially the aliens trick. Note that aliens trick is a bit more general than LP duality, and may work for non-linear problems. However, LP problems is quite large class of cases for which aliens trick will consistently work, without need to prove it every time.

Bonus: Solve the dual problem directly, without taking a second dual of it.

Duality... On subsets?!
1430G - Yet Another DAG Problem. You're given a weighted DAG on 𝑛≤18
 vertices. You need to assign each vertex an integer 𝑎𝑖
, so that for each edge 𝑖→𝑗
 the value of 𝑎𝑖−𝑎𝑗>0
 and the total sum of 𝑤𝑖𝑗(𝑎𝑖−𝑎𝑗)
 is minimized.

Primal formulation
Dual formulation
Solution
Nope, no subsets or bitmasks here. The intended solution uses them, hence 𝑛≤18
, but the solution with LP duality is polynomial, so it provides a significant improvement in the complexity of the algorithm.

Inefficient slope trick
713C - Sonya and Problem Wihtout a Legend. You're given the array 𝑎1,…,𝑎𝑛
. You may increase or decrease some elements of the array arbitrary number of times. What is the smallest number of changes you need to do to make the array non-decreasing?

The problem is famous for being exemplar problem for "slope trick", still it can also be solved less efficiently with duality.

Primal formulation
Dual formulation
Solution
Bonus: Can you solve the dual problem, i.e. find 𝜆1,…,𝜆𝑛
, faster than 𝑂(𝑛2)
?

Further exercises
I heard that the following problems can also be solved by LP duality, so you may practice on them if you want more.

1307G - Cow and Exercise.
Baekjoon Online Judge — Ito.
Tags tutorial, linear programming, duality

Solving AGC 058 D with multivariate generating functions

By adamant, history, 16 months ago, In English
Hi everyone!

Today, I participate in AtCoder contest, which I quite rarely do. Of course, my performance was quite poor, but I stumbled with quite nice idea along the way. To those who didn't read it, the problem goes as follows:

AGC 058 — Yet Another ABC String. How many ternary strings are there such that they contain exactly 𝐴
 characters a, exactly 𝐵
 characters b and exactly 𝐶
 characters c, and do not have any sub-string that is a cyclic shift of abc?

My approach is very different from the one in the editorial, and seems insightful to me, so I decided to share it in detail.

Besides, I can't reject the request by Um_nik to explain it further :D

Walks on finite automata
We may construct a directed graph, such that each edge is marked by a, b or c, and so that the string is valid if and only if there is a walk from the starting vertex so that concatenation of characters on traversed edges form the string. The graph looks like this:


Graph of allowed walks
In the graph with have 7
 vertices:

Starting vertex in the middle;
Vertices 𝑎
, 𝑏
 and 𝑐
 denote the last character and the fact that pre-last character is not important;
Vertices 𝑎𝑏
, 𝑏𝑐
 and 𝑐𝑎
 denote the last two characters.
As you see, going from 𝑎𝑏
 by 𝑐
, from 𝑏𝑐
 by 𝑎
 or from 𝑐𝑎
 by 𝑏
 is not allowed. The graph may be further simplified to only 3
 vertices:


Reduced graph of allowed walks
Indeed, we can't traverse from vertices 𝑎𝑏
, 𝑏𝑐
 and 𝑐𝑎
 in one another, so we may as well remove them completely.

Multivariate generating function
Let 𝐹(𝑎,𝑏,𝑐)
 be the generating function on the number of allowed walks, so that [𝑎𝐴𝑏𝐵𝑐𝐶]𝐹(𝑎,𝑏,𝑐)
 is the number of allowed walks that have 𝐴
 characters a, 𝐵
 characters b and 𝐶
 characters c. Then it may be expressed as

𝐹=𝑎𝐹𝑎+𝑏𝐹𝑏+𝑐𝐹𝑐,
where 𝐹𝑎
, 𝐹𝑏
 and 𝐹𝑐
 are the generating functions for walks starting in 𝑎
, 𝑏
 and 𝑐
 correspondingly.

From the graph structure it follows that

⎧⎩⎨⎪⎪𝐹𝑎𝐹𝑏𝐹𝑐===(𝑎+𝑏𝑎)𝐹𝑎(𝑏+𝑐𝑏)𝐹𝑏(𝑐+𝑎𝑐)𝐹𝑐+++𝑏2𝐹𝑏𝑐2𝐹𝑐𝑎2𝐹𝑎+++𝑐𝐹𝑐𝑎𝐹𝑎𝑏𝐹𝑏+++1+𝑏,1+𝑐,1+𝑎.
Note that this is also one of common approaches to construct a regular expression for given DFA.

This may be expressed as a system of linear equations, and the whole answer is given as

𝐹=(𝑎𝑏𝑐)⎛⎝⎜⎜⎜1−𝑎−𝑏𝑎−𝑎−𝑎2−𝑏21−𝑏−𝑐𝑏−𝑏−𝑐−𝑐21−𝑐−𝑎𝑐⎞⎠⎟⎟⎟−1⎛⎝⎜⎜1+𝑏1+𝑐1+𝑎⎞⎠⎟⎟.
I'll be honest here, I don't exactly know the way to compute it manually, but matrixcalc suggests that the answer is

𝐹(𝑎,𝑏,𝑐)=3𝑎𝑏𝑐−𝑎−𝑏−𝑐𝑎+𝑏+𝑐−2𝑎𝑏𝑐−1.
Note that the computation is not exactly pretty:


Matrixcalc "suggestion"
Um_nik orz for computing it manually!

What to do with the generating function?
I was hesitant to publish this blog until I got the actual working solution here. Big thanks to Golovanov399 for sharing it!

Well, first of all multiplying both parts by the denominator we get that

𝐹(𝑎,𝑏,𝑐)=(𝑎+𝑏+𝑐)𝐹(𝑎,𝑏,𝑐)−2𝑎𝑏𝑐𝐹(𝑎,𝑏,𝑐)+𝐹0(𝑎,𝑏,𝑐),
where 𝐹0(𝑎,𝑏,𝑐)=3𝑎𝑏𝑐−(𝑎+𝑏+𝑐)
 is insignificant to higher terms of 𝐹
.

From the formula above it follows that

𝐹[𝐴][𝐵][𝐶]=𝐹[𝐴−1][𝐵][𝐶]+𝐹[𝐴][𝐵−1][𝐶]+𝐹[𝐴][𝐵][𝐶−1]−2𝐹[𝐴−1][𝐵−1][𝐶−1],
where 𝐹[𝐴][𝐵][𝐶]=[𝑎𝐴𝑏𝐵𝑐𝐶]𝐹(𝑎,𝑏,𝑐)
 is the answer to the problem. Not exactly the result that is obvious from the statement, is it?

However, the problem requires the solution in 𝑂(𝐴+𝐵+𝐶)
 and, luckily, it is also possible to find it from the generating function!

Let 𝐺(𝑎,𝑏,𝑐)=1𝑎+𝑏+𝑐−2𝑎𝑏𝑐−1
. Note that

[𝑎𝐴𝑏𝐵𝑐𝐶]𝐹=3[𝑎𝐴−1𝑏𝐵−1𝑐𝐶−1]𝐺−[𝑎𝐴−1𝑏𝐵𝑐𝐶]𝐺−[𝑎𝐴𝑏𝐵−1𝑐𝐶]𝐺−[𝑎𝐴𝑏𝐵𝑐𝐶−1]𝐺,
hence the computation of [𝑎𝐴𝑏𝐵𝑐𝐶]𝐹
 can be reduced to 4
 computations of similar thing in 𝐺
.

And to compute [𝑎𝐴𝑏𝐵𝑐𝐶]𝐺
 you should note that

𝐺=∑𝑡=0∞(𝑎+𝑏+𝑐−2𝑎𝑏𝑐)𝑡=∑𝑖,𝑗,𝑘,𝑙(𝑖+𝑗+𝑘+𝑙𝑖,𝑗,𝑘,𝑙)𝑎𝑖𝑏𝑗𝑐𝑘(−2𝑎𝑏𝑐)𝑙.
Among all the terms on the right, only min(𝐴,𝐵,𝐶)
 contribute to [𝑎𝐴𝑏𝐵𝑐𝐶]𝐺
 and we may find them all by iterating over 𝑙
.

Here, (𝑖+𝑗+𝑘+𝑙𝑖,𝑗,𝑘,𝑙)=(𝑖+𝑗+𝑘+𝑙)!𝑖!𝑗!𝑘!𝑙!
 is a multinomial coefficient.

Tags tutorial, generating function, dfa
Vote: I like it+163Vote: I do not like it
Author adamantPublication date 16 months agoComments 2

Problems that I authored so far

By adamant, history, 13 months ago, In English
Hi everyone!

Today I saw a discussion in AC Discord server about how many problems some people made for different competitions. It was sparkled by this CF entry. I haven't keep track of this before, so it made me curious to go through everything that I made and publish in a comprehensive list. I'm doing it mostly out of curiosity, but maybe it might be interesting for someone else too :)

#	Date	Problem	Contest	Comment
1	Jan 2016	Sasha and Swaps	Ad Infinitum 14	Find a 𝑇
-th root of a given permutation, while minimizing the number of swaps in which the root may be decomposed.
2	Aug 2015	Sasha and swag strings	Ptz Summer 2015. MIPT Contest	Compute the total number of distinct substrings on all edges of a given string's suffix tree.
3	Sep 2015	Alice and Bob (and string)	Hackerrank World Cup Finals	A game, for which a game graph is a suffix automaton of reversed string and you use its suffix link tree as a suffix tree to get the answer.
4	Apr 2016	Dihedral Subgroup	Ad Infinitum 15	Given 𝑛
, find the smallest symmetric group 𝑆𝑚
 that contains the dihedral group 𝐷𝑛
 as a subgroup.
5	Sep 2016	Gears of War	Week of Code 23	Simple problem on bipartiteness.
6	Sep 2016	Lighthouse	Week of Code 23	Implementation grid problem.
7	Sep 2016	Treasure Hunting	Week of Code 23	Simple geometry problem. Has 1-line solution with complex numbers.
8	Sep 2016	Unexpected Problem	Week of Code 23	When is it true that 𝑠𝑡=𝑡𝑠
 for strings 𝑠
 and 𝑡
?
9	Sep 2016	Gravity Tree	Week of Code 23	Data structures on trees.
10	Sep 2016	Enclosure	Week of Code 23	Construct a polygon with given sides 𝐿1,…,𝐿𝑛
 and maximum area.
11	Sep 2016	Sasha and the Swaps II	Week of Code 23	Find the number of ways to represent a permutation as 𝑘
 swaps.
12	Dec 2016	The Axis of Awesome	Ad Infinitum 17	Given 𝑛
 points in 3D, add minimum number of points so that sum of squared distances to any axis passing through the origin is the same.
13	Apr 2016	667B - Coat of Anticubism	CF Round #349	Given 𝐿1,…,𝐿𝑛−1
, add shortest 𝐿𝑛
 so that it's possible to make a polygon with such side lengths.
14	Apr 2016	666E - Forensic Examination	CF Round #349	Construct a suffix structure for each vertex of the segment tree.
15	Dec 2017	901A - Hashing Trees	CF Round #453	Construct counter-examples for specific tree hashing approach.
16	Dec 2017	901B - GCD of Polynomials	CF Round #453	Construct two polynomials with smallest coefficients and largest possible amount of Euclidean algorithm steps for GCD.
17	Dec 2017	901E - Cyclic Cipher	CF Round #453	Solve a system of equations with a circulant matrix.
18	Dec 2017	906D - Power Tower	CF Round #454	Compute 𝑎(𝑎(…𝑎𝑟)𝑙+1)𝑙
 modulo 𝑚
.
19	Dec 2017	906E - Reverses	CF Round #454	Reverse some substrings of 𝐴
 to obtain 𝐵
. Involves palindromes.
20	Sep 2019	1220G - Geolocation	CF Round #586	Given 𝑛
 random points (𝑥𝑖,𝑦𝑖)
 and unordered set of distances 𝜌𝑖
 to some (𝑥,𝑦)
, find all possible (𝑥,𝑦)
. Input is randomized.
21	Jul 2020	1375I - Cubic Lattice	CF Global Round 9	Given 𝑛
 points (𝑥𝑖,𝑦𝑖,𝑧𝑖)
, find a cubic lattice with largest cell size that contains all the given points. Involves integer quaternions.
22	Jul 2017	Tree Expectancy	July Challenge 2017	Find the expected number of vertices having exactly one child in a random ordered tree.
23	Sep 2018	Table Game	September Challenge 2018	A simple 2-dimensional game.
24	Aug 2019	FourSquareSum	SRM 764	You're given 𝑎,𝑏,𝑐,𝑑
 such that 𝑎2+𝑏2+𝑐2+𝑑2=2𝑛
.
Find 𝑠,𝑥,𝑦,𝑧
 such that 𝑠2+𝑥2+𝑦2+𝑧2=𝑛
.
25	Aug 2018	Alice and Bob and a string	Ptz Summer 2018. MIPT Contest	Similar to "Alice and Bob (and string)", but now characters are appended on both sides.
26	Feb 2019	102129A - Tritwise Mex	Ptz Winter 2019. OKC 1	Tritwise mex convolution.
27	Feb 2019	102129B - Associativity Degree	Ptz Winter 2019. OKC 1	Construct a binary operation with a specified number of associative triplets.
28	Feb 2019	102129C - Medium Hadron Collider	Ptz Winter 2019. OKC 1	Long story short, do some combinatorics or polynomial GCD.
29	Feb 2019	102129D - Basis Change	Ptz Winter 2019. OKC 1	Change the basis of linear recurrence in a specific manner.
30	Feb 2019	102129E - Scored Nim	Ptz Winter 2019. OKC 1	Some simple game.
31	Feb 2019	102129F - Milliarium Aureum	Ptz Winter 2019. OKC 1	Data structures on a tree.
32	Feb 2019	102129G - Permutant	Ptz Winter 2019. OKC 1	Find the determinant of a matrix, in which each row is the same permutation of the previous one.
33	Feb 2019	102129H - Game Of Chance	Ptz Winter 2019. OKC 1	Compute a gain expectation in randomized game.
34	Feb 2019	102129I - Incomparable Pairs	Ptz Winter 2019. OKC 1	Compute the number of substring pairs of a given string that are not substrings of each other.
35	Feb 2019	102129J - The Zong of the Zee	Ptz Winter 2019. OKC 1	I don't remember, but it was somehow inspired by Sunless Sea.
36	Feb 2019	102129K - Expected Value	Ptz Winter 2019. OKC 1	Constraints should push you into 𝑂(𝑛2)
 dp, right?
37	Aug 2019	102354A - Square Root Partitioning	Ptz Summer 2019. OKC 2	Find the number of ways to split 𝑎1‾‾√,…,𝑎𝑛‾‾√
 in equal sum sets.
38	Aug 2019	102354B - Yet Another Convolution	Ptz Summer 2019. OKC 2	max|𝑎𝑖−𝑏𝑗|
 convolution over gcd(𝑖,𝑗)=𝑘
.
39	Aug 2019	102354C - Money Sharing	Ptz Summer 2019. OKC 2	Some warm-up problem.
40	Aug 2019	102354D - Magic Strings	Ptz Summer 2019. OKC 2	Compute a number of distinct subsequences of specific string pattern.
41	Aug 2019	102354E - Decimal Expansion	Ptz Summer 2019. OKC 2	Compute the expansion of 910⋅99100⋅9991000⋅…
42	Aug 2019	102354F - Cosmic Crossroads	Ptz Summer 2019. OKC 2	Find a rotation that matches two sets of random points in 3D.
43	Aug 2019	102354H - Defying Gravity	Ptz Summer 2019. OKC 2	Notice something about symmetry in 2D.
44	Aug 2019	102354I - From Modular to Rational	Ptz Summer 2019. OKC 2	Given 𝑝𝑞
 modulo 𝑚>(𝑝+𝑞)2
, find 𝑝
 and 𝑞
.
45	Aug 2019	102354J - Tree Automorphisms	Ptz Summer 2019. OKC 2	Find a generating set for an automorphism group of a given tree.
46	Feb 2023	104234A - Square Sum	OCPC 2023. OKC 3	Count solutions to 𝑥2+𝑦2≡𝑧(mod𝑚)
.
47	Feb 2023	104234B - Super Meat Bros	OCPC 2023. OKC 3	Given linear recurrences 𝑎𝑖
 and 𝑏𝑗
, find a linear recurrence for
𝑐𝑘=∑𝑖+𝑗=𝑘(𝑘𝑖)𝑎𝑖𝑏𝑗.
48	Feb 2023	104234C - Testing Subjects Usually Die	OCPC 2023. OKC 3	Guess random number with memory loss.
49	Feb 2023	104234D - Triterminant	OCPC 2023. OKC 3	Terry Tao solved this on my MathOverflow question 5 years ago.
50	Feb 2023	104234E - Garbage Disposal	OCPC 2023. OKC 3	Too similar to 1818B - Indivisible?
51	Feb 2023	104234H - Graph Isomorphism	OCPC 2023. OKC 3	Check if a graph has at most 𝑛
 distinct isomorphic graphs.
52	Feb 2023	104234I - DAG Generation	OCPC 2023. OKC 3	Find the probability of two DAGs generated by a given process to be the same DAG.
53	Feb 2023	104234J - Persian Casino	OCPC 2023. OKC 3	What if Prince of Persia cheated in casino using the dagger of time?
54	Feb 2023	104234K - Determinant, or...?	OCPC 2023. OKC 3	Given 𝑎𝑖
, find det𝐴
 for 𝐴𝑖𝑗=𝑎𝑖|𝑗
.
55	Feb 2023	104234L - Directed Vertex Cacti	OCPC 2023. OKC 3	Count digraphs on 𝑛
 vertices such that their SCCs are cycles or isolated vertices and they have 𝑚
 edges outside SCCs.
56	Feb 2023	104234M - Siteswap	OCPC 2023. OKC 3	I like juggling.
57	Apr 2023	1818A - Politics	CF Round #869	Wouldn't you be upset if somebody disagrees with you?
58	Apr 2023	1818B - Indivisible	CF Round #869	Too similar to 104234E - Garbage Disposal?
59	Apr 2023	1817C - Similar Polynomials	CF Round #869	Given 𝐴(𝑥)
 and 𝐵(𝑥)
, find 𝑠
 such that 𝐵(𝑥)=𝐴(𝑥+𝑠)
.
60	Apr 2023	1817D - Toy Machine	CF Round #869	This game was inspired by a puzzle in God of War.
61	Apr 2023	1817E - Half-sum	CF Round #869	This problem was inspired by 102129K - Expected Value.
62	Apr 2023	1817F - Entangled Substrings	CF Round #869	Find number of pairs of substrings 𝑎
 and 𝑏
 that only occur in 𝑠
 as substrings of 𝑎𝑐𝑏
 for some 𝑐
.
I think that's mostly it. There might be some other problems here and there which were difficult for me to retrieve. I also prepared a number of problems for internal educational use and some problems based on somebody else's ideas, but I think they shouldn't be included here.

Well, it seems I haven't set any new problems in a while. Hope it will change soon!

UPD: Added OKC 3 and CF Round #869.

Tags problem list, curiosity

Osijek Competitive Programming Camp 2023 winter

By adamant, history, 12 months ago, In English
Hi everyone!


I am happy to announce the Osijek Competitive Programming Camp that is scheduled to happen on 18-26 February 2023 in Osijek, Croatia. The camp is hosted by the Department of Mathematics at J. J. Strossmayer University of Osijek and is brought to you by the organizing team of adamant (that's me!), -is-this-fft-, mblazev, Tomx and antontrygubO_o.

The camp is inspired by various competitive programming camps that we attended during our active years in ICPC, and is aimed to help college students prepare for ICPC regional contests and finals. The camp will consist of 7 ICPC-style contests and 2 days off.

Up until now, most top competitive programming training camps have been organised in Russia, or by Russian entities. Due to the current situation in the world, attending such camps is not possible or desirable for a large number of participants, and we hope to offer a European alternative to Russian camps.

This is the first edition of the camp, and we all are very excited about it and hope that we may see you soon!

Details
Participation fee for onsite participants is 150€ per person. It does not include regular meals, travel or accommodation.

If you want to participate, but are unable to come onsite, we offer a reduced fee of 100€ per person for online participation. It is also possible to reduce fees individually if you are unable to attend some of the contests (e.g. because of the overlap with SWERC).

We support and empathize with those affected by the ongoing war in Ukraine, therefore we offer a 100€ discount for affected individuals and teams affiliated with Ukrainian institutions. In other words, the fees would be 50€ and 0€ per person for onsite and online participation correspondingly.

The expected starting time for the contests is 10am CET. For online participants, it is still a preferred starting time, but if it is not an option for you, we will also offer an opportunity to compete virtually within 24 hours after the main window is over.

Participants
If you are interested in participating, please fill the form here.

We ask you to register before February 10 if you want to participate online and before February 4 if you want to participate onsite.

If you want to participate onsite, especially if you require visa support, we would urge you to register as soon as possible, as visa processing may take 30+ days.

Also, if your university has a lively ICPC community that may be interested in attending the camp, and you have some contacts of people in charge (e.g. coaches) we would highly appreciate if you could fill the form here, so that we can send an invitation. Thanks!

Problemsetters
We'd like to praise the authors of the contests for the camp:

amiya — a guy who changed CF username every year and labeled the contest by prime numbers.
conqueror_of_tourist — ICPC 2022 world finalist. Codeforces LGM in Python.
antontrygubO_o — IMO 2017 gold medalist, NAC 22 winner, contest coordinator, Codeforces LGM.
Snow-Flower — ICPC 2023 world finalist.
rivalq — Codeforces grandmaster, ICPC 2021 and 2022 world finalist.
Adam_GS — Codeforces grandmaster, EJOI 2021 gold medalist.
ToxicPie9 — ICPC 2021 world finalist.
adamant — a maintainer of cp-algorithms.com, author of nearly 50 competitive programming problems.
Sponsors
Last but not least, we would like to say special thanks to our sponsors, who make the camp possible:

  
If you are interested in sponsoring next editions of the camp or have any questions, please feel free to reach out at ocpc@mathos.hr.

Finally, we would also like to thank Codeforces for guidance and promoting this announcement to the main page, eolymp for providing us an online judge for the contests and the Department of Mathematics at J. J. Strossmayer University of Osijek for all their organizational support and providing us a physical location to conduct the camp.

 Announcement of Osijek Competitive Programming Camp, Winter 2023, Day 9: Magical Story of LaLa (The 1st Universal Cup. Stage 14: Ranoa)
Tags training camp, osijek, icpc

Permutation group basis construction (Schreier–Sims algorithm)

By adamant, history, 11 months ago, In English
Hi everyone!

I had this blog drafter for quite a while, but the $300 contest motivated me to actually finish and publish it. I don't care about the prize that much as I think many other blogs published recently are talking about something even cooler. But I like to make people suffer, so I will apply anyway to make peltorator read even more stuff >:)

Besides, the contest makes it harder to stay on contribution top, so I got to do something to stay afloat. That being said...

Today I'd like to write about an algorithm that solves the following problem:

You're given a set of permutations 𝐺={𝑔1,…,𝑔𝑚}
. Find the size |⟨𝑔1,…,𝑔𝑚⟩|
 of the group generated by 𝐺
.

At first, I wanted to get into a lot of group-theoretic details with proofs and all, but then I felt like it would make the article harder to comprehend, so I will try to keep group-theoretic stuff to minimum, while telling more about the high-level idea.

Prerequisites
It is strongly recommended to be familiar with some simpler basis constructing algorithms, e.g. Gauss elimination or XOR basis.

Familiarity with basic group theory (e.g. definitions) will also be beneficial.

What does the statement mean?
In this article, we will treat permutations as a bijective functions from {1,…,𝑛}
 to itself.

For example, the permutation 𝜎=(122331)
 is perceived as a function 𝜎
, such that 𝜎(1)=2
, 𝜎(2)=3
 and 𝜎(3)=1
.

Then, we may as well define the composition 𝜎1𝜎2
 of permutations 𝜎1
 and 𝜎2
 as (𝜎1𝜎2)(𝑘)=𝜎1(𝜎2(𝑘))
.

By the group ⟨𝐺⟩
 generated by 𝐺
 we mean the set of permutation that we will get by repeatedly composing permutations with each other in all possible ways, until no new permutations may be produced. Further we will rely on the following properties of permutations:

There is an identity permutation 𝑒(𝑘)=𝑘
, such that 𝑒𝜎=𝜎𝑒=𝜎
 for any 𝜎
;
For every permutation 𝜎
, there is unique 𝜎−1
 such that 𝜎(𝜎−1(𝑘))=𝜎−1(𝜎(𝑘))=𝑘
, that is 𝜎𝜎−1=𝜎−1𝜎=𝑒
.
Together, the properties above just say that permutations form a group.

High-level idea
General idea of the algorithm is to find 𝑘
 sets 𝐺1,…,𝐺𝑘⊂𝐺
 such that every element 𝑔∈𝐺
 may be uniquely represented as 𝑔=𝑔1𝑔2…𝑔𝑘
, where 𝑔1∈𝐺1,…,𝑔𝑘∈𝐺𝑘
. Then it will also hold that |𝐺|=|𝐺1||𝐺2|…|𝐺𝑘|
.

It would be even nicer to make 𝐺1,…,𝐺𝑘
 such that finding 𝑔1𝑔2…𝑔𝑘
 for any given 𝑔
 is a reasonably simple task.

Which 𝐺1,…,𝐺𝑘
 to use?
This idea above is quite similar to representing any vector 𝑟
 of a linear space as a linear combination 𝑟=𝑎1𝑟1+⋯+𝑎𝑘𝑟𝑘
 of basis vectors. The basis 𝑟1,…,𝑟𝑘
 is usually constructed in a row-echelon form, meaning that each vector has a specific coordinate associated with it, so that all basis vectors with higher indices have zero coefficient in this coordinate. In this way, the coefficient 𝑎𝑖
 may be found just by looking on the corresponding coordinate of 𝑟−𝑎1𝑟1−⋯−𝑎𝑖−1𝑟𝑖−1
.

Generally, in linear spaces we may allow zero vectors in the basis and choose 𝑟1,…,𝑟𝑛
 in such way that 𝑟𝑖
 has zero value in all coordinates 𝑗<𝑖
. Then, the vector 𝑎𝑖𝑟𝑖
 is uniquely determined by the 𝑖
-th coefficient of 𝑟−𝑎1𝑟1−⋯−𝑎𝑖−1𝑟𝑖−1
.

Getting back to permutations, 𝐺𝑖
 may be interpreted as a set of all possible values of 𝑎𝑖𝑟𝑖
. So instead of representing the vector 𝑟
 as a linear combination of 𝑎𝑖𝑟𝑖
, we represent the permutation 𝑔
 as a composition of 𝑔𝑖
. In linear spaces, we wanted 𝑎𝑖𝑟𝑖
 to have the value 0
 in coordinates 𝑗<𝑖
. And also for 𝑎𝑖𝑟𝑖
 to be uniquely determined by the 𝑖
-th coordinate of 𝑟−𝑎1𝑟1−⋯−𝑎𝑖−1𝑟𝑖−1
.

What would it mean for us in permutation context?

What is the equivalent of 𝑟−𝑎1𝑟1−⋯−𝑎𝑖−1𝑟𝑖−1
 in permutations?

We ultimately want 𝑔=𝑔1…𝑔𝑛
. Cancelling first 𝑖−1
 permutations on both sides, we get to 𝑔′=𝑔−1𝑖−1…𝑔−11𝑔
.

What is the equivalent of coordinates in permutations?

It's the values 𝜎(1),𝜎(2),…,𝜎(𝑛)
.

What is the equivalent of 𝑗
-th coordinate being 0
 for 𝑗<𝑖
?

We needed it, so that the composition of 𝑔−1𝑖
 and 𝑔′
 always preserved 𝑔′(𝑗)=𝑗
 for 𝑗<𝑖
.

By definition it means that 𝑔−1𝑖(𝑔′(𝑗))=𝑔′(𝑗)=𝑗
, which only happens when 𝑔−1𝑖(𝑗)=𝑔𝑖(𝑗)=𝑗
.

What is the equivalent of "𝑎𝑖𝑟𝑖
 is uniquely determined by the 𝑖
-th coordinate of 𝑟−𝑎1𝑟1−⋯−𝑎𝑖−1𝑟𝑖−1
"?

In the very end we ultimately want 𝑔−𝑛𝑛…𝑔−11𝑔=𝑒
, where 𝑒
 is the identity permutation 𝑒(𝑘)=𝑘
.

To achieve this, we must pick 𝑔𝑖
 in such way that 𝑔−1𝑖
 will turn 𝑔′(𝑖)
 into 𝑖
.

By definition, it means that 𝑔−1𝑖(𝑔′(𝑖))=𝑖
, or equivalently 𝑔𝑖(𝑖)=𝑔′(𝑖)
.

In other words, we must have 𝑎(𝑖)≠𝑏(𝑖)
 for any 𝑎,𝑏∈𝐺𝑖
.

That being said, Schreier-Sims algorithm constructs 𝑛
 sets 𝐺1,…,𝐺𝑛
 such that

every element 𝑔𝑖∈𝐺𝑖
 has 𝑔𝑖(𝑗)=𝑗
 for 𝑗<𝑖
;
all elements 𝑔𝑖∈𝐺𝑖
 have distinct 𝑔𝑖(𝑖)
;
The sets 𝐺
 and 𝐺1∪𝐺2∪⋯∪𝐺𝑛
 generate the same permutation group.
I will use Python in my code examples below. Let schreier_sims(S) be the function that takes a list of permutations 𝐺
 and returns 𝐺1,…,𝐺𝑛
. Each 𝐺𝑖
 is also represented as a list of permutations. Then checking that an element belongs to 𝐺
 may be done as follows:

# compute the composition p[i] = p1[p2[i]]
def apply(p1, p2):
    return tuple(p1[p2[i]] for i in range(len(p2)))

# compute the inverse permutation ans[p[i]] = i
def inverse(p):
    ans = [0] * len(p)
    for i in range(len(p)):
        ans[p[i]] = i
    return tuple(ans)

# compute the sets G1, ..., Gn
def schreier_sims(S):
    ...

# G is the list of Schreier-Sims basis sets,
# p is the permutation that we need to check
def belongs(G, p):
    for i in range(len(G)):
        for g in G[i]:
            if p[i] == g[i]:
                p = apply(inverse(g), p)
    return all(p[j] == j for j in range(len(p)))

belongs(schreier_sims([[1, 2, 0]]), [2, 0, 1]) # True
belongs(schreier_sims([[1, 2, 0]]), [2, 1, 0]) # False
Is this enough?
No! There is an additional implicit assumption that we have made here. Specifically, that every permutation 𝑔∈𝐺
 such that 𝑔(𝑗)=𝑗
 for 𝑗<𝑖
 can be generated just by permutations from the sets 𝐺𝑖,𝐺𝑖+1,…,𝐺𝑛
. While this is a somewhat natural assumption if you're familiar with Gauss algorithm, it isn't necessarily true.

It could be that combining permutations from 𝐺𝑖
, you will get a permutation 𝑔𝑖
 such that 𝑔𝑖(𝑖)=𝑖
 and then you will have to apply it to make 𝑔
 an identity permutation in the end. Therefore, we need a stronger set of constraints:

every element 𝑔𝑖∈𝐺𝑖
 has 𝑔𝑖(𝑗)=𝑗
 for 𝑗<𝑖
;
all elements 𝑔𝑖∈𝐺𝑖
 have distinct 𝑔𝑖(𝑖)
;
For any permutation 𝑔∈⟨𝐺⟩
 such that 𝑔(𝑗)=𝑗
 for 𝑗<𝑖
, it is also true that 𝑔∈⟨𝐺𝑖,𝐺𝑖+1,…,𝐺𝑛⟩
.
In group theory terms, the last condition means that the set of permutations 𝐺1∪⋯∪𝐺𝑛
 is a strong generating set of ⟨𝐺⟩
.

Note that this is very similar to how we construct the basis for vectors over remainders modulo possibly composite number 𝑚
 (see e.g. the article by errorgorn). Key difference with vectors over fields here is the possible existence of torsions, when it is possible to annihilate the 𝑖
-th coordinate of a vector by multiplying the vector with a non-zero constant (thus, possibly not annihilating the remaining coordinates).

Similarly with permutations, it may be possible to annihilate the 𝑖
-th term of a permutation from 𝐺𝑖
 by applying other permutations from 𝐺𝑖
 to it, without making the permutation as a whole the identity permutation, hence it might be still needed in the decomposition unless 𝐺1,…,𝐺𝑛
 constitutes a strong generating set.

How to construct 𝐺1,…,𝐺𝑛
?
Schreier-Sims algorithms construct a strong generating set iteratively. Let 𝐺(𝑖)
 be the set of permutation 𝑔∈⟨𝐺⟩
, such that 𝑔(𝑗)=𝑗
 for 𝑗<𝑖
. In particular, 𝐺(1)=⟨𝐺⟩
. Note that 𝐺(𝑖)
 is closed under composition, that is 𝑔1𝑔2∈𝐺(𝑖)
 for any 𝑔1,𝑔2∈𝐺(𝑖)
, because

𝑔1(𝑔2(𝑗))=𝑔1(𝑗)=𝑗
for any 𝑗<𝑘
. In group-theoretic terms, 𝐺(𝑖)
 is called a stabilizer subgroup of 1,2,…,𝑖−1
.

From the definition of a strong generating set above it follows that

𝐺(𝑖)=⟨𝐺𝑖∪𝐺𝑖+1∪⋯∪𝐺𝑛⟩.
The iteration of the Schreier-Sims algorithm assumes that we have a generating set of permutation 𝑆
 such that 𝐺(𝑖)=⟨𝑆⟩
, and from that it finds a set 𝐺𝑖
 and a new generating set 𝑆′
 such that 𝐺(𝑖)=⟨𝐺𝑖∪𝑆′⟩
, and 𝐺(𝑖+1)=⟨𝑆′⟩
. In this way, it is possible to apply similar steps iteratively to construct 𝐺𝑖+1,𝐺𝑖+2,…,𝐺𝑛
.

Finding 𝐺𝑖
To find 𝐺𝑖
, we should find a representative for every possible value of 𝑔(𝑖)
 for 𝑔∈𝐺(𝑖)
. In other words, for every obtainable 𝑗≠𝑖
, we should find a permutation 𝑔𝑗∈𝐺(𝑖)
 such that 𝑔𝑗(𝑖)=𝑗
. Once we apply 𝑔−1𝑗
, to the permutation 𝑔
 having 𝑔(𝑖)=𝑗
, we would make its 𝑖
-th term being equal to 𝑖
 and therefore it will move to 𝐺(𝑖+1)
, which is dealt with later on.

That being said, 𝐺𝑖
 will consist exactly of 𝑔𝑗
 for all obtainable 𝑗
. The set of all possible 𝑔(𝑖)
 is called the orbit of the number 𝑖
. We can find and organize the orbit by building a so-called Schreier tree. The tree consists of all possible values of 𝑔(𝑖)
, and edges of the tree correspond to permutations that should be applied to change one element of the orbit to another.

Now, let 𝑔𝑗∈𝐺(𝑖)
 be such that 𝑔𝑗(𝑖)=𝑗
. Then for any 𝑔∈𝑆
 it holds that (𝑔𝑔𝑗)(𝑖)=𝑔(𝑗)
. This fact allows to construct the Schreier tree with a simple depth-first search. We will store actual permutations 𝑔𝑗
 in an array orbit:

# orbit[j] = permutation g in <S> s.t. g[i] = j
def build_schreier_tree(i, S, orbit):
    for g in S:
        if g[i] not in orbit:
            orbit[g[i]] = apply(g, orbit[i])
            # g[i] = j
            build_schreier_tree(g[i], S, orbit)
The running time of the construction above is 𝑂(𝑛|𝑆|+𝑛2)
.

Finding 𝑆′
Assume that we have 𝑆
 s.t. ⟨𝑆⟩=𝐺(𝑖)
 and 𝐺𝑖
, represented by the elements of the orbit of 𝑖
. Then the generating set 𝑆′
 such that ⟨𝑆′⟩=𝐺(𝑖+1)
 can be found from what is called the Schreier's lemma:

𝑆′={(𝑔𝑠𝑢)−1𝑠𝑔𝑢|𝑢∈𝚘𝚛𝚋𝚒𝚝𝑖,𝑠∈𝑆}
Here, 𝚘𝚛𝚋𝚒𝚝𝑖
 are the possible values of 𝑔(𝑖)
 for 𝑔∈𝐺(𝑖)
 and 𝑔𝑗∈𝐺𝑖
 is a representative having 𝑔𝑗(𝑖)=𝑗
.

Indeed, consider an element 𝑠′=𝑠1𝑠2…𝑠𝑘∈𝐺(𝑖+1)
. Let 𝑟𝑖=𝑠𝑖𝑠𝑖+1…𝑠𝑘
, then we may rewrite it as

𝑠′=𝑠1(𝑔𝑟2(𝑖)𝑔−1𝑟2(𝑖))𝑠2(𝑔𝑟3(𝑖)𝑔−1𝑟3(𝑖))…(𝑔𝑟𝑘𝑔−1𝑟𝑘)𝑠𝑘,
which, using that 𝑠′(𝑖)=𝑖
 and 𝑟𝑘(𝑖)=𝑠𝑘(𝑟𝑘+1(𝑖))=𝑠𝑘(𝑢)
 for 𝑟𝑘+1(𝑖)=𝑢
, in turn further rewrites as

𝑠′=(𝑔−1𝑖𝑠1𝑔𝑟2(𝑖))(𝑔−1𝑟2(𝑖)𝑠2𝑔𝑟3(𝑖))…(𝑔−1𝑟𝑘(𝑖)𝑠𝑘𝑔𝑖)=𝑠′1𝑠′2…𝑠′𝑘.
The process of generating such set is straight-forward:

# Given <S> = G^(i) and
# the orbit representatives of i,
# find S' s.t. <S'> = G^(i+1)
def make_gen(S, orbit):
    n = len(next(iter(S)))
    newS = set()
    for s in S:
        for u in orbit:
            newS.add(reduce(apply, [inverse(orbit[s[u]]), s, orbit[u]]))
    return newS
Note that the size of 𝑆′
 generated in this way may be as high as 𝑛|𝑆|
, which would allow an exponential growth if not halted.

Making sure 𝑆′
 has a reasonable size
For any set 𝑆′
, it is possible to construct another set 𝑆″
 such that ⟨𝑆′⟩=⟨𝑆″⟩
, but |𝑆″|≤𝑛2
. There are several ways to construct such sets, for example Jerrum's filter even guarantees that |𝑆″|≤𝑛
, but at the cost of higher construction complexity. In Schreier-Sims algorithm, however, since the construction and sifting is repeated several times iteratively, the preferred way is Sims filter. It guarantees that for any pair (𝑖,𝑗)
 there is at most one permutation 𝑔∈𝑆″
 such that 𝑔(𝑘)=𝑘
 for 𝑘<𝑖
 and 𝑔(𝑖)=𝑗
.

The construction process for it is fairly straightforward and essentially repeats the Gauss method:

# Sifts S and returns S'
# such that <S> = <S'>
# and |S'| <= n^2
def normalize(S):
    n = len(next(iter(S)))
    newS = set()
    base = [{} for i in range(n)]
    for s in S:
        for x in range(0, n):
            if s[x] != x:
                if s[x] in base[x]:
                    s = apply(inverse(s), base[x][s[x]])
                else:
                    base[x][s[x]] = s
                    newS.add(s)
                    break
    return newS
Getting it all together
Gluing all the pieces mentioned above together, we get the following algorithm:

def schreier_sims(S):
    n = len(next(iter(S)))
    ans = []
    for i in range(n):
        orbit = {}
        # initiate with identity permutation
        orbit[i] = tuple(range(n))
        # find G_i = orbit
        build_schreier_tree(i, S, orbit)
        # add G_i to the answer
        ans += [[orbit[j] for j in orbit]]
        # generate S' and make sure it is at most n^2 in size
        S = normalize(make_gen(S, orbit))
    return ans
Complexity analysis here is not very pleasant and yields something around 𝑂(𝑛6)
. It is possible to further reduce it by roughly a factor of 𝑛
 by terminating the sifting process earlier, if sufficient amount of uniformly generated elements of 𝐺
 are sifted through.

Example problem
There was a problem in Petrozavodsk Summer-2011. Moscow SU Unpredictable Contest that asked specifically to find the size of a permutation group for 𝑛≤50
. If you don't have an access to opentrains, I have uploaded this specific problem to the mashup which you can access by the invitation link, and here is my submission to it using the Schreier-Sims algorithm: [submission:188935679].

A bit of self-advertisement
By the way, I'm organizing a competitive programming camp this February! And you would make me very happy if you participate in it :)

Tags permutations, group theory, xor basis, gauss, tutorial

CDQ convolution (online FFT) generalization with Newton method

By adamant, history, 11 months ago, In English
Hi everyone!

This is yet another blog that I had drafted for quite some time, but was reluctant to publish. I decided to dig it up and complete to a more or less comprehensive state for the $300 contest.

Essentially, the blog tells how to combine CDQ technique for relaxed polynomial multiplication ("online FFT") with linearization technique from Newton method (similar approach is used in the first example of the ODE blog post by amiya), so that the functions that typically require Newton's method can be computed online as well. I will try to briefly cover the general idea of "online FFT" too and provide some examples, in case you're not well familiar with it. That being said...

Consider the following setting:

There is a differentiable function 𝐹(𝑥)
 such that 𝐹(0)=0
 and a polynomial 𝑓(𝑥)
.

You want to compute first 𝑛
 coefficients of a formal power series 𝑔(𝑥)
 such that 𝑔(𝑥)=𝐹(𝑓(𝑥))
. However, the series 𝑓(𝑥)
 is not known in advance. Instead, the 𝑘
-th coefficient of 𝑓(𝑥)
 is given to us after we compute the 𝑘
-th coefficient of 𝑔(𝑥)
.

Looks familiar? No? Ok, let's make a brief detour first.

CDQ convolution
General idea of CDQ technique is described in the following simple scheme: To compute something on the [𝑙,𝑟)
 interval,

Compute it on [𝑙,𝑚)
 for 𝑚=𝑙+𝑟2
,
Compute the influence of [𝑙,𝑚)
 onto [𝑚,𝑟)
,
Compute everything else in [𝑚,𝑟)
 recursively,
Merge the results.
This approach is very versatile, and In convolution context, it's commonly known as "online FFT". It has the following typical formulation:

Standard formulation
We want to compute a sequence 𝑐0,𝑐1,…,𝑐𝑛
, such that

𝑐𝑘=∑𝑖+𝑗=𝑘−1𝑎𝑖𝑏𝑗,
where 𝑎0,𝑎1,…,𝑎𝑛
 and 𝑏0,𝑏1,…,𝑏𝑛
 are not known in advance, but 𝑎𝑘
 and 𝑏𝑘
 are revealed to us after we compute 𝑐𝑘
.

In a more polynomial-like manner, we may formulate it as

𝐶(𝑥)=𝑥𝐴(𝑥)𝐵(𝑥),
where the 𝑘
-th coefficient of the polynomials 𝐴(𝑥)
 and 𝐵(𝑥)
 is revealed to us as we compute the 𝑘
-th coefficient of 𝐶(𝑥)
.

Examples
Polynomial exponent. Assume you want to compute 𝑄(𝑥)=𝑒𝑃(𝑥)
 for a given 𝑃(𝑥)
.

reduction
Jetpack. You want to get from (0,0)
 to (𝑛,0)
. On each step, you increase you 𝑥
-coordinate by 1
, and your 𝑦
 coordinate changes to 𝑦+1
 if you use the jetpack or to max(0,𝑦−1)
 if you don't. At the same time, you can only have 𝑦>0
 for at most 2𝑘
 steps. How many ways are there to get to (𝑛,0)
 under the given constraints?

reduction
Gennady Korotkevich Contest 5 — Bin. Find the number of full binary trees with 𝑛
 leaves such that for every vertex with two children, the number of leaves in its left sub-tree doesn’t exceed the number of leaves in its right sub-tree by more than 𝑘
.

reduction
Solution
Okay, how to solve this? Let's recall the convolution formula

𝑐𝑘=∑𝑖+𝑗=𝑘−1𝑎𝑖𝑏𝑗.
Assume that we want to compute 𝑐𝑘
 for 𝑘
 in [𝑙,𝑟)
 and we already know values of 𝑐𝑘
, 𝑎𝑖
 and 𝑏𝑗
 for 𝑖,𝑗,𝑘∈[𝑙,𝑚)
. Consider the contribution to 𝑐𝑘
 for 𝑘∈[𝑚,𝑟)
 of (𝑖,𝑗)
 pairs such that both 𝑖
 and 𝑗
 are below 𝑚
 and at least one of them is above or equal to 𝑙
. For each such 𝑎𝑖
, values of interest for 𝑗
 range from 0
 to min(𝑚,𝑟−𝑙)
.

Correspondingly, for each 𝑏𝑗
, values of interest for 𝑖
 range from 0
 to min(𝑙,𝑟−𝑙)
. In both cases, the contribution may be computed with an appropriate convolution of polynomials of size at most 𝑟−𝑙
. Note that in the second case we used min(𝑙,𝑟−𝑙)
 instead of min(𝑚,𝑟−𝑙)
 to avoid double-counting pairs in which both 𝑖
 and 𝑗
 are 𝑙
 or above.

We make two recursive calls and use extra 𝑂(𝑛log𝑛)
 time to consolidate them, making it for overall 𝑂(𝑛log2𝑛)
 time.

Generalization
Now back to the beginning. The examples above typically expect that the right-hand side is formulated in a way that is kind of similar to convolutions. But there are a lot of functions, for which it's not really possible to do so. Try the following ones in a similar setting (i.e. the coefficients of 𝑓(𝑥)
 are given after the coefficients of 𝑔(𝑥)
 are computed):

𝑔(𝑥)=𝑔(𝑥)=𝑔(𝑥)=𝑥⋅𝑒𝑓(𝑥)𝑥⋅log11−𝑓(𝑥)𝑥⋅11−𝑓(𝑥)
Those are the functions that are quite typical in formal power series (for their interpretation, see here). You may probably rephrase them in a convolution-like manner, so that CDQ is applicable, but you would need to do something ad hoc for each individual function. It doesn't seem very convenient, so it only makes sense to try and find some generic enough approach. What is a generic formulation here?

𝑔(𝑥)=𝐹(𝑓(𝑥)).
And you may note that what all these functions 𝐹(…)
 have in common is that they're differentiable. Let's use it in a similar way to what we do in Newton's method and rewrite the right hand side in the following way:

𝑔(𝑥)=𝐹(𝑓0)+𝐹′(𝑓0)(𝑓−𝑓0)+𝑂((𝑓−𝑓0)2).
This formula generally works for any 𝑓0
. In particular, let 𝑓0
 be first 𝑛
 coefficients of 𝑓
, then (𝑓−𝑓0)2
 is divisible by 𝑥2𝑛
. In other words,

𝑔(𝑥)≡𝐹(𝑓0)+𝐹′(𝑓0)(𝑓−𝑓0)(mod𝑥2𝑛).
In this formula, we still do not know 𝑓(𝑥)
 completely. But what's important is that it is no longer an argument of some generic function 𝐹(…)
, so the right-hand side is now a linear function of 𝑓(𝑥)
! This is exactly the formulation for which we learned to apply CDQ convolution above, that is 𝑔(𝑥)=𝐴(𝑥)𝑓(𝑥)+𝐵(𝑥)
, where

𝐴(𝑥)=𝐵(𝑥)=𝐹′(𝑓0),𝐹(𝑓0)−𝑓0𝐹′(𝑓0)
are specific constant polynomials in this context. This sub-problem is solvable in 𝑂(𝑛log2𝑛)
 with the standard CDQ technique, and since it allows us to double the number of known coefficients at each step, the overall running time is also 𝑂(𝑛log2𝑛)
, assuming that we're able to compute 𝐹(…)
 and 𝐹′(…)
 in 𝑂(𝑛log2𝑛)
 as well.

Tags polynomials, online fft, cdq, newton, tutorial

Unlabeling combinatorial species (cycle index series)

By adamant, history, 11 months ago, In English
Hi everyone!

In my previous blog, I wrote about how generating functions can be used to enumerated labeled species. In this blog, I want to continue the topic by writing about how one can account for different kinds of symmetries when counting different combinatorial structures. Ultimately, we will end up deriving and hopefully understanding the analogue of Pólya enumeration theorem in species.

Difficulty: ★★★★☆

Prerequisites:

Familiarity with combinatorial species (see my prev. blog), OR
Very good intuition with enumerative combinatorics, genfuncs and recap below.
I will try to use plain language rather than formulas as much as possible, as it seems to be the preferred format for readers.

Recap
Below is a very brief recap of the most important things from the previous article. I tried to keep it as informal as possible.

Previously on combinatorial species
If 𝐴(𝑥)
 and 𝐵(𝑥)
 are the generating functions for species 𝐴
 and 𝐵
, then 𝐴(𝐵(𝑥))
 is the generating function for their composition.

It is a fundamental result for labeled species. Unfortunately, there is no simple analogue with unlabeled structures, and deriving the composition formula in some reasonable formulation for them is the ultimate goal of this article.

How unlabeled structures work
Let's take a closer look on the equivalence classes that are formed when we enumerate unlabeled structures. What is common for the elements of an equivalence class? How to compute the size of an equivalence class? How to enumerate the elements of a given equivalence class, if a single representative is known?

Let |𝐴|=𝑛
. Consider an object 𝑜∈𝐹(𝐴)
 of the species 𝐹
. If we use a permutation 𝜎:𝐴→𝐴
 on the underlying set of atoms, it will produce a new object 𝐹𝜎(𝑜)
. There is a total of 𝑛!
 possible permutations 𝜎
 of 𝑛
 atoms, and when we apply 𝐹𝜎
 to a specific object 𝑜
 for all permutations 𝜎
, we obtain all the elements of its equivalence class defined above.

Claim 1. For every object 𝑜′
 that may be produced as a result of applying 𝐹𝜎
 to 𝑜
, the number of permutations 𝜎
 that produce 𝑜′
 is the same as the number of permutations that produce 𝑜
 itself.

Explanation. Let 𝜏
 be a permutation such that 𝐹𝜏(𝑜)=𝑜′
. Then for any permutation 𝜎
 such that 𝐹𝜎(𝑜)=𝑜
, there is a permutation 𝜏∘𝜎
 such that 𝐹𝜏∘𝜎(𝑜)=𝐹𝜏(𝐹𝜎(𝑜))=𝑜′
. Same is true in the other direction.

The permutations 𝜎
 such that 𝐹𝜎(𝑜)=𝑜
 are called the automorphisms of the object 𝑜
. So among 𝑛!
 values of 𝐹𝜎(𝑜)
, if we consider them as a multiset, each unique one is counted |Aut(𝑜)|
 times, where Aut(𝑜)={𝜎:𝐹𝜎(𝑜)=𝑜}
 is the set of automorphisms of 𝑜
.

This allows us to say that the size of the equivalence class of 𝑜
 is exactly 𝑛!|Aut(𝑜)|
.

Burnside lemma
The result above is very useful, as now instead of counting the number of equivalence classes we instead can "distribute" the counting among all the elements, that is we may represent the total number of equivalence classes as

∑𝑜∈𝐹(𝐴)|Aut(𝑜)|𝑛!.
In this way, the elements from the same equivalence class, when summed up together, will produce 1
.

On the other hand, the sum of |Aut(𝑜)|
 can be reformulated as follows:

∑𝑜∈𝐹(𝐴)|Aut(𝑜)|=|{(𝑜,𝜎):𝐹𝜎(𝑜)=𝑜}|=∑𝜎∈𝑆𝑛|Fix(𝜎)|,
where Fix(𝜎)
 is the number of fixed points of 𝐹𝜎
, that is Fix(𝜎)={𝑜∈𝐹(𝐴):𝐹𝜎(𝑜)=𝑜}
.

This gives the following alternative formula for the number of the equivalence classes:

1𝑛!∑𝜎∈𝑆𝑛|Fix(𝜎)|,
also known as the Burnside lemma. That being said, the number of unlabeled structure on 𝑛
 atoms of the species equates to the average number of fixed points over all permutations of length 𝑛
. This key observation allows us to apply key properties of expected values to compute the number of equivalence classes, in particular use the linearity of expected value.

Automorphisms and fixed points for compositions
To construct an 𝐹∘𝐺
 structure, we first build an 𝐹
-structure on a set 𝐵={1,2,…,𝑘}
, and then substitute the 𝑖
-th of its atoms with a 𝐺
-structure built on the set 𝐴𝑖
. Then, we treat the disjoint union 𝐴=𝐴1⊔𝐴2⊔⋯⊔𝐴𝑘
 as the full set of atoms. So, an object 𝑜
 of species 𝐹∘𝐺
 can be described by an object 𝑓∈𝐹(𝐵)
 and a family of objects 𝑔1,…,𝑔𝑘
, where 𝑔𝑖∈𝐺(𝐴𝑖)
.

Automorphisms of a given object
Consider a permutation 𝜎:𝐴→𝐴
. What should hold for it to be an automorphism of 𝑜
?

First of all, it should preserve the structure of 𝑓
. The object 𝑓
 is built on top of the sets 𝐴1,…,𝐴𝑘
, and to preserve the structure of 𝑓
, the permutation 𝜎
 should only map these sets to each other. In other words, it should map a set 𝐴𝑖
 to a set 𝐴𝑗
 such that |𝐴𝑖|=|𝐴𝑗|
, and the function 𝜌(𝑖)=𝑗
 defined from such mapping should be a permutations, which is, in turn, an automorphism of 𝑓
.

Then, if we look on a transition from 𝐴𝑖
 to 𝐴𝜌(𝑖)
, the permutation 𝜎
 should be consistent with the mapping 𝜌
. In other words, if we restrict the domain of 𝜎
 from the whole 𝐴
 to its subset 𝐴𝑖
, the resulting mapping 𝜎𝑖
 should map the object 𝑔𝑖
 into the object 𝑔𝜌(𝑖)
.

That being said, each permutation of interest 𝜎:𝐴→𝐴
 can be described by a permutation 𝜌:𝐵→𝐵
, which is an automorphism of 𝑓
, and a family of bijections 𝜎𝑖:𝐴𝑖→𝐴𝜌(𝑖)
, such that 𝜎𝑖
 maps 𝑔𝑖
 to 𝑔𝜌(𝑖)
.

Fixed points of a given permutation
Such setting allows us to think about what should hold for an object 𝑜∈(𝐹∘𝐺)(𝐴)
 to be a fixed point of 𝜎
?

The object 𝑓
 must be a fixed point of 𝜌
;
For each 𝜎𝑖:𝐴𝑖→𝐴𝑗
 it must hold that 𝐹𝜎𝑖(𝑔𝑖)=𝑔𝑗
.
The first criteria is understandable and is in line with what we have studied so far. What about the second, though? When is it true?

Essentially, it means that in a cycle decomposition of the permutation 𝜌
, for each cycle 𝑖1→𝑖2→⋯→𝑖𝑚→𝑖1
 we're allowed to choose a single distinct element 𝑔𝑖1
 in such way that it is a fixed point of a permutation 𝜏=𝜎𝑖𝑚∘⋯∘𝜎𝑖2∘𝜎𝑖1
, and all the other ones will be uniquely defined by the mappings 𝜎𝑖1,𝜎𝑖2,…,𝜎𝑖𝑚−1
.


Objects 𝑔𝑖1,…,𝑔𝑖𝑚
 on a cycle 𝑖1→𝑖2→⋯→𝑖𝑚→𝑖1
Let |𝐴𝑖1|=⋯=|𝐴𝑖𝑚|=𝑡
. What do we get if we average the amount of fixed points 𝑔𝑖1
 of 𝜏
 over all permutations 𝜏
? It follows from the Burnside lemma that we get the number of distinct unlabeled structures of species 𝐺
 on 𝑡
 atoms. The neat part here is that for each cycle in 𝜌
 it could be done independently, multiplying the amounts to get the average amount for a fixed permutation 𝜌
.

Averaging over all permutations
In other words, to count the average amount of fixed points of 𝜎:𝐴→𝐴
, we can do the following:

Consider a permutation 𝜌:𝐵→𝐵
;
Pick a fixed point of 𝐹𝜌
 as an 𝐹
-structure;
For each cycle of 𝜌
, pick an unlabeled structure 𝑔
 of 𝐺
 and duplicate it on the whole cycle 𝑔𝑖1,…,𝑔𝑖𝑚
;
Average the counts over all permutations 𝜌
 by dividing with 𝑘!
.
These considerations boil down to the following essential formula:

(𝐹∘𝐺)(𝑥)=∑𝑘=0∞1𝑘!∑𝜌∈𝑆𝑘|Fix𝐹(𝜌)|∏𝑗=1𝑘𝐺(𝑥𝑗)𝜌𝑗,
where 𝜌𝑗
 is the number of cycles of length 𝑗
 in 𝜌
, and the ordinary generating functions 𝐺(𝑥)
 and (𝐹∘𝐺)(𝑥)
 are defined as

𝐺(𝑥)=(𝐹∘𝐺)(𝑥)=∑𝑛=0∞𝑥𝑛𝑛!∑𝜎∈𝑆𝑛|Fix𝐺(𝜎)|,∑𝑛=0∞𝑥𝑛𝑛!∑𝜎∈𝑆𝑛|Fix𝐹∘𝐺(𝜎)|.
In other words, the functions are defined in such way that [𝑥𝑛]𝐺(𝑥)
 and [𝑥𝑛](𝐹∘𝐺)(𝑥)
 are the numbers of unlabeled 𝐺
-structures and (𝐹∘𝐺)
-structures correspondingly on 𝑛
 atoms, corresponding to OGFs defined for unlabeled structures in the previous article.

Intuitively, 𝐺(𝑥𝑗)
 is a generating functions for sets of 𝑗
 equal unlabeled structures of species 𝐺
, so we pick |𝑐|
 same 𝐺
-structures for each cycle 𝑐
 of length |𝑐|
 and then multiply the generating functions over all cycles, as it is done independently.

If you're not convinced yet, you can find a bit more technical and rigorous (but less intuitive) computation below to get the same result.

Derivation
Cycle index series
looking on the formula above, we may define a multivariate formal power series

𝑍𝐹(𝑡1,𝑡2,𝑡3,…)=∑𝑘=0∞1𝑘!∑𝜌∈𝑆𝑘|Fix𝐹(𝜌)|𝑡𝜌11…𝑡𝜌𝑘𝑘,
which allows to rewrite (𝐹∘𝐺)(𝑥)
 as the formal power series composition:

(𝐹∘𝐺)(𝑥)=𝑍𝐹(𝐺(𝑥),𝐺(𝑥2),𝐺(𝑥3),…).
The multivariate power series 𝑍𝐹(𝑡1,𝑡2,𝑡3,…)
 is called the cycle index series of the species 𝐹
 and it holds a tremendous amount of information about the structure of the species. In particular, we may note that the exponential generating function of the species is simply

𝐹(𝑥)=𝑍𝐹(𝑥,0,0,…),
As to count labeled structures it is sufficient to count the fixed points of identity permutations (for which all cycles have length 1
).

and the ordinary generating function of unlabeled structures of the species is nothing but

𝐹(𝑥)=𝑍𝐹(𝑥,𝑥2,𝑥3,…),
as we may perceive the species as a composition of itself with an "atom" species 𝑥
.

This is the composition formula for the unlabeled structures that we sought for.

Why is it called a cycle index series?
The definition of a cycle index series above may be rewritten in terms of automorphisms:

𝑍𝐹(𝑡1,𝑡2,𝑡3,…)=∑𝑘=0∞∑𝑜∈𝐹(𝐵𝑘)1𝑘!∑𝜌∈Aut(𝑜)𝑡𝜌11…𝑡𝜌𝑘𝑘,
where 𝐵𝑘={1,2,…,𝑘}
.

Substituting 1𝑘!=|Aut(𝑜)|𝑘!1|Aut(𝑜)|
, and recalling that the equivalence class of 𝑜
 has 𝑘!|Aut(𝑜)|
 objects, we may rewrite the sum above as

𝑍𝐹(𝑡1,𝑡2,𝑡3,…)=∑𝑘=0∞∑𝑜∈𝐹˜(𝐵𝑘)1|Aut(𝑜)|∑𝜌∈Aut(𝑜)𝑡𝜌11…𝑡𝜌𝑘𝑘,
where 𝐹˜(𝐵𝑘)
 is the set of representatives of unlabeled equivalence classes of 𝐹(𝐵𝑘)
.

For a given set of permutations that is closed under composition (so-called permutation group) 𝐺
, the polynomial

𝑍(𝐺)(𝑡1,𝑡2,…,𝑡𝑘)=1|𝐺|∑𝜌∈𝐺𝑡𝜌11…𝑡𝜌𝑘𝑘
is known more widely as the cycle index of 𝐺
. That being said, the cycle index series 𝑍𝐹(𝑡1,𝑡2,…)
 is the formal sum of cycle indices over all unlabeled structures of the species 𝐹
 of all possible sizes.

Atom colorings
Note that for in all discussions above we assumed that either all atoms are distinct from each other (labeled) or indistinguishable (unlabeled). In a bit more generic setting we may assume that each atom is colored in one of 𝑠
 distinct colors.

In this case, the number of indistinguishable structures built on such set of atoms is 𝑍(𝐺)(𝑠,𝑠,…,𝑠)
, as we enumerated fixed points of 𝜌
 by coloring all atoms on each cycle in the same color.

Correspondingly, if we want to also keep track of the total number of atoms in cycle index series, we should use

𝑓(𝑠)=𝑍𝐹(𝑠𝑥,𝑠𝑥2,𝑠𝑥3,…).
Then, the coefficient [𝑥𝑛]𝑓(𝑠)
 will denote the number of 𝐹
-structure on 𝑛
 atoms, such that each atom is colored in one of 𝑠
 distinct colors.

Examples
There are two famous species 𝐹
 with known composition formulas. Specifically, the cycles and sets.

Let's derive explicit form for their cycle index series, and from that obtain the operators CYC
 and MSET
:

Cycles
Consider a cycle of length 𝑛
. What do its automorphisms look like? Its automorphisms are:

A permutation with this cycle as its cycle presentation;
Powers of this permutation.
This is a total of 𝑛
 elements. When we raise the cycle to the power 𝑘
, we obtain a set of gcd(𝑘,𝑛)
 of length 𝑛/gcd(𝑛,𝑘)
 each.

For example, raising single cyclic shift [2,3,4,5,6,1]
 with cycle presentation (1 2 3 4 5 6)
 to the power 3
, we get a permutation [4,5,6,1,2,3]
 with a cycle presentation (1 4)(2 5)(3 6)
. Note that all cycles of length 𝑛
 are isomorphic to each other, meaning that there is a unique unlabeled cycle of length 𝑛
 with the cycle index

𝑍(𝐶𝑛)(𝑡1,𝑡2,…,𝑡𝑛)=1𝑛∑𝑘=1𝑛𝑡gcd(𝑘,𝑛)𝑛/gcd(𝑘,𝑛)=1𝑛∑𝑑|𝑛𝜑(𝑛𝑑)𝑡𝑑𝑛/𝑑.
The last formula is due to the fact that there are 𝜑(𝑛𝑑)
 numbers 𝑘
 such that gcd(𝑘,𝑛)=𝑑
. If we sum it up over all 𝑛
, we get

𝑍𝐶(𝑡1,𝑡2,…)=∑𝑛=1∞1𝑛∑𝑑|𝑛𝜑(𝑛𝑑)𝑡𝑑𝑛/𝑑,
then substituting 𝑘=𝑛𝑑
, we get

𝑍𝐶(𝑡1,𝑡2,…)=∑𝑘=1∞𝜑(𝑘)𝑘∑𝑑=1∞𝑡𝑑𝑘𝑑,
making it into

𝑍𝐶(𝑡1,𝑡2,…)=∑𝑘=1∞𝜑(𝑘)𝑘log11−𝑡𝑘
From this, the unlabeled composition formula of CYC
 species of cycles and arbitrary species 𝑓
 with OGF 𝑓(𝑥)
 is:

CYC𝑓(𝑥)=∑𝑘=1∞𝜑(𝑘)𝑘log11−𝑓(𝑥𝑘)
Sets
Now, in case of sets, we would get the same set no matter how we rearrange its elements. Again, there is a unique unlabeled set on 𝑛
 vertices, and its automorphisms are all possible permutations, hence we have

𝑍(𝑆𝑛)(𝑡1,𝑡2,…,𝑡𝑛)=1𝑛!∑𝜌∈𝑆𝑛∏𝑐∈𝜌𝑡|𝑐|.
Assuming that there are 𝑗𝑘
 cycles of length 𝑘
, the formula above rewrites as

𝑍(𝑆𝑛)(𝑡1,𝑡2,…)=∑𝑗1+2𝑗2+3𝑗3+⋯+𝑛𝑗𝑛=𝑛∏𝑘=1𝑛𝑡𝑗𝑘𝑘𝑘𝑗𝑘𝑗𝑘!.
Indeed, the number of ways to make a permutation with given 𝑗1,…,𝑗𝑛
 is

𝑛!1𝑗1𝑗1!2𝑗2𝑗2!…𝑛𝑗𝑛𝑗𝑛!,
as for 𝑗𝑘
 cycles of length 𝑘
, you divide by 𝑘𝑗𝑘
 to not double-count cyclic shifts and by 𝑗𝑘!
 to not double-count rearrangements of cycles.

On the other hand, if we sum it up over all 𝑛
, we get

𝑍𝑆(𝑡1,𝑡2,…)=∑𝑛=0∞∑𝑗1+2𝑗2+3𝑗3+⋯+𝑛𝑗𝑛=𝑛∏𝑘=1𝑛𝑡𝑗𝑘𝑘𝑘𝑗𝑘𝑗𝑘!.
Adding a new variable 𝑦
 to track the sum of 𝑗1+2𝑗2+⋯+𝑛𝑗𝑛
, we can rewrite it as product of sums:

𝑍𝑆(𝑦,𝑡1,𝑡2,…)=∏𝑘=1∞∑𝑗𝑘=0∞(𝑡𝑘𝑦𝑘)𝑗𝑘𝑘𝑗𝑘𝑗𝑘!=∏𝑘=1∞exp(𝑡𝑘𝑦𝑘𝑘).
Finally, substituting 𝑦=1
, we get the sum over all possible 𝑛
 as

𝑍𝑆(𝑡1,𝑡2,…)=exp(∑𝑘=1∞𝑡𝑘𝑘)
and the composition formula with set species

MSET𝑓(𝑥)=exp(∑𝑘=1∞𝑓(𝑥𝑘)𝑘)
The results above also yield simplified expression for 𝑍(𝑆𝑛)
:

𝑍(𝑆𝑛)(𝑡1,𝑡2,…,𝑡𝑛)=[𝑦𝑛]∏𝑘=1𝑛exp(𝑡𝑘𝑦𝑘𝑘).
Cycle index composition
In a very generic setting, one may prove the general formula:

𝑍𝐹∘𝐺(𝑡1,𝑡2,…)=𝑍𝐹(𝑍𝐺(𝑡1,𝑡2,…),𝑍𝐺(𝑡2,𝑡4,…),𝑍𝐺(𝑡3,𝑡6,…),…).
Derivation, explanation and proof of this formula is left to the reader as an exercise.

Further reading
Symbolic method (Combinatorics) — Wikipedia
Cycle index — Wikipedia
Asymmetry index series
In this article, we studied how to account for symmetries (automorphisms) when we transition from enumerating labeled structures to enumerating unlabeled ones. Another use-case which is worth studying are so-called asymmetric structures, that is the structures that only have the identity permutation as their single automorphism. In such structures, any unlabeled structure has exactly 𝑛!
 ways to turn it into a labeled one, meaning that the OGF of the unlabeled structures and the EGF of corresponding labeled structures coincide.

In a similar fashion, it is possible to introduce a so-called "asymmetry index series", which we may then use to compose operators like CYC
 and SET
 on the generating functions of asymmetric structures. Maybe I will write more about it in future blog entries...

Tags tutorial, generating function, species, burnside lemma, polya enumeration

How to sum up all natural numbers (and their non-negative powers)

By adamant, history, 10 months ago, In English
Hi everyone!

This is the blog for infinite sums. For finite sums of power, see here.

As everybody knows, it is true that

1+2+3+4+⋯=−112
Moreover, it is widely known that

1+1+1+1+⋯=−12
Historically, it seems that the result was first discovered by Ramanujan, who described it in one of his letters to G. Hardy as

"If I tell you this you will at once point out to me the lunatic asylum as my goal"
In this blog, I want to explain how this summation could be easily done with the competitive programmer's favorite tool: generating functions. Moreover, I will explain how to compute the infinite sums of form 1𝑘+2𝑘+3𝑘+4𝑘+⋯=𝑆𝑘
 for any non-negative integer 𝑘
.

All your problems will go away with this simple genfunc trick. Just use...
Consider the exponential generating function

𝑆(𝑥)=∑𝑘=0∞𝑆𝑘𝑘!𝑥𝑘.
Expanding 𝑆𝑘
 into infinite sum, we get

𝑆(𝑥)=∑𝑘=0∞∑𝑡=1∞𝑡𝑘𝑥𝑘𝑘!=∑𝑡=1∞𝑒𝑡𝑥=𝑒𝑥1−𝑒𝑥
It doesn't converge at 𝑥=0
, where we typically analyze genfuncs, but should it bother us? The expression expands as a Laurent series

𝑆(𝑥)=−1𝑥−12−𝑥12+𝑥3720−𝑥530240+…,
which means that

101112131415++++++202122232425++++++303132333435++++++404142434445++++++………………======−1/2,−1/12,0,1/120,0,−1/252.
If you do some investigation on the sequence of denominators, you'll notice that it is OEISable and is related to the so-called Bernoulli numbers. On closer inspection, you may notice that, generally,

𝑆𝑘=(−1)𝑘𝐵𝑘+1𝑘+1,
where 𝐵𝑘
 is the 𝑘
-th Bernoulli number. But what does it have to do with the task at hand?

A complex problem, a complex solution...
Consider another way to define the sum of powers of natural numbers, using 𝑠
 as a parameter:

𝜁(𝑠)=1−𝑠+2−𝑠+3−𝑠+4−𝑠+…
The function 𝜁(𝑠)
 converges for any 𝑠>1
, which, using some complex analysis magic, allows to find its unique analytic continuation on the whole complex plane ℂ
, except for the point 𝑠=1
. In other words, there is a meaningful definition of

𝜁(−𝑘)=1𝑘+2𝑘+3𝑘+4𝑘+…
for any integer 𝑘>0
. Okay then, what are the values of such function? I don't really know, but Wikipedia says that

𝜁(−𝑘)=(−1)𝑘𝐵𝑘+1𝑘+1.
Well, that's nice to know! We have no idea about Bernoulli numbers, or any other properties of Riemann zeta functions 𝜁(𝑠)
, yet we obtained some meaningful result with seemingly completely wrong formal manipulations. So, if we ever want to compute the infinite sum

𝑆𝑘=1𝑘+2𝑘+3𝑘+4𝑘+…,
we should just expand the formal power series 𝑥𝑒𝑥1−𝑒𝑥=(1−𝑒𝑥𝑥)−1𝑒−𝑥
 and the answer would be its (𝑘+1)
-th coefficient, divided by 𝑘!
.

By the way, Wikipedia also mentions that 𝑡𝑒𝑡−1
 is the exponential generating function for the Bernoulli numbers, which is also consistent with the results above.

Tags fun with genfuncs, tutorial

Osijek Competitive Programming Camp 2023 winter — wrap

By adamant, history, 10 months ago, In English
Hi everyone!


Sponsored by
  
The Osijek competitive programming camp (also see the announcement on Codeforces) just concluded last Sunday, on February 26, and I'd like to write this blog post as a wrap to this winter's iteration of the camp. Overall, 74 teams joined the camp, and 13 of them attended the camp onsite in Osijek. Tähvend (-is-this-fft-) and I (adamant), as organizers, were also there!

  	  
Thanks to pauldiac for all the photos!
The camp consisted of 7 contests, and 2 days off. On the first day off, the onsite participants had an opportunity to go to bowling, and on the second day off, to play paintball (see the photos). As for the contests, two of them were shared with the Universal Cup, so that our participants didn't have to choose what to attend between the camp contests and the UniCup. On top of that, three other contests will be used in the Universal Cup for later stages. Here is a brief contest summary:

Authors	Contest	Scoreboard	UniCup
antontrygubO_o	Day 1: Anton Trygub Contest	scoreboard	Stage 4: Ukraine
adamant, fedimser	Day 2: Oleksandr Kulkov Contest 3	scoreboard	Zaporizhia stage
rivalq	Day 4: Jatin Garg Contest	scoreboard	—
Adam_GS, ToxicPie9	Day 5: Adam Gąsienica-Samek Contest	scoreboard	—
amiya	Day 6: Yuhao Du Contest 11	scoreboard	Zhejiang stage
conqueror_of_tourist	Day 8: Dilhan Salgado Contest	scoreboard	Stage 5: Osijek
Snow-Flower	Day 9: Magical Story of LaLa	scoreboard	Ranoa stage
Combined scoreboard, also ITMO rating table (thanks, awoo!). Congratulations to the top teams (by ITMO score):

HoMaMaOvO (maroonrk, hos.lyric, maspy)
University of Warsaw: Polish Mafia (Swistakk, Radewoosh, mnbvmar)
Kyoto University: Heno World (heno239, yamunaku, Moririn2528)
And also to the top onsite teams (by ITMO score):

University of Oxford: nwerc is bad (Ronnie007, gamegame, Sorting)
University of Zagreb: Zagreb (dorijanlendvaj, keko37, ppavic)
School No. 14 in Wrocław + School No. 8 in Warsaw: PokorFanClub (kamilszymczak1, Rafi22, surgutti)
I can't express enough how happy I am that the idea worked out, and we gathered so many cool people to participate in this! I'm very optimistic towards the idea of conducting it again sometime, and as such I wanted to make an announcement:

Call for problemsetters
We're looking for problemsetters for the next iteration of the camp!

Specific dates are to be announced later on, but you may express your interest by filling in this form, and we will contact you. Once the camp is conducted, we will offer you a monetary compensation (expected to be 1000+€, depending on whether we're successful with finding sponsors and attracting participants for the next iteration), and also fully waive the participation fees for you and your team if you want to attend the rest of the camp. Moreover, you can use your author compensation to sponsor participants who would like to attend the camp. In this case, the participants in question will get a 30% discount from their fees!

Feedback and our responses
Now, that being said, I'd like to use this opportunity to reply to some feedback that we received and explain our perspective a bit more, and also tell more about how we're going to address it. Here are the key points:

A lot of contests are used in the Universal Cup
To begin with, I'd like to explain a bit more on what happened here. Initially, our idea was to share 2 contests with Universal Cup, so that our participants do not have to choose between participating in the camp, and participating in the Universal Cup contests. Then, some of our authors (well, myself included) also expressed interest in offering their contests to the Universal Cup, as we all really love this initiative and want to support it, and, of course, would also love to get more exposure to the content that we create.

So, we ended up proposing 3 more contests to the Universal Cup, and I see that it was somewhat upsetting to some of our participants, as they paid the participation fees expecting an exclusive experience, but could have participated in the contests during the Universal Cup instead. Here are the main reasons why I felt like allowing the contests to be used in the Universal Cup is okay:

Most of the contests are original, and were prepared specifically for the camp. Major part of the fees is going towards authors compensation, and it is likely that the contests would not exist as they are, if not for the camp;
I feel like it is generally better if, after being used for their main purpose, the contests are made public, rather than if they just rot away in the author's Polygon, as it serves the competitive programming community as a whole;
Other camps typically do not restrict authors from re-using the contests. People often publish their contests to gym right after Ptz (or after a silence window, if they're re-used in OpenCup or other camps), and most of the contests are also published to opentrains;
It is generally good for the camp, especially right after it is established, to get more exposure to the community, so that people could better understand what contest difficulty and quality they should expect in the next iterations;
But making people feel upset or deceived because of the re-used contests is totally not something we intended, and I apologize to anyone who may feel this way as a result. It wasn't planned in advance that so much contests would be re-used so soon, and I feel that we failed to be transparent or considerate enough about it.

I see several options on how it can be improved for future iterations, such as, for example, making some stricter regulations on how many contests can be shared with major competitions like UniCup, or establishing a longer silence window before they can be published. I don't have a concrete decision here yet, but what I can promise is that our policies in this regard will be clear, transparent and upfront for future iterations of the camp.

Issues with eolymp
We got a lot of feedback regarding our judging system, eolymp. The system is, indeed, a bit raw at the moment, as, to my knowledge, it's only in the beta as a service for external competitions. Nevertheless, I have a very positive and optimistic outlook for the system, as the eolymp team was very helpful and responsive to any questions that we had during the contests.

We have collected all the feedback from participants regarding the system, and shared it with the eolymp team, so I hope that all or the vast majority of issues would be resolved by the next iteration of the camp.

Quality of the contests
Some of participants mentioned that, as contests are mostly prepared by a single author, the style of each contest affects the overall algorithmic topics of the problems within a contest too much, making the contests a bit unbalanced. Another piece if feedback was that there was a bit too much of math problems.

While I think that it is important to train on contests of different styles (and different biases towards some topics), I generally agree that almost every contest had such bias, and many of them were skewed towards mathematics, so we will try to diversify a bit contest authors used within some of the contests for future iterations. Thanks for the feedback!

Quality of the editorials
A lot of feedback mentioned that some editorials were difficult to understand due to being hard to follow, or using references to some advanced techniques, or sometimes even research papers. There were also mentions of technical issues.

As for the technical issues, it was quite hard to avoid them, given that it's the first time we're having such experience. Surely, we know much more about how editorials can be organized now, and it should also warrant less technical issues. As for improving editorials in general, we got a lot of feedback that we should provide more explanation for harder problems and the referenced techniques, and should use more slides, as the participants seemed to really like hand-drawn slides by -is-this-fft-. Duly noted, we will communicate this to our authors in the future iterations, and also try our best to explain more when we're making analyses onsite!

Discounts
When we announced the camp, we have only offered discounts to Ukrainian teams, and the teams that are skipping some of the contests. While reaching out to participants globally, we got a lot of requests for a discount, which we had to decline, as we did not plan for it originally, and felt like it wouldn't be fair to offer discounts beyond what was initially announced publicly.

However, we recognize that there is a significant difference in global markets which make it especially hard for participants from some regions to participate. For the next iteration of the camp, we will consider what we can do to make the camp more accessible to people from troubled regions (possibly, with merit-based criteria). I apologize to those whom we couldn't provide this opportunity for this iteration. Perhaps, you may participate in some of the contests virtually, as significant amount of them is mirrored in the Universal Cup now?

Intersection with SWERC
Unfortunately, we were not able to schedule the camp in a way that would allow us to avoid the intersection with SWERC. We had very small window for scheduling the camp, as moving the dates further would overlap with the studies at the university, and moving it earlier would overlap with the Petrozavodsk camp, and leave our participants too little time to make a decision on whether they want to attend.

Ultimately, we scheduled the camp in a way that it only intersects with SWERC on first few days, and suggesting SWERC teams to participate partially (with appropriate discounts). It seems that it was still a very inconvenient option for them, as only 2 teams decided to attend (online). We will try our best to schedule the camp better next time, to avoid such overlaps.

To end on a bright note, we're looking forward to future iterations of the camp, and will do our best to make them better :)

Tags ocpc, osijek, training camp
Vote: I like it+328Vote: I do not like it
Author adamantPublication date 10 months agoComments 1

The 1st Universal Cup. Stage 7: Zaporizhzhia

By adamant, history, 10 months ago, In English
Hello everyone! The 7th Stage of the 1st Universal Cup: Zaporizhzhia, will be held on March 11th, 2023.

The contest is based on the Day 2: Oleksandr Kulkov Contest 3 from the Osijek Competitive Programming Camp 2023. Please, don't participate if you have seen these problems.

Authors of the problems: adamant, fedimser, I would also like to thank -is-this-fft- for the help with the preparation of the contest.

We hope you will like the problems!

You can participate in the contest in the following three time windows:

Mar 11th 13:00 — 18:00 (UTC +8)
Mar 11th 19:00 — 24:00 (UTC +8)
Mar 12th 02:00 — 07:00 (UTC +8)
Please note that you can see two scoreboards in DOMjudge. The 'Local Scoreboard' shows the standings ONLY IN THE CURRENT TIME WINDOW. And the 'Combined Scoreboard' shows all participants, including the onsite participants, and the cup participants in the previous time windows.

Contest link: https://domjudge.qoj.ac/

Universal Cup Scoreboard: https://qoj.ac/ucup/scoreboard

About Universal Cup:

Universal Cup is a non-profit organization dedicated to providing trainings for competitive programming teams. Up to now, there are more than 200 teams from all over the world registering for Universal Cup.

A more detailed introduction: https://codeforces.com/blog/entry/111672

Register a new team: https://ucup.ac/register

 Announcement of OCPC 2023, Oleksandr Kulkov Contest 3
Tags ucup, universal cup, osijek, ocpc

Oleksandr Kulkov Contest 3 in gym + tutorial and comments

By adamant, history, 9 months ago, In English
Hi everyone!

As the Universal Cup stage 7 which mirrored my contest from the Osijek Competitive Programming Camp 2023 just concluded, I decided to also upload the contest to gym as OCPC 2023, Oleksandr Kulkov Contest 3, and post the tutorial here. I hope you enjoyed the contest!

My previous ICPC-style contests:

Oleksandr Kulkov Contest 1 in gym
Oleksandr Kulkov Contest 2 in gym
Compiled PDF Tutorial for all problems: link.

To make it worth a separate blog post, I will also add some brief meta comments :)

P.S. Congratulations to the team USA1 (ecnerwala, ksun48, scott_wu) for solving all the problems in-contest!

104234A - Square Sum
Author: adamant

In number theory, it is a semi well-known fact that the number of solutions to 𝑥2+𝑦2≡𝑧(mod𝑝)
 only depends on whether 𝑧
 is divisible by 𝑝
. In particular, for 𝑧=1
 the number of solutions is found as

𝑝−(−1𝑝),
where (−1𝑝)
 is the Legendre symbol.

This fact is quite interesting on its own and it got me curious on how to count the solutions for generic modulo 𝑚
. While it is considered standard fact that modulo 𝑚
 is equivalent to modulo 𝑝𝑘
 if you factorize it, I rarely ever saw problems that actually rely on it explicitly, so I felt like there should be one.

Besides that, while the solution expects transition from 𝑝𝑘−1
 to 𝑝𝑘
, I believe one could as well transition from 𝑝𝑘
 to 𝑝2𝑘
, making for a much more efficient solution of the factorization of 𝑚
 is known and much larger. Of course, this idea would be very similar to Hensel lifting, also known in polynomial context as Newton's method.

Tutorial
104234B - Super Meat Bros
Author: adamant
See also: Hadamard product and binomial convolution of linear recurrences.

I saw several problems which required one to compute characteristic polynomial for the Hadamard product of two linear recurrences, but not for binomial convolution, despite the latter being based on the same ideas. Well, this problem is aimed to make binomial convolution of linear recurrences represented among competitive programming problems too :)

Tutorial
104234C - Testing Subjects Usually Die
Author: adamant

A version of this problem with 𝑐=0
 and 𝑐=1
 was proposed to me by a colleague when I was working at think-cell. It got me curious, how the transition from 𝑐=0
 to 𝑐=1
 would look like if we make it continuous, hence it inspired me to make this problem.

Tutorial
104234D - Triterminant
Author: adamant
See also: What are the properties of this polynomial sequence?

I actually came up with this polynomial sequence quite a long time ago, and there was even a reply by Terry Tao himself on the mathoverflow question about it that I asked. Since then, with the help of gamegame, I was able to develop much more intuitive and simple explanation for its major properties. There was another problem partially relying on this sequence: 901B - GCD of Polynomials.

One could use this sequence to get Accepted on that problem (though there were much simpler constructions too).

Tutorial
104234E - Garbage Disposal
Author: adamant

There were several harder ideas revolving around this problem. You may try to solve them if you feel bored:

For each garbage type, you know its amount and for each garbage bin you know its capacity. Is it possible to dispose of all garbage?
Same question as above, but the criterion is gcd(𝑥,𝑦)>1
 instead of gcd(𝑥,𝑦)=1
.
Tutorial
104234F - Palindromic Polynomial
Author: fedimser

The problem turned out much harder than I anticipated, and requiring some decent amount of casework. Nevertheless, I find the fact that 𝑃(𝑥)=𝑥𝑛𝑃(𝑥−1)
 for such polynomials, and how it is used in the solution, very beautiful (even though a bit well-known).

Tutorial
104234G - Palindromic Differences
Author: fedimser

The initial version of the problem also guaranteed that the input array is a permutation, but we had to change this, as in this way the solution might've been too OEISable.

Tutorial
104234H - Graph Isomorphism
Author: adamant

While the intended solution relies on some advanced group theoretic facts (see e.g. here), I still felt like the solution itself is cute and could justify the problem. I also hope that there is a simpler explanation for the solution specifically in the graph case (by looking on the degrees, perhaps). Have you found a simpler solution?

Tutorial
104234I - DAG Generation
Author: adamant

Initially, I wanted the problem about generating rooted trees, rather than DAGs. In this case, one would have to solve the equation

(𝑥∂∂𝑥)𝑘𝑇(𝑥)=𝑥𝑒𝑇(𝑥),
or similar to check if 𝑘
 generated trees are same. Then, one would use generalized CDQ convolution to solve it, and the equation itself follows from the number of topological orderings of a tree. But, ultimately, I decided that it would be more interesting to explore the connection between DAGs and their topological orderings, as I thought that the change in summation ordering here (to sum up DAGs for given topsort instead of topsorts for given DAG) is neat. One downside is that in this way the solution is formulaic and OEISable, perhaps?

Tutorial
104234J - Persian Casino
Author: adamant

Well, not much to comment here. I like Prince of Persia games, esp. the sands of time trilogy, and I would imagine that this would happen if Prince was using his time unwind abilities in a more casual manner. After all, I did a lot of save-load cheating with in-game casinos in Yakuza games :)

Tutorial
104234K - Determinant, or...?
Author: adamant

While most people probably solved it using the identity for the determinant of block matrices, my original inspiration was the formula for the determinant of circulant matrices. While circulant matrices correspond to cyclic convolutions, the matrix in question here corresponds (to some extent) to the or-convolution of two arrays, which provides some additional context as to why the solution is so similar to inverse sum over subsets, which is also used to compute such convolutions.

Tutorial
104234L - Directed Vertex Cacti
Author: adamant

As probably majority of participants who solved it during the contest, I found the fact accidentally and couldn't believe my eyes. Only much later, Endagorion and Electric-tric helped me make sense of it and find an actual meaningful bijective proof for the answer.

Tutorial
104234M - Siteswap
Author: adamant

I actually like juggling (though I can only do so with 3 balls), and the siteswap concept is quite nice, so I just wanted to make more people aware of it.

Tutorial
 Tutorial of OCPC 2023, Oleksandr Kulkov Contest 3
Tags okc, ocpc, universal cup, tutorial

Unexpected application of cosines

By adamant, history, 9 months ago, In English
Hi everyone!

This blog is inspired by some contestant solutions to 104234F - Palindromic Polynomial. In short, problem asks you to find a polynomial such that 𝐴(𝑥𝑖)=𝑦𝑖
 for several given pairs (𝑥𝑖,𝑦𝑖)
, and the coefficients of 𝐴(𝑥)
 read the same left-to-right and right-to-left (i.e., they form a palindrome). The intended solution utilizes the fact that for such polynomial, it always holds that

𝐴(𝑥)=𝑥𝑛𝐴(𝑥−1),
as 𝑥𝑛𝐴(𝑥−1)
 is exactly the polynomial 𝐴(𝑥)
 with reversed coefficients (assuming the degree is 𝑛
).

Representing palindromic polynomials with polynomials in 𝑥+1𝑥
Several solutions from contestants, however, relied on a different criterion for 𝐴(𝑥)
. Rather than using the identity above, participants found out that palindromic polynomials can always be represented as

𝐴(𝑥)=𝑥𝑑𝐵(𝑥+1𝑥),
when its degree 2𝑑
 is even, or as

𝐴(𝑥)=𝑥𝑑(1+𝑥)𝐵(𝑥+1𝑥),
when its degree 2𝑑+1
 is odd. In both cases, 𝐵(𝑥)
 is a polynomial of degree 𝑑
. This representation allows for much simpler solution, but in this blog we will talk about why it exists in the first place, rather than how to use it to solve the problem.

Thanks to ecnerwala for explaining me this approach in more detail!

Getting rid of odd degrees
First things first, we should notice that a palindromic polynomial 𝐴(𝑥)
 of odd degree always has a root 𝑥=−1
, as it is an alternating sum of the polynomial's coefficients, and coefficients from the right half go into this sum with different sign from corresponding coefficients in the left half. Then, dividing 𝐴(𝑥)
 by 1+𝑥
 we get a palindromic polynomial of degree 2𝑑
 instead of 2𝑑+1
.

This allows us to focus on the polynomials of even degree.

Representing polynomials of even degrees with 𝑥𝑘+1𝑥𝑘
First of all we may notice that palindromic polynomials of even degree 2𝑑
 can always be represented as

𝐴(𝑥)=∑𝑖=0𝑑𝑎𝑖(𝑥𝑑−𝑖+𝑥𝑑+𝑖)=𝑥𝑑∑𝑖=0𝑑𝑎𝑖(𝑥𝑖+1𝑥𝑖)
for some sequence 𝑎0,𝑎1,…,𝑎𝑑
 which is easily derived from the coefficients of 𝐴(𝑥)
. Now we should show that the sum

∑𝑖=0𝑑𝑎𝑖(𝑥𝑖+1𝑥𝑖)
can be rewritten as

𝐵(𝑥+1𝑥)=∑𝑖=0𝑑𝑏𝑖(𝑥+1𝑥)𝑖
with some sequence 𝑏0,𝑏1,…,𝑏𝑑
.

Changing basis from 𝑥𝑘+1𝑥𝑘
 to (𝑥+1𝑥)𝑘
Consider the sequence of Laurent polynomials (i.e. polynomials, in which negative degrees are allowed)

𝐴𝑘=(𝑥+1𝑥)𝑘,
and another sequence of Laurent polynomials

𝐵𝑘=𝑥𝑘+1𝑥𝑘.
As it turns out, these two sequences have the same linear span. What it practically means, is that every element of 𝐵𝑘
 may be represented as a finite linear combination of the elements of 𝐴𝑘
, and vice versa. It is somewhat trivial in one direction:

𝐴𝑘=∑𝑖=0𝑘(𝑘𝑖)𝑥𝑘−2𝑖=∑𝑖=0⌊𝑘/2⌋(𝑘𝑖)(𝑥𝑘−2𝑖+1𝑥𝑘−2𝑖).
With some special consideration for even 𝑘
, as the middle coefficient should be additionally divided by 2
. You can represent it with lower triangular matrix with 1
 on the diagonal (except for the first row where it's 12
), which would mean that the matrix is invertible and there is, indeed, the representation for 𝐵𝑘
 in terms of 𝐴𝑘
 as well.

But what about cosines?
We will need a bit of complex numbers magic here. The explanation above, while technically correct, doesn't really shed any light on why it is true at all. To better understand it, consider the substitution 𝑥=𝑒𝑖𝜃
, then one can use the explicit formula for cos𝜃
:

cos𝜃=𝑒𝑖𝜃+𝑒−𝑖𝜃2,
and notice that

𝐴𝑘=2𝑘cos𝑘𝜃,𝐵𝑘=2cos𝑘𝜃.
Then, it is a well known fact that cos𝑘𝜃
 can be represented in terms of 1,cos𝜃,cos2𝜃,…,cos𝑘𝜃
. To prove it, consider the identity

𝑒𝑖𝜃=cos𝜃+𝑖sin𝜃,
then on one hand

𝑒𝑖𝑘𝜃=cos𝑘𝜃+𝑖sin𝑘𝜃,
on the other hand

𝑒𝑖𝑘𝜃=(𝑒𝑖𝜃)𝑘=(cos𝜃+𝑖sin𝜃)𝑘=∑𝑗=0𝑘(𝑘𝑗)𝑖𝑗sin𝑗𝜃cos𝑘−𝑗𝜃.
Now, we're only interested in the real part of the said expression, hence the one with even 𝑗
. Using sin2𝜃=1−cos2𝜃
, we get

cos𝑘𝜃=∑𝑗=0⌊𝑘/2⌋(𝑘2𝑗)(cos2𝜃−1)𝑗cos𝑘−2𝑗𝜃.
Hence, if we define

𝑇𝑘(𝑥)=∑𝑗=0⌊𝑘/2⌋(𝑘2𝑗)(𝑥2−1)𝑗𝑥𝑘−2𝑗,
we can see that 𝑇𝑘(cos𝑥)=cos𝑘𝑥
. The sequence of polynomials 𝑇0,𝑇1,𝑇2,…
 defined in such way is also known as Chebyshev polynomials. In the context of our problem, it would mean that

𝑇𝑘(𝑥+𝑥−12)=𝑥𝑘+𝑥−𝑘2,
meaning that the desired transition could be expressed as

∑𝑖=0𝑑𝑎𝑖(𝑥𝑖+1𝑥𝑖)=2∑𝑖=0𝑑𝑎𝑖𝑇𝑖(𝑥+𝑥−12)=𝐵(𝑥+1𝑥).
Tags tutorial, cosinus, polynomials, chebyshev

Fourier transform on groups. Part 1: Irreducible representations

By adamant, history, 9 months ago, In English
Hi everyone!

Consider the following problem. You're given two arrays 𝑎1,𝑎2,…,𝑎𝑛!
 and 𝑏1,𝑏2,…,𝑏𝑛!
.

You need to compute the array 𝑐1,𝑐2,…,𝑐𝑛!
, such that

𝑐𝑘=∑𝑖∘𝑗=𝑘𝑎𝑖𝑏𝑗,
where 𝑖∘𝑗
 denotes the composition of the 𝑖
-th and the 𝑗
-th lexicographically smallest permutations of 𝑛
 elements.

Generally, typical approach to such things would be to escape into space in which the convolution corresponds to the component-wise product of two vectors, and then to transform back into initial space. For example, with the summation rule being 𝑖+𝑗≡𝑘(mod𝑛)
, it is well known that the corresponding transform is exactly the discrete Fourier transform of vectors 𝑎𝑖
 and 𝑏𝑗
.

But with more generic groups, things may be much more complicated, and I will write a series of blog posts explaining it.

Fourier transform on groups. Part 1: Irreducible representations
Fourier transform on groups. Part 2: Characters
Prerequisites
Difficulty: ★★★★★

One is expected to have some reasonable intuition with group theory and be well-familiar with linear algebra, and be familiar with some tangent definitions (e.g. what is a ring, what is a field, what is a linear span, what is a vector space, dot product, hermite product etc). Familiarity with discrete Fourier transform, and similar transforms used to compute convolutions is also essential.

Useful definitions from group theory and linear algebra


What kind of transform are we looking for?
For now, let's work in a more generic setting. Assume that we have an operation ∘
, and we want to find the convolution

𝑐𝑘=∑𝑖∘𝑗=𝑘𝑎𝑖𝑏𝑗.
Let's analyse what we are doing for different operations:

For 𝑖∘𝑗=(𝑖+𝑗)mod𝑛
, we use the fact that the DFT of 𝑐
 is the point-wise product of DFTs of 𝑎
 and 𝑏
;
For 𝑖∘𝑗=𝑖⊕𝑗
, we use the fact that the WHT of 𝑐
 is the point-wise product of WHTs of 𝑎
 and 𝑏
;
For 𝑖∘𝑗=𝑖|𝑗
, we use the fact that sum over subset transform of 𝑐
 is the point-wise product of such transforms of 𝑎
 and 𝑏
.
One way to define a unifying framework here is to introduce a formal variable 𝑥
 such that 𝑥𝑖𝑥𝑗=𝑥𝑖∘𝑗
. You can read further about such approach in this blog about subset convolution. Using such formal variable, we can define the convolution as

𝐶(𝑥)=∑𝑘∈𝐺𝑐𝑘𝑥𝑘=∑𝑘∈𝐺𝑥𝑘∑𝑖∘𝑗=𝑘𝑎𝑖𝑏𝑗=∑𝑖∈𝐺∑𝑗∈𝐺𝑎𝑖𝑏𝑗𝑥𝑖∘𝑗=𝐴(𝑥)𝐵(𝑥).
Then, we find some sequence 𝑥1,𝑥2,…
 of objects such that the formula 𝑥𝑖𝑡𝑥𝑗𝑡=𝑥𝑖∘𝑗𝑡
 is true for every 𝑡
, and then we could evaluate the "polynomials" 𝐴(𝑥)
 and 𝐵(𝑥)
 in 𝑥1,𝑥2,…
 and interpolate the polynomial 𝐶(𝑥)
 from 𝐶(𝑥𝑡)=𝐴(𝑥𝑡)𝐵(𝑥𝑡)
.

Examples
In the case of 𝑖+𝑗(mod𝑛)
, the sequence 𝑥1,𝑥2,…
 consists of the roots of the polynomial 𝑥𝑛−1
, as

𝑥𝑖𝑡𝑥𝑗𝑡=𝑥(𝑖+𝑗)mod𝑛𝑡
for any root 𝑥𝑡
 of 𝑥𝑛−1
. For 𝑖⊕𝑗
, specific 𝑥𝑡
 are corner points of the hypercube [−1,1]𝑛
, so 𝑥𝑖𝑡
 rewrites as

𝑥𝑖1𝑡1𝑥𝑖2𝑡2…𝑥𝑖𝑛𝑡𝑛,
where 𝑥𝑡𝑘
 is the 𝑘
-th coordinate of 𝑥𝑡
 (that is, either 1
 or −1
), and 𝑖1,…,𝑖𝑛
 is the binary representation of 𝑖
. In this way, it held that

𝑥𝑖𝑡𝑥𝑗𝑡=𝑥𝑖1𝑡1𝑥𝑖2𝑡2…𝑥𝑖𝑛𝑡𝑛𝑥𝑗1𝑡1𝑥𝑗2𝑡2…𝑥𝑗𝑛𝑡𝑛=𝑥𝑖1⊕𝑗1𝑡1𝑥𝑖2⊕𝑗2𝑡2…𝑥𝑖𝑛⊕𝑗𝑛𝑡𝑛=𝑥𝑖⊕𝑗𝑡.
And with 𝑖|𝑗
, it's also very similar, except for the evaluation takes place in [0,1]𝑛
 instead of [−1,1]𝑛
.

Formalization
When talking about 𝑥1,𝑥2,…
 above, we named them "objects", and we required that we're able to raise them to the power 𝑖
, where 𝑖
 is the element of a set on which the operation 𝑖∘𝑗
 is defined. To make things formal, we may interpret each 𝑥𝑡
 as a function

𝑥𝑡:𝐺→𝑅,
such that

𝑥𝑡(𝑖)⋅𝑥𝑡(𝑗)=𝑥𝑡(𝑖∘𝑗),
where 𝑅
 is some ring (complex numbers or remainders modulo 𝑚
 for the discrete Fourier transform). For the sake of notational convenience, we will continue to denote 𝑥𝑡(𝑖)
 as 𝑥𝑖𝑡
 to treat it as an object with the operation ∘
 in its powers.

Def. 1. Let 𝐺1
 and 𝐺2
 be two sets with binary operations ∘1
 and ∘2
 defined on them. A function 𝑥:𝐺1→𝐺2
, such that

𝑥(𝑖)∘2𝑥(𝑗)=𝑥(𝑖∘1𝑗)
is called a homomorphism from 𝐺1
 to 𝐺2
.

In other words, what we're really looking for as the "objects" for evaluation and interpolation of such polynomials are homomorphisms from the set 𝐺
 with operation ∘
 into the multiplicative group of some ring.

Group representations
The functions 𝑥1,𝑥2,…
 are defined above very generically, as they do not expect any additional constraints on the set 𝐺
, on which the operation ∘
 is defined, nor on the ring 𝑅
. To make it more meaningful, we will now have to restrict the definition a bit.

Def. 2. Let 𝑉
 be a vector space. Then, its automorphism group Aut𝑉
 is the group of all invertible linear maps of 𝑉
 on itself.

Def. 3. A representation of a group 𝐺
 in a vector space 𝑉
 over ℂ
 is a homomorphism 𝑥:𝐺→Aut𝑉
.

Def. 4. The dimension 𝑛
 of the vector space 𝑉
 is called the degree of the representation 𝑥
.

Note: Generally, the elements of 𝑉
 can be identified (by picking a basis) with the elements of the vector space ℂ𝑛
, and the elements of Aut𝑉
 can be identified with the elements of the general linear group 𝐺𝐿(𝑛,ℂ)
, that is with invertible matrices of size 𝑛×𝑛
. It is useful to keep in mind when dealing with implementations, but we will stick to the coordinate-free approach for now, to better understand the high-level underlying structure of objects at hand.

Example. For the cyclic group 𝐶𝑛
, we can define a parametrized family of representations on 𝐺𝐿(1,ℂ)
:

𝑥𝑘(𝑗)=𝑥𝑗𝑘=𝑒2𝜋𝑖𝑗𝑘𝑛,
in which case it indeed holds that

𝑥𝑎𝑘⋅𝑥𝑏𝑘=𝑒2𝜋𝑖𝑎𝑘𝑛𝑒2𝜋𝑖𝑏𝑘𝑛=𝑒2𝜋𝑖(𝑎+𝑏)𝑘𝑛=𝑥(𝑎+𝑏)mod𝑛𝑘.
As you may recognize, this family of representations is used to compute the standard discrete Fourier transform.

Using the definitions above, we can say that we should find a sequence of representations 𝑥1,𝑥2,…
, then we compute 𝐴(𝑥𝑡)
 and 𝐵(𝑥𝑡)
 for each 𝑡
, from which we get the expression for 𝐶(𝑥𝑡)
 as 𝐶(𝑥𝑡)=𝐴(𝑥𝑡)⋅𝐵(𝑥𝑡)
. While this underlines the general idea, it's still unclear how to choose the set of representations which would allow to invert the transform and recover the sequence 𝑐
.

What representations are considered disitinct?
One of important things that was needed for interpolation of polynomials to be possible, is that the points, in which interpolation is done, are distinct. Let's think for a moment, how to determine if two representations 𝑥
 and 𝑦
 are distinct. Let's say that 𝑥
 acts on the vector space 𝑉
 and 𝑦
 acts on vector space 𝑊
. We can say that the representations 𝑥
 and 𝑦
 are similar if we can map their underlying spaces to each other in such a way that there is a correspondence between how 𝑥
 acts on 𝑉
, and how 𝑦
 acts on 𝑊
. Let's formalize it.


An equivariant map 𝑓
 should be consistent with how 𝑥𝑔
 and 𝑦𝑔
 act on 𝑉
 and 𝑊
Def. 5. Let 𝑥
 and 𝑦
 be representations of 𝐺
 in vector spaces 𝑉
 and 𝑊
 correspondingly. An equivariant map from 𝑉
 to 𝑊
 is a linear map 𝑓:𝑉→𝑊
 such that for any element of the group 𝑔∈𝐺
, and any element of the vector space 𝑣∈𝑉
, applying 𝑥𝑔
 to 𝑣
 and then moving to 𝑊
 via 𝑓
 is the same as first moving to 𝑊
 via 𝑓
, and then applying 𝑦𝑔
, that is 𝑓(𝑥𝑔(𝑣))=𝑦𝑔(𝑓(𝑣))
.

In other words, 𝑓𝑥𝑔=𝑦𝑔𝑓
 for any element 𝑔∈𝐺
, where concatenation denotes the composition of linear maps. If the equivariant map 𝑓
 is bijective, it is called an isomorphism of the representations 𝑥
 and 𝑦
, and the representations themself are called isomorphic. We will denote representation isomorphism as 𝑥∼𝑦
.

Note: If you're familiar with linear algebra, you can recognize that the isomorphism of representations can also be defined as

𝑥𝑔=𝑓−1𝑦𝑔𝑓
that should hold for some invertible linear map 𝑓:𝑉→𝑊
 and all 𝑔∈𝐺
. From linear algebra perspective it corresponds to how linear maps change after change of basis denoted by the transition matrix 𝑓
. Note that if 𝑥
 and 𝑦
 are isomorphic in the sense defined above, then for any polynomial 𝐴(𝑥)
 it holds that 𝐴(𝑥)=𝑓−1𝐴(𝑦)𝑓
, meaning that evaluating in two representations that are isomorphic to each other does not provide any more information than evaluating in only one of them.

Unitary representations
Consider a representation 𝑥:𝐺→Aut𝑉
. From the previous section it follows that any representation obtained by changing the basis of 𝑉
 is isomorphic to 𝑥
. Of all the possible ways to pick the basis of 𝑉
, there is one that benefits us greatly when we analyze it.

As a finite-dimensional space, 𝑉
 can be equipped with an inner product (⋅,⋅)
. The concept of inner product generalizes the notion of dot product in Euclidean spaces. Let's recall the definition:

Recall the definition
It turns out, that when 𝐺
 is finite, it is always possible to define the linear product ⟨⋅,⋅⟩
 on 𝑉
 in a way that is consistent with the representation, that is, for any 𝑢,𝑣∈𝑉
 and any 𝑔∈𝐺
, it would hold that ⟨𝑢,𝑣⟩=⟨𝑥𝑔𝑢,𝑥𝑔𝑣⟩
.

Proof
In matrix form it would mean that 𝑣∗𝑢=𝑣∗(𝑥𝑔)∗𝑥𝑔𝑢
, from which follows that (𝑥𝑔)∗𝑥𝑔=𝐼
, hence (𝑥𝑔)∗=(𝑥𝑔)−1
 for each 𝑔∈𝐺
. It means that in the orthonormal basis associated with this inner product (which can be constructed via Gram-Schmidt process) every 𝑥𝑔
 is a unitary transformation (analogue of orthogonal transformations for vector spaces over complex numbers).

The existence of such inner product for representations is crucial in establishing some facts below.

Irreducible representations
With more typical convolutions, the representations were always 1
-dimensional. One important observation about potentially multi-dimensional representations is that they sometimes contain other, "smaller" representations in them.

Def. 6. A representation 𝑥
 is called reducible if the space 𝑉
 has a non-trivial subspace 𝑊
 that is invariant under each 𝑥𝑔
.

Def. 7. The representation 𝑦
 obtained by reducing 𝑥
 on an invariant subspace 𝑊
 is called a subrepresentation of 𝑥
.

Example. Consider the cyclic group 𝐶2
 and the representation 𝑥
 defined as

𝑥0=(1001),𝑥1=(0110).
It acts on the 2
-dimensional space ℂ2
. Then, 𝑥0
 applied to a vector (𝑥,𝑦)
 doesn't change it, while 𝑥1
 swaps the coordinates, making it into (𝑦,𝑥)
. The transforms 𝑥0
 and 𝑥1
 have two common non-trivial invariant subspaces, each spanned on the vectors (1,1)
 and (1,−1)
 correspondingly. This allows to reduce the whole representation on 𝐺𝐿(2,ℂ)
 to two subrepresentations on 𝐺𝐿(1,ℂ)
.

The first representation is 𝑥0=1
 and 𝑥1=1
, in the subspace spanned on (1,1)
, while the second representation is 𝑥0=1
 and 𝑥1=−1
 in the subspace spanned on (1,−1)
. In this way, we can reduce the initial 2
-dimensional representation to a pair of 1
-dimensional representations.

Def. 8. The direct sum of representations 𝑥
 and 𝑦
 in vector spaces 𝑉
 and 𝑊
 is a representation 𝑧
 such that for any 𝑔∈𝐺
, the transformation 𝑧𝑔
 is a direct sum of 𝑥𝑔
 and 𝑦𝑔
, acting on the direct sum of vector spaces 𝑉⊕𝑊
.

Note: In other words, each element of the vector space 𝑉⊕𝑊
 can be represented as a pair (𝑣,𝑤)
 of an element 𝑣∈𝑉
 and an element 𝑤∈𝑊
. Then, 𝑧𝑔
 applied to such vector would yield a vector (𝑥𝑔𝑣,𝑦𝑔𝑤)
. In matrix form, we can rewrite in a block-diagonal manner:

𝑧𝑔(𝑣𝑤)=(𝑥𝑔00𝑦𝑔)(𝑣𝑤)=(𝑥𝑔𝑣𝑦𝑔𝑤).
Def. 9. A representation 𝑥
 is called decomposible if it is isomorphic to a direct sum of non-trivial representations 𝑥1
 and 𝑥2
.

It can be proven that if 𝑥
 is a reducible representation of a finite group, then it is also decomposible. In other words, for any representation 𝑥
 of a finite group, there is a set of irreducible representations 𝑥1,…,𝑥𝑘
 such that 𝑥∼𝑥1+⋯+𝑥𝑘
. This result is also known in the representation theory as Maschke’s theorem.

Proof
Why irreducible representations are of interest?
Assume that the representation 𝑥
 is the direct sum of 𝑥1
 and 𝑥2
, that is (in the basis corresponding to 𝑉=𝑉1⊕𝑉2
)

𝑥𝑔=(𝑥𝑔100𝑥𝑔2),
then the result of computing the "polynomial" 𝐴(𝑥)
 can be represented as

𝐴(𝑥)=∑𝑔∈𝐺𝑎𝑔𝑥𝑔=∑𝑔∈𝐺𝑎𝑔(𝑥𝑔100𝑥𝑔2)=(𝐴(𝑥1)00𝐴(𝑥2)).
Hence instead of the value 𝐴(𝑥)
 we may consider the pair of values 𝐴(𝑥1)
 and 𝐴(𝑥2)
. So it makes sense to only consider irreducible representations, as they carry the same amount of information about 𝐺
 as their direct sum.

Fourier transform on finite groups
Let's do our first stop here and reflect on what we learnt so far. We learnt how to compare representations with each other (by telling if they are isomorphic) and we learnt that only irreducible representations are of interest, as any other representations can be expressed as direct sum of irreducible ones. This already allows us to define the direct (but not the inverse) Fourier transform on finite groups.

Def. 10. Let

𝐴(𝑥)=∑𝑔∈𝐺𝑎𝑔𝑥𝑔,
where 𝑥
 is a placeholder "variable". Then the Fourier transform of 𝐴(𝑥)
 is the sequence 𝐴(𝑥1),𝐴(𝑥2),…
, where 𝑥1,𝑥2,…
 are pairwise distinct (not isomorphic to each other) irreducible representations of 𝐺
.

Well, this partially deals with the evaluation part, but still leaves us with a lot of questions. How many irreducible representations are there? Are they even finite? Is it possible to compute the transform efficiently for generic groups? And, of course, how the hell do we do the inverse transform from this? We will deal with these questions step by step over the next parts of the series, so stay tuned!

Tags tutorial, hardcore, group theory, representation theory, convolution, fast fourier transform

How to read my blogs

By adamant, history, 9 months ago, In English
Hi everyone!

I saw that a lot of people write meta blogs recently. As my blog count approaches 90, I feel like it may make a lot of sense to write another one that kind of focuses on the philosophy behind my blogs and explains potential readers what they should expect from my blogs and how to approach them. So, without further ado...

Tldr.
Read my blogs if and only if the general setup itself appeals to you. Don't feel obliged to do it to get better at competitive programming.
Don't be driven away by seemingly dense math notation if the topic looks interesting. Ask questions if something is difficult to comprehend, I will explain in a more detail and it will help others too.
Be ready to fill some possible gaps in the story yourself, as an exercise.
Be open-minded about analyzing setup in general, rather than working towards predetermined goal.
Be prepared to see too little explanation on how abstract nonsense from my blogs connects with concrete applications.
Give feedback, it motivates me a lot and helps make future blogs better.
Be interested
First thing you should decide for yourself when reading my blogs is to understand if it is of interest to you. And I want to be as transparent here as possible, my blogs are not designed to be useful for competitive programming. I mean, of course I have some blogs that might be, but only in very rare circumstances I have a goal of helping someone perform better in competitive programming when writing a blog.

My main motivation is to share something that I found interesting or fascinating in one way another, and I mostly expect the same motivation from any potential readers. So, in my opinion, most of the time you can decide on whether you should read my blog or not by reading the title and skimming through first 2-3 paragraphs. Does the setup and the problem at hand sound exciting to you? If yes, please tag along for the journey! If not, you probably won't miss anything important by skipping it :)

Don't be afraid
One piece of feedback that I often get is that my blogs are scary, and many potential readers think that they require some very advanced math to comprehend. For example, it seems that many people assumed that my blogs on combinatorial species require some advanced knowledge in category theory. In reality though, while there were some links to category theory stuff in the blog, they only scratched the very surface and very basic definitions of the subject, which I myself only got to know while writing the blog, and all of it was defined in the blog itself, so it was not a requirement to be familiar with the concepts in advance.

I may have a lot of links to advanced stuff, e.g. Wikipedia pages, in my blogs, but their purpose is mostly to provide additional context and some connection to a grander picture, as well as some opportunities for further learning to a curious reader, rather than to indicate some kind of prerequisite for reading the blog. I try, especially in later blogs, to be more or less clear about the hard prerequisites for reading my blogs (and try to keep them low whenever possible).

If you really struggle with understanding something, but the topic is interesting to you, feel free to reach out to me directly, I may try to explain some stuff in a more clear manner, or even improve the blog a bit :)

Be ready to think
Another piece of feedback that I often get is that my blogs are often terse and lack examples. While I try my best to act on it in my later blogs, I think that in learning it is generally very important to be able to come with your own examples and think through tersely described explanations to fill some gaps on your own. I recognize that it might be unpleasant to do so, and I'm trying to ease your pain, but ultimately I think that the best way to really understand something is to read some very high-level description and try to reinvent it based only on that little information that you have.

I think, this is the best way to develop your own intuition, and carve a better, simpler path to something that others take for granted. I spend some crazy amount of time doing just that, trying to reinvent the wheel, to come up with my very own examples and ways to define and do things and this is exactly how I managed to come up with the stuff described here or here or here or here or here or... Well, you get the idea. I learnt and found out so much in exactly this way, I couldn't recommend it enough :)

Accept that the journey is more important than the result
I once received a piece of feedback that there are essentially two ways of explaining things:

From result to research. This is western approach, when we start by clearly defining the end result towards which we are working first, and then explain how we reach it.
From research to result. This approach is more common in the eastern part of the world (say, in Russia). In this way, we do not have a pre-existing result that we're working towards, but rather we start with a generic, somewhat obscure, setup and just analyze it in detail, to learn what results follow from it.
While the first is argued to be much better for understanding, as it is easier for people to connect what we're doing right now with what they're going to get in the end, I vastly prefer the second approach. There are quite a lot of reasons for it:

It is what you naturally do when you approach new open-ended problems. You do not know in advance what you will get when dealing with the problem, you do not even know if you will find out anything at all. You start with a setup, and you find the conclusions about it, you analyze its properties, until you eventually find out something of interest. Thus, more often than not, I write things in this way simply because it corresponds to how I found them out in the first place.
Generally, proving the known result and finding out a new unknown result are vastly different things. A lot of textbooks just give you the theorem and provide with the simplest or shortest proof available. While such proofs may be shorter and easier to comprehend, they throw away what I think is the most important part: reproducibility. You should generally be able to reproduce the result even if you forgot some details about it. And motivation behind the proofs play tremendous role here. If you understand the motivation, you're likely be able to reproduce the proof just using the same natural reasoning. If you don't, most likely you won't be able to reproduce it.
And I can't stress enough how important it is, in my opinion, to provide motivation for any step that you make. When you start with a setup and do not know the result beforehand, motivation most of the times is very simple: you just do what you think is natural to do to analyze the problem. Structuring the blogs in such a way, generally, allow me to avoid a pitfall of many textbooks when they have a sequence of lemmas that eventually allow you to prove the end result, but for each of them, you have no idea why would you ever make such assumption of you were to analyze the setting on your own. Yet, I hear the feedback, so you may notice that in my later blogs I try to provide some additional insight as to what are the core results and applications that we're going to derive.

Think abstractly
Okay, this is a bit of my personal preference towards abstract nonsense over concrete stuff. I feel like, generally, concrete formulations often have a lot of diversions. You may be able to solve concrete instance of the problem, but if it is some kind of approach that may be applied to many similar problems, it is important to understand what is the actual underlying structure of the things at hand that allows them to behave in a similar way.

And it is much better when we're able to unearth this underlying structure that connects different instances of similar problems together. While it may seem that analyzing concrete examples may shed some light on what's going on, my personal experience tends to say that it is too easy to get distracted into a very specific way of doing things and miss the common structure if you work with concrete stuff too much. This is also the reason my blogs lack examples that often. I don't work with them much on my own, and even consider them harmful to some extent :)

Give feedback
I can't thank enough all the people who reached out to me to tell when something in my blogs was difficult to comprehend or plainly wrong. While I may not always be able to right all wrongs, I take feedback very seriously, crave for it (well, yes, I am an attention and validation seeker alright) and try my best to act on it. So, if you see any issues in my blogs (or contrary to it, like it and want to praise it), please feel free to tell! This will help me in writing future blogs, and give me some motivation, as I would know that the blogs are of interest to somebody. I must say that these days it especially warms my heart when someone I barely know approaches me in real life and tells that my blogs helped them in one way or another :)

Tags meta, instruction

Fourier transform on groups. Part 2: Characters

By adamant, history, 9 months ago, In English
Hi everyone!

Consider the following problem. You're given two arrays 𝑎1,𝑎2,…,𝑎𝑛!
 and 𝑏1,𝑏2,…,𝑏𝑛!
.

You need to compute the array 𝑐1,𝑐2,…,𝑐𝑛!
, such that

𝑐𝑘=∑𝑖∘𝑗=𝑘𝑎𝑖𝑏𝑗,
where 𝑖∘𝑗
 denotes the composition of the 𝑖
-th and the 𝑗
-th lexicographically smallest permutations of 𝑛
 elements.

Generally, typical approach to such things would be to escape into space in which the convolution corresponds to the component-wise product of two vectors, and then to transform back into initial space. For example, with the summation rule being 𝑖+𝑗≡𝑘(mod𝑛)
, it is well known that the corresponding transform is exactly the discrete Fourier transform of vectors 𝑎𝑖
 and 𝑏𝑗
.

But with more generic groups, things may be much more complicated. This is the second blog in a series of blog posts explaining it.

Fourier transform on groups. Part 1: Irreducible representations
Fourier transform on groups. Part 2: Characters
Great thanks to Endagorion and Golovanov399 for helping to understand the theory for this part!

In this blog, we will learn the basics of character theory which will allow us to define the inverse Fourier transform on finite groups.

Prerequisites
Difficulty: ★★★★★

One is expected to have some reasonable intuition with group theory and be well-familiar with linear algebra, and be familiar with some tangent definitions (e.g. what is a ring, what is a field, what is a linear span, what is a vector space, dot product, hermite product etc). Familiarity with discrete Fourier transform, and similar transforms used to compute convolutions is also essential.

Hom(𝑉,𝑊)
 as a representation
As this section is particularly dense on the mathematical notation, I decided to hide it under spoiler tag. It is highly recommended to familiarize with it, as it provides a whole lot of necessary intuition and motivation as to why exactly we're introducing the characters the way we do in the next section. However, if you're more interested in concrete results, and are fine with skipping less relevant details, you can proceed to the next section directly.

Or you can see for yourself how deep the rabbit hole goes...
Characters
The result above allows us to introduce the key concept of the representation theory, the characters of the representation.

Def. 3. Let 𝑥
 be the representation of a group 𝐺
 in a vector space 𝑉
. The character of a group element 𝑔∈𝐺
 is defined as

𝜒(𝑔)=tr𝑥𝑔.
Let 𝜒
 and 𝜃
 be the characters of irreducible representations 𝑥
 and 𝑦
. Then, using the results above, we get

1|𝐺|∑𝑔∈𝐺𝜒(𝑔)⋅𝜃(𝑔)∗={1,0,𝑥∼𝑦,𝑥≁𝑦.
The expression above can be treated as an inner product ⟨𝜃,𝜒⟩
 on the vector space ℂ|𝐺|
.

Orthonormality of the characters
One particularly nice property of characters is that if 𝑥∼𝑥1+𝑥2
, and the representations 𝑥1
 and 𝑥2
 have characters 𝜒1
 and 𝜒2
, then the characters 𝜒
 of 𝑥
 can be obtained as the sum of characters of 𝑥1
 and 𝑥2
, that is

𝜒(𝑔)=𝜒1(𝑔)+𝜒2(𝑔).
Let 𝑥1,𝑥2,…
 be the set of pairwise non-isomorphic, irreducible representations, and let 𝜒1,𝜒2,⋯∈ℂ|𝐺|
 be their character vectors. Then, as was already noted above, ⟨𝜒𝑖,𝜒𝑗⟩=𝛿𝑖𝑗
, that is, the inner product is equal to 1
 when 𝑖=𝑗
 and 0
 otherwise. In particular, it means that irreducible characters are linearly independent, so there may be at most |𝐺|
 irreducible representations.

Another corollary here is that every representation 𝑥
 is uniquely determined by its characters (up to isomorphism). Indeed, let

𝑥∼𝑣1𝑥1+𝑣2𝑥2+…,
where 𝑣1,𝑣2,…
 are non-negative integers and 𝑣𝑖𝑥𝑖∼𝑥𝑖+𝑥𝑖+…
, repeated 𝑣𝑖
 times. Then

𝜒=𝑣1𝜒1+𝑣2𝜒2+…,
and the numbers 𝑣1,𝑣2,…
 are uniquely determined as 𝑣𝑖=⟨𝜒,𝜒𝑖⟩
 due to orthonormality of 𝜒1,𝜒2,…
. From this also follows that

⟨𝜒,𝜒⟩=𝑣21+𝑣22+…
Then, since any representation of a finite group can be decomposed into a direct sum of irreducible representations, we can deduce that the representation 𝑥
 of a finite group 𝐺
 is irreducible if and only if for its characters holds

⟨𝜒,𝜒⟩=1|𝐺|∑𝑔∈𝐺𝜒(𝑔)𝜒(𝑔)∗=1.
Regular representation
Def. 4. The regular representation of 𝐺
 is a representation 𝑥
 in the space ℂ|𝐺|
, such that 𝑥𝑔(𝑒ℎ)=𝑒𝑔ℎ
 for a basis 𝑒1,𝑒2,…,𝑒|𝐺|
.

This representation, while being very simple and straight-forward in its definition, has a bunch of very interesting properties in terms of its characters. First of all, we may notice that in this representation, the characters are expressed as

𝑟(𝑔)=tr𝑥𝑔={|𝐺|,0,𝑔=1,𝑔≠1.
This is due to the fact that 𝑥1=𝐼
 is the identity matrix, as 𝑥1𝑒ℎ=𝑒1ℎ=𝑒ℎ
, while any other 𝑥𝑔
 has only zero elements on the diagonal, if it is represented as a matrix, hence it has a zero trace. This fact allows to express the coefficients in the decomposition of the regular representation character 𝑟
 into irreducible characters as

⟨𝑟,𝜒𝑖⟩=1|𝐺|𝑟(1)𝜒∗𝑖(1)=𝜒∗𝑖(1)=dim𝑉𝑖,
where 𝑉𝑖
 is the space of the irreducible representation 𝑥𝑖
. Here we used the fact that 𝜒(1)
 always equates to the dimension of 𝑉
, as 𝑥1
 is always the unit matrix. That being said, each unique irreducible representation 𝑥𝑖
 is included in the regular representation 𝑥
 with multiplicity that is equal to the degree of 𝑥𝑖
. From this also follows that

|𝐺|=1|𝐺|𝑟(1)𝑟(1)∗=⟨𝑟,𝑟⟩=(deg𝑥1)2+(deg𝑥2)2+…,
where deg𝑥𝑖=dim𝑉𝑖
. Another important result here is that 𝑟
 is expressed as

𝑟=𝜒1deg𝑥1+𝜒2deg𝑥2+…,
which allows us to reformulate initial formula for 𝑟(𝑔)
 as

1|𝐺|∑𝑖𝜒𝑖(𝑔)deg𝑥𝑖={1,0,𝑔=1𝑔≠1.
Note: Rings a bell yet? Recall the identity

1𝑛∑𝑗=0𝑛−1𝜔𝑗𝑘𝑛={1,0,𝑘≡0(mod𝑛),𝑘≢0(mod𝑛).,
where 𝜔𝑛
 is the 𝑛
-th root of unity s. t. 𝜔𝑛𝑛=1
. And, indeed, 𝜒𝑗(𝑘)=𝜔𝑘𝑗𝑛
 are the 1
-dimensional irreducible characters of the cyclic group, which gives the connection of the standard discrete Fourier transform with the formula above.

Inverse Fourier transform
Now, assume once again that

𝐹(𝑥)=∑𝑔∈𝐺𝑓𝑔𝑥𝑔,
and that we know 𝐹(𝑥1),𝐹(𝑥2),…
 for all irreducible representations 𝑥1,𝑥2,…
 of 𝐺
. How do we recover 𝑓1,𝑓2,…,𝑓|𝐺|
?

First of all, let's think for a moment why it is possible at all. Above, while analyzing the regular representation, we found out that

|𝐺|=(deg𝑥1)2+(deg𝑥2)2+…
It is a very reassuring fact for us, because |𝐺|
 is the dimension of the 𝑓1,𝑓2,…,𝑓|𝐺|
, if we treat it as a vector. On the other hand, each 𝐹(𝑥𝑖)
 can be represented as a square matrix of degree deg𝑥𝑖
, meaning that overall the space of all possible 𝐹(𝑥1),𝐹(𝑥2),…
 has a dimension which is equal to the sum of the squared deg𝑥𝑖
. So, at the very least we clearly see that the dimensions match up, heavily hinting that the inverse transform should exist. How should such transform look like?

Let's once again consider the regular representation 𝑥
. As we found out above,

𝑥∼𝑥1deg𝑥1+𝑥2deg𝑥2+…
So, if we compute 𝐹(𝑥)
 in the regular representation 𝑥
 (assuming the basis corresponds to the decomposition), what we get is a block-diagonal matrix, where the diagonal has deg𝑥1
 blocks that are equal to 𝐹(𝑥1)
, then deg𝑥2
 blocks that are equal to 𝐹(𝑥2)
, and so on. We can introduce a natural inner product on the linear space of all possible values of 𝐹(𝑥)
, defined in the coordinate form as

⟨𝐹,𝐺⟩=∑𝑖,𝑗𝐹(𝑥)𝑖𝑗𝐺(𝑥)∗𝑖𝑗.
What is even more important is that the same inner product can be rewritten in a coordinate-agnostic form as

⟨𝐹,𝐺⟩=∑𝑖(∑𝑗𝐹(𝑥)𝑖𝑗𝐺(𝑥)∗𝑖𝑗)=∑𝑖(𝐹(𝑥)𝐺(𝑥)∗)𝑖𝑗=tr[𝐹(𝑥)𝐺(𝑥)∗].
Note that for 𝐹(𝑥)=𝑥𝑔
 and 𝐺(𝑥)=𝑥ℎ
, by definition of the regular representation, it holds that

⟨𝑥𝑔,𝑥ℎ⟩=tr[𝑥𝑔(𝑥ℎ)∗]=tr𝑥𝑔ℎ−1={|𝐺|,0,𝑔=ℎ,𝑔≠ℎ.
as (𝑥ℎ)∗=(𝑥ℎ)−1=𝑥ℎ−1
 for unitary matrices, and 𝑥𝑔𝑥ℎ−1=𝑥𝑔ℎ−1
. Hence, if we redefine the inner product to be divided by |𝐺|
, the family of transforms 𝑥𝑔
 for all 𝑔∈𝐺
 would form an orthonormal basis in the space of all possible 𝐹(𝑥)
. On the other hand, since 𝐹(𝑥)
 and 𝐺(𝑥)
 are block-diagonal with same sizes of blocks, we can further rewrite it as

⟨𝐹,𝐺⟩=1|𝐺|∑𝑖deg𝑥𝑖tr[𝐹(𝑥𝑖)𝐺(𝑥𝑖)∗].
Using this formula, which is also known as the Plancherel formula, the decomposition in this basis is expressed through the inner product as

𝑓𝑔=⟨𝐹,𝑥𝑔⟩=1|𝐺|∑𝑖𝑑𝑖tr[𝑦𝑖𝑥𝑔−1𝑖]
where 𝑦𝑖=𝐹(𝑥𝑖)
, 𝑑𝑖=deg𝑥𝑖
, and 𝑥1,𝑥2,…
 are all pairwise non-isomorphic irreducible representations of 𝐺
. The formula above is known as the inverse Fourier transform on finite groups.

Extra: Conjugacy classes
Let's recall the following definition from a group theory:

Def. 5. The elements 𝑎,𝑏∈𝐺
 are conjugate if there is an element 𝑔∈𝐺
 such that 𝑎=𝑔−1𝑏𝑔
.

Conjugacy is an equivalence relation, meaning that we can define the conjugacy classes:

Def. 6. A conjugacy class is an equivalence class in 𝐺
 under the conjugacy relation.

If 𝑎
 and 𝑏
 are conjugate, for the representation 𝑥
 it means that 𝑥𝑎=(𝑥𝑔)−1𝑥𝑏𝑥𝑔
, so the linear maps 𝑥𝑎
 and 𝑥𝑏
 must be similar. In particular, it means that 𝑥𝑎
 and 𝑥𝑏
 have the same characteristic polynomial. Thus, tr𝑥𝑎=tr𝑥𝑏
, hence 𝜒(𝑎)=𝜒(𝑏)
.

In other words, for any conjugacy class 𝐾
, the character is the same for all elements of the conjugacy class. Let 𝜒(𝐾)
 be the value of the character on the conjugacy class 𝐾
, then the inner product of characters rewrites as

⟨𝜃,𝜒⟩=1|𝐺|∑𝐾|𝐾|⋅𝜒(𝐾)⋅𝜃(𝐾)∗,
and we may treat the characters as vectors over ℂ𝑘
 instead of ℂ|𝐺|
, where 𝑘
 is the number of conjugacy classes in 𝐺
. This allows us to provide a more strict upper bound for the maximum possible number of the irreducible representations of 𝐺
. Since the characters of irreducible representations are also orthonormal over ℂ𝑘
, it means that there may be no more than 𝑘
 pairwise non-isomorphic irreducible representations of 𝐺
. And this bound is, in fact, always achieved!

Proof that it is always achieved
It means that the number of non-isomorphic irreducible representations over the field ℂ
 is exactly the number of conjugacy classes in the group 𝐺
. Then, to find specific representations, it is often possible (in particular, in the case of permutation group 𝑆𝑛
) to find a bijection between conjugacy classes and irreducible pairwise non-isomorphic representations. Finding irreducible representations for the symmetric group 𝑆𝑛
 will be the main topic of the next blog in the series, so stay tuned!

Tags tutorial, hardcore, group theory, representation theory, convolution, fast fourier transform

ZFC considered harmful

By adamant, history, 8 months ago, In English
Hi everyone!

As it is widely known, Zermelo–Fraenkel set theory with the axiom of choice, also widely known as ZFC, has several fatal flaws:

Nobody remembers the axioms accurately;
in ZFC, it is always valid to ask of a set ‘what are the elements of its elements?’, and in ordinary mathematical practice, it is not.
From now on, please use the Lawvere's Elementary Theory of the Category of Sets instead of ZFC:

Composition of functions is associative and has identities
There is a set with exactly one element
There is a set with no elements
A function is determined by its effect on elements
Given sets 𝑋
 and 𝑌
, one can form their cartesian product 𝑋×𝑌
Given sets 𝑋
 and 𝑌
, one can form the set of functions from 𝑋
 to 𝑌
Given 𝑓:𝑋→𝑌
 and 𝑦∈𝑌
, one can form the inverse image 𝑓−1(𝑦)
The subsets of a set 𝑋
 correspond to the functions from 𝑋
 to {0,1}
The natural numbers form a set
Every surjection has a right inverse
Only by switching to a superior set of set theory axioms we can save mathematics. Thank you for your attention.

P.S. On a more serious note, I think that their approach is quite interesting and it is useful to revisit the fundamentals once in a while. Overall, as highlighted in An Infinitely Large Napkin, we understand things much better when we think about them as "sets and structure-preserving maps between them" rather than just "sets and their elements", as suggested by ZFC.

Tags shitpost, set theory
Vote: I like it+37Vote: I do not like it
Author adamantPublication date 8 months agoComments 41

Codeforces Round #869

By adamant, history, 8 months ago, In English
Hi everyone!

It's been a while since I set problems for Codeforces competitions, so I hope that you had sufficient opportunity to rest before dealing with my problems again. Oh, no-no, there's nothing to worry about! Probably! I promise!

jeroenodb and adamant (that's me!) are happy to invite you to Codeforces Round 869 (Div. 1) and Codeforces Round 869 (Div. 2) which are going to take place at Saturday, April 29, 2023 at 09:35UTC-5. Participants with a rating strictly lower than 1900 will participate in Division 2, while participants with a rating of at least 1900 will participate in Division 1.

All the problems are prepared by us, and we would like to thank:

dario2994 for rejecting sufficient amount of problems for me to be able to set Oleksandr Kulkov Contest 3, for thorough and dedicated coordination and for suggesting one of the problem ideas for the round.
izban, gamegame, tfg, fedimser, Endagorion, nor, rivalq, errorgorn, antontrygubO_o, redpanda, Runtime-Terr0r, izhang, Everule, NemanjaSo2005, ak2006, ayhan23, SlavicG, c_plus_plus, amiya, willy108, madlogic, LeticiaFCS, tanus_era, nigus, Plurm, mathdude42, SajidZakaria, ace-in-the-hole, InternetPerson10, alwyn, -is-this-fft-, DJeniUp and dorijanlendvaj for testing and/or valuable feedback and insights about the problems.
MikeMirzayanov for Codeforces and Polygon and KAN for all his effort in coordinating coordinators :)
In both divisions, you will have 2 hours and 15 minutes to solve 6 problems.

Score distribution in both divisions: 500 — 1000 — 1250 — 2000 — 2500 — 3000.

Good luck and have fun!

The editorial is out.

Congratulations to the winners!

Div. 1:

A_G
tourist
ecnerwala
fantasy
QAQAutoMaton
Div. 2:

RGB_ICPC4
zhukovaliza
lintd
-LAP-
STOC
 Announcement of Codeforces Round 869 (Div. 1)
 Announcement of Codeforces Round 869 (Div. 2)
Tags 869, div1 + div2

Arrow product: How to enumerate directed graphs

By adamant, history, 8 months ago, In English
Hi everyone!

In this blog, I would like to present a somewhat novel concept in competitive programming, which is, while being relatively simple, allows us to enumerate directed graphs with specific type of strongly connected components. This allows us to easily and uniformly compute generating functions e.g. for acyclic digraphs or strongly connected digraphs. The formalism introduced here is also very intuitive at that.

I initially wanted to make a problem based on the content of this blog to appear in today's round, but after some discussions we decided to exclude it, as the contest already had a sufficient amount of difficult problems. You may try the original problem [problem:441297A] via the invitation link.

Prerequisites
You are expected to be familiar with exponential generating functions, and the exponential formula for connected structures, that is, that if 𝐴(𝑥)
 is the EGF for combinatorial structures 𝐴
, then exp𝐴(𝑥)
 is the EGF corresponding to sets of isolated instances of 𝐴
.

Directed acyclic graphs
To warm-up, consider the following problem:

You're given an integer 𝑛
. You need to count the number of distinct directed acyclic graphs on 𝑛
 vertices.

In other words, the problem asks you to find the 𝑛
-th entry of the sequence A003024. One direct and somewhat well-known way to solve this problem is to use the inclusion-exclusion principle based on the number of the source vertices of the graph:

𝑎𝑛=∑𝑖=1𝑛(−1)𝑖+1(𝑛𝑖)2𝑖(𝑛−𝑖)𝑎𝑛−𝑖
In the formula above, we choose 𝑖
 out of 𝑛
 vertices to be the source vertices in (𝑛𝑖)
 ways, and then we choose how we draw the arcs from the source vertices to all the other in 2𝑖(𝑛−𝑖)
 ways, and we build a DAG on the remaining 𝑛−𝑖
 vertices. To avoid double-counting, we do inclusion-exclusion on the number of source vertices 𝑖
.

Is this fft?
The formula above, used as it is, allows to compute 𝑎𝑛
 in 𝑂(𝑛2)
. But what if we want to do it faster? Let's rewrite it as

𝑎𝑛𝑛!=∑𝑖=1𝑛2𝑖(𝑛−𝑖)(−1)𝑖+1𝑖!𝑎𝑛−𝑖(𝑛−𝑖)!
It almost looks like a convolution, except for the nasty 2𝑖(𝑛−𝑖)
 part. Luckily, we have a trick up our sleeve to deal with it! Recall the following formula from the chirp Z-transform that allows to rewrite 𝑎𝑏
 in terms of 𝑎+𝑏
, 𝑎
 and 𝑏
:

𝑎𝑏=(𝑎+𝑏2)−(𝑎2)−(𝑏2).
Applying this formula, we get

𝑎𝑛𝑛!2(𝑛2)=∑𝑖=1𝑛(−1)𝑖+1𝑖!2(𝑖2)𝑎𝑛−𝑖(𝑛−𝑖)!2(𝑛−𝑖  2),
and moving the sum to the right, we may write it in a bit more uniform way as

∑𝑖=0𝑛(−1)𝑖+1𝑖!2(𝑖2)𝑎𝑛−𝑖(𝑛−𝑖)!2(𝑛−𝑖  2)={1,0,𝑛=0,𝑛≠0.
In terms of generating functions, it rewrites in a much simpler form as

𝐿(𝑒−𝑥)⋅𝐿(𝐷(𝑥))=1,
where

𝐷(𝑥)=∑𝑛=0∞𝑎𝑛𝑥𝑛𝑛!
is the exponential generating function of the acyclic digraphs, and 𝐿(⋅)
 is a function defined on formal power series, which divides the 𝑘
-th coefficient of the formal power series by 2(𝑘2)
. In other words, we get the formula

𝐿(𝐷(𝑥))=1𝐿(𝑒−𝑥)
and first 𝑛
 coefficients of the right-hand side can be computed in 𝑂(𝑛log𝑛)
 with power series inversion.

Arrow product
Let's stop for a moment and think a bit more generically about what just happened. We found out that the product

𝐿(𝐴(𝑥))⋅𝐿(𝐵(𝑥))
corresponds to a special kind of convolution defined as

𝑐𝑘=∑𝑖+𝑗=𝑘2𝑖𝑗𝑎𝑖𝑏𝑗
The formula above is very similar to general convolution as sum of 𝑎𝑖𝑏𝑗
, which is known to enumerate the pairs of an element described by the generating function 𝐴
 and an element described by the generating function 𝐵
. And the additional 2𝑖𝑗
 multiplier means that, in addition to picking an element of type 𝐴
 on 𝑖
 "atoms" and an element of type 𝐵
 on 𝑗
 atoms, we connect each pair of atoms of these elements in one of 2
 ways. This can be interpreted e.g as follows:


Element of the arrow product of digraphs (first interpretation)
Illustration from the paper Symbolic method and directed graph enumeration
We either draw, or do not draw a (directed) edge from the vertices of an object of type 𝐴
 to the vertices of an object of type 𝐵
;
We either draw, or do not draw an (undirected) edge between the vertices of an object of type 𝐴
 and an object of type 𝐵
;
We always draw a (directed) edge between each pair of vertices, and choose a direction in one of 2
 ways;
We always draw an (undirected) edge between each pair of vertices, and color it in one of 2
 colors.
A combinatorial structure built on top of structures of types 𝐴
 and 𝐵
 in accordance to one of the above interpretations is called the arrow product of the structures 𝐴
 and 𝐵
. This interpretation allows to think of the task above from a different perspective.

As a notational convenience, we will use the symbol ↷
 for such product, so that

𝐴(𝑥)↷𝐵(𝑥)=∑𝑘=0∞𝑥𝑘∑𝑖+𝑗=𝑘2𝑖𝑗𝑎𝑖𝑏𝑗,
where 𝑎𝑖=[𝑥𝑖]𝐴(𝑥)
 and 𝑏𝑗=[𝑥𝑗]𝐵(𝑥)
.

Enumerating DAGs
Let's mark the sources of a digraph with special variable 𝑢
. In other words, let 𝐷(𝑥,𝑢)
 be a generating function such that the coefficient near 𝑥𝑛𝑢𝑘
 corresponds to the number 𝑎𝑛,𝑘
 of DAGs with 𝑛
 vertices and 𝑘
 source vertices, divided by 𝑛!
 (so it is EGF in terms of the number of vertices, as they're considered pairwise distinct, and OGF in terms of the number of sources, as they're considered indistinguishable). With definition above in mind, we can write the following equation for 𝐷(𝑥,𝑢)
:

𝐷(𝑥,𝑢+1)=𝑒𝑥𝑢↷𝐷(𝑥)
In the formula above, 𝑢+1
 after expansion can be interpreted in such way that now the coefficient near 𝑥𝑛𝑢𝑘
 corresponds to the number of DAGs that have 𝑛
 vertices overall and 𝑘
 of their sources are specifically marked. In other way, (𝑢+1)𝑡
 expands in a way that expanding it into 𝑢
 corresponds to marking the vertex, while expanding it into 1
 corresponds to leaving it as is.

Then, we can represent the class of objects enumerated by 𝐷(𝑥,𝑢+1)
 as the arrow product of a set of marked source vertices, corresponding to the generating function 𝑒𝑢𝑥
, and of a directed graph without any marked vertices, corresponding to 𝐷(𝑥)
. Please refer to the diagram below for better understanding, the vertices on the left part will become marked source vertices and enumerated by 𝑢
, while the source vertices on the right part will remain unmarked.


𝐷(𝑥,𝑢+1)
 represented as an arrow product
Illustration from the paper Symbolic method and directed graph enumeration
Now, using the definition of 𝐷(𝑥,𝑢)
 we should note that 𝐷(𝑥,1)=𝐷(𝑥)
 and 𝐷(𝑥,0)=1
. Thus, substituting 𝑢=−1
, we get

𝑒−𝑥↷𝐷(𝑥)=1
which is equivalent to the already known formula

𝐿(𝐷(𝑥))=1𝐿(𝑒−𝑥).
Enumerating digraphs with given type of SCCs
Now, thinking about this problem a bit more generically, we may thing of DAGs as directed graphs such that each of their strongly connected components is a single vertex. So, the class of all possible strongly connected components has an EGF 𝐴(𝑥)=𝑥
. But the same reasoning applies to any other kind of digraphs with specified type of strongly connected components.

For examples we can use 𝐴(𝑥)=log11−𝑥
 for graphs whose SCCs are simple cycles, or we can put 𝐴(𝑥)
 to be an EGF of strongly connected graphs, to obtain EGF for all possible digraphs. Let 𝐺(𝑥,𝑢)
 be generating function for digraphs having only strongly connected components enumerated by the EGF 𝐴(𝑥)
, such that the coefficient near 𝑥𝑛𝑢𝑘
 corresponds to the number of digraphs of type 𝐺
 such that they have 𝑛
 vertices and 𝑘
 "source" strongly connected components (i.e. components without incoming edges).


𝐺(𝑥,𝑢+1)
 represented as the arrow product of 𝑒𝐴(𝑥)𝑢
 and 𝐺(𝑥)

Illustration from the paper Symbolic method and directed graph enumeration
Then, with the same logic, we can write that

𝐺(𝑥,𝑢+1)=𝑒𝐴(𝑥)𝑢↷𝐺(𝑥)
from which one may again use the substitution 𝑢=−1
, and get

𝑒−𝐴(𝑥)↷𝐺(𝑥)=1
Which leads us to the general formula

𝐿(𝐺(𝑥))=1𝐿(𝑒−𝐴(𝑥)).
Finding EGF for strongly connected digraphs
The formula above can also be used to find 𝐴(𝑥)
 when 𝐺(𝑥)
 is known. If we use 𝐴(𝑥)=𝑆(𝑥)
, where 𝑆(𝑥)
 is the EGF of all strongly connected digraphs, then 𝐺(𝑥)
 would be the EGF of all possible digraphs. On the other hand, it is known that the number of digraphs on 𝑛
 vertices is known to be 2𝑛(𝑛−1)
 (for each of 𝑛(𝑛−1)
 ordered pairs we either draw a directed edge or not). In terms of 𝐿(⋅)
 it means

𝐺(𝑥)=𝐿−1(𝐿−1(𝑒𝑥)),
from which one gets that

𝐿(𝑒−𝑆(𝑥))=1𝐿−1(𝑒𝑥),
which in turn rewrites as

𝑆(𝑥)=−log𝐿−1(1𝐿−1(𝑒𝑥))
which can also be computed in 𝑂(𝑛log𝑛)
 with formal power series inversion and logarithms.

Enumerating with edges
Now, what if we want a generating function 𝐴(𝑥,𝑤)
 such that a coefficient near 𝑥𝑛𝑤𝑚
 corresponds to the number of digraphs of type 𝐴
 that have 𝑛
 vertices and 𝑚
 edges? Again, we would divide this coefficient by 𝑛!
, but not 𝑚!
, as we generally consider vertices distinct (enumerated/labeled), while edges indistinguishable from each other. Proper way to define an arrow product in this case would be

𝐴(𝑥,𝑤)↷𝑤𝐵(𝑥,𝑤)=∑𝑘=0∞𝑥𝑘∑𝑖+𝑗=𝑘(1+𝑤)𝑖𝑗𝑎𝑖(𝑤)𝑏𝑗(𝑤)
where 𝑎𝑖(𝑤)=[𝑥𝑖]𝐴(𝑥,𝑤)
 and 𝑏𝑗(𝑤)=[𝑥𝑗]𝐵(𝑥,𝑤)
. Here, 1+𝑤
 semantically means that we decide to either draw (or refrain from it) an edge, which is either undirected, or is always directed from left to right (depending on the context). Then, expanding into 𝑤
 means drawing an edge, while expanding into 1
 would mean not doing it.

We can also define 𝐿𝑤(𝐴(𝑥,𝑤))
 to be a function that divides [𝑥𝑘]𝐴(𝑥,𝑤)
 by (1+𝑤)(𝑘2)
, in which case we will get the familiar identity

𝐿𝑤(𝐴(𝑥,𝑤))⋅𝐿𝑤(𝐵(𝑥,𝑤))=𝐴(𝑥,𝑤)↷𝑤𝐵(𝑥,𝑤).
In this notion, the formula for SCCs still holds:

𝑒−𝐴(𝑥,𝑤)↷𝑤𝐺(𝑥,𝑤)=1
assuming that 𝐺(𝑥,𝑤)
 enumerates graphs whose strongly connected components only belong to 𝐴(𝑥,𝑤)
.

Enumerating directed cacti for free
Consider the problem 104234L - Directed Vertex Cacti. It essentially asks us to find [𝑥𝑛𝑤𝑚]𝐺(𝑥,𝑤)
 of the generating function of graphs 𝐺
 such that each of their strongly connected components is a simple cycle, and 𝑤
 only enumerates edges outside the SCCs. What would be a generating function 𝐴(𝑥)
 for such SCCs? Nothing else than

log11−𝑥,
as it is a well-known generating function for simple cycles. Then, using the formula above, we get

(1−𝑥)↷𝑤𝐺(𝑥,𝑤)=1,
hence

𝐿𝑤(𝐺(𝑥,𝑤))=11−𝑥=∑𝑛=0∞𝑥𝑛,
where 𝐿𝑤(⋅)
 divides [𝑥𝑘]𝐺(𝑥,𝑤)
 by (1+𝑤)(𝑘2)
. Therefore, we get

𝐺(𝑥,𝑤)=∑𝑛=0∞(1+𝑤)(𝑛2)𝑥𝑛,
meaning that [𝑥𝑛𝑦𝑚]𝐺(𝑥,𝑤)=((𝑛2)𝑚)
.

Enumerating edge cacti
Looking for a challenge? I have some! Use this invitation link, and you can try solving [problem:441297A], much harder version of the problem above, where cactus is a graph where "each edge belongs to at most one cycle" rather than each vertex. As far as I know, there are no closed form solution, so you would really need to use some generatingfunctionology to solve it.

Tags dag, scc, generating function

Big integers with negative digits: The Trygub numbers

By adamant, history, 8 months ago, In English
Hi everyone!

Today we will talk about something that might be useful in programming competitions. Yay! Also great thanks to antontrygubO_o for sharing this with me and others in a Discord server.

Let's implement a data structure that maintains a big number 𝑁
 in base 𝑏
 and supports the following:

Given (possibly negative) integers |𝑥|,|𝑦|≤𝑛
, add 𝑥𝑏𝑦
 to 𝑁
.
Given 𝑘
 and assuming 𝑁≥0
, print the 𝑘
-th digit of 𝑁
.
Check if 𝑁
 is positive, negative or equals to 0
.
Each operation should take at most 𝑂(log𝑛)
 amortized time and 𝑂(𝑞)
 memory. While some approach you may think of immediately would imply using segment tree, or a similar structure, the solution proposed here only requires std::map, so it's much shorter and easier to implement (at the slight expense of increased constant factor). It may be used in the following problems:

#2302. 「NOI2017」整数
1817E - Half-sum
1810F - M-tree
If you implement the big integers in these numbers the standard way (i.e. keeping digits in the [0,𝑏)
 segment, carefully executing carries, etc), you will quickly learn that you may get in trouble because you may be forced to do and undo a lot of carry operations which chain up and so you need to frequently change large segments between the values of 0
 and 𝑏−1
.

Now, stop being fooled by the non-negative propaganda! You don't have to do it! Let's give ourselves some slack and allow negative digits. Well, just a bit of them. Instead of maintaining digits in [0,𝑏)
, let's now maintain them in the interval (−𝑏,𝑏)
. It seems like a tiny change, but the effect is tremendous. On one hand, the representation of any number is not unique anymore. On the other hand, when we actually reach the value 𝑏
 or −𝑏
, we wrap them back to 0
, and carry 1
 or −1
 to the next digit correspondingly.

Noticed anything? The carry now wraps us from the endpoints of the interval to its middle instead of from one endpoint to another! It would be easy to add 1
 to a particular bit, turn it into 𝑏
 and cause a chain of carries by it. But! If after that we add −1
 to the same bit, it will not wrap all the bits back to 𝑏−1
! It will just change this specific bit to −1
! So, we give up the uniqueness of the representation, but we gain a whole lot of stability in exchange.

The C++ implementation for the first two queries is also quite concise:

code
I tested it on #2302. 「NOI2017」整数 and it works!

P.S. Applying it to 1817E - Half-sum and 1810F - M-tree is left to the curious reader as an exercise :)

P.P.S. Is this trick well-known? Does it have a name?

Tags bignum

Shift of polynomial sampling points

By adamant, history, 8 months ago, In English
Hi everyone!

Previously, I had a blog on how, given 𝑠
 and a polynomial 𝑓(𝑥)
, to compute coefficients of the polynomial 𝑓(𝑥+𝑠)
.

Today we do it in values space. Consider the problem Library Judge — Shift of Sampling Points of Polynomial. In this problem, you're given 𝑠
 and the values 𝑓(0),𝑓(1),…,𝑓(𝑛)
 of a polynomial of degree at most 𝑛
 and you need to find 𝑓(𝑠),𝑓(𝑠+1),…,𝑓(𝑠+𝑛)
.

In particular, I used this to generate test cases for 1817C - Similar Polynomials, there might be other applications too.

General linear recurrence shift
If you have further questions about something here, please refer to this blog

Let's, for a moment, consider even more generic problem now. Let 𝑓0,𝑓1,…
 be a linear recurrence, such that

𝑓𝑚=𝑎1𝑓𝑚−1+⋯+𝑎𝑛+1𝑓𝑚−(𝑛+1).
Given 𝑠
 and 𝑓0,…,𝑓𝑛
, we need to find 𝑓𝑠,…,𝑓𝑠+𝑛
. The problem generalizes that of finding specific value 𝑓𝑠
.

Generally, we can say that 𝑓𝑠,…,𝑓𝑠+𝑛
 are the coefficients near 𝑥0,𝑥−1,…,𝑥−𝑛
 of 𝑥𝑠𝑔(𝑥−1)
, where

𝑔(𝑥)=∑𝑘=0∞𝑓𝑘𝑥𝑘
is the generating function of 𝑓𝑘
. On the other hand, 𝑔(𝑥)=𝑝(𝑥)𝑞(𝑥)
, where

𝑞(𝑥)=1−∑𝑘=1𝑛+1𝑎𝑘𝑥𝑘,
and 𝑝(𝑥)
 is a polynomial of degree at most 𝑛
. Let

𝐴(𝑥)=𝑥𝑛+1−∑𝑘=1𝑛+1𝑎𝑘𝑥(𝑛+1)−𝑘=𝑥𝑛+1𝑞(𝑥−1),
then 𝐷(𝑥)𝑔(𝑥−1)=𝑥𝑛+1𝑝(𝑥−1)
 only has positive powers of 𝑥
 if 𝐷(𝑥)
 is divisible by 𝐴(𝑥)
. Thus, the coefficients near non-positive powers of 𝑥𝑠𝑔(𝑥−1)
 will not change if we replace 𝑥𝑠
 by its remainder modulo 𝐴(𝑥)
. So, we need coefficients near 𝑥0,𝑥−1,…,𝑥−𝑛
 of the product 𝑅(𝑥)𝑔(𝑥−1)
, where 𝑅(𝑥)≡𝑥𝑠(mod𝐴)
. Note that only the first 2𝑛+1
 coefficients of 𝑔(𝑥)
 affect the result, hence the whole problem may be solved in 𝑂(𝑛log𝑛log𝑠)
 for finding 𝑅(𝑥)
 and then 𝑂(𝑛log𝑛)
 to find 𝑓𝑠,…,𝑓𝑠+𝑛
.

If you're not comfortable working with negative coefficients, a possibly simpler way to look on it is to notice that

𝑓𝑠=[𝑥𝑛+1](𝑓0𝑥𝑛+1+𝑓1𝑥𝑛+⋯+𝑓𝑛+1)⋅(𝑥𝑠mod𝐴).
On the other hand, changing the first polynomial to 𝑓1𝑥𝑛+1+⋯+𝑓𝑛+2
 would yield 𝑓𝑚+1
 as a result. Altogether, it means that

(𝑓0𝑥2𝑛+1+𝑓1𝑥2𝑛+⋯+𝑓2𝑛+1)⋅(𝑥𝑠mod𝐴)
would yield the values 𝑓𝑠+𝑛,…,𝑓𝑠+1,𝑓𝑠
 in its coefficients near 𝑥𝑛+1,𝑥𝑛+2,…,𝑥2𝑛+1
.

Shift in polynomials
In polynomials, it is possible to implement the solution above in 𝑂(𝑛log𝑛)
 instead of 𝑂(𝑛log𝑛log𝑠)
. For this we should note that 𝑓(0),𝑓(1),…
 also form a linear recurrence with a very specific characteristic polynomial 𝑞(𝑥)=(1−𝑥)𝑛+1
.

Perhaps the simplest way to show it is by noticing that taking finite difference Δ𝑓(𝑘)=𝑓(𝑘)−𝑓(𝑘−1)
 for 𝑛+1
 times will yield a zero polynomial, as Δ
 reduces the degree by 1
. Using a function 𝑆
 such that 𝑆𝑓(𝑘)=𝑓(𝑘−1)
, we can write it as

Δ𝑛+1𝑓(𝑘)=(1−𝑆)𝑛+1𝑓(𝑘)=∑𝑖=0𝑛+1(−1)𝑖(𝑛+1𝑖)𝑓(𝑘−𝑖)=0,
which shows that (1−𝑥)𝑛+1
 is the characteristic polynomial of the corresponding recurrence. Thus,

𝑅(𝑥)≡𝑥𝑠(mod(𝑥−1)𝑛+1)
can be transformed via substitution 𝑥=𝑡+1
 into

𝑅(𝑡+1)≡(𝑡+1)𝑠(mod𝑡𝑛+1).
The identity above allows us to find

𝑅(𝑡+1)=∑𝑘=0𝑛(𝑠𝑘)𝑡𝑘,
from which we can obtain the final result by substituting 𝑡=𝑥−1
 back

𝑅(𝑥)=∑𝑘=0𝑛(𝑠𝑘)(𝑥−1)𝑘,
which can then be computed as a regular Taylor shift by −1
 of 𝑅(𝑡+1)
 in 𝑂(𝑛log𝑛)
.

You can also refer to my solution on the Library Judge.

UPD: It was brought to my attention that 𝑅(𝑥)
 can be computed in linear time, as

[𝑥𝑘]𝑅(𝑥)=∑𝑖=𝑘𝑛(𝑠𝑖)(𝑖𝑘)(−1)𝑖−𝑘=(𝑠𝑘)∑𝑖=𝑘𝑛(𝑠−𝑘𝑖−𝑘)(−1)𝑖−𝑘.
Then, using (𝑛𝑟)=(𝑛−1𝑟−1)+(𝑛−1𝑟)
, we transform the later sum into a telescopic one:

∑𝑖=𝑘𝑛(−1)𝑖−𝑘[(𝑠−𝑘−1𝑖−𝑘−1)+(𝑠−𝑘−1𝑖−𝑘)]=(−1)𝑛−𝑘(𝑠−𝑘−1𝑛−𝑘).
Thus, the full expression for 𝑅(𝑥)
 is

𝑅(𝑥)=∑𝑘=0𝑛(−1)𝑛−𝑘(𝑠𝑘)(𝑠−𝑘−1𝑛−𝑘)𝑥𝑘.
UPD2: Golovanov399 mentioned in the comments that there is a much simpler way of doing it for polynomials using the representation

𝑔(𝑥)=𝑝(𝑥)(1−𝑥)𝑛+1
of the generating function. As (1−𝑥)𝑛+1
 is a very well-behaved function, one may notice that

1(1−𝑥)𝑛+1=∑𝑘=0∞(𝑛+𝑘𝑛)𝑥𝑘,
then one can find 𝑓(𝑠),…,𝑓(𝑠+𝑛)
 by multiplying 𝑝(𝑥)
, which is obtained as

𝑝(𝑥)=𝑔(𝑥)(1−𝑥)𝑛+1mod𝑥𝑛+1,
with the corresponding segment of coefficients of (1−𝑥)−(𝑛+1)
 near the 𝑠
-th coefficient.

UPD3: It was brought to my attention that it is possible to do this in just a single convolution. Upon some further discussion with Golovanov399, we found out that the Lagrange interpolation on 𝑓(0),…,𝑓(𝑛)
 can actually be written simply as

𝑓(𝑘)=∑𝑖=0𝑛(𝑘𝑖)(𝑛−𝑘𝑛−𝑖)𝑓(𝑖).
In particular, it also gives a much simpler and more concise expression for 𝑅(𝑥)
 from above:

𝑥𝑘≡∑𝑖=0𝑛(𝑘𝑖)(𝑛−𝑘𝑛−𝑖)𝑥𝑖(mod𝑥𝑛+1).
We also derived the following formula:

∂𝑛+1∂𝑥𝑛+1((𝑥𝑦−1)𝑛𝑛!log11−𝑥)≡𝑦𝑛+11−𝑥𝑦(mod(𝑦−1)𝑛+1)
Applying the function 𝑇(𝑦𝑖)=𝑓(𝑖)
 to it, we get the identity for 𝑘≥1
:

[𝑥𝑛+𝑘](∑𝑖=0𝑛(−1)𝑛−𝑖𝑓(𝑖)𝑖!(𝑛−𝑖)!𝑥𝑖)(∑𝑗=1∞𝑥𝑗𝑗)=(𝑘−1)!(𝑛+𝑘)!𝑓(𝑛+𝑘),
which allows to find 𝑓(𝑠),…,𝑓(𝑠+𝑛)
 in a single convolution with a sub-range of the coefficients of log11−𝑥
.

The result above can also be found directly from manipulating coefficients in Lagrange interpolation formula. I kind of like it, but unfortunately we do not yet know if there is any meaningful interpretation to the result in terms of 𝑥
 and 𝑦
.

Questions to audience
Is there a simpler solution here, preferably without heavy low-level work with coefficients...
Is it possible to compute 𝑅(𝑥)
 directly, rather than as a Taylor shift?
Are there other viable applications to doing this?
Tags polynomials

Pythagorean triples and Pell's equations

By adamant, history, 8 months ago, In English
Hi everyone!

Recently I started solving projecteuler, and while doing so I encountered two concepts about which I heard before, but I didn't really bother to learn them. I made a few notes to myself about how they work, and thought it could be useful for somebody else too. This blog focuses on Pythagorean triples and Pell's equations, which are recurrent concepts on projecteuler.

Great thanks to nor, Endagorion, Golovanov399 and Neodym for useful discussions about these topics.

Pythagorean triples
Pythagorean triple is a triple 𝑎,𝑏,𝑐∈ℤ
 such that

𝑎2+𝑏2=𝑐2.
It is possible to parameterize them in a way that allows to find all triples such that 𝑎,𝑏,𝑐≤𝑛
 in 𝑂(𝑛√)
.

Rational points on a unit circle
To do that, let's divide the equation by 𝑐2
 to get

(𝑎𝑐)2+(𝑏𝑐)2=1.
This reduces the task of finding Pythagorean triples to finding rational points (𝑥,𝑦)
 on a unit circle.

And there is a nice and simple way to find all such points. To do this, we should notice that a ray going from (0,−1)
 into a rational point (𝑥,𝑦)
 will intersect the line 𝑦=0
 in a rational point (𝑝𝑞,0)
. So, we can find all rational points (𝑥,𝑦)
 by projecting the point (𝑝𝑞,0)
 back onto unit circle for each 𝑝𝑞
.


We consider a ray from (0,−1)
 into (𝑝𝑞,0)
 to find rational points on the unit circle
Note: The correct value for the numerator of 𝑦
 in the image is 𝑞2−𝑝2
The direction vector of a ray from (0,−1)
 to (𝑝𝑞,0)
 is (𝑝𝑞,1)
, so the equation of the ray is 𝑝𝑦−𝑥𝑞=−𝑝
. Let's solve the system

{𝑥2+𝑦2=1,𝑝𝑦−𝑥𝑞=−𝑝.
After expressing 𝑦
 as a function of 𝑥
 like

𝑦=𝑥𝑞−𝑝𝑝,
we put it back into the first equation and get the result:

𝑥=2𝑞𝑝𝑝2+𝑞2,𝑦=𝑞2−𝑝2𝑝2+𝑞2
As it turns out, the projection point on a unit circle is also always rational, so we found a bijection between rational points in ℝ
 and rational points on the unit circle.

As a geometric inversion
It was also pointed to me that the bijection between the Pythagorean triples and the rational numbers can be perceived as an inversion transform on a complex plane. General formula for the inversion in the point 𝑂
 with the radius 𝑟
 is

(𝑧−𝑂)⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯(𝑧′−𝑂)=𝑟2,
where 𝑧−𝑂⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯
 is complex conjugate. Therefore,

𝑧↦𝑟2𝑧−𝑂⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯⎯+𝑂.
In our particular case we use 𝑂=−𝑖
 and 𝑟=2‾√
. Then the image of a real number 𝑡
 is

𝑡↦2𝑡−𝑖−𝑖,
which rewrites as

2(𝑡+𝑖)−𝑖(1+𝑡2)1+𝑡2=2𝑡1+𝑡2+𝑖1−𝑡21+𝑡2.
Then it follows from geometric inversion properties that all such points lie on the unit circle. And for 𝑡=𝑝𝑞
 it is

𝑝𝑞↦2𝑝𝑞𝑝2+𝑞2+𝑖𝑞2−𝑝2𝑝2+𝑞2.
Back to integers
This translates back into integer triples as

𝑎=2𝑝𝑞,𝑏=𝑞2−𝑝2,𝑐=𝑝2+𝑞2
Note that for integers such parameterization is incomplete, as the fraction 𝑥=𝑎𝑐
 could as well correspond to 𝑘𝑎𝑘𝑐
 for some integer 𝑘
. Therefore, all possible triplets are obtained by multiplying vectors (𝑎,𝑏,𝑐)
 of the form defined above with all possible integer constants 𝑘
.

Pell's equations
Another somewhat recurring theme is Pell's equations. These are the equations that look like

𝑥2−𝐷𝑦2=1.
We're looking for positive integer solutions (𝑥,𝑦)
 to it. To understand the solutions better, we can relax it to inequalities

0<𝑥2−𝐷𝑦2≤1.
The two inequalities above are equivalent to the equation when (𝑥,𝑦)
 are restricted to integers. Now, by dividing with 𝑦2
 we turn it into

𝐷‾‾√<𝑥𝑦≤𝐷+1𝑦2‾‾‾‾‾‾‾√.
The only possible lattice points satisfying the inequalities above are actual solutions to 𝑥2−𝐷𝑦2=1
. It means that the set of lattice points in the quater-plane 𝑥,𝑦≥0
, for which 𝑥>𝑦𝐷‾‾√
, is a subset of the area surrounded by the curve 𝑥2−𝐷𝑦2=1
. As this area is strictly convex (see the picture below), the subset can only intersect the boundary of the area in the convex hull of the subset. In other words, all solutions must lie on the convex hull of lattice points 𝑥>𝑦𝐷‾‾√
 on 𝑥,𝑦≥0
.

Note that being on the convex hull of lattice points is a necessary, but generally not sufficient, condition for a solution.


Convergents (red) correspond to corners of the convex hull of lattice points
From the theory of continued fractions it is known that such points are exactly the convergents 𝑥𝑦
 of 𝐷‾‾√
. Knowing this, and trying all convergents in order we will eventually find at least one non-trivial solution, from which it will be possible to find all the others (see below).

Solutions as a subgroup of 2×2
 matrices with determinant 1
The text above shows that if solution exists, it must be a convergent of 𝐷‾‾√
. But still, we would like to know which particular convergents are the solutions and what is their general structure. To better understand it, let's rewrite the equation as

det(𝑥𝑦𝐷𝑦𝑥)=1.
That's a very nice representation because of the following facts:

If an integer matrix has determinant 1
, its inverse is also an integer matrix with determinant 1
.
If two matrices have determinant 1
, so does their product.
So, let's consider the inverse of the matrix under the determinant. It is

(𝑥𝑦𝐷𝑦𝑥)−1=(𝑥−𝑦−𝐷𝑦𝑥),
corresponding to the fact that if (𝑥,𝑦)
 is an integer solution to the Pell's equation, so is (𝑥,−𝑦)
. Now to the product:

(𝑥0𝑦0𝐷𝑦0𝑥0)(𝑥1𝑦1𝐷𝑦1𝑥1)=(𝑥0𝑥1+𝐷𝑦0𝑦1𝑥0𝑦1+𝑦0𝑥1𝐷(𝑥0𝑦1+𝑦0𝑥1)𝑥0𝑥1+𝐷𝑦0𝑦1)
In other words, if (𝑥0,𝑦0)
 and (𝑥1,𝑦1)
 are solutions, so is (𝑥0𝑥1+𝐷𝑦0𝑦1,𝑥0𝑦1+𝑦0𝑥1)
. This means that the full set of integer solutions to the Pell's equations in fact form a group with the group operation defined as above. Note that the point (𝑥,𝑦)=(1,0)
 corresponds to the identity element of such group. Note that for (𝑥0,𝑦0)=(𝑥1,𝑦1)
 the result is (𝑥2+𝐷𝑦2,2𝑥𝑦)
.

Solutions as a subgroup of ℤ[𝐷‾‾√]
Another, perhaps simpler way to look on it is to consider numbers of kind 𝑎+𝑏𝐷‾‾√
. For such numbers, it's natural to define multiplication

(𝑎+𝑏𝐷‾‾√)(𝑐+𝑑𝐷‾‾√)=(𝑎𝑐+𝑏𝑑𝐷)+(𝑎𝑐+𝑏𝑑)𝐷‾‾√
and the multiplicative norm |𝑎+𝑏𝐷‾‾√|=(𝑎+𝑏𝐷‾‾√)(𝑎−𝑏𝐷‾‾√)=𝑎2−𝑏2𝐷
. Then it's also quite easy to see that the numbers satisfying |𝑎+𝑏𝐷‾‾√|=1
 form a group, as the norm is multiplicative, that is |𝐴𝐵|=|𝐴|⋅|𝐵|
.

Periodicity of the continued fraction
Consider the continued fraction 𝐷‾‾√=[𝑎0;𝑎1,𝑎2,…]
 and its complete quotients 𝑠𝑘=[𝑎𝑘;𝑎𝑘+1,…]
. We can represent them as

𝑠𝑘=𝐷√+𝑥𝑘𝑦𝑘,
where 𝑥𝑘
 and 𝑦𝑘
 are rational numbers. In fact, we can even prove that 𝑥𝑘
 and 𝑦𝑘
 are integers, and that

𝐷𝑞2𝑘−𝑝2𝑘𝐷𝑞𝑘𝑞𝑘−1−𝑝𝑘𝑝𝑘−1=𝑦𝑘+1(−1)𝑘+1,=𝑥𝑘+1(−1)𝑘
for any convergent 𝑝𝑘−1𝑞𝑘−1=[𝑎0;𝑎1,…,𝑎𝑘−1]
. The proof is a bit technically involved, thus I will place it under the spoiler.

Proof
The identity above means that |𝐷𝑞2𝑘−𝑝2𝑘|=1
 if and only if 𝑦𝑘+1=1
, that is 𝑠𝑘+1=𝐷‾‾√+𝑥𝑘+1
. As it turns out, this is tightly related to the periodicity of the continued fraction of 𝐷‾‾√
, and only ever happens at 𝑘=−1
 or after we just completed another period. To better understand this, we should prove the following fact:

𝑥+𝑦𝐷‾‾√=[(𝑎0;𝑎1,…,𝑎𝑘)]⟺{𝑥+𝑦𝐷‾‾√>1,−1<𝑥−𝑦𝐷‾‾√<0
That is, 𝑥+𝑦𝐷‾‾√∈ℚ[𝐷‾‾√]
 has a periodic fraction if and only if it's larger than 1
 and −1<𝑥−𝑦𝐷‾‾√<0
.

Proof
The result above allows us to prove that 𝐷‾‾√
 is periodic starting with 𝑎1
, because −1<𝑠∗1<0
 from the very beginning. It, in turn, means that when we get to 𝑠𝑘+1=𝐷‾‾√+𝑥𝑘+1
 it must hold that 𝑥𝑘+1=⌊𝐷‾‾√⌋=𝑎0
, otherwise 𝑠∗𝑘+1
 won't be in the interval (−1,0)
. But this immediately tells us that 𝑠𝑘+2=𝑠1
.

Finding all solutions
In other words, the first non-trivial solution to |𝑥2−𝑦2𝐷|=1
 is given by the convergent 𝑝𝑘𝑞𝑘
, where

𝐷‾‾√=[𝑎0;(𝑎1,…,𝑎𝑘+1)],
and all the non-negative solutions are the convergents with indices 𝑡(𝑘+1)−1
 for integer 𝑡≥0
. This observation also allows to find the explicit correspondence between a convergent 𝑝𝑘𝑞𝑘
 and its matrix representation. It generally holds for convergents that

(𝑝𝑘+1𝑞𝑘+1𝑝𝑘𝑞𝑘)=(𝑎0110)(𝑎1110)…(𝑎𝑘+1110).
If |𝑝2𝑘−𝐷𝑞2𝑘|=1
, we can additionally derive that

(𝑝𝑘𝑞𝑘𝐷𝑞𝑘𝑝𝑘)=(𝑎0110)(𝑎1110)…(𝑎𝑘+1110)(𝑎0110)−1.
Proof
It makes a lot of sense, as applying the transform defined by such matrix to any convergent 𝑝𝑡𝑞𝑡
 will simply add another period in front of it, thus moving it into 𝑝𝑡+𝑘+1𝑞𝑡+𝑘+1
. This gives explicit isomorphism between addition in convergent's indices and group operation in the solution group in terms of matrices. This also highlights that the solution group is cyclic, that is all solutions are obtained as powers of the smallest solution 𝑝𝑘𝑞𝑘
, which can be found with continued fractions.

Summary
In other words, to find all solutions to the Pell's equation 𝑥2−𝐷𝑦2=1
, one may find the smallest solution as the first convergent 𝑝𝑘𝑞𝑘
 of 𝐷‾‾√=[𝑎0;𝑎1,𝑎2,…]
 such that 𝑝2𝑘−𝐷𝑞2𝑘=1
. Then all the other solutions are obtained as

𝑥+𝑦𝐷‾‾√=(𝑝𝑘+𝑞𝑘𝐷‾‾√)𝑡
for all integer 𝑡≥0
.

Tags project euler, mathematics, continued fraction, diophantine equation, tutorial

Integer solutions to x² + y² = z

By adamant, history, 7 months ago, In English
Hi everyone!

As I continue working through Project Euler, I want to write a blog about another piece of general mathematical knowledge that is both interesting on its own and might be useful in some problems. Consider the following Diophantine equation:

𝑥2+𝑦2=𝑧.
We assume that we're given a specific number 𝑧∈ℤ
, and we need to check if there are 𝑥,𝑦∈ℤ
 for which the identity above holds. Then, if such numbers exist we should find them and report. Example of a problem that might need it:

Timus — 1593. Square Country. Version 2. You're given an integer 𝑛
. Find minimum 𝑘
 such that 𝑛=𝑎21+⋯+𝑎2𝑘
.

Tl;dr.
Let 𝑧=2𝑘0𝑝𝑘11…𝑝𝑘𝑛𝑛𝑝𝑘𝑛+1𝑛+1…𝑝𝑘𝑚𝑚
, where 𝑝1,…,𝑝𝑛
 are different prime numbers with remainder 3
 modulo 4
, and 𝑝𝑛+1,…,𝑝𝑚
 are different prime numbers with remainder 1
 modulo 4
. Then there are two cases. If any of 𝑘1,…,𝑘𝑛
 is odd, there are no solutions. Otherwise there is always a solution 𝑧=𝑥2+𝑦2
 that looks like

𝑥+𝑖𝑦=(1+𝑖)𝑘0𝑝𝑘1/21…𝑝𝑘𝑛/2𝑛(𝑥𝑛+1+𝑖𝑦𝑛+1)𝑘𝑛+1…(𝑥𝑚+𝑖𝑦𝑚)𝑘𝑚,
where 𝑖2=−1
 and 𝑥2𝑘+𝑦2𝑘=𝑝2𝑘
 for 𝑘
 from 𝑛+1
 to 𝑚
. For each 𝑝𝑘
, to find such 𝑥𝑘,𝑦𝑘
 we need to find an integer 𝑖
 such that 𝑖2≡−1(mod𝑝)
, then find a minimum 𝑥𝑘=𝑖𝑦𝑘mod𝑝𝑘
 for 1≤𝑦𝑘<𝑝𝑘‾‾√
. This is doable in 𝑂(log𝑝𝑘)
.

And if we want to count solutions, their number is given by Jakobi's two-square theorem: The number of ways of representing 𝑧
 as the sum of two squares is 4(𝑑1(𝑧)−𝑑3(𝑧))
, where 𝑑𝑘(𝑧)
 is the number of divisors of 𝑧
 that have remainder 𝑘
 modulo 4
.

From exact equation to mod𝑧
First of all, let's solve a bit easier problem. One obvious thing we can do is to take remainder modulo 𝑧
 on both parts:

𝑥2+𝑦2≡0(mod𝑧).
This relaxed version is equivalent to finding 𝑥,𝑦,𝑘∈ℤ
 such that

𝑥2+𝑦2=𝑘𝑧.
Hmmm... Remainders modulo arbitrary number 𝑧
 is not the most pleasant thing to work on directly. But remainders modulo prime number 𝑝
 are usually nice. On the other hand, if 𝑝
 is some prime factor of 𝑧
 and there is a solution for 𝑧
, it means that there will as well be a solution for 𝑝
 with 𝑘=𝑧𝑝
. So, let's assume 𝑧
 to be prime, for now.

From arbitrary 𝑧
 to prime 𝑝
Now, we have another equation

𝑥2+𝑦2≡0(mod𝑝),
where 𝑝
 is a prime number. What's good about prime numbers is that remainders modulo prime numbers form a field (i.e. they work very similarly to rationals, and we can expect similar results to hold). For 𝑝=2
, there is a non-trivial solution 𝑥=𝑦=1
. What about odd numbers 𝑝
? There are two cases to consider, as the remainder of 𝑝
 modulo 4
 is either 1
 or −1
.

Fermat's theorem on sums of two squares tells us that for an odd prime 𝑝
, the solution exists if and only if 𝑝
 has a remainder 1
 modulo 4
. Moreover, the sum of two squares theorem tells us that the number 𝑧
 is expressible as a sum of two squares if and only if its prime decomposition does not have a term 𝑝𝑘
, where 𝑝≡−1(mod4)
, and 𝑘
 is odd. Let's find out why.

𝑝≡1(mod4)
Of course, it's not yet clear why these two cases are important. Let's assume that there is an integer 𝑖
 such that

𝑖2≡−1(mod𝑝),
that is there is a remainder modulo 𝑝
 which behaves very similarly to imaginary unit from complex numbers. Then

𝑥2+𝑦2≡(𝑥+𝑖𝑦)(𝑥−𝑖𝑦)(mod𝑝).
This reduces the initial equation to a union of linear equations

[𝑥+𝑖𝑦≡0(mod𝑝),𝑥−𝑖𝑦≡0(mod𝑝).
For each 𝑦
, except 𝑦=0
, there are 2
 possible values of 𝑥=±𝑖𝑦
, so there are a total of 2𝑝+1
 solutions. Noteworthy, it is always possible to find a pair of solutions (𝑥,𝑦)
 such that 1≤𝑥,𝑦<𝑝√
, which means that 𝑥2+𝑦2=𝑝
 is satisfied exactly.

How to find it? Find 𝑖
, and consider the minimum value of 𝑖𝑘mod𝑝
 among 1≤𝑘<𝑝√
. Due to pigeonhole principle, there will be 𝑘1≠𝑘2
 such that 𝑖(𝑘1−𝑘2)mod𝑝≤𝑝√
. This is actually very similar to 102354I - From Modular to Rational!

Now, when does such 𝑖
 exist and how to find it? It is known that remainders modulo 𝑝
 have a primitive root 𝑔
 such that its powers from 0
 to 𝑝−2
 run through all possible remainders modulo 𝑝
. Note that for odd 𝑝
 it always holds that

−1≡𝑔𝑝−12(mod𝑝).
Then, if such 𝑖
 exists we should be able to find it from

𝑖2≡𝑔𝑝−12(mod𝑝)⟺𝑖≡𝑔𝑝−14(mod𝑝).
Well, technically −𝑔𝑝−14
 also can be used as 𝑖
, but it's not that important. What's important is that it is possible to do as above only when 𝑝−1
 is divisible by 4
. In other words, when 𝑝≡1(mod4)
.

𝑝≡−1(mod4)
Now, let's think about the other case. If there is no such 𝑖
, we can introduce it! Now, we can formally consider numbers that look like 𝑥+𝑖𝑦
, where 𝑖
 is not a remainder modulo 𝑝
. Numbers of this kind, if treated formally, also form a field. If you're familiar with field theory, I should mention that it is isomorphic to the Galois field 𝐺𝐹(𝑝2)
. If you're not familiar with it, ignore what I just wrote.

The thing now is that we can try to find all solutions in this new, extended field. And it reduces to the same union of equations

[𝑥+𝑖𝑦≡0(mod𝑝),𝑥−𝑖𝑦≡0(mod𝑝),
so for every 𝑦
, the only possible solutions are 𝑥=±𝑖𝑦
. The problem is, this time such 𝑥
 would not be a remainder modulo 𝑝
, unless 𝑦=0
. Instead, it will be an "imaginary" solution. So, the only "real" solution is 𝑥≡𝑦≡0(mod𝑝)
. It means that all solutions to

𝑥2+𝑦2=𝑘𝑝
look like 𝑥=𝑝𝑥′
 and 𝑦=𝑝𝑦′
. Thus,

𝑝2𝑥′2+𝑝2𝑦′2=𝑘𝑝.
So, if 𝑘
 is not divisible by 𝑝
, there are no solutions. Otherwise 𝑘=𝑝𝑘′
 reduces it to

𝑥′2+𝑦′2=𝑘′,
after which similar argument could be applied. So, if 𝑘′
 is divisible by an odd power of such 𝑝
, there are no solutions. We're only one step away from solving the whole 𝑥2+𝑦2=𝑧
 problem now, assuming that we know the factorization of 𝑧
.

Back to arbitrary 𝑧
Now we need to use one more fact from complex numbers. There, we can introduce a norm

‖𝑥+𝑖𝑦‖=(𝑥+𝑖𝑦)(𝑥−𝑖𝑦)=𝑥2+𝑦2.
Its crucial property for this task is that it is multiplicative, that is

‖(𝑎+𝑖𝑏)(𝑐+𝑖𝑑)‖=‖𝑎+𝑖𝑏‖⋅‖𝑐+𝑖𝑑‖.
This gives the Brahmagupta–Fibonacci identity

(𝑎2+𝑏2)(𝑐2+𝑑2)=(𝑎𝑐−𝑏𝑑)2+(𝑎𝑑+𝑏𝑐)2,
from which it follows that if we can represent 𝑧
 as a product of several several numbers that are expressible as a sum of two squares, we can use the identity above to also express 𝑧
 as a sum of two squares. In complex number terms, it means that we will find a complex number 𝑥+𝑖𝑦
 such that ‖𝑥+𝑖𝑦‖=𝑧
.

Repeating what's written in tl'dr section, let 𝑧=2𝑘0𝑝𝑘11…𝑝𝑘𝑛𝑛𝑝𝑘𝑛+1𝑛+1…𝑝𝑘𝑚𝑚
, where 𝑝1,…,𝑝𝑛
 are different prime numbers with remainder 3
 modulo 4
, and 𝑝𝑛+1,…,𝑝𝑚
 are different prime numbers with remainder 1
 modulo 4
. Then there are two cases. If any of 𝑘1,…,𝑘𝑛
 is odd, there are no solutions. Otherwise there is always a solution 𝑧=𝑥2+𝑦2
 that looks like

𝑥+𝑖𝑦=(1+𝑖)𝑘0𝑝𝑘1/21…𝑝𝑘𝑛/2𝑛(𝑥𝑛+1+𝑖𝑦𝑛+1)𝑘𝑛+1…(𝑥𝑚+𝑖𝑦𝑚)𝑘𝑚,
where 𝑖2=−1
 and 𝑥2𝑘+𝑦2𝑘=𝑝2𝑘
 for 𝑘
 from 𝑛+1
 to 𝑚
. This result is tightly connected to the following ones:

Classification of Gaussian primes. A Gaussian integer 𝑎+𝑖𝑏
 is prime if and only if either of the following is true:

𝑎=0
 or 𝑏=0
 and the non-zero number is prime with remainder 3
 modulo 4
,
𝑎2+𝑏2
 is 2
 or a prime number with remainder 1
 modulo 4
.
The result also allows to factor Gaussian integers by representing their norm as a sum of two squares in the way suggested above.

Jakobi's two-square theorem. The number of ways of representing 𝑧
 as the sum of two squares is 4(𝑑1(𝑧)−𝑑3(𝑧))
, where 𝑑𝑘(𝑧)
 is the number of divisors of 𝑧
 that have remainder 𝑘
 modulo 4
.

If there is a prime divisor with remainder 3
 mod 4
, it's 0
. Otherwise, it is 4
 times the number of divisors of 𝑝𝑘𝑛+1𝑛+1…𝑝𝑘𝑚𝑚
. We may interpret it that the divisor decides how much of multipliers in the product would correspond to 𝑥𝑘+𝑖𝑦𝑘
, and how much to 𝑦𝑘+𝑖𝑥𝑘
, after which 4
 accounts for all the possible ways to multiply 𝑥+𝑖𝑦
 with 1
, −1
, 𝑖
 or −𝑖
, which accounts for all possible combinations of (±𝑥,±𝑦)
.

And, of course, there are similar results for 3
 squares and 4
 squares:

Legendre's three-square theorem,
Lagrange's four-square theorem and Euler's four-square identity.
Tags number theory, tutorial

Computing n! modulo pᵏ for small p

By adamant, history, 7 months ago, In English
Hi everyone!

There is an article on cp-algorithms about how to compute 𝑛!
 modulo prime number 𝑝
. Today I was wondering, how to do it if we need to compute modulo 𝑝𝑘
 rather than just 𝑝
. Turns out one can do it efficiently if 𝑝
 is small.

Motivational example
Xiaoxu Guo Contest 3 — Binomial Coefficient. Given 1≤𝑛,𝑘≤1018
, find (𝑛𝑘)
 modulo 232
.

There are also some projecteuler problems that require it, including with several queries of distinct 𝑛
 and 𝑘
.

Task formulation and outline of result
To clarify the task a bit, our ultimate goal here is to be able to compute e.g. binomial coefficients modulo 𝑝𝑘
. Thus, what we really need is to represent 𝑛!=𝑝𝑡𝑎
, where gcd(𝑎,𝑝)=1
, and then report 𝑡
 and 𝑎
 modulo 𝑝𝑘
. This is sufficient to also compute (𝑛𝑟)
 modulo 𝑝𝑘
.

We will show that, assuming that polynomial multiplication of size 𝑛
 requires 𝑂(𝑛log𝑛)
 operations, and assuming that arithmetic operations modulo 𝑝𝑘
 take 𝑂(1)
, we can find 𝑡
 and 𝑎
 in 𝑂(𝑑2+𝑑𝑘log𝑘)
, where 𝑑=log𝑝𝑛
. It requires 𝑂(𝑝𝑘log2𝑘)
 pre-computation that takes 𝑂(𝑝𝑘log𝑘)
 memory.

Algorithm when 𝑝𝑘
 is also small
First, let's solve a simpler version of this problem.

Library Judge — Binomial Coefficient. Given 𝑛
, 𝑘≤1018
 and 𝑚≤106
, print (𝑛𝑘)mod𝑚
 for 𝑇≤2⋅105
 test cases.

Let's denote 𝑛!𝑝
 as the product of all numbers from 1
 to 𝑛
 that are co-prime with 𝑝
. That is,

𝑛!𝑝=∏1≤𝑗≤𝑛gcd(𝑗,𝑝)=1𝑗.
In this notion, assuming 𝑘=⌊𝑛𝑝⌋
, we can represent 𝑛!
 as 𝑛!=𝑝𝑘𝑘!𝑛!𝑝
, which expands recursively into

𝑛!=∏𝑖=0∞𝑝⌊𝑛𝑝𝑖+1⌋⌊𝑛/𝑝𝑖⌋!𝑝
On practice, we only care about 𝑖≤log𝑝𝑛
, and we may pre-compute all possible values of 𝑎!𝑝
 modulo 𝑝𝑘
 in advance in 𝑂(𝑝𝑘)
 for all 𝑎<𝑝𝑘
, which then allows us to compute the full formula in 𝑂(𝑑)
, where 𝑑=log𝑝𝑛
. To do this, we note that

𝑎!𝑝≡(𝑝𝑘−1)!⌊𝑎/𝑝𝑘⌋𝑝(𝑎mod𝑝𝑘)!𝑝(mod𝑝𝑘),
and (𝑝𝑘−1)!𝑝≡−1(mod𝑝𝑘)
 for 𝑝>2
 and 𝑝𝑘=4
. For other values with 𝑝=2
 it is 1
 instead of −1
.

Please find the implementation in Python below for details:

Code
It works 5.5s on the aforementioned Library Judge problem.

Algorithm with polynomials
Great thanks to Endagorion for sharing this algorithm with me!

Let's look further into the formulas from the section above. There is a part contributed by numbers divisible by 𝑝
, which can be reduced to computing 𝑘!
, and a part contributed by numbers not divisible by 𝑝
. Let 𝑛=𝑎0+𝑎1𝑝+⋯+𝑎𝑑𝑝𝑑
, then

∏1≤𝑗≤𝑛gcd(𝑗,𝑝)=1𝑗=∏𝑖=0𝑑∏1≤𝑗≤𝑎𝑖𝑝𝑖gcd(𝑗,𝑝)=1(⌊𝑛𝑝𝑖+1⌋𝑝𝑖+1+𝑗).
To compute it quickly, we can define a family of polynomial 𝑃𝑖,𝑎(𝑥)
 for 0≤𝑎≤𝑝
 such that

𝑃𝑖,𝑎(𝑥)=∏1≤𝑗≤𝑎𝑝𝑖−1gcd(𝑗,𝑝)=1(𝑥𝑝𝑖+𝑗),
so the value of the factorial would be represented as

𝑛!=𝑝𝑘𝑘!∏𝑖=0𝑑𝑃𝑖+1,𝑎𝑖(⌊𝑛𝑝𝑖+1⌋),
and expanding 𝑘!
 it then rewrites into

𝑛!=∏𝑖=0𝑑𝑝⌊𝑛𝑝𝑖+1⌋∏𝑗=0𝑑−𝑖𝑃𝑗+1,𝑎𝑖+𝑗(⌊𝑛𝑝𝑖+𝑗+1⌋),
which simplifies as

𝑛!=∏𝑖=0𝑑𝑝⌊𝑛𝑝𝑖+1⌋∏𝑗=0𝑖𝑃𝑗+1,𝑎𝑖(⌊𝑛𝑝𝑖+1⌋)
Now, what would it take us to use this setup? First of all, notice that 𝑃𝑖,𝑎(𝑥)
 can be computed from one another:

𝑃𝑖,𝑎(𝑥)=∏1≤𝑗≤𝑎𝑝𝑖−1gcd(𝑗,𝑝)=1(𝑥𝑝𝑖+𝑗)=∏𝑡=0𝑎−1∏1≤𝑗≤𝑝𝑖−1gcd(𝑗,𝑝)=1(𝑥𝑝𝑖+𝑡𝑝𝑖−1+𝑗)=∏𝑡=0𝑎−1𝑃𝑖−1,𝑝(𝑝𝑥+𝑡).
Note that for shorter and more consistent implementation, this recurrent formula also mostly works for 𝑖=1
 if we put 𝑃0,𝑝(𝑥)=𝑥+1
, but for 𝑃1,𝑝
 we should go up to 𝑝−2
 instead of 𝑝−1
. We should also note that for 𝑃𝑖,𝑎(𝑥)
, we only care for coefficients up to 𝑥⌊𝑥/𝑖⌋
, as the larger ones are divisible by 𝑝𝑘
.

This allows to compute 𝑃𝑖,𝑎(𝑥)
 in 𝑂(𝑝𝑘log𝑘𝑖)
 for all 𝑎
 for a given 𝑖
. Over all 𝑖
 from 1
 to 𝑘
 it sums up to 𝑂(𝑝𝑘log2𝑘)
 time and 𝑂(𝑝𝑘log𝑘)
 memory. Then, evaluating all the polynomials for any specific 𝑎
 requires 𝑂(𝑑2+𝑑𝑘log𝑘)
 operations, where 𝑑=log𝑝𝑛
.

As it requires some manipulations with polynomials, I implemented it in Python with sympy just as a proof of concept:

Code
Algorithm with 𝑝
-adic logarithms and exponents
The algorithm above requires some heavy machinery on polynomials. I also found out an alternative approach that allows to compute 𝑡
 and 𝑎
 for any given 𝑛
 divisible by 𝑝
 in 𝑂(𝑑𝑘2)
 with 𝑂(𝑝𝑘)
 precomputation, where 𝑑=log𝑝𝑛
. It doesn't rely on polynomial operations, except for Lagrange interpolation.

Let 𝑛=𝑝𝑡+𝑏
, where 0≤𝑏<𝑝
, then we can represent its factorial as

𝑛!=𝑝𝑡𝑡!∏𝑖=1𝑏∏𝑗=0𝑡(𝑝𝑗+𝑖)∏𝑖=𝑏+1𝑝−1∏𝑗=0𝑡−1(𝑝𝑗+𝑖).
We can further rewrite it as

𝑛!=𝑝𝑡𝑡!(𝑝−1)!𝑡𝑏!∏𝑖=1𝑏∏𝑗=0𝑡(1+𝑗𝑖𝑝)∏𝑖=𝑏+1𝑝−1∏𝑗=0𝑡−1(1+𝑗𝑖𝑝)
Let's learn how to compute the product

𝐴(𝑏,𝑡)=∏𝑖=1𝑏∏𝑗=0𝑡(1+𝑗𝑖𝑝),
as with it we can represent the factorial as

𝑛!=𝑝𝑡𝑡!(𝑝−1)!𝑡𝑏!𝐴(𝑏,𝑡)𝐴(𝑝−1,𝑡−1)𝐴(𝑏,𝑡−1).
We can take a 𝑝
-adic logarithm (please refer to this article for definitions and properties) from the product to get

log𝐴(𝑏,𝑡)=∑𝑖=1𝑏∑𝑗=0𝑡log(1+𝑗𝑖𝑝)=∑𝑧=1∞(−1)𝑧+1𝑝𝑧𝑧∑𝑖=1𝑏𝑖−𝑧∑𝑗=0𝑡𝑗𝑧
We can precompute sums of 𝑖−𝑧
 for each 𝑧
 up to 𝑘
 and 𝑏
 up to 𝑝−1
 in 𝑂(𝑝𝑘)
, and we can find the sum of 𝑗𝑧
 for 𝑗
 from 0
 to 𝑡
 in 𝑂(𝑧)
 via Lagrange interpolation. Therefore, with 𝑂(𝑝𝑘)
 precomputation, we can compute log𝐴(𝑏,𝑡)
 in 𝑂(𝑘2)
, which then allows to find 𝑛!
 in 𝑂(𝑑𝑘2)
, where 𝑑=log𝑝𝑛
. I implemented it in Python, as a proof of concept (incl. fast sums over 𝑖
 and 𝑗
):

Code
Note: Due to inherent properties of 𝑝
-adic logarithms and exponents, the algorithm only works with 𝑝>2
, and it might return −𝑛!
 instead with 𝑝=2
. This is because explog𝑧=−𝑧
 with 𝑝=2
 when 𝑧
 has remainder 3
 modulo 4
. It is accounted for in the code by tracking the number of integers below 𝑛
 that have remainder 3
 modulo 4
.

I also tested the solution on the motivational problem and got AC in Python: submission.

Some other approaches
There is also a blog by prabowo, based apparently on another blog by Min_25, and a scholarly article by Andrew Granville. I'm not sure whether they talk about the same algorithms as described here.

Tags tutorial, p-adic numbers, factorial

Solving the "simple math problem" with generating functions

By adamant, history, 6 months ago, In English
Hi everyone!

Some time ago the following "simple math problem" was shared in a Discord chat:

 

As a lover of simple math problems, I couldn't just pass by this problem. It turned out much harder than any other genfunc problem that I solved before, as the structure of the answer depends on the parity of 𝑛
 and 𝑚
, and it's not very natural to track it through genfuncs. It took me few months, I even called for help from higher powers (i.e. sent a pm to Elegia) but I finally have a solution that I somewhat like.

Finding a closed-form genfunc
First of all, it's really inconvenient to sum up from 0
 to ⌊𝑚/2⌋
 or ⌊𝑛/2⌋
. What we can do is to introduce variables 𝑎=𝑛−2𝑗
 and 𝑏=𝑚−2𝑖
, after which the expression under the sum changes to

(𝑖+𝑗𝑗)2(𝑎+𝑏𝑎).
As we want to sum it up over all 𝑖,𝑗,𝑎,𝑏
 such that 𝑎+2𝑗=𝑛
 and 𝑏+2𝑖=𝑚
, we can keep track of these equations with two formal variables 𝑥
 and 𝑦
. Then, the sum that we're interested in expresses as

[𝑥𝑛𝑦𝑚]∑𝑖,𝑗,𝑎,𝑏(𝑖+𝑗𝑖)2(𝑎+𝑏𝑎)𝑥2𝑗+𝑎𝑦2𝑖+𝑏=[𝑥𝑛𝑦𝑚](∑𝑎,𝑏(𝑎+𝑏𝑎)𝑥𝑎𝑦𝑏)(∑𝑖,𝑗(𝑖+𝑗𝑖)2𝑥2𝑗𝑦2𝑖).
This is the product of two generating functions. The first one is standard and collapses with 𝑠=𝑎+𝑏
 substitution:

∑𝑎,𝑏(𝑎+𝑏𝑎)𝑥𝑎𝑦𝑏=∑𝑠(𝑥+𝑦)𝑠=11−𝑥−𝑦.
The second generating function is trickier, and collapses into

∑𝑖,𝑗(𝑖+𝑗𝑖)2𝑥2𝑗𝑦2𝑖=11−2(𝑦2+𝑥2)+(𝑦2−𝑥2)2√.
Unfortunately, I don't have a simple explanation to it, as it is obtained via reduction to a well-known bivariate generating function for Legendre polynomials (see this Mathematics Stack Exchange question for details). So, the problem reduces to finding

[𝑥𝑛𝑦𝑚]11−𝑥−𝑦11−2(𝑦2+𝑥2)+(𝑦2−𝑥2)2‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√
Adding series multisection
Since the second function in the product only depends on 𝑥2
 and 𝑦2
, we have an expression of form 𝐺(𝑥,𝑦)=𝐴(𝑥,𝑦)⋅𝐵(𝑥2,𝑦2)
.

It would make sense to do a series multisection (aka roots of unity filter) to represent it as a combination of summands that look like 𝐴′(𝑥2,𝑦2)⋅𝐵(𝑥2,𝑦2)
, so that we can go back from (𝑥2,𝑦2)
 to (𝑥,𝑦)
 and analyze them as 𝐴′(𝑥,𝑦)⋅𝐵(𝑥,𝑦)
. This way we detach from possible dependence on the parities of the powers of coefficients. The multisection is generally done as

𝐴(𝑥)=𝐴(𝑥)+𝐴(−𝑥)2+𝐴(𝑥)−𝐴(−𝑥)2,
where the first summand only has even powers of 𝑥
, and the second summand only has odd powers of 𝑥
. In our case, we need to apply it first to the variable 𝑥
, and the to 𝑦
. However, we can do a little shortcut, as after doing so, the common denominator of the resulting expression should be

(1−𝑥−𝑦)(1+𝑥−𝑦)(1−𝑥+𝑦)(1+𝑥+𝑦),
meaning that the numerator must be

(1+𝑥−𝑦)(1−𝑥+𝑦)(1+𝑥+𝑦),
which expands to

11−𝑥−𝑦=[1−𝑥2−𝑦2]+[𝑥−𝑥3+𝑥𝑦2]+[𝑦+𝑥2𝑦−𝑦3]+[2𝑥𝑦](1−𝑥−𝑦)(1+𝑥−𝑦)(1−𝑥+𝑦)(1+𝑥+𝑦)
Solving for parities
The 4
 distinct summands in the numerator correspond to all possible combinations of parities of powers of 𝑥
 and 𝑦
. But what is really striking here is that the denominator expands to

(1−𝑥−𝑦)(1+𝑥−𝑦)(1−𝑥+𝑦)(1+𝑥+𝑦)=1−2(𝑦2+𝑥2)+(𝑦2−𝑥2)2,
meaning that it is exactly the expression under the square root of the second function multiplier. Therefore, the full problem reduces to finding (and computing an appropriate linear combination of) [𝑥𝑛𝑦𝑚]
 of the function

𝐹(𝑥,𝑦)=[1−2(𝑦+𝑥)+(𝑦−𝑥)2]−3/2.
This function is very similar to

𝐺(𝑥,𝑦)=[1−2(𝑦+𝑥)+(𝑦−𝑥)2]−1/2,
about which we already know that [𝑥𝑛𝑦𝑚]𝐺(𝑥,𝑦)=(𝑛+𝑚𝑛)2
. Indeed, we can notice that

{∂∂𝑥𝐺(𝑥,𝑦)=(1−𝑥+𝑦)𝐹(𝑥,𝑦),∂∂𝑦𝐺(𝑥,𝑦)=(1+𝑥−𝑦)𝐹(𝑥,𝑦),
therefore

𝐹(𝑥,𝑦)=12[∂∂𝑥𝐺(𝑥,𝑦)+∂∂𝑦𝐺(𝑥,𝑦)],
from which it follows that

[𝑥𝑛𝑦𝑚]𝐹(𝑥,𝑦)=12[(𝑛+1)(𝑛+𝑚+1𝑛+1)2+(𝑚+1)(𝑛+𝑚+1𝑚+1)2]
This allows to solve the problem in 𝑂(1)
 per (𝑛,𝑚)
 pair with 𝑂(𝑛+𝑚)
 pre-computation:

code
Alternative approaches
The answer to the whole problem can be further simplified as

𝑓(𝑛,𝑚)=⌊(𝑛+𝑚)/2+1⌋(⌊𝑛/2⌋+⌈𝑚/2⌉⌊𝑛/2⌋)(⌊𝑚/2⌋+⌈𝑛/2⌉⌊𝑚/2⌋)
Thanks to Endagorion for telling me about this form! One can prove it by inspecting how 𝑓(𝑛,𝑚)
 relates to 𝑓(𝑛−1,𝑚)
 and 𝑓(𝑛,𝑚−1)
, but apparently there is also a bijective proof. You may also refer to this publication for further details and alternative proofs.

Follow-up questions to interested audience
Is there a sufficiently simple way to solve this problem without just "observing" some weird facts?
Is there a more direct way to find the closed form on the second function? (see this question).
UPD: I found out how to compute the genfunc for (𝑖+𝑗𝑖)2
 directly, without using Legendre polynomials.

Consider a bivariate polynomial 𝑄𝑛(𝑥,𝑦)
 defined as

𝑄𝑛(𝑥,𝑦)=∑𝑘=0𝑛(𝑛𝑘)2𝑥𝑘𝑦𝑛−𝑘=[𝑡𝑛](𝑥+𝑡)𝑛(𝑦+𝑡)𝑛.
We need to sum it up over all 𝑛
, and we know from Legendre polynomials that 𝑄𝑛(𝑥)=(𝑦−𝑥)𝑛𝑃𝑛(𝑦+𝑥𝑦−𝑥)
.

But let's analyze 𝑄𝑛(𝑥,𝑦)
 on its own:

[𝑡𝑛](𝑥+𝑡)𝑛(𝑦+𝑡)𝑛=[𝑡𝑛](𝑡(𝑡+𝑥+𝑦)+𝑥𝑦)𝑛.
This expands into

∑𝑘(𝑛𝑘)𝑥𝑘𝑦𝑘[𝑡𝑘](𝑡+𝑥+𝑦)𝑛−𝑘=∑𝑘(𝑛𝑘)(𝑛−𝑘𝑘)𝑥𝑘𝑦𝑘(𝑥+𝑦)𝑛−2𝑘.
Note that

(𝑛𝑘)(𝑛−𝑘𝑘)=(𝑛𝑘,𝑘,𝑛−2𝑘)=(𝑛2𝑘)(2𝑘𝑘),
hence we want to compute the sum

∑𝑛,𝑘(𝑛2𝑘)(2𝑘𝑘)𝑥𝑘𝑦𝑘(𝑥+𝑦)𝑛−2𝑘.
Let's sum up over 𝑛
 for each fixed 𝑘
:

∑𝑛(𝑛2𝑘)(𝑥+𝑦)𝑛−2𝑘=∑𝑡(2𝑘+𝑡2𝑘)(𝑥+𝑦)𝑡=1(1−𝑥−𝑦)2𝑘+1.
Thus, we want to compute

∑𝑘(2𝑘𝑘)𝑥𝑘𝑦𝑘(1−𝑥−𝑦)2𝑘+1.
On the other hand we know that

∑𝑘𝑥𝑘4𝑘(2𝑘𝑘)=11−𝑥√,
thus the sum above compacts into

11−𝑥−𝑦11−4𝑥𝑦(1−𝑥−𝑦)2√=1(1−𝑥−𝑦)2−4𝑥𝑦‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾‾√
which then expands into the same expression in the denominator.

Tags generating function, simple math

Useful substitutions with generating functions

By adamant, history, 6 months ago, In English
Hi everyone!

Today I would like to write about some identities that might come handy when using generating functions to solve competitive programming problems. I will also try to include some useful examples about them.

Some notation
For brevity, we will sometimes skip the specific bounds in indexed sums, meaning that the summation happens among all valid indices. Please also read the information below if you're not familiar with generating functions, or want to brush up on some aspects.

Definitions and notation
Multinomial coefficients
If you're not familiar with them, multinomial coefficients are defined as

(𝑛𝑎1,…,𝑎𝑟)=𝑛!𝑎1!…𝑎𝑟!,
where 𝑎1+⋯+𝑎𝑟=𝑛
. When dealing with identities that contain binomial coefficients, we often can combine them and "reshuffle" into another combination of binomial coefficients which is more convenient to us. This is because multinomial coefficients can be expanded as

(𝑛𝑎1,…,𝑎𝑛)=(𝑛𝑎1)(𝑛−𝑎1𝑎2)(𝑛−𝑎1−𝑎2𝑎3)…(𝑎𝑟−1+𝑎𝑟𝑎𝑟−1)=(𝑎1+𝑎2𝑎2)(𝑎1+𝑎2+𝑎3𝑎3)…(𝑛𝑎𝑟)
In other words, we can represent any multinomial coefficient as a chain of binomial coefficients which account for each 𝑎𝑖
 one piece at a time. We will see a lot of examples of this below, mostly with trinomial coefficients and for now we can look on the following one:

(𝑛𝑘)(𝑛−𝑘𝑘)=(𝑛𝑘,𝑘,𝑛−2𝑘)=(𝑛2𝑘)(2𝑘𝑘),
which allows to fully exclude 𝑛
 from one of the coefficients which will then simplify our work with the part that is related to 𝑛
.

Binomial theorem
Let's start with the simplest substitution ever. We most likely know the binomial theorem:

(𝑥+𝑦)𝑛=∑𝑘(𝑛𝑘)𝑥𝑘𝑦𝑛−𝑘
This allows us, whenever we have a binomial coefficient in our expression, to substitute it with

(𝑛𝑘)=[𝑥𝑘](1+𝑥)𝑛=[𝑥𝑛−𝑘](1+𝑥)𝑛=[𝑥𝑛]𝑥𝑘(1+𝑥)𝑛
The last identity is especially useful, as it allows to drag [𝑥𝑛]
 outside of the summation.

Example 1. Consider the sum from one of Elegia's blog posts:

∑𝑘(𝑛𝑘)2(3𝑛+𝑘2𝑛).
Assume that you're given several values of 𝑛
 and you need to compute it in 𝑂(1)
 with 𝑂(𝑛)
 pre-processing.

Solution
Exercise 1: Solve ProjectEuler 831 — Triple Product.

Negative binomial expansion
Now, let's learn few more well-known expansions. You most likely know that

11−𝑥=1+𝑥+𝑥2+…
This expression is also the consequence of the binomial expansion, as it also allows to use values of 𝑛
 that are not positive integer. So, being more general, we can write that

(1−𝑥)−𝑛=∑𝑘(−𝑛𝑘)(−1)𝑘𝑥𝑘,
but at the same time the binomial coefficient rewrites as

(−𝑛𝑘)=(−𝑛)(−𝑛−1)…(−𝑛−(𝑘−1))𝑘!=(−1)𝑘(𝑛+𝑘−1𝑘).
The result above allows us to rewrite

1(1−𝑥)𝑘+1=∑𝑡(𝑡+𝑘𝑘)𝑥𝑡
This, in turn, means that while (𝑛𝑘)
 is often substituted with [𝑥𝑘](1+𝑥)𝑛
, there is another useful substitution:

(𝑎+𝑏𝑎)=[𝑥𝑎]1(1−𝑥)𝑏+1=[𝑥𝑏]1(1−𝑥)𝑎+1
To differentiate between the two, the substitution (𝑛𝑘)=[𝑥𝑘](1+𝑥)𝑛
 is typically more useful when 𝑛
 is constant and summation goes over 𝑘
. Conversely, when 𝑘
 is constant and summation goes over 𝑛
, it would likely be more convenient to use the substitution above.

Inverse square root expansion
Another important case of binomial expansion is when 𝑛=−12
. Consider the expansion

11−𝑥√=∑𝑘(−1/2𝑘)(−𝑥)𝑘.
We can rewrite the fractional binomial as

(−1/2𝑘)=(−1/2)(−1/2−1)…(−1/2−𝑘+1)𝑘!=(−1)𝑘2𝑘1⋅3⋅5⋅…⋅(2𝑘−1)𝑘!=(−1)𝑘2𝑘(2𝑘−1)!!𝑘!,
where 𝑘!!=𝑘(𝑘−2)(𝑘−4)…
 is the double factorial of 𝑘
. Note that (2𝑘)!!(2𝑘−1)!!=(2𝑘)!
 and that (2𝑘)!!=2𝑘𝑘!
.

Thus, multiplying the numerator and denominator with 2𝑘𝑘!
, we get

=(−1)𝑘4𝑘(2𝑘)!𝑘!2=(−1)𝑘4𝑘(2𝑘𝑘),
which means that

11−𝑥‾‾‾‾‾√=∑𝑘𝑥𝑘4𝑘(2𝑘𝑘)
This identity allows to collapse some more complicated sums that involve (2𝑘𝑘)
, as it means that

(2𝑘𝑘)=[𝑥𝑘]11−4𝑥‾‾‾‾‾‾√
Example 2. The problem 446227I — O Andar do Trêbado reduces to computing

[𝑥𝑛](1+𝑥+𝑥2)𝑛.
It's well known that this is doable in 𝑂(𝑛)
. But in this problem we need to do it for all 𝑛
 up to some limit.

Solution
Note that the linked problem also allowed 𝑂(𝑛2)
 solutions. Thanks to tfg for telling me about the problem!

Example 3. The function 𝑓(𝑛,𝑚)
 defined below can be computed in 𝑂(1)
 with 𝑂(𝑛+𝑚)
 pre-computation:

𝑓(𝑛,𝑚)=∑𝑖,𝑗(𝑖+𝑗𝑖)2(𝑛+𝑚−2𝑖−2𝑗𝑛−2𝑗).
Solution
Example 4. Find the closed form of a function

𝐺(𝑥,𝑦)=∑𝑖,𝑗(𝑖+𝑗𝑖)2𝑥𝑖𝑦𝑗.
Solution
Example 5. Find the closed form of a function

𝐺(𝑡,𝑥)=∑𝑛𝑡𝑛𝑃𝑛(𝑥),
where 𝑃𝑛(𝑥)
 is the sequence of Legendre polynomials, defined for this example as

𝑃𝑛(𝑥)=12𝑛∑𝑘(𝑛𝑘)2(𝑥−1)𝑘(𝑥+1)𝑛−𝑘.
Solution
Note: Although examples 4 and 5 don't look useful for competitive programming, they can be used to solve the example 3, which is based on a real problem from some training camp (and also some prior publications, apparently).

Powers of 𝑛
Another less known, but quite useful substitution involves the exponential function:

𝑒𝑛𝑥=∑𝑘𝑛𝑘𝑥𝑘𝑘!,
from which it follows that we can use the substitution

𝑛𝑘=[𝑥𝑘𝑘!]𝑒𝑛𝑥
Let's see how it can be used to solve some problems.

Example 6. Given 𝑚
, we want to compute 𝑓(𝑘)
 in 𝑂(1)
 per 𝑘
 with 𝑂(𝑘log𝑘)
 pre-computation:

𝑓(𝑘)=∑𝑛=0𝑚−1𝑛𝑘.
Solution: Using the substitution above, we turn the sum into the geometric progression:

𝑓(𝑘)=[𝑥𝑘𝑘!]∑𝑛=0𝑚−1𝑒𝑛𝑥=[𝑥𝑘𝑘!]1−𝑒𝑚𝑥1−𝑒𝑥.
Well, not exactly new, as I already wrote a blog about it quite some time ago...

Example 7. Given 𝐴(𝑥)=𝑎0+𝑎1𝑥+…
, we want to quickly compute 𝑓(𝑘)
 defined as

𝑓(𝑘)=∑𝑛𝑎𝑛𝑛𝑘.
Solution: Using the same substitution, we get

𝑓(𝑘)=[𝑥𝑘𝑘!]∑𝑛𝑎𝑛𝑒𝑛𝑥=[𝑥𝑘𝑘!]𝐴(𝑒𝑥).
Well, still not new, as I had another blog about it some time ago.

At the same time, the result above assumes that we're able to compute series expansion of 𝐴(𝑒𝑥)
 up to 𝑘
 sufficiently quickly. Which might turn out to be quite problematic if 𝐴(𝑥)
 is sufficiently arbitrary. However, when 𝐴(𝑥)
 is sufficiently nice, there is an interesting side effect of such solution. It allows 𝐴(𝑥)
 to have quite a large amount of terms if it still can be represented in a "nice" way.

For this, we rely on the following statement:

Conjecture: Let 𝐹(𝑥)
, 𝐺(𝑥)
 and 𝐻(𝑥)
 be formal power series, and let 𝐺0=[𝑥0]𝐺(𝑥)
. Then the following implication stands:

𝐹(𝑥)≡𝐻(𝑥)(mod(𝑥−𝐺0)𝑘)⟹𝐹(𝐺(𝑥))≡𝐻(𝐺(𝑥))(mod𝑥𝑘)
In particular, as [𝑥0]𝑒𝑥=1
, it means that

𝐹(𝑥)≡𝐻(𝑥)(mod(𝑥−1)𝑘)⟹𝐹(𝑒𝑥)≡𝐻(𝑒𝑥)(mod𝑥𝑘)
Proof
From the result above it follows that to find first 𝑘
 elements of 𝐹(𝑒𝑥)
, we can instead find 𝐻(𝑥)
 such that

𝐹(𝑥)≡𝐻(𝑥)(mod(𝑥−1)𝑘),
and compute first 𝑘
 elements of 𝐻(𝑒𝑥)
 instead. This allows to efficiently solve the following problem:

Example 8. 392C - Yet Another Number Sequence. Let 𝑓𝑛=𝑓𝑛−1+𝑓𝑛−2
. Given 𝑚
 and 𝑘
, compute 𝑓(𝑘)
 in 𝑂(𝑘log𝑘)
:

𝑓(𝑘)=∑𝑛=0𝑚−1𝑓𝑛𝑛𝑘.
Solution
Note: Apparently, this problem is also a subject of one of Elegia's blogs, but it is difficult for me to translate on my own...

Exercise 2. Find a way to solve example 8 in 𝑂(𝑘)
.

Hint
Example 9. Solve Library Judge — Sum of Exponential times Polynomial Limit. Given 𝑟≠1
 and 𝑑
, compute 𝑓(𝑑)
 in 𝑂(𝑑)
, where

𝑓(𝑑)=∑𝑖=0∞𝑟𝑖𝑖𝑑.
Solution
Exercise 3. Solve Library Judge — Sum of Exponential times Polynomial. Given 𝑟
, 𝑑
 and 𝑛
, compute 𝑓(𝑑)
 in 𝑂(𝑑)
, where

𝑓(𝑑)=∑𝑖=0𝑛−1𝑟𝑖𝑖𝑑.
Exercise 4. Solve CodeChef — Quasi-Polynomial Sum.

Tags generating function, tutorial

Dirichlet convolution. Part 1: Fast prefix sum computations

By adamant, history, 6 months ago, In English
Hi everyone!

Suppose that you need to compute some sum of a number-theoretic function that has something to do with divisors:

∑𝑘=1𝑛𝜑(𝑘)=?∑𝑘=1𝑛∑𝑑|𝑘𝑑2=??∑𝑥=1𝑛∑𝑦=1𝑥gcd(𝑥,𝑦)=?!?
As it turns out, such and many similar sums can be computed with Dirichlet convolution in 𝑂(𝑛2/3)
, and in this article we will learn how.

Let 𝑓(𝑛)
 and 𝑔(𝑛)
 be two arithmetic functions. Let 𝐹(𝑛)
 and 𝐺(𝑛)
 be their prefix sums, that is

𝐹(𝑛)=∑𝑖=1𝑛𝑓(𝑖),𝐺(𝑛)=∑𝑗=1𝑛𝑔(𝑗).
We need to compute a prefix sum of the Dirichlet convolution (𝑓∗𝑔)(𝑛)
. In this article, we will consider some general methods, and show how to do so in 𝑂(𝑛2/3)
 if we can compute prefix sums of 𝐹(𝑛)
 and 𝐺(𝑛)
 in all possible values of ⌊𝑛/𝑘⌋
 in this complexity.

Part 1: Fast prefix sum computation
Part 2: Dirichlet series and prime counting
ecnerwala previously mentioned that it is possible, but did not go into much detail. There is also a blog by Nisiyama_Suzune, which covers prefix sums of Dirichlet inverses in 𝑂(𝑛2/3)
.

Dirichlet convolution
Recall that the Dirichlet convolution of arithmetic functions 𝑓(𝑛)
 and 𝑔(𝑛)
 is defined as

(𝑓∗𝑔)(𝑛)=∑𝑑|𝑛𝑓(𝑑)𝑔(𝑛𝑑)=∑𝑖𝑗=𝑛𝑓(𝑖)𝑔(𝑗)
Dirichlet convolution is often of interest in the context of multiplicative functions (i.e., functions such that 𝑓(𝑎)𝑓(𝑏)=𝑓(𝑎𝑏)
 for gcd(𝑎,𝑏)=1
), as the result can be proven to also be a multiplicative function. Moreover, quite often functions of interest can be expressed as a Dirichlet convolution. For example, the divisor function

𝜎𝑎(𝑛)=∑𝑑|𝑛𝑑𝑎
can be represented as a Dirichlet convolution of the constant function 𝑓(𝑛)=1
 and the power function 𝑔(𝑛)=𝑛𝑎
. There are also other similar identities, for example, for the Euler's totient function 𝜑(𝑛)
, one can notice that

∑𝑑|𝑛𝜑(𝑑)=𝑛.
This is due to the fact that 𝜑(𝑛𝑑)
 is the number of integers 𝑥
 such that gcd(𝑥,𝑛𝑑)=1
, or, equivalently, gcd(𝑥,𝑛)=𝑑
. This implies that ℎ(𝑛)=𝑛
 can be represented as the Dirichlet convolution of 𝑓(𝑛)=1
 and 𝜑(𝑛)
.

Hyperbola method
For now assume that we can compute any of 𝑓
, 𝑔
, 𝐹
, 𝐺
 in 𝑂(1)
. We can rewrite the prefix sum of the Dirichlet convolution as

∑𝑡≤𝑛(𝑓∗𝑔)(𝑡)=∑𝑡≤𝑛∑𝑑|𝑡𝑓(𝑑)𝑔(𝑡𝑑)=∑𝑖𝑗≤𝑛𝑓(𝑖)𝑔(𝑗).
In the expression above, at least one of 𝑖
 and 𝑗
 must be less than 𝑛√
. This allows us to rewrite the expression as

∑𝑖𝑗≤𝑛𝑓(𝑖)𝑔(𝑗)=∑𝑖=1⌊𝑛√⌋𝑓(𝑖)∑𝑗=1⌊𝑛/𝑖⌋𝑔(𝑗)+∑𝑗=1⌊𝑛√⌋𝑔(𝑗)∑𝑖=1⌊𝑛/𝑗⌋𝑓(𝑖)−∑𝑖=1⌊𝑛√⌋𝑓(𝑖)∑𝑗=1⌊𝑛√⌋𝑔(𝑗)=∑𝑖=1⌊𝑛√⌋𝑓(𝑖)𝐺(⌊𝑛/𝑖⌋)+∑𝑗=1⌊𝑛√⌋𝑔(𝑗)𝐹(⌊𝑛/𝑗⌋)−𝐹(⌊𝑛√⌋)𝐺(⌊𝑛√⌋)
Thus, in this situation we can find a prefix sum of the Dirichlet convolution in 𝑂(𝑛√)
.


Summation blocks in 𝑖𝑗≤15

Points in the green block are double-counted and should be subtracted
The representation of the prefix summation, as written above, is commonly known as the Dirichlet hyperbola method. Graphically, such summation splits lattice points below the 𝑖𝑗=𝑛
 hyperbola into three parts (see the picture above). Besides exact computations, this method can also be used to get asymptotic estimations for summations like the one above.

Exercise 1: Solve Project Euler — #401 Sum of Squares of Divisors: Compute the prefix sum of 𝜎2(𝑛)=∑𝑑|𝑛𝑑2
 up to 𝑛=1015
.

Choosing a better splitting point
Now, what if computing 𝐹(𝑖)
 and 𝐺(𝑗)
 requires vastly unbalanced amount of effort?

Example: Let's say that we want to compute to compute

ℎ(𝑛)=∑𝑎|𝑛∑𝑏|𝑎𝑏2,
the sum of squares of divisors of all divisors of 𝑛
. It can be represented as a Dirichlet convolution of 𝑓(𝑛)=1
 and 𝑔(𝑛)=𝜎2(𝑛)
. For the first one, prefix sums can be computed in 𝑂(1)
, and for the second one they can be computed in 𝑂(𝑛√)
, see excercise 1.

In this case, we can bound the sums by a point (𝑘,𝑙)
 on the rectilinear convex hull of points under the hyperbola 𝑘𝑙=𝑛
:

∑𝑖𝑗≤𝑛𝑓(𝑖)𝑔(𝑗)=∑𝑖=1𝑘𝑓(𝑖)𝐺(⌊𝑛/𝑖⌋)+∑𝑗=1𝑙𝑔(𝑗)𝐹(⌊𝑛/𝑗⌋)−𝐹(𝑘)𝐺(𝑙)
See the picture below for a graphical representation.


For 𝑛=15
, we can choose 𝑘=2
 and 𝑙=5
 instead of 𝑘=𝑙=3
Let's now assume that we can compute 𝐹(𝑛)
 in 𝑛𝛼
 and 𝐺(𝑛)
 in 𝑛𝛽
 for 0≤𝛼,𝛽<1
. What is the optimal splitting pair (𝑘,𝑙)
?

Note that for (𝑘,𝑙)
 to be on the rectilinear convex hull of points below 𝑘𝑙=𝑛
, it is necessary and sufficient to satisfy 𝑘𝑙≤𝑛
 and (𝑘+1)(𝑙+1)>𝑛
, so we may essentially assume that 𝑘𝑙∼𝑛
. What is the time complexity required to compute the full sum? It is

∑𝑖=1𝑘𝑂(𝑛𝛽𝑖𝛽)+∑𝑗=1𝑙𝑂(𝑛𝛼𝑗𝛼)=𝑂(𝑛𝛽𝑘𝛽−1+𝑛𝛼𝑙𝛼−1)
To minimize the total complexity, we should make the two summands equal. Substituting 𝑙∼𝑛/𝑘
, we get

𝑛𝛽=𝑛𝑘𝛼+𝛽−2⟹𝑘=𝑛1−𝛽2−𝛼−𝛽
In particular, for 𝛼=0
 and 𝛽=1/2
 we get 𝑘=𝑛1/3
 and 𝑙=𝑛2/3
, making for the total complexity 𝑂(𝑛2/3)
.

Adding precomputation
Okay now, but what if 𝛼
 and 𝛽
 are roughly the same, and also reasonably huge? For example 𝛼=𝛽=1/2
 or even 𝛼=𝛽=2/3
.

Due to symmetry, it's still optimal to split in 𝑘=𝑙=⌊𝑛√⌋
, so we can't make it better by choosing a better splitting point. What we can do, however, is to precompute prefix sums of 𝑓(𝑛)
 and 𝑔(𝑛)
 up to some point 𝑛√≤𝑡<𝑛
.

It's typically possible to do so with linear sieve (see here for details) in 𝑂(𝑡)
, after which the complexity for the full computation becomes

𝑂(𝑡)+∑𝑖=1𝑛/𝑡𝑂(𝑛𝛼𝑖𝛼)+𝑂(𝑛√)=𝑂(𝑡+𝑛𝛼𝑛𝛼−1/𝑡𝛼−1+𝑛√)=𝑂(𝑡+𝑛𝑡𝛼−1+𝑛√).
Optimizing for 𝑡
 makes first 2
 summands equal, thus 𝑡=𝑛1/(2−𝛼)
. This leads to 𝑂(𝑛2/3)
 for 𝛼=1/2
 or 𝑂(𝑛3/4)
 for 𝛼=2/3
.

Adding even more precomputation
The result above is still not quite satisfactory. We see that 𝑛2/3
 is a kind of magical bound, to which a lot of things reduce. Given that, it would be very nice to stick to it, that is to somehow guarantee that if we can compute prefix sums of 𝑓(𝑛)
 and 𝑔(𝑛)
 in 𝑂(𝑛2/3)
, then we also can compute prefix sums of (𝑓∗𝑔)(𝑛)
 in 𝑂(𝑛2/3)
. Turns out that it is possible to do! We just need a bit stronger guarantees.

Claim: Let 𝑓(𝑛)
 and 𝑔(𝑛)
 be such that 𝐹(⌊𝑛/𝑘⌋)
 and 𝐺(⌊𝑛/𝑘⌋)
 are known for all possible arguments. Then we can compute prefix sum 𝐻(𝑛)
 of ℎ(𝑛)=(𝑓∗𝑔)(𝑛)
 in 𝑂(𝑛√)
. Moreover, we can find 𝐻(⌊𝑛/𝑘⌋)
 for all possible arguments in 𝑂(𝑛2/3)
.

Note 1: There might only be at most 2𝑛√
 distinct values of ⌊𝑛/𝑘⌋
, because either 𝑘
 or ⌊𝑛/𝑘⌋
 must be smaller than 𝑛√
.

Note 2: The result above allows us to repeatedly compute Dirichlet convolutions without blowing the complexity, as it will pin to 𝑂(𝑛2/3)
.

Proof: The result about 𝐻(𝑛)
 immediately follows from the hyperbola method.

Additionally, using the identities ⌊⌊𝑛/𝑎⌋𝑏⌋=⌊𝑛𝑎𝑏⌋
 and ⌊⌊𝑛𝑘⌋‾‾‾‾√⌋=⌊𝑛𝑘‾‾√⌋
, we can compute 𝐻(⌊𝑛/𝑘⌋)
 with the hyperbola method as

𝐻(⌊𝑛/𝑘⌋)=∑𝑖=1⌊𝑛/𝑘√⌋𝑓(𝑖)𝐺(⌊𝑛𝑘𝑖⌋)+∑𝑗=1⌊𝑛/𝑘√⌋𝑔(𝑗)𝐹(⌊𝑛𝑘𝑗⌋)−𝐹(⌊𝑛𝑘‾‾√⌋)𝐺(⌊𝑛𝑘‾‾√⌋)
in 𝑂(𝑛𝑘‾‾√)
, as any individual summand is computed in 𝑂(1)
 after pre-computation.

Note that ⌊𝑡√⌋
 always lies in the corner of the rectilinear convex hull of points below 𝑘𝑙=𝑡
. There are two cases two consider here:

The corner is "concave", that is ⌊𝑡√⌋(⌊𝑡√⌋+1)≤𝑡
. It means that ⌊𝑡√⌋=⌊𝑡/𝑙⌋
 for ⌊𝑡√⌋<𝑙≤⌊𝑡⌊𝑡√⌋⌋
.
The corner is "convex", that is ⌊𝑡√⌋(⌊𝑡√⌋+1)>𝑡
. It means that ⌊𝑡√⌋=⌊𝑡/𝑙⌋
 for ⌊𝑡⌊𝑡√⌋+1⌋<𝑙≤⌊𝑡√⌋
.
Note 3: Graphically, on the picture above the corner (3,3)
 is "concave", and the corner (3,5)
 is "convex".

In either case, it means that ⌊𝑡√⌋
 can always be represented as ⌊𝑡/𝑙⌋
 for some 𝑙
. Applying this result to ⌊𝑛/𝑘⌋
 instead of 𝑡
, we deduce that ⌊𝑛𝑘‾‾√⌋
 can be represented as ⌊⌊𝑛/𝑘⌋𝑙⌋=⌊𝑛𝑘𝑙⌋
, thus the values of 𝐹(𝑛)
 and 𝐺(𝑛)
 in ⌊𝑛𝑘‾‾√⌋
 must also be known, and do not contribute any extra burden to the computation time.

Thus, the overall complexity is the sum of 𝑛𝑘‾‾√
 over all values of ⌊𝑛/𝑘⌋
, in which we compute it with the hyperbola method. Since we can pre-compute these values up to 𝑛/𝑘∼𝑛2/3
 in 𝑂(𝑛2/3)
 with the linear sieve, we're only interested in values of 𝑘
 up to 𝑛1/3
, which takes

∑𝑘=1𝑛√3𝑛𝑘‾‾√∼∫0𝑛√3𝑛𝑥‾‾√𝑑𝑥=2𝑛2/3,
proving the overall complexity of 𝑂(𝑛2/3)
.

Dirichlet inverse
Example: Library Judge — Sum of Totient Function. Find the sum of 𝜑(𝑛)
 up to 𝑛≤1010
.

Solution: Explained below, see submission for implementation details.

The result above provides us with an efficient method of computing the prefix sums of the convolution of two sequences. However, it often happens that we can compute the prefix sums of the convolution (𝑓∗𝑔)(𝑛)
, and of one of the functions 𝑓(𝑛)
, but not of the other.

For example, consider the prefix sums of the Euler's totient function 𝜑(𝑛)
. We already know that the Dirichlet convolution of 𝜑(𝑛)
 and 𝑓(𝑛)=1
 is ℎ(𝑛)=𝑛
. We can easily compute prefix sums of 𝑓(𝑛)
 and ℎ(𝑛)
. But computing them for 𝜑(𝑛)
 is much more difficult.

What we can do here is consider the Dirichlet inverse of 𝑓(𝑛)=1
. Dirichlet inverse of a function 𝑓(𝑛)
 is the function 𝑓−1(𝑛)
 such that (𝑓∗𝑓−1)(𝑛)
 equates to the Dirichlet neutral function, which is defined as 1
 for 𝑛=1
 and to 0
 otherwise. As follows from its name, Dirichlet convolution of any function with the Dirichlet neutral yields the initial function.

From Möbius inversion formula, the inverse for it is known to be 𝜇(𝑛)
, the Möbius function, hence we can represent 𝜑(𝑛)
 as

𝜑(𝑛)=∑𝑑|𝑛𝑑𝜇(𝑛𝑑),
that is as the Dirichlet convolution of 𝑓(𝑛)=𝑛
 and 𝜇(𝑛)
. This reduces the task of finding prefix sums for 𝜑(𝑛)
 to finding prefix sums of the Dirichlet convolution of 𝑓(𝑛)=𝑛
 and 𝜇(𝑛)
. But how do we find the prefix sums of 𝜇(𝑛)
 (also known as the Mertens function)?

Claim: Let 𝑓(𝑛)
 be such that we can find values of 𝐹(⌊𝑛/𝑘⌋)
 for all possible arguments in 𝑂(𝑛2/3)
. Then we can also find the prefix sums for all possible arguments of the Dirichlet inverse 𝑓−1(𝑛)
 in 𝑂(𝑛2/3)
.

Proof: Let 𝐹−1(𝑛)
 be the prefix sum of 𝑓−1(𝑛)
 up to 𝑛
. The hyperbola formula allows us to compute 𝐹−1(𝑛)
 in 𝑂(𝑛√)
, granted that we already know all values of 𝐹(⌊𝑛/𝑘⌋)
 and 𝐹−1(⌊𝑛/𝑘⌋)
:

𝐹−1(𝑛)=1𝑓(1)[1+𝐹(⌊𝑛√⌋)𝐹−1(⌊𝑛√⌋)−∑𝑖=1⌊𝑛√⌋𝑓−1(𝑖)𝐹(⌊𝑛/𝑖⌋)−∑𝑖=2⌊𝑛√⌋𝑓(𝑖)𝐹−1(⌊𝑛/𝑖⌋)]
Note: For non-zero multiplicative functions, 𝑓(1)=1
.

With this in mind, we can compute all values of 𝐹−1(𝑛)
 up to 𝑂(𝑛2/3)
 with linear sieve, and then compute 𝐹−1(⌊𝑛/𝑘⌋)
 one by one in the increasing order of arguments.

Each value is computed in 𝑂(𝑛𝑘‾‾√)
 starting with 𝑘≈𝑛√3
 and going up to 𝑘=1
. This gives the 𝑂(𝑛2/3)
 algorithm to find the Dirichlet inverse, and its analysis is essentially the same as with the algorithm for the Dirichlet convolution.

As a proof of concept, I have implemented the code to compute the sums of 𝜇(𝑛)
 and 𝜑(𝑛)
 up to a given bound:

code
You can use 𝑀(1012)=62366
 and Φ(1012)=303963550927059804025910
 to check your implementation.

Exercise 2: Solve Project Euler — #625 Gcd Sum: Compute 𝐺(𝑛)=∑𝑗=1𝑛∑𝑖=1𝑗gcd(𝑖,𝑗)
 for 𝑛=1011
.

Exercise 3: Solve Project Euler — #351 Hexagonal Orchards.

Further reading
[Tutorial] Math note — linear sieve — how to compute prefix of multiplicative functions up to 𝑛
 in 𝑂(𝑛)
 with linear sieve.
[Tutorial] Math note — Möbius inversion — how to apply Möbius inversion to some number-theoretic problems.
[Tutorial] Math note — Dirichlet convolution — another entry on Dirichlet convolution inversion in 𝑂(𝑛2/3)
.
Summing Multiplicative Functions (Pt. 1) — more on computing prefix sums of multiplicative functions.
UPD: Turns out, it is also possible to compute prefix sums in ⌊𝑛/𝑘⌋
 for Dirichlet convolution and Dirichlet inverse in 𝑂(𝑛2/3)
 while only using 𝑂(𝑛√)
 memory, and without linear sieving:

code
See the comment below for details.

Tags tutorial, dirichlet convolution

Dirichlet convolution. Part 2: Dirichlet series and prime counting

By adamant, history, 6 months ago, In English
Hi everyone!

Recently I've published a blog about how one can multiply and divide sequences with Dirichlet convolution. In this blog, we will learn about a convenient framework to reason about them in a "coordinate-free" notation, similar to how generating functions are used to analyze sequences under the regular convolution.

Part 1: Fast prefix sum computation
Part 2: Dirichlet series and prime counting
We will learn how to deal with Dirichlet multiplication and division in the framework of Dirichlet series, and derive a number of well known number theoretic results from this perspective. While doing so, we will learn about Riemann zeta function and will have a glimpse into why it is so important in analyzing behavior of prime numbers.

We will also learn how Dirichlet series framework helps us to, given 𝑔(1),…,𝑔(𝑛)
, to compute 𝑓(1),…,𝑓(𝑛)
 in 𝑂(𝑛log𝑛)
 such that 𝑔(𝑛)
 is the Dirichlet product of 𝑓(𝑛)
 with itself, repeated 𝑘
 times. Besides that, we will learn how to count prime numbers below 𝑛
 in 𝑂(𝑛2/3)
 using logarithms on Dirichlet series.

Convolutions recap
Let 𝑎1,𝑎2,…
 and 𝑏1,𝑏2,…
 be two sequences. We define their Dirichlet convolution as

(𝑎∗𝑏)𝑘=∑𝑖𝑗=𝑘𝑎𝑖𝑏𝑗.
Before working with it, let's recall what we did with other convolutions. Assume, more generically, that we want to analyze

𝑐𝑘=∑𝑖∘𝑗=𝑘𝑎𝑖𝑏𝑗.
What we typically do to work with such convolutions is we introduce a family of functions 𝜌𝑖
 such that

𝜌𝑖𝜌𝑗=𝜌𝑖∘𝑗,
With these objects in mind, we can rewrite the expression above as

∑𝑘𝑐𝑘𝜌𝑘=∑𝑘∑𝑖∘𝑗=𝑘𝑎𝑖𝑏𝑗𝜌𝑖∘𝑗=(∑𝑖𝑎𝑖𝜌𝑖)(∑𝑗𝑏𝑗𝜌𝑗).
Note that we vaguely described 𝜌𝑖
 as "functions" without going into detail about their type and/or domain. This is somewhat intentional, as specific applications may imply different specifications. Consider some standard examples (essentially taken from here).

Examples
Dirichlet series
Now, what should we use for 𝑖∘𝑗=𝑖𝑗
, where 𝑖𝑗
 means simple multiplication? We already noticed that multiplying two exponents with the same base yields to summation in powers as 𝑥𝑖𝑥𝑗=𝑥𝑖+𝑗
. But what happens if we multiply two exponents with the same power? We get multiplication in bases, that is 𝑥𝑠𝑦𝑠=(𝑥𝑦)𝑠
. Thus, we define

𝜌𝑖(𝑠)=𝑖−𝑠,
where 𝑠
 is a positive integer greater than 1
. Using such functions, we define the Dirichlet series of a sequence 𝑎1,𝑎2,…
 as

𝐴(𝑠)=∑𝑖=1∞𝑎𝑖𝑖𝑠
This allows us to rewrite Dirichlet convolution as

𝐶(𝑠)=∑𝑘𝑐𝑘𝑘𝑠=∑𝑘∑𝑖𝑗=𝑘𝑎𝑖𝑏𝑗(𝑖𝑗)𝑠=(∑𝑖𝑎𝑖𝑖𝑠)(∑𝑗𝑏𝑗𝑗𝑠)=𝐴(𝑠)𝐵(𝑠)
Note: We use 𝑖−𝑠
 instead of 𝜌𝑖(𝑠)=𝑖𝑠
 by convention, and so that Dirichlet series converges for integer 𝑠>1
.

With formal power series, it is common to write [𝑥𝑘]𝐹(𝑥)
 to denote the coefficient near 𝑥𝑘
 of the expansion of 𝐹(𝑥)
. For notational convenience, we will also write [𝑛−𝑠]𝐹(𝑠)
 to denote the coefficient near 𝑛−𝑠
 of the Dirichlet series.

Dirichlet power
Consider the following problem from this blog by amiya:

102471C - Dirichlet $k$-th root. Given 𝑔(1),…,𝑔(𝑛)
 and 𝑘
, find 𝑓(1),…,𝑓(𝑛)
 such that (𝑓∗⋯∗𝑓)𝑘(𝑛)=𝑔(𝑛)
.

Solution. When talking about quickly raising functions to arbitrary powers, we generally have two options to proceed.

First option is to represent the power as 𝐹𝑘(𝑠)=exp[𝑘log𝐹(𝑠)]
. Another option is to take the derivative and represent 𝐺=𝐹𝑘
 as

𝐺′𝐹=𝑘𝐹𝑘𝐹′=𝑘𝐺𝐹′.
The later is then used to represent the coefficients of 𝑄(𝑠)
 as a reasonably small recurrence. For Dirichlet series,

∂∂𝑠1𝑛𝑠=∂∂𝑠𝑒−𝑠ln𝑛=−ln𝑛𝑛𝑠,
therefore the derivative of the full Dirichlet series rewrites as

𝐹′(𝑠)=−∑𝑛𝑓(𝑛)ln𝑛𝑛𝑠.
Expanding both sides of 𝐺′𝐹=𝑘𝐺𝐹′
 into Dirichlet product, we get

∑𝑎𝑏=𝑛𝑓(𝑏)𝑔(𝑎)ln𝑎=𝑘∑𝑎𝑏=𝑛𝑔(𝑎)𝑓(𝑏)ln𝑏
Since individual terms of Dirichlet convolution involve much less summands than regular convolution (and the number of summands amortizes to 𝑂(𝑛log𝑛)
 over all indices), we could use the identity above to find 𝑓(𝑛)
 from the values of 𝑔(𝑛)
 and smaller values of 𝑓(𝑛)
. Except for, we need to print exact answer modulo prime number... What do we do about it?

Instead of ln𝑛
, we need to use some function that behaves reasonably similar to ln𝑛
 modulo prime numbers. But what kind of function could it be? What properties of ln𝑛
 do we actually rely on in this recurrence? Well, first thing to notice is that we don't really need 𝑒
 to be the base of the logarithm, as we may multiply both sides by a constant to substitute it with some other base.

But that still won't do it. Instead, we should substitute the derivative with some other operation 𝐷[⋅]
, such that

𝐷[𝐹(𝑠)𝑘]=𝑘𝐹(𝑠)𝑘−1𝐷[𝐹(𝑠)],
for any positive integer number 𝑘
, and 𝐷[𝐹(𝑠)]
 is something with a simple form. By induction, any operation 𝐷[⋅]
 satisfying Leibniz rule

𝐷[𝐹(𝑠)𝐺(𝑠)]=𝐹(𝑠)𝐷[𝐺(𝑠)]+𝐷[𝐹(𝑠)]𝐺(𝑠)
will do. This is, in fact, how derivatives are defined in abstract algebra (see also closely related arithmetic derivative).

Since standard derivative multiplied 𝑓(𝑛)
 with −ln𝑛
, it's reasonable to assume that the function we're looking for also applies to Dirichlet series component-wise, multiplying each 𝑓(𝑛)
 with another function ℎ(𝑛)
. Then, the product rule rewrites as

ℎ(𝑛)∑𝑎𝑏=𝑛𝑓(𝑎)𝑔(𝑏)=∑𝑎𝑏=𝑛[ℎ(𝑎)+ℎ(𝑏)]𝑓(𝑎)𝑔(𝑏).
From this it follows that ℎ(𝑛)
 could be any function such that ℎ(𝑎𝑏)=ℎ(𝑎)+ℎ(𝑏)
 for any positive integer 𝑎,𝑏
. This is it, this is the most important property of ln𝑛
 in our context that we were looking for! Such functions are known as totally additive functions, and one natural example of it could be the prime omega function Ω(𝑛)
, which is the number of prime divisors of 𝑛
, counted with multiplicities.

Multiplicative functions
In number theory, first and most natural example of where Dirichlet series pop up are multiplicative functions. Let's see why.

A multiplicative function is a function 𝑓:ℤ+↦ℂ
 such that 𝑓(𝑎𝑏)=𝑓(𝑎)𝑓(𝑏)
 for any 𝑎,𝑏
 such that gcd(𝑎,𝑏)=1
.

From fundamental theorem of arithmetic it follows that multiplicative functions are fully defined by their values in powers of primes:

𝑛=𝑝𝑑11…𝑝𝑑𝑘𝑘⟹𝑓(𝑛)=𝑓(𝑝𝑑11)…𝑓(𝑝𝑑𝑘𝑘).
Therefore, when working with multiplicative functions, it is often convenient, for each prime number 𝑝
 to consider the sequence

𝑓(1),𝑓(𝑝),𝑓(𝑝2),…,
and sometimes even its ordinary generating function (also known as the Bell series):

𝐹𝑝(𝑥)=∑𝑘𝑓(𝑝𝑘)𝑥𝑘,
as the Dirichlet product over just powers of primes corresponds to the regular product of such generating functions:

(𝑓∗𝑔)(𝑝𝑘)=∑𝑝𝑖𝑝𝑗=𝑝𝑘𝑓(𝑝𝑖)𝑔(𝑝𝑗)=∑𝑖+𝑗=𝑘𝑓(𝑝𝑖)𝑔(𝑝𝑗)=[𝑥𝑘]𝐹𝑝(𝑥)𝐺𝑝(𝑥)
This connection re-expresses itself even more when treated through Dirichlet series framework. Let's, similarly to generating functions above, define the 𝑝
-component of the multiplicative function's Dirichlet series 𝐹𝑝(𝑠)
 as

𝐹𝑝(𝑠)=∑𝑘𝑓(𝑝𝑘)𝑝𝑘𝑠
Using the notation of 𝑝
-components, we can then find out that

𝐹(𝑠)=∑𝑛𝑓(𝑛)𝑛𝑠=∑𝑛𝑓(𝑝𝑑11)…𝑓(𝑝𝑑𝑘𝑘)𝑝−𝑑1𝑠1…𝑝−𝑑𝑘𝑠𝑘=(∑𝑝1𝑓(𝑝𝑑11)𝑝−𝑑1𝑠1)(∑𝑝2𝑓(𝑝𝑑22)𝑝−𝑑2𝑠2)⋯=∏𝑝 prime𝐹𝑝(𝑠)
In other words, the full Dirichlet series can be represented as an infinite product over all primes (so-called Euler product):

∑𝑛𝑓(𝑛)𝑛𝑠=∏𝑝 prime𝐹𝑝(𝑠)
From this, for example, immediately follows that the Dirichlet convolution of multiplicative functions is a multiplicative function, as

𝐹(𝑠)𝐺(𝑠)=∏𝑝 prime𝐹𝑝(𝑠)∏𝑝 prime𝐺𝑝(𝑠)=∏𝑝 prime𝐹𝑝(𝑠)𝐺𝑝(𝑠).
This result is quite important, as it allows us to analyze Dirichlet series of multiplicative functions as infinite sums or infinite products of good-looking functions, depending on what properties are of higher interest to us.

Examples and their Dirichlet series
In this section, we consider some well-known examples of multiplicative functions (taken mostly from here) and how their Dirichlet series are computed. As the main purpose of this section is to get a grasp of various perspectives used to work with Dirichlet series, we will consider several possible derivations for each function.

The constant function 𝐼(𝑛)=1
. Since 𝐼(𝑝𝑘)=1
, its 𝑝
-components look like

𝜁𝑝(𝑠)=∑𝑘1𝑝𝑘𝑠=11−𝑝−𝑠
Therefore, its Dirichlet series can be represented as

𝜁(𝑠)=∑𝑛1𝑛𝑠=∏𝑝 prime11−𝑝−𝑠
This particular Dirichlet series is also known as the Riemann zeta function. From the result above it follows that multiplying the Dirichlet series of a multiplicative function 𝑓(𝑛)
 with the Riemann zeta function, results into a multiplicative function 𝑔(𝑛)
 such that

𝑔(𝑛)=∑𝑑|𝑛𝑓(𝑑).
The Möbius function. In number theory, one often wants to inverse such transform, that is, to find 𝑓(𝑛)
 from a known 𝑔(𝑛)
. As we already noticed above, 𝐺(𝑠)=𝜁(𝑠)𝐹(𝑠)
, thus we can express 𝐹(𝑠)
 as 𝐹(𝑠)=𝜁−1(𝑠)𝐺(𝑠)
. If you're familiar with Möbius inversion, you already know what happens next. We need to find the function 𝜁−1(𝑠)
 explicitly, and we can do so as

𝜁−1(𝑠)=∏𝑝 prime(1−𝑝−𝑠).
If we fully expand it, we get

𝜁−1(𝑠)=∑𝑘(−1)𝑘∏𝑝1,…,𝑝𝑘 prime1𝑝𝑠1…𝑝𝑠𝑘=∑𝑛𝜇(𝑛)𝑛𝑠
In the identity above, we introduced the function 𝜇(𝑛)
 such that

𝜇(𝑛)={(−1)𝑘,0,𝑛=𝑝1…𝑝𝑘,otherwise.
The function 𝜇(𝑛)
 is also known as the Möbius function. It is a multiplicative function, that can be defined in powers of primes as 𝜇(𝑝𝑘)=[𝑘=0]−[𝑘=1]
, where [⋅]
 is the Iverson bracket, which evaluates to 1
 if the expression inside is true and to 0
 otherwise.

From this, we also get the Möbius inversion formula:

𝑓(𝑛)=∑𝑑|𝑛𝜇(𝑛𝑑)𝑔(𝑑).
The power function 𝑓(𝑛)=𝑛𝑎
. Since 𝑓(𝑝𝑘)=𝑝𝑘𝑎
, its 𝑝
-components look like

𝐹𝑝(𝑠)=∑𝑘𝑝𝑘𝑎𝑝𝑘𝑠=11−𝑝𝑎−𝑠
Thus, its Dirichlet series can be represented as

𝐹(𝑠)=∏𝑝 prime11−𝑝𝑎−𝑠=𝜁(𝑠−𝑎)
Of course, the same result can also be obtained directly as

𝐹(𝑠)=∑𝑛𝑛𝑎𝑛𝑠=∑𝑛1𝑛𝑠−𝑎=𝜁(𝑠−𝑎).
The divisor function 𝜎𝑎(𝑛)=∑𝑑|𝑛𝑑𝑎
. Since 𝜎𝑎(𝑝𝑘)=∑𝑖≤𝑘𝑝𝑖𝑎
, its 𝑝
-components look like

𝐹𝑝(𝑠)=∑𝑘1𝑝𝑘𝑠∑𝑖≤𝑘𝑝𝑖𝑎=∑𝑖𝑝𝑖𝑎∑𝑘≥𝑖1𝑝𝑘𝑠=∑𝑖𝑝𝑎𝑖𝑝−𝑖𝑠1−𝑝−𝑠=11−𝑝−𝑠11−𝑝𝑎−𝑠
This, in turn, means that its Dirichlet series can be represented as

𝐹(𝑠)=∏𝑝 prime11−𝑝−𝑠11−𝑝𝑎−𝑠=𝜁(𝑠)𝜁(𝑠−𝑎)
Another way to get the same result directly is as follows:

𝐹(𝑠)=∑𝑑𝑑𝑎∑𝑘1𝑑𝑠𝑘𝑠=∑𝑑,𝑘1𝑑𝑠−𝑎𝑘𝑠=𝜁(𝑠)𝜁(𝑠−𝑎).
And yet another way to get it is to express 𝜎𝑎(𝑛)
 as the Dirichlet convolution of 𝑓(𝑛)=1
 and 𝑔(𝑛)=𝑛𝑎
. The first one has Dirichlet series 𝜁(𝑠)
, and the second one has Dirichlet series 𝜁(𝑠−𝑎)
.

The Euler's totient function 𝜑(𝑛)
. Since 𝜑(𝑝𝑘)=𝑝𝑘−𝑝𝑘−1
, its 𝑝
-components look like

𝐹𝑝(𝑠)=1+(𝑝−1)∑𝑘≥1𝑝𝑘−1𝑝𝑘𝑠=1+𝑝−1𝑝∑𝑘≥11𝑝𝑘(𝑠−1)=1+𝑝−1𝑝𝑝1−𝑠1−𝑝1−𝑠=1−𝑝−𝑠1−𝑝1−𝑠
Multiplying it over all 𝑝
, we get

𝐹(𝑠)=∏𝑝 prime1−𝑝−𝑠1−𝑝1−𝑠=𝜁(𝑠−1)𝜁(𝑠)
Another way to derive it is to express 𝑔(𝑛)=𝑛
 as the Dirichlet convolution of 𝜑(𝑛)
 and 𝑓(𝑛)=1
:

∑𝑑|𝑛𝜑(𝑛𝑑)=𝑛,
as 𝜑(𝑛𝑑)
 is the amount of numbers 𝑘
 from 1
 to 𝑛
 such that gcd(𝑘,𝑛)=𝑑
. But the Dirichlet series of 𝑔(𝑛)
 is known to be 𝜁(𝑠−1)
 and for 𝑓(𝑛)
 it's 𝜁(𝑠)
, so this Dirichlet convolution, indeed, rewrites in terms of Dirichlet series as 𝜁(𝑠)𝐹(𝑠)=𝜁(𝑠−1)
.

Square-free indicator. Let 𝑓(𝑛)
 a function equals to 1
 if 𝑛
 is square-free and to 0
 otherwise. Its 𝑝
-components are

𝐹𝑝(𝑠)=1+1𝑝𝑠=1−𝑝−2𝑠1−𝑝−𝑠
Thus, the Dirichlet series of 𝑓(𝑛)
 is found as

𝐹(𝑠)=∏𝑝 prime1−𝑝−2𝑠1−𝑝−𝑠=𝜁(𝑠)𝜁(2𝑠)
General power function. If 𝐹(𝑠)
 is the Dirichlet series of 𝑓(𝑛)
, then 𝐹(𝑠−𝑎)
 is the Dirichlet series of 𝑓(𝑛)𝑛𝑎
.

Totally multiplicative functions
A totally multiplicative function is a function 𝑓:ℤ+→ℂ
, such that 𝑓(𝑎)𝑓(𝑏)=𝑓(𝑎𝑏)
 for any 𝑎,𝑏
.

For a totally multiplicative functions, its Dirichlet series is even more restricted, as the function is fully determined by its values in primes:

𝑛=𝑝𝑑11…𝑝𝑑𝑘𝑘⟹𝑓(𝑛)=𝑓(𝑝1)𝑑1…𝑓(𝑝𝑘)𝑑𝑘,
and its 𝑝
-components must look like

𝐹𝑝(𝑠)=∑𝑘𝑓(𝑝)𝑘𝑝−𝑘𝑠=11−𝑓(𝑝)𝑝−𝑠,
thus the Dirichlet series of a totally multiplicative function can be represented as

𝐹(𝑠)=∏𝑝 prime11−𝑓(𝑝)𝑝−𝑠
Noteworthy, expanding the inverse of such function gives us

𝐹−1(𝑠)=∑𝑘(−1)𝑘∏𝑝1,…,𝑝𝑘𝑓(𝑝1)…𝑓(𝑝𝑘)𝑝𝑠1…𝑝𝑠𝑘=∑𝑛𝜇(𝑛)𝑓(𝑛)𝑛𝑠
In other words, the inverse 𝑔(𝑛)
 of a totally multiplicative function is 𝑔(𝑛)=𝜇(𝑛)𝑓(𝑛)
. In particular, for power function 𝑓(𝑛)=𝑛𝑎
, which is totally multiplicative, it means that the Dirichlet series of its inverse is

𝜁−1(𝑠−𝑎)=∑𝑛𝜇(𝑛)𝑛𝑎𝑛−𝑠
Another example of a totally multiplicative function, which often occurs in a competitive programming context, is the Legendre symbol:

(𝑎𝑝)=⎧⎩⎨⎪⎪1−10if 𝑎 is a quadratic residue modulo 𝑝 and 𝑎≢0(mod𝑝),if 𝑎 is a quadratic nonresidue modulo 𝑝,if 𝑎≡0(mod𝑝),
which can as well be defined explicitly as

(𝑎𝑝)≡𝑎𝑝−12(mod𝑝) and (𝑎𝑝)∈{−1,0,1}.
It is totally multiplicative in its top argument, as

(𝑎𝑝)(𝑏𝑝)=(𝑎𝑏𝑝).
exp
, log
 and prime-counting
Library Judge — Counting Primes. Count the number of primes no more than 𝑛≤1011
.

This approach was originally shared by ecnerwala in this comment.

We can apply logarithms and exponents to Dirichlet series, as defined by their series representations:

log11−𝑥=∑𝑘=1∞𝑥𝑘𝑘,exp𝑥=∑𝑘=0∞𝑥𝑘𝑘!.
Applying the logarithm series definition to 𝜁(𝑠)
, we get

log𝜁(𝑠)=∑𝑝 primelog11−𝑝−𝑠=∑𝑝 prime∑𝑘=1∞𝑝−𝑘𝑠𝑘=∑𝑘=1∞1𝑘∑𝑝 prime1(𝑝𝑘)𝑠
In other words,

[𝑛−𝑠]log𝜁(𝑠)={1𝑘,0,𝑛=𝑝𝑘,otherwise.
That being said, if we compute prefix sums of a function with Dirichlet series log𝜁(𝑠)
 up to 𝑛
, we can then subtract corresponding counts over 𝑝2,𝑝3,…
 to get the prime-counting function 𝜋(𝑛)
. But how do we find the prefix sums of the logarithm of 𝜁(𝑠)
?

We can use another series representation of the logarithm as

log(1+𝑥)=−∑𝑘=1∞(−1)𝑘𝑥𝑘𝑘,
and plug in 𝜁(𝑠)−1
 instead of 𝑥
. Note that when 𝑓(1)=0
, the values of 𝐹(𝑠)𝑘
 have a quickly vanishing prefix, as

[𝑛−𝑠]𝐹𝑘(𝑠)=∑𝑎1…𝑎𝑘=𝑛𝑎1,…,𝑎𝑘≥2𝑓(𝑎1)…𝑓(𝑎𝑛).
Therefore, the prefix of (𝜁(𝑠)−1)𝑘
 consists entirely of zeros for 𝑛<2𝑘
. Thus, we're only interested in 𝐹𝑘(𝑠)
 for 𝑘≤log2𝑛
. To compute these powers, we can use the approach shared by ecnerwala in the comment to my previous blog about Dirichlet convolutions.

We can split the space of 𝑥𝑦𝑧≤𝑛
 into 𝑂(𝑛2/3)
 rectanges [𝑥0,𝑥1]×[𝑦0,𝑦1]×[𝑧0,𝑧1]
 and use such splitting to compute prefix sums over Dirichlet series 𝐻(𝑠)=𝐹(𝑠)𝐺(𝑠)
 or 𝐹(𝑠)=𝐻(𝑠)/𝐺(𝑠)
, assuming that the prefix sums of the series on the right-hand side are known. Doing it as is would take 𝑂(𝑛2/3log𝑛)
, but we can speed up the whole computation. Let 𝑡≈𝑛1/6
, then

𝜁𝑡(𝑠)=∏𝑝 prime𝑝>𝑡11−𝑝−𝑠
has an empty prefix of length 𝑡
, and we will only need first log𝑛log𝑡
 values of (𝜁𝑡(𝑠)−1)𝑘
 instead of log𝑛
 to compute the prefix sums of log𝜁𝑡(𝑠)
 up to 𝑛
. To find 𝜁𝑡(𝑠)
 itself, we should use the fact that multiplying with 1−𝑝−𝑠
 can be done in 𝑂(𝑛√)
, as after such multiplication 𝐹(⌊𝑛/𝑘⌋)
 simply gets reduced by 𝐹(⌊𝑛/(𝑘𝑝)⌋)
.

After this, we can add the contributions of first 𝑡
 terms back into logarithm to recover log𝜁(𝑠)
 from log𝜁𝑡(𝑠)
.

Note 1: As ecnerwala mentioned, this approach could more generally be used to sum up any totally multiplicative function over all primes up to 𝑛
. Indeed, the logarithm of the Dirichlet series of a totally multiplicative function is

log𝐹(𝑠)=log∏𝑝 prime11−𝑝(𝑠)𝑝−𝑠=∑𝑘=1∞1𝑘∑𝑝 prime𝑓(𝑝)𝑘(𝑝𝑘)𝑠
Thus, as we see,

[𝑛−𝑠]log𝐹(𝑠)={𝑓(𝑝)𝑘𝑘,0,𝑛=𝑝𝑘,otherwise.
Note 2: The approach above can also be used to compute prefix sums of logarithms and exponents of any Dirichlet series. Generally, we can represent any Dirichlet series as an infinite product over not necessarily prime numbers as

∑𝑛=1∞𝑔(𝑛)𝑛𝑠=∏𝑛=1∞11−𝑓(𝑛)𝑛−𝑠
In his comment, ecnerwala calls 𝑔(𝑛)
 the pseudo-Euler transform of 𝑓(𝑛)
, in reference to Euler transform of integer sequences. Similarly to what we did with primes, we can find 𝑓(1),…,𝑓(𝑡)
 for 𝑡≈𝑛1/6
 and divide 𝐺(𝑠)
 by corresponding fractions to get

𝐺𝑡(𝑠)=∏𝑛>𝑡11−𝑓(𝑛)𝑛−𝑠,
so that we can find prefix sums of 𝐺𝑡(𝑠)
 in 𝑂(𝑛2/3)
, and then amend them with logarithms of fractions to get log𝐺(𝑠)
. To find 𝐺𝑡(𝑠)
 itself we, again, shout notice that, again, multiplying 𝐺(𝑠)
 with 1−𝑓(𝑡)𝑡−𝑠
 can be done in 𝑂(𝑛√)
, as after such multiplication, the value 𝐹(⌊𝑛/𝑘⌋)
 gets reduced by 𝑓(𝑡)𝐹(⌊𝑛/(𝑘𝑡)⌋)
.

Note 3: The logarithm log𝜁(𝑠)
 connects the Riemann zeta function with the prime zeta function

𝑃(𝑠)=∑𝑝 prime1𝑝𝑠.
This connection, in turn, allows to express the prime zeta function in terms of Riemann zeta function through generalized Möbius inversion:

log𝜁(𝑠)=∑𝑘=1∞𝑃(𝑘𝑠)𝑘⟺𝑃(𝑠)=∑𝑘=1∞𝜇(𝑘)𝑘log𝜁(𝑘𝑠)
See also
Counting primes in 𝑂̃ (𝑛2/3)
 — also the comment by ecnerwala on the approach through Dirichlet series.
Lucy's Algorithm + Fenwick Trees — another approach to count primes up to 𝑛
 in 𝑂(𝑛2/3log1/3𝑛)
.
Tags dirichlet convolution, tutorial

Stirling numbers with fixed n and k

By adamant, history, 6 months ago, In English
Hi everyone!

In combinatorics, you often need to compute Stirling numbers for various reason. Stirling numbers of the first kind count the number of permutations of 𝑛
 elements with 𝑘
 disjoint cycles, while Stirling numbers of the second kind count the number of ways to partition a set of 𝑛
 elements into 𝑘
 nonempty subsets. Besides that, they're often arise when you need to change between powers of 𝑥
 and rising/falling factorials. There are three problems on Library Checker that go as follows:

Library Checker — Stirling Number of the First Kind. Given 𝑁
, find 𝑠(𝑁,𝑘)
 for 0≤𝑘≤𝑁
.

Library Checker — Stirling Number of the Second Kind. Given 𝑁
, find 𝑆(𝑁,𝑘)
 for 0≤𝑘≤𝑁
.

Library Checker — Stirling Number of the First Kind (Fixed K). Given 𝑁
 and 𝐾
, find 𝑠(𝑛,𝐾)
 for 𝐾≤𝑛≤𝑁
.

Here 𝑠(𝑛,𝑘)
 are Stirling numbers of the first kind, and 𝑆(𝑛,𝑘)
 are Stirling numbers of the second kind. Let's solve them!

Definitions
There are various ways to define Stirling numbers. In this blog, we will follow the algebraic approach to highlight their connection.

Def.: The falling factorial is the polynomial (𝑥)𝑛
 defined as

(𝑥)𝑛=𝑥(𝑥−1)…(𝑥−𝑛+1)𝑛 factors=∏𝑖=0𝑛−1(𝑥−𝑖)
Def.: The rising factorial is the polynomial 𝑥(𝑛)
 defined as

𝑥(𝑛)=𝑥(𝑥+1)…(𝑥+𝑛−1)𝑛 factors=∏𝑖=0𝑛−1(𝑥+𝑖)
Note: From the definitions above, it follows that 𝑥(𝑛)=(𝑥+𝑛−1)𝑛=(−1)𝑛(−𝑥)𝑛
.

We will start with Stirling numbers of the second kind, as the proof is simpler for them.

Def.: Stirling numbers of the second kind are the coefficients 𝑆(𝑛,𝑘)
 in the expansion of 𝑥𝑛
 into falling factorials:

𝑥𝑛=∑𝑘=0𝑛𝑆(𝑛,𝑘)(𝑥)𝑘
Claim: The value 𝑆(𝑛,𝑘)
 equates to 𝐶(𝑛,𝑘)
 the number of ways to partition 𝑛
 distinct elements into 𝑘
 non-empty sets.

Note: We assume that there is a unique way to partition a set of 0
 elements into 0
 non-empty sets.

Proof
Def.: Stirling numbers of the first kind are the coefficients 𝑠(𝑛,𝑘)
 in the expansion of (𝑥)𝑛
 into powers of 𝑥
:

(𝑥)𝑛=∑𝑘=0𝑛𝑠(𝑛,𝑘)𝑥𝑘
Note: From the definition above, and the fact that 𝑥(𝑛)=(−1)𝑛(−𝑥)𝑛
, it follows that

𝑥(𝑛)=∑𝑘=0𝑛(−1)𝑛−𝑘𝑠(𝑛,𝑘)𝑥𝑘=∑𝑘=0𝑛|𝑠(𝑛,𝑘)|𝑥𝑘.
Claim: The absolute value |𝑠(𝑛,𝑘)|
 equates to 𝑐(𝑛,𝑘)
, is the number of permutations of length 𝑛
 with 𝑘
 disjoint cycles.

Proof
Generating functions
Finding Stirling numbers with fixed 𝑛
 or 𝑘
 will most certainly involve working with polynomials. As it's usually most convenient to explain polynomial operations in the framework of generating functions, let's find them for Stirling numbers.

Stirling numbers of the second kind
As we already mentioned, 𝑆(𝑛,𝑘)
 is the number of ways to partition a set of 𝑛
 distinct objects into 𝑘
 non-empty sets. The exponential formula tells us that if 𝐹(𝑥)
 is the exponential generating function for objects of some kind, then exp𝐹(𝑥)
 is the generating function for sets of such objects, also meaning that exp𝐹(𝑥)−1
 is the generating function for non-empty sets of such objects.

As things enumerated by 𝑆(𝑛,𝑘)
 can be described as "sets of 𝑘
 non-empty sets of objects", the exponential generating function should look roughly as 𝑒𝑒𝑥−1
, where 𝑥
 is a formal variable keeping track of "base" objects. However, for Stirling numbers we need to keep track of the base objects (their total amount sums to 𝑛
), and of the non-empty sets, in which we split them (their amount sums to 𝑘
).

To account for non-empty sets, we will introduce another variable 𝑦
, and use the generating function

𝐹(𝑥,𝑦)=𝑒𝑦(𝑒𝑥−1)
instead. In this way, when the external exponent is expanded into powers of (𝑒𝑥−1)
, each summand (𝑒𝑥−1)𝑘𝑘!
, corresponding to sets of 𝑘
 non-empty sets will be additionally multiplied by 𝑦𝑘
. Thus, we get the expansion into generating function as

∑𝑛=0∞∑𝑘=0𝑛𝑆(𝑛,𝑘)𝑥𝑛𝑛!𝑦𝑘=𝑒𝑦(𝑒𝑥−1)
Fixed 𝑘
Using the power series definition of exponent, we can then expand it as

𝑒𝑦(𝑒𝑥−1)=∑𝑘=0∞𝑦𝑘(𝑒𝑥−1)𝑘𝑘!,
from which we get the exponential generating function with a fixed 𝑘
:

𝑆(𝑛,𝑘)=[𝑥𝑛𝑛!](𝑒𝑥−1)𝑘𝑘!
Fixed 𝑛
Library Checker — Stirling Number of the Second Kind. Given 𝑁
, find 𝑆(𝑁,𝑘)
 for 0≤𝑘≤𝑁
.

But what if we want to expand with fixed 𝑛
? No worries! Let 𝐹(𝑦)
 be the ordinary generating function of 𝑆(𝑛,𝑘)
 with fixed 𝑛
.

From the definition of Stirling numbers, considering 𝑡=0,1,…
 we get

𝑡𝑛=∑𝑘=0𝑛𝑆(𝑛,𝑘)𝑡!(𝑡−𝑘)!.
Right-hand side can be perceived as a convolution of 𝐹(𝑦)
 and 𝑒𝑦
, as the later expands into sum of 𝑦𝑡−𝑘(𝑡−𝑘)!
. Thus,

∑𝑡=0∞𝑡𝑛𝑦𝑡𝑡!=𝑒𝑦𝐹(𝑦),
from which follows the generating function with a fixed 𝑛
:

𝑆(𝑛,𝑘)=[𝑦𝑘]𝑒−𝑦∑𝑡=0∞𝑡𝑛𝑦𝑡𝑡!
Defined this way, the generating function on the right-hand side is also known as the 𝑛
-th Touchard polynomial.

Expanded, the expression above transforms into the well-known explicit formula

𝑆(𝑛,𝑘)=1𝑘!∑𝑡=0𝑘(−1)𝑘−𝑡(𝑘𝑡)𝑡𝑛=∑𝑡=0𝑘(−1)𝑘−𝑡𝑡𝑛𝑡!(𝑘−𝑡)!
which can also be obtained through the principle of inclusion and exclusion. The formula above can also be used to compute a specific 𝑆(𝑛,𝑘)
 in 𝑂(𝑘log𝑘𝑛)
 without Fourier transform or other complicated algorithms.

Stirling numbers of the first kind
Generally, a permutation can be represented as a set of cycles. The exponential generating function for cycles is known to be

log11−𝑥=∑𝑘=1∞𝑥𝑘𝑘.
Now, we can mark each cycle by an additional formal variable 𝑦
 and apply exponential formula. However, we should note that for signed Stirling numbers, we should multiply the coefficient near 𝑥𝑛𝑦𝑘
 with (−1)𝑛−𝑘
, which is done by substituting 𝑥
 and 𝑦
 with −𝑥
 and −𝑦
:

𝐹(𝑥,𝑦)=𝑒−𝑦log11+𝑥=𝑒𝑦log(1+𝑥)=(1+𝑥)𝑦.
So, similarly to Stirling numbers of the second kind, we get the expansion

∑𝑛=0∞∑𝑘=0𝑛𝑠(𝑛,𝑘)𝑥𝑛𝑛!𝑦𝑘=(1+𝑥)𝑦
Unlike generating functions for the Stirling numbers of the second kind, this one can be checked in a very straightforward way:

(1+𝑥)𝑦=∑𝑛=0∞(𝑦𝑛)𝑥𝑛=∑𝑛=0∞𝑥𝑛𝑛!(𝑦)𝑛=∑𝑛=0∞𝑥𝑛𝑛!∑𝑘=0𝑛𝑠(𝑛,𝑘)𝑦𝑘.
Fixed 𝑘
Library Checker — Stirling Number of the First Kind (Fixed K). Given 𝑁
 and 𝐾
, find 𝑠(𝑛,𝐾)
 for 𝐾≤𝑛≤𝑁
.

One obvious way to find the generating function with fixed 𝑘
 is to expand

𝑒𝑦log(1+𝑥)=∑𝑘=0∞log(1+𝑥)𝑘𝑘!𝑦𝑘,
so we get the exponential generating function with a fixed 𝑘
:

𝑠(𝑛,𝑘)=[𝑥𝑛𝑛!]log(1+𝑥)𝑘𝑘!
Fixed 𝑛
Library Checker — Stirling Number of the First Kind. Given 𝑁
, find 𝑠(𝑁,𝑘)
 for 0≤𝑘≤𝑁
.

Sasha and the Swaps II. Given 𝑛
, find the number of distinct permutations obtainable with exactly 𝑘
 swaps for each 𝑘
 from 1
 to 𝑛−1
.

For a fixed 𝑛
, we already know that the generating function would be

(𝑦)𝑛=𝑦(𝑦−1)…(𝑦−𝑛+1).
Computing it as is with divide and conquer approach, we would get 𝑂(𝑛log2𝑛)
. Can we do better? Sure thing! We can represent it as

(𝑦)2𝑛=(𝑦)𝑛(𝑦−𝑛)𝑛,
therefore we can first compute (𝑦)𝑛
, then do a Taylor shift to find (𝑦−𝑛)𝑛
 and multiply them in 𝑂(𝑛log𝑛)
. The overall time is

𝑇(𝑛)=𝑇(𝑛2)+𝑂(𝑛log𝑛)=𝑂(𝑛log𝑛).
Tags tutorial, stirling numbers

Evaluation and interpolation on geometric progression

By adamant, history, 6 months ago, In English
Hi everyone!

Two problems were recently added to Library Checker:

Multipoint Evaluation (Geometric Sequence). Given 𝑓(𝑥)
, find 𝑓(𝑎𝑟𝑖)
 for the given 𝑎,𝑟
 and 0≤𝑖<𝑚
.
Polynomial Interpolation (Geometric Sequence). Given 𝑓(𝑎𝑟𝑖)=𝑦𝑖
 for 0≤𝑖≤𝑛
, find 𝑓(𝑥)
 of degree 𝑛
.
Note: We can divide or multiply the 𝑘
-th coefficient of initial/resulting polynomial by 𝑎𝑘
, so we may assume 𝑎=1
.

Today we'll learn how to solve both of them in 𝑂(𝑛log𝑛)
.

Evaluation at 𝑟𝑘
Well, the first one is also known as chirp Z-transform and it's by now standard (see here or here). It uses the fact that

(𝑎+𝑏2)−(𝑎2)−(𝑏2)=𝑎𝑏,
which is also important e.g. for graphic generating functions. Using this identity, we can represent

𝑓(𝑟𝑘)=∑𝑗=0𝑛𝑓𝑗𝑟𝑘𝑗=𝑟−(𝑘2)∑𝑗=0𝑛(𝑓𝑗𝑟−(𝑗2))𝑟(𝑘+𝑗2).
In other words, the sequence 𝑐𝑘=𝑟(𝑘2)𝑓(𝑟𝑘)
 is the cross-correlation of the sequences 𝑎𝑖=𝑓𝑖𝑟−(𝑖2)
 and 𝑏𝑗=𝑟(𝑗2)
.

This approach, commonly known as Bluestein algorithm, allows to solve the first task in 𝑂(𝑛log𝑛)
. But what about the second?

Interpolation at 𝑟𝑘
It gets suspiciously difficult at this point! Naturally, we may try to invert the Bluestein algorithm, but we will fail. Why? Well, if we know 𝑓(𝑟𝑘)
, we can recover 𝑐𝑘
 directly. We also know the values of 𝑏𝑗
. So, it shouldn't be difficult to recover 𝑎𝑖
? Let's write it as a linear system:

⎛⎝⎜⎜⎜⎜𝑐0𝑐1…𝑐𝑛⎞⎠⎟⎟⎟⎟=⎛⎝⎜⎜⎜⎜𝑏0𝑏1⋮𝑏𝑛𝑏1𝑏2⋮𝑏𝑛+1……⋱…𝑏𝑛𝑏𝑛+1⋮𝑏2𝑛⎞⎠⎟⎟⎟⎟⎛⎝⎜⎜⎜⎜𝑎0𝑎1…𝑎𝑛⎞⎠⎟⎟⎟⎟
It would be natural to be able to reverse cross-correlation in a similar manner to how we can inverse regular convolution. Well, it doesn't work this way. Why? Let's look on the system for convolution of 𝑎𝑖
 and 𝑏𝑗
 instead of cross-correlation:

⎛⎝⎜⎜⎜⎜𝑐0𝑐1…𝑐𝑛⎞⎠⎟⎟⎟⎟=⎛⎝⎜⎜⎜⎜𝑏0𝑏1⋮𝑏𝑛0𝑏0⋮𝑏𝑛−1……⋱…00⋮𝑏0⎞⎠⎟⎟⎟⎟⎛⎝⎜⎜⎜⎜𝑎0𝑎1…𝑎𝑛⎞⎠⎟⎟⎟⎟
Notice anything? Yep. When we invert convolution, we, implicitly, find an inverse for a lower-triangular matrix. But with a cross-correlation, it is no longer the case. So, what matrix do we actually work with? Generally, matrices like the one we have for cross-correlation are known as Hankel matrices or, if we reverse the rows, as Toeplitz matrices. From their definition, it seems that inverting a cross-correlation is, in the general case, equivalent to multiplying a vector with an inverse of a general Hankel/Toeplitz matrix.

Okay. How fast can we do so in general case? Wikipedia points to so-called Levinson recursion, which runs in 𝑂(𝑛2)
, and trying to find anything faster doesn't provide anything fruitful enough: fastest known algorithms seem to all be 𝑂(𝑛log2𝑛)
 and based on divide and conquer, similar to general solution for polynomial interpolation. If we want 𝑂(𝑛log𝑛)
 algorithm, we must do something beyond inverting general cross-correlation from Bluestein algorithm, and we must use the fact that we're working with a geometric progression.

Lagrange interpolation
let's recall how we could interpolate this polynomial with Lagrange interpolation:

𝑓(𝑥)=∑𝑖=0𝑛𝑦𝑖∏𝑗≠𝑖𝑥−𝑥𝑗𝑥𝑖−𝑥𝑗.
For geometric progression 1,𝑧,𝑧2,…,𝑧𝑛
, we have a special case 𝑥𝑖=𝑧𝑖
 which allows us to greatly simplify this expression. First of all, we should notice that now we're able to compute denominator near each 𝑦𝑖
 in 𝑂(𝑛)
 by representing it as

∏𝑗≠𝑖(𝑧𝑖−𝑧𝑗)=𝑧𝑛𝑖∏𝑗≠𝑖(1−𝑧𝑗−𝑖)=𝑧𝑛𝑖∏𝑗=1𝑖(1−𝑧−𝑗)∏𝑗=1𝑛−𝑖(1−𝑧𝑗).
So, we can compute the denominator near 𝑦𝑖
 as 𝑧𝑛𝑖
, times a prefix product of 1−𝑧−𝑘
, times a prefix product of 1−𝑧𝑘
. Now let 𝑦′𝑖
 be 𝑦𝑖
, divided by corresponding denominators. We can find all 𝑦′𝑖
 in 𝑂(𝑛log𝑛)
 with the decomposition above.

After that what remains is for us to compute the remaining sum, which looks like

𝑓(𝑥)=∑𝑖=0𝑛𝑦′𝑖∏𝑗≠𝑖(𝑥−𝑧𝑗).
Partial fraction decomposition
Let's divide each side by 𝑄𝑛(𝑥)=∏𝑗=0𝑛(𝑥−𝑧𝑗)
, then the expression rewrites as

𝑓(𝑥)𝑄𝑛(𝑥)=∑𝑗=0𝑛𝑦′𝑗𝑥−𝑧𝑗.
Hmm, so, if we could find right-hand side as a formal power series, we might recover 𝑓(𝑥)
 by multiplying RHS with 𝑄𝑛(𝑥)
 and then only keeping first 𝑛+1
 coefficients. Of course, two questions arise immediately when we think about it:

How to find first 𝑛+1
 coefficients of the right-hand side?
How to compute 𝑄𝑛(𝑥)
 quickly enough?
How to expand right-hand side?
To answer both questions, let's consider polynomial reversal

𝑄𝑅𝑛(𝑥)=∏𝑗=0𝑛(1−𝑧𝑗𝑥).
With this, the expression above rewrites as

𝑓𝑅(𝑥)𝑄𝑅𝑛(𝑥)=∑𝑗=0𝑛𝑦′𝑗1−𝑧𝑗𝑥.
That's much better! Now we can expand right hand side as 11−𝑥=1+𝑥+𝑥2+𝑥3+…
, and get...

∑𝑗=0𝑛𝑦′𝑗1−𝑧𝑗𝑥=∑𝑗=0𝑛𝑦′𝑗∑𝑖=0∞𝑧𝑗𝑖𝑥𝑖=∑𝑖=0∞𝑥𝑖∑𝑗=0𝑛𝑦′𝑗𝑧𝑖𝑗.
Huh?! The coefficient near 𝑥𝑖
 in this expression is the value of 𝑔(𝑥)=∑𝑗=0𝑛𝑦′𝑗𝑥𝑗
 in the point 𝑥=𝑧𝑖
!

So, we can find first 𝑛+1
 coefficients of the right-hand side via regular chirp Z-transform in 𝑂(𝑛log𝑛)
!

How to compute 𝑄𝑛(𝑥)
 quickly?
And what about 𝑄𝑅𝑛(𝑥)
? Turns out, we can also find it in 𝑂(𝑛log𝑛)
! For this, we notice that

𝑄𝑛(𝑥)=𝑄𝑡(𝑥)𝑄𝑛−𝑡(𝑧𝑡𝑥)
for any 𝑡
. If we know 𝑄𝑛−𝑡(𝑥)
, finding 𝑄𝑛−𝑡(𝑧𝑡𝑥)
 can be done by multiplying the 𝑘
-th coefficient of 𝑄𝑛−𝑡(𝑥)
 with 𝑧𝑡𝑘
. From this, we notice that we can compute 𝑄𝑛(𝑥)
 with a process similar to binary exponentiation by picking 𝑡=⌊𝑛/2⌋
. The procedure then works in

𝑇(𝑛)=𝑇(𝑛2)+𝑂(𝑛log𝑛)=𝑂(𝑛log𝑛).
Altogether it solves the whole interpolation problem in 𝑂(𝑛log𝑛)
. Please refer to my submission on Library Checker for implementation.

Tags tutorial, chirp z-transform, polynomial interpolation

Implementing Dinitz on bipartite graphs

By adamant, history, 5 months ago, In English
Hi everyone!

if you read about Dinic algorithm on CP-Algorithms, especially the section about how it works on unit networks, you'll notice that its supposed runtime for bipartite matching is 𝑂(𝐸𝑉‾‾√)
, where 𝐸
 is the total number of edges, and 𝑉
 is the total number of vertices of a bipartite graph. Let's try it out on the following problem:

Library Checker — matching on Bipartite Graph. Given a bipartite graph with 𝑉,𝐸≤2⋅105
, find the maximum matching.

The implementation of the Dinitz algorithm that I typically use looks as follows:

code
Now, if we make a submission with this implementation, we get... Time Limit Exceeded?! Okay, that's weird. Apparently, roughly the fastest practical algorithm for maximum bipartite matching is so-called Hopcroft-Karp algorithm. Maybe it has a better complexity? Nope, Wikipedia page says it's 𝑂(𝐸𝑉‾‾√)
. Maybe it's somehow fundamentally different from Dinitz? No, not really — Wikipedia page explicitly states that it is essentially a special case of Dinitz algorithm. So... What gives such bad performance?

Of course, there can simply be a bug in my Dinitz algorithm implementation, but after some checks, it seems that it's not the case. On particular failing test-case, we can see that the algorithm really just executes around 200
 iterations of traversing the whole flow network of roughly 8⋅105
 edges. So, it seems that the core reason is just the enormous constant-time overhead.

So... If Hopcroft-Karp is just the special case of Dinitz algorithm, applied to bipartite graphs, how do we make it 50x faster?

Blocking Kuhn algorithm
UPD: It seems that, if applied as is (without randomization), this heuristics can be broken by the test case from this comment by dacin21. If you want to use it, please also include randomization of vertices/edge, as in this submission.

It may seem a bit weird, but let's first talk about something different. One standard and pretty well-known algorithm for bipartite matching is Kuhn's algorithm. It's known to run 𝑂(𝑉𝐸)
, which wouldn't work as is for this problem, but there are some optimizations to it that will actually allow to get AC on this problem. Yes, you heard it right, with proper heuristics, you can get AC on this problem with presumable 𝑂(𝑉𝐸)
 runtime, while proven 𝑂(𝐸𝑉‾‾√)
 runtime algorithm fails. So... What is standard Kuhn's algorithm? It looks like this:

code
For each vertex in the right part, we keep an array match, so that match[u] is −1
, if it's not matched with anything, and match[u] = v if it is matched with the vertex 𝑣
 from the left part. Then, for each vertex 𝑣
 from the left part, we try to match it with some vertex 𝑢
 from the right part. If 𝑢
 is not matched — that's great, we can just match it with 𝑣
. Otherwise, we try to recursively match the vertex match[u] with something different. if we succeed, the vertex 𝑢
 is free now, and we can match 𝑣
 with it. Otherwise, we go to the next 𝑢
 in the adjacency list of 𝑣
.

Of course, using this implementation, we get Time Limit Exceeded. But! We can change just a few lines to get Accepted instead. Here:

for(int i = 0; i < n; i++) {
    visited.assign(n, 0);
    kuhn(i);
}
Written as it is now, the algorithm is essentially equivalent to Ford-Fulkerson algorithm, as we find augmenting paths one by one and push the flow alongside the path. After that, we fully reset visited tags and run everything from scratch. But the core idea of Dinitz algorithm is to push as much flow as possible before resetting visited tags and trying pushing it again. A flow found this way is called "blocking flow", as it blocks as from finding further augmenting paths without updating the residual network. So... let's do just that. Try to match as much vertices as possible before resetting visited:

vector<bool> paired(n);
for(bool repeat = true; repeat;) {
    repeat = false;
    visited.assign(n, 0);
    for(int i = 0; i < n; i++) {
        if(!paired[i]) {
            paired[i] = kuhn(i);
            repeat |= paired[i];
        }
    }
}
And doing just that is enough to get Accepted on the problem using just 1583ms out of 5s! Not bad for a 𝑂(𝑉𝐸)
 algorithm, huh? This heuristic was already described 8 years ago in this Russian blog by Burunduk1, but I haven't seen its highlight in English yet.

Anyway, adding a bit more optimizations and heuristics, you can bring it down to 986ms, while still using Kuhn's algorithm.

Dinitz on bipartite graphs
But what if we still want something faster? After all, we still didn't use the Dinitz's algorithm most important concept. Layered networks. The implementation above finds blocking flow in some network, which is defined by depth-first search. But to provably constrain the number of iterations of finding blocking flow, we need to make sure that it is a specific network. Namely, the layered network defined by shortest paths from source to each vertex. Without constructing the flow network explicitly, we still can do it like this:


vector<int> dist;
bool bfs() {
    dist.assign(n, n);
    int que[n];
    int st = 0, fi = 0;
    for(int v = 0; v < n; v++) {
        if(!paired[v]) {
            dist[v] = 0;
            que[fi++] = v;
        }
    }
    bool rep = false;
    while(st < fi) {
        int v = que[st++];
        for(auto e: g[v]) {
            int u = match[e];
            rep |= u == -1;
            if(u != -1 && dist[v] + 1 < dist[u]) {
                dist[u] = dist[v] + 1;
                que[fi++] = u;
            }
        }
    }
    return rep;
}
In this implementation, rather than initiating the bfs queue with source, we initiate it with all vertices that are directly reachable from it (that is, vertices of the left part that are not paired yet). After that, we find the shortest distance from such vertices to each vertex in the left part, and we can use this information to implicitly only traverse the layered network:

vector<size_t> ptr;
bool kuhn(int v) {
    for(size_t &i = ptr[v]; i < size(g[v]); i++) {
        int &u = match[g[v][i]];
        if(u == -1 || (dist[u] == dist[v] + 1 && kuhn(u))) {
            u = v;
            paired[v] = true;
            return true;
        }
    }
    return false;
}
This time, similar to Dinitz algorithm, we maintain a pointer to the last checked edge in each vertex. Then, the main loop looks like

while(bfs()) {
    ptr.assign(n, 0);
    for(int v = 0; v < n; v++) {
        if(!paired[v]) {
            kuhn(v);
        }
    }
}
Implemented this way, it gets Accepted in just 160ms, which is a huge improvement over blocking Kuhn's algorithm which ran in 960ms at best, and certainly over Dinitz algorithm in explicitly constructed flow network, which couldn't even pass under 5s... Note: This is my interpretation of how Dinitz on bipartite graphs should look like if we store the network implicitly. I'm not entirely sure if it coincides with Hopcroft-Karp algorithm, because I don't know it.

Tags tutorial, bipartite matching, hopcroft-karp, dinitz

I'm adamant, AMA

By adamant, history, 5 months ago, In English
Hi everyone!

Recently, my interactions with Codeforces met certain milestones. Here they are:

My total blog count passed over 100:

 

I reached the rated contribution top:

 

I'm red again for the first time since 2021:

 

To celebrate these 3 happy occasions, I want to make an AMA session. There were some AMA from people that are generally much more popular than I am (see here, here and here), and I am mentally preparing to see that nobody comes to the party, but who knows? :)

So... Let's go. Ask me anything you want in the comments.

Tags ask me away

Osijek Competitive Programming Camp 2023 fall — wrap

By adamant, history, 2 months ago, In English
Hi everyone!


Supported by
  
 
The Osijek competitive programming camp (also see the announcement on Codeforces) concluded on September 24, and I'd like to write this blog post as a wrap to this fall's iteration of the camp. Overall, 62 teams joined the camp, and 9 of them attended the camp onsite in Osijek. Tähvend (-is-this-fft-) and I (adamant), as organizers, were also there! And we would like to say a big thanks to ajovanov who immensely helped us with the organisation onsite.

Compared to previous camp, we grew in supporters a bit, as this time we were also supported by Wolfram Research, who offered all camp participants free 6 months of Wolfram|One Personal Edition and Wolfram|Alpha Pro, and by Art of Problem Solving who supported us by a few AoPS 25$ coupons, which we presented to the best onsite team.

On top of that, similar to the winter edition of the camp, each onsite participant was provided by 2 t-shirts, one from the camp itself and another from our sponsor, Jane Street.

More importantly, we have established contact with ICPC global, and for this installment of the camp, we were also sponsored by ICPC foundation. We are very happy about this opportunity to develop a stronger tie with official ICPC, and we are looking forward to the products of this collaboration in the future.

  	  
You can find the remaining photos here
The camp consisted of 7 contests, and 2 days off. On the first day off, the onsite participants had an opportunity to go to laser tag, and on the second day off, to visit an escape room and a local zoo. As for the contests, you may check the combined scoreboard for further information. Congratulations to the top teams!

As promised in the announcement, we will maintain a silence period until the end of November, so none of our contest will go public until then. After the date, at least one contest will be used in the universal cup, and we also plan to upload at least one other contest to Codeforces, so that potential participants of the camp can have a demo of our contest quality and difficulty levels.

Yet again, I'm very happy that the idea of the camp works out, and as such I want to make an announcement:

Call for problemsetters
We're looking for problemsetters for the next iteration of the camp!

Specific dates are to be announced later on, but you may express your interest by writing a direct message to me (adamant) or -is-this-fft-, and we will get back to you. Once the camp is conducted, we will offer you a monetary compensation.

The compensation is expected to be ~2000€ for the contest itself, and also ~400€ if you agree to conduct the contest analysis (subject to withholding tax by Croatian laws). The numbers are preliminary and may change depending on how things go with sponsors and the number of participants. We will also fully waive the participation fees for you and your team if you want to attend the rest of the camp.

Tags ocpc, training camp

MacMahon's master theorem

By adamant, history, 3 weeks ago, In English
Hi everyone!

Mandatory orz to Elegia whose blog introduced me to MMT as an elegant approach to prove Dixon's identity.

Today, I would like to write about MacMahon's master theorem (MMT). It is a nice result on the intersection of combinatorics and linear algebra that provides an easy proof to some particularly obscure combinatorial identities, such as the Dixon's identity:

∑𝑘(−1)𝑘(𝑎+𝑏𝑎+𝑘)(𝑏+𝑐𝑏+𝑘)(𝑐+𝑎𝑐+𝑘)=(𝑎+𝑏+𝑐𝑎,𝑏,𝑐).
Besides that, MMT has large implications in Quantum Physics, which we will hopefully discuss. For now, let's formulate MMT.

MMT. Let 𝐀=[𝑎𝑖𝑗]𝑛×𝑛
, 𝐗=diag(𝑥1,…,𝑥𝑛)
, 𝐭=(𝑡1,…,𝑡𝑛)
, 𝐱=(𝑥1,…,𝑥𝑛)
 and 𝐤=(𝑘1,…,𝑘𝑛)≥0
. Then,

[𝐭𝐤]∏𝑖=1𝑛(∑𝑗=1𝑛𝑎𝑖𝑗𝑡𝑗)𝑘𝑖=[𝐱𝐤]det(𝐈−𝐗𝐀)−1
where 𝐭𝐤
 stands for 𝑡𝑘11…𝑡𝑘𝑛𝑛
, and 𝐱𝐤
, correspondingly, for 𝑥𝑘11…𝑥𝑘𝑛𝑛
, and 𝐈=[𝛿𝑖𝑗]𝑛×𝑛
 is the identity matrix.

With MMT, the Dixon's identity is proven as elegantly as this:

Proof
Let's now learn how to prove MMT itself.

Variable-less MMT
Now, let's look into MMT and try to understand what does it actually stand for? First of all, we can multiply both sides by 𝐱𝐤
 and sum them up over all 𝐤≥0
. In this case, the RHS, by definition, will turn into det(𝐈−𝐗𝐀)−1
, while the LHS will turn into

∑𝐤≥0[𝐭𝐤]∏𝑖=1𝑛(∑𝑗=1𝑛𝑎𝑖𝑗𝑥𝑖𝑡𝑗)𝑘𝑖.
We can now do the substitution 𝐁=𝐗𝐀
 and use 𝑏𝑖𝑗=𝑎𝑖𝑗𝑥𝑖
 to reformulate MMT in the equivalent form

∑𝐤≥0[𝐭𝐤]∏𝑖=1𝑛(∑𝑗=1𝑛𝑏𝑖𝑗𝑡𝑗)𝑘𝑖=det(𝐈−𝐁)−1
This form is more convenient for us to work with, as it doesn't depend on any variables in RHS.

LHS as the sum of permanents
Now, let's take a closer look on the individual summands of the LHS. What do they enumerate?

[𝐭𝐤]∏𝑖=1𝑛(∑𝑗=1𝑛𝑏𝑖𝑗𝑡𝑗)𝑘𝑖=∑(𝑗1,…,𝑗𝑘)∏𝑠=1𝑘𝑏𝑖𝑠𝑗𝑠,
where (𝑖1,…,𝑖𝑘)
 is a tuple such that 1≤𝑖1≤⋯≤𝑖𝑘≤𝑛
 and each 𝑖
 occurs exactly 𝑘𝑖
 times in it. Correspondingly, (𝑗1,…,𝑗𝑘)
 are all possible rearrangements of such tuple. So, we go over all rearrangements of (𝑖1,…,𝑖𝑘)
 and sum up the product of 𝑏𝑖𝑠𝑗𝑠
 over 𝑠
.

Doesn't it look familiar? If we consider all permutations of (1,…,𝑘)
 rather than direct rearrangements of (𝑖1,…,𝑖𝑘)
, this will just be a permanent of a (𝑘1+⋯+𝑘𝑛)×(𝑘1+⋯+𝑘𝑛)
 matrix, in which the element 𝑏𝑖𝑗
 occurs as a block of size 𝑘𝑖×𝑘𝑗
.

This concept usually occurs in literature defined even for two distinct vectors 𝐤≥0
 and 𝐥≥0
, so that 𝑏𝑖𝑗
 occurs in a block of size 𝑘𝑖×𝑙𝑗
. The resulting matrix is typically denoted as 𝐁(𝐤,𝐥)
. So, e.g. for 𝑛=3
, 𝐤=(3,2,1)
 and 𝐥=(1,2,3)
, 𝐁(𝐤,𝐥)
 would look like

𝐁=⎛⎝⎜⎜𝑏11𝑏21𝑏31𝑏12𝑏22𝑏32𝑏13𝑏23𝑏33⎞⎠⎟⎟↦⎛⎝⎜⎜⎜⎜⎜⎜⎜⎜𝑏11𝑏11𝑏11𝑏21𝑏21𝑏31𝑏12𝑏12𝑏12𝑏12𝑏12𝑏12𝑏22𝑏22𝑏22𝑏22𝑏32𝑏32𝑏13𝑏13𝑏13𝑏13𝑏13𝑏33𝑏13𝑏13𝑏33𝑏23𝑏23𝑏23𝑏23𝑏23𝑏23𝑏33𝑏33𝑏33⎞⎠⎟⎟⎟⎟⎟⎟⎟⎟=𝐁(𝐤,𝐥).
 
 
 
 
 
 
 
 
 
Note that to go back to rearrangements of (𝑖1,…,𝑖𝑘)
 rather than permutations of (1,…,𝑘)
 we should divide the permanent by 𝑘1!…𝑘𝑛!
, which is commonly denoted in literature as 𝐤!
. In this terms, MMT rewrites concisely as

∑𝐤≥0per𝐁(𝐤,𝐤)𝐤!=det(𝐈−𝐁)−1
Or, in terms of the original matrix 𝐀
, as

∑𝐤≥0per𝐀(𝐤,𝐤)𝐱𝐤𝐤!=det(𝐈−𝐗𝐀)−1
Sums of permanents as traces of symmetric powers
Now, let's denote |𝐤|=𝑘1+⋯+𝑘𝑛
. We can show that

∑|𝐤|=𝑘per𝐁(𝐤,𝐤)𝐤!=trS𝑘(𝐁),
where S𝑘(𝐁)
 is the so-called symmetric power of 𝐁
.

Let's properly define it to better understand what this means. Before doing so, we should define tensor products. Let 𝑉
 and 𝑊
 be vector spaces with bases 𝐵𝑉
 and 𝐵𝑊
 correspondingly. Then, their tensor product 𝑉⊗𝑊
 is the vector space with associated bilinear operation ⊗:𝑉×𝑊→𝑉⊗𝑊
, such that the set {𝑣⊗𝑤:𝑣∈𝐵𝑉,𝑤∈𝐵𝑊}
 forms a basis of 𝑉⊗𝑊
.

If we represent vectors in coordinate form, one possible way to represent tensor products is via Kronecker product operation applied to coordinate arrays. When tensor product is defined on vectors, it is also implicitly defined on linear maps. So, if 𝐴:𝑉1→𝑉2
 and 𝐵:𝑊1→𝑊2
 are linear maps, then their tensor product is the linear map 𝐴⊗𝐵:𝑉1⊗𝑊1→𝑉2⊗𝑊2
, such that (𝐴⊗𝐵)(𝑣⊗𝑤)=(𝐴𝑣)⊗(𝐵𝑤)
. If 𝐴
 and 𝐵
 are represented by matrices, 𝐴⊗𝐵
 can be represented as their Kronecker product.

To define symmetric powers, we should consider 𝑉⊗𝑉
, a tensor product of 𝑉
 with itself. Assuming that 𝑉
 has a basis 𝑒1,…,𝑒𝑛
, we can say that 𝑉⊗𝑉
 has a basis {𝑒𝑖⊗𝑒𝑗:1≤𝑖,𝑗≤𝑛}
, so we can represent any 𝑣∈𝑉⊗𝑉
 as an array [𝑣𝑖𝑗]𝑛×𝑛
 such that

𝑣=∑𝑖,𝑗𝑣𝑖𝑗(𝑒𝑖⊗𝑒𝑗).
Now, of all vectors 𝑣∈𝑉⊗𝑉
 we can consider a subset of symmetric vectors, that is those for which 𝑣𝑖𝑗=𝑣𝑗𝑖
 for any (𝑖,𝑗)
. Such vectors form a subspace S2(𝑉)
 of 𝑉⊗𝑉
, also called the symmetric square of 𝑉
. As a subspace of 𝑉
, symmetric square has a basis

{𝑒𝑖𝑗=𝑒𝑖⊗𝑒𝑗+𝑒𝑗⊗𝑒𝑖:1≤𝑖≤𝑗≤𝑛}.
Finally, we can define the symmetric power S𝑘(𝑉)
 of 𝑉
 as a subspace of 𝑉⊗𝑘=𝑉⊗⋯⊗𝑉𝑘
 formed by symmetric arrays. By this we mean that any element of 𝑉⊗𝑘
 can be represented as an array [𝑣𝑖1…𝑖𝑘]𝑛×⋯×𝑛
 of size 𝑛𝑘
, such that

𝑣=∑𝑖1,…,𝑖𝑘𝑣𝑖1,…,𝑖𝑘(𝑒𝑖1⊗⋯⊗𝑒𝑖𝑘).
With this definition, the 𝑘
-th symmetric power of 𝑉
 is a subspace of 𝑉⊗𝑘
 formed of all vectors 𝑣
 whose coordinates wouldn't change if we rearrange vectors 𝑒1,…,𝑒𝑛
 in any way. We can also define its basis as

{𝑒𝐢=∑(𝑗1,…,𝑗𝑘)(𝑒𝑗1⊗⋯⊗𝑒𝑗𝑘):1≤𝑖1≤⋯≤𝑖𝑘≤𝑛},
where (𝑗1,…,𝑗𝑘)
 go over all possible rearrangements of (𝑖1,…,𝑖𝑘)
. Looks familiar, doesn't it?

Now, what's even more important is that we can also define the symmetric power S𝑘(𝐴)
 of any linear map 𝐴:𝑉→𝑉
 as the reduction of 𝐴⊗𝑘
 on S𝑘(𝑉)
. Now, we can find its trace by going over all basis vectors in S𝑘(𝑉)
 and summing up their contribution to the trace:

trS𝑘(𝐁)=∑𝐢𝑒𝐢⋅S𝑘(𝐁)𝑒𝐢𝑒𝐢⋅𝑒𝐢.
Noting that (𝑣1⊗⋯⊗𝑣𝑘)⋅(𝑤1⊗⋯⊗𝑤𝑘)=(𝑣1⋅𝑤1)…(𝑣𝑘⋅𝑤𝑘)
, we can see that 𝑒𝐢⋅𝑒𝐢
 is the total number of rearrangements (𝑗1,…,𝑗𝑘)
 of (𝑖1,…,𝑖𝑘)
. This is because, when expanded, each rearrangement will only give 1
 with itself, while giving 0
 with any other. Conversely, 𝑒𝐢⋅S𝑘(𝐁)𝑒𝐢
 expands into summation over rearrangements (𝑙1,…,𝑙𝑘)
 and (𝑗1,…,𝑗𝑘)
 of (𝑖1,…,𝑖𝑘)
:

∑(𝑙1,…,𝑙𝑘)∑(𝑗1,…,𝑗𝑘)∏𝑠=1𝑘𝑏𝑙𝑠𝑗𝑠.
When we divide this whole sum by 𝑒𝐢⋅𝑒𝐢
, it is essentially equivalent to fixing (𝑙1,…,𝑙𝑘)=(𝑖1,…,𝑖𝑘)
, making the sum into

trS𝑘(𝐁)=∑𝐢∑(𝑗1,…,𝑗𝑘)∏𝑠=1𝑘𝑏𝑖𝑠𝑗𝑠=∑|𝐤|=𝑘per𝐁(𝐤,𝐤)𝐤!,
which proves that the LHS of the MMT is the sum of trS𝑘(𝐁)
 over 𝑘≥0
.

Inverse determinants as traces of symmetric powers
Now, let's take a closer look at det(𝐈−𝐁)−1
. Let 𝜆1,…,𝜆𝑛
 be eigenvalues of 𝐁
, then

det(𝐈−𝐁)−1=∏𝑖=1𝑛11−𝜆𝑖=∏𝑖=1𝑛∑𝑘=0∞𝜆𝑘𝑖=∑𝐤≥0𝜆𝐤.
We may show that the eigenvalues of S𝑘(𝐁)
 are, in fact, all possible values of 𝜆𝐤
 where |𝐤|=𝑘
. To show this, assume in the basis construction above that 𝑒1,…,𝑒𝑛
 are all eigenvectors of 𝐁
 that also form the basis. In this case, 𝑒𝐢
 would also be an eigenvector of S𝑘(𝐁)
 with the eigenvalue 𝜆𝐤
, where 𝐤=(𝑘1,…,𝑘𝑛)
 and 𝑘𝑖
 is the multiplicity of 𝑖
 in (𝑖1,…,𝑖𝑘)
. On the other hand, the sum of all eigenvalues of a particular linear map is simply the trace of the linear map, hence

∑𝑘=0∞trS𝑘(𝐁)=det(𝐈−𝐁)−1,
concluding the proof of MacMahon's master theorem.

Applications to Quantum Physics
Consider the expression det(𝐈−𝐁)
 without taking its inverse:

det(𝐈−𝐁)=∏𝑖=1𝑛(1−𝜆𝑖)=∑𝑘=0∞(−1)𝑘∑|𝐢|=𝑘𝜆𝐢,
where this time 𝐢
 iterates over tuples (𝑖1,…,𝑖𝑘)
 such that 1≤𝑖1<⋯<𝑖𝑘≤𝑛
 instead of 1≤𝑖1≤⋯≤𝑖𝑘≤𝑛
, thus ensuring that only pairwise distinct eigenvalues participate. Summed this way, eigenvalues also sum up to a trace of a certain power of 𝐁
. However, this time it's not symmetric power of 𝐁
, but instead exterior power, denoted Λ𝑘(𝐁)
.

While symmetric power is a subspace of 𝑉⊗𝑘
 composed by symmetric tensors, exterior power is composed of antisymmetric tensors instead, so in its basis construction some rearrangements are multiplied by −1
, if the parity of the rearrangement is odd. In this way,

det(𝐈−𝐁)=∑𝑘=0𝑛(−1)𝑘trΛ𝑘(𝐁).
Now, plugging 𝑡𝐀
 instead of 𝐁
 into MMT, we get the following result:

1∑𝑘=0𝑛(−1)𝑘trΛ𝑘(𝐀)𝑡𝑘=∑𝑘=0∞trS𝑘(𝐀)𝑡𝑘
which is also known as the boson-fermion correspondence, where bosons are commuting particles corresponding to symmetric powers, while fermions are skew-commuting particles corresponding to exterior powers.

I suppose one other way to look at it is to treat trS𝑘(𝐀)
 as the complete homogeneous symmetric polynomial ℎ𝑘(𝜆1,…,𝜆𝑛)
, and trΛ𝑘(𝐀)
 as the elementary symmetric polynomial 𝑒𝑘(𝜆1,…,𝜆𝑛)
, which then implies the correspondence between them.

Multivariate Lagrange inversion
MMT is, in fact, a linear case of a more general result, known as multivariate Lagrange implicit function theorem (LIFT).

Multivariate LIFT: Let 𝐱=(𝑥1,…,𝑥𝑛)
 and let 𝑅1(𝐱),…,𝑅𝑛(𝐱)∈𝐊[[𝐱]]
 be such that

𝑅𝑗=𝑥𝑗𝐺𝑗(𝑅1,…,𝑅𝑛),
where 𝐺1(𝐮),…,𝐺𝑛(𝐮)∈𝕂[[𝐮]]
 for 𝐮=(𝑢1,…,𝑢𝑛)
. Then, for any 𝐹(𝐮)∈𝕂[[𝐮]]
 and 𝐚=(𝑎1,…,𝑎𝑛)
 it holds that

[𝐱𝐚]𝐹(𝑅1,…,𝑅𝑛)=[𝐮𝐚]𝐹(𝐮)𝐺𝐚(𝐮)det(𝛿𝑖𝑗−𝑢𝑗𝐺𝑖(𝐮)∂𝐺𝑖(𝐮)∂𝑢𝑗)
where the expression under the determinant is the (𝑖,𝑗)
-th element of the matrix. For 𝐺𝑖(𝐮)=∑𝑗=1𝑛𝑎𝑖𝑗𝑢𝑗
 we get MMT.

I previously made a blog post about univariate LIFT, but this one here seems significantly harder to comprehend to me.

P.S. One more interesting related identity is (as found on Mathematics StackExchange):

∑𝐤,𝐥≥0per𝐀(𝐤,𝐥)𝐱𝐤𝐲𝐥𝐤!𝐥!=𝑒𝐱⊤𝐀𝐲
Meaning that MMT essentially extracts diagonal elements from this bivariate genfunc. Nice to know, I guess?

Tags combinatorics, generating functions, linear algebra, tensor

ICPC Swiss Subregional 2023-2024 in gym

By adamant, history, 2 weeks ago, In English
Hi everyone!

Today, ETH Zürich, EPFL and the University of Lugano selected their teams for SWERC using the individual contest prepared by BenniKartefla, Lebossle, MihneaDAVID, OhLee, SnowfuryGiant, adamant, deepThought, ghassan, johannesk, majk, monika.

Special thanks to Suika_predator, fallleaves01, Heltion, Okrut, AsiBasi, atli164 and Tagl for testing it!

The contest is now uploaded to the Codeforces gym at 2023-2024 ICPC, Swiss Subregional.

Congratulations to the newly formed ICPC teams! Contest tutorial:

A
104854A - Arthur The Ant
Firstly you must figure out how many days need to pass until it's possible to travel between two lily pads. For two lily pads (𝑥1,𝑦1)
 and (𝑥2,𝑦2)
, it can be calculated as.
⌊𝑎𝑏𝑠(𝑥1−𝑥2)+𝑎𝑏𝑠(𝑦1−𝑦2)2⌋
Consider this as a graph were each lily pad is a vertex and edges between all pairs with weight equal to the number of days it takes to travel between the vertices.

Go through the edges in increasing order of weight. If we add them one by one to the graph then at some point (𝑛,𝑚)
 will be reachable from (1,1)
 and the first time this happens is the minimum number of days. Clearly if two vertices are already connected we don't need to add an edge. Thus it follows that we only need to consider edges of a minimum spanning tree.

If you had a Team reference document or were allowed to google you could simply write up an (𝑘log𝑘)
 algorithm for Rectilinear minimum spanning trees. But the limits are not very tight to allow people to come up with an (𝑘log2𝑘)
 algorithm by themselves. We describe one such method briefly.

If it's possible to travel from (1,1)
 to (𝑛,𝑚)
 in 𝑑
 days, then it is also possible in 𝑑+1
 days. So we binary search the answer. For each point (𝑥𝑖,𝑦𝑖)
 we translate it to (𝑥𝑖+𝑦𝑖,𝑥𝑖−𝑦𝑖)
 as to rotate the grid by 45 degrees making the problem just manhattan distances. Sort all points by x-coordinate and do a sweep line to connect vertices by edges, with union find to check if vertices are connected and to connect them.

Time complexity is (𝑘log𝑘log(𝑛+𝑚))
, or (𝑘log𝑘)
 if you can implement rectilinear minimum spanning tree.

B
104854B - Beautiful Contest
By the constraint that each problem is roughly twice as difficulty as the next one in the contest, we can see that the size of a contest is bounded by log2(106)≈20
.

Consider a binary tree rooted at 1
 with vertex 𝑖
 having two children 2𝑖
 and 2𝑖+1
. Vertex 𝑖
 will represent problems of difficulty 𝑖
.

At each vertex 𝑖
 we store a multiset 𝑆𝑖
 of beauty values and two values 𝑒𝑛𝑑𝑖
 and 𝑎𝑛𝑠𝑖
. Were 𝑒𝑛𝑑𝑖
 is the maximum beauty of a contest that has the smallest difficulty problem as problem 𝑖
, and 𝑎𝑛𝑠𝑖
 is the maximum among all 𝑒𝑛𝑑𝑗
 where 𝑗
 is in the subtree of 𝑖
.

Initially we set all 𝑎𝑛𝑠𝑖=𝑒𝑛𝑑𝑖=0
.

When we receive a command for 𝑑
 𝑏
, we either add or remove a beauty value from 𝑆𝑑
 of 𝑏
, then we update the tree. Only the vertices on the path from 𝑑
 to the root need to be updated. Which we must do in order from vertex 𝑑
 to the root as follows.


while d >= 1:
    end[d] = max(0, max(end[2d], end[2d+1]) + MAX(S))
    ans[d] = max(end[d], ans[2d], ans[2d+1])
    d /= 2

Verbally we update 𝑒𝑛𝑑𝑑
 by picking the most beautiful problem of difficulty 𝑑
 and the beauty of selecting the rest of the problems higher than 𝑑
.

Afterwards we output 𝑎𝑛𝑠1
.

Each update takes at most 20
 steps and inserting/removing from the multiset takes 𝑂(log𝑛)
 time so we end up with a time complexity of 𝑂(𝑛log𝑛)
.

C
104854C - Continued Fractions
One solution is to use Pollard's Rho algorithm to factorize 𝑝
, however it is not required and there exists a simpler solution.

Note that 𝑎+𝑥𝑏+𝑥=1+𝑎−𝑏𝑏+𝑥
.

So for integrity three we have three equations
1+𝑎−𝑏𝑏
1+𝑎−𝑏𝑏+1
1+𝑎−𝑏𝑏+2
So we need 𝑏
, 𝑏+1
 and 𝑏+2
 to divide 𝑎−𝑏
. Since 𝑚𝑎𝑥(𝑔𝑐𝑑(𝑏,𝑏+1),𝑔𝑐𝑑(𝑏,𝑏+2),𝑔𝑐𝑑(𝑏+1,𝑏+2))≤2
 we have that 𝑏(𝑏+1)(𝑏+2)
 must divide 2(𝑎−𝑏)
.

Note that 𝑎=𝑝⋅𝑏
, so we can rewrite the equations:

𝑏(𝑏+1)(𝑏+2)
 must divide 2((𝑝−1)𝑏)
, or rather (𝑏+1)(𝑏+2)
 must divide 2(𝑝−1)
. Thus we can write
𝑘⋅(𝑏+1)⋅(𝑏+2)=2(𝑝−1)
It must hold that 𝑚𝑖𝑛(𝑘,𝑏+1,𝑏+2)≤2(𝑝−1)‾‾‾‾‾‾‾‾√3
. So we can iterate over all small values of 𝑘
 and 𝑏
 and check for integrity.

Complexity 𝑂(𝑝√3)
D
104854D - District 42
Concatenate all numbers from 1
 to 𝑛
 together in a string, 𝑠=12345678910111213....
, then iterate through the string s and count the number of times 𝑠𝑖=4∧𝑠𝑖+1=2
.

Time complexity is (𝑛)
.

E
104854E - Elimination Bracket
First thing to notice is that there always exists an optimal solution with some prefix of question marks replaced by ')' and the rest replaced with '(' (Left as an exercise for the reader).

Now we must figure how large this prefix should be, we can compute the answer for each prefix/suffix and then take the maximum. Define 𝑀𝑗
 as how many pairs '(', ')' match in the prefix up to 𝑗
 and 𝐿𝑗
 how many '(' are left unmatched if we replace all '?' with ')', this can be calculate in linear time. Similarly define 𝑀′𝑗
 as how many pairs '(', ')' match in the suffix up to 𝑗
 and 𝐿𝑗
 how many ')' are left unmatched if we replace all '?' with '('.

The answer is now
max1≤𝑖≤𝑛𝑀𝑖+𝑀′𝑖+1+2𝑚𝑖𝑛(𝐿𝑖,𝐿′𝑖+1)
As we can match '(' that are left in the prefix with ')' that are left in the suffix, which will add 2 times the minimum of '(' and ')'.

Time complexity is (𝑛)
.

F
104854F - Factorial Prime
There is only one prime number that can be written as a factorial, that is 2
. So if 𝑥≥2
, then you print 2
, otherwise you print −1
.

G
104854G - Guess Gauss
The sum ∑𝑛𝑖=1𝑖
 has a closed formula 𝑛(𝑛+1)2
. We are given 𝑑=𝑚(𝑚+1)2−𝑛(𝑛+1)2
, and need to find all pairs (𝑛,𝑚)
 that satisfy this equation.

Consider that 𝑑
 can also be expressed as
(𝑛+1)+(𝑛+2)+⋯+(𝑚−1)+𝑚
𝑛⋅(𝑚−𝑛)+1+2+⋯+(𝑚−𝑛)
𝑛⋅(𝑚−𝑛)+(𝑚−𝑛)(𝑚−𝑛+1)2
Let 𝑘=𝑚−𝑛
𝑛⋅𝑘+𝑘(𝑘+1)2=𝑑
𝑛=𝑑−𝑘(𝑘+1)2𝑘
We can see that 𝑘
 is bounded, 𝑘≤𝑑‾‾√
, so we can iterate through all possible values of 𝑘
, check if 𝑑−𝑘(𝑘+1)2
 is divisible by 𝑘
. If that is the case we set 𝑛=𝑑−𝑘(𝑘+1)2𝑘
 and 𝑚=𝑛+𝑘
.

Time complexity is (𝑑‾‾√)
.

H
104854H - Homogeneous Mixings
The problem boils down to the following: You're given counts 𝑐1,…,𝑐𝑘
 for 𝑘=26
 being the size of alphabet, and 𝑐𝑖
 being the number of corresponding letters, so 𝑐1+⋯+𝑐𝑘=𝑛
. You need to count how many out of 𝑆=(𝑛𝑐1,…,𝑐𝑘)
 arrangements of letters don't have adjacent letters of the same kind. Without loss of generality, assume that 𝑐1,…,𝑐𝑘≥1
.

To re-state the problem, let 𝑆𝑡
 be the number of arrangements such that they have exactly 𝑡
 maximal non-empty blocks of consecutive equal letter. Clearly, 𝑆1+⋯+𝑆𝑛=𝑆
, and our ultimate goal is to find the value of 𝑆𝑛𝑆
. This can be done via inclusion-exclusion.

Let 𝑇𝑖
 be the number of arrangements of given letters into 𝑖
 non-empty blocks of consecutive letters. Note that, for 𝑇𝑖
, we do not require that the blocks are maximal (i.e. neighbouring blocks might actually use the same letter), and each sequence of letters is counted for every possible way of splitting it into 𝑖
 blocks (so, might be counted more than once).

Then, it follows from the PIE that
𝑆𝑛=∑𝑖=1𝑛(−1)𝑛−𝑖𝑇𝑖.
Does it simplify the problem in any way? Yes, to some extent. Let's say that of all the blocks, 𝑖1
 consist of the letter 1
, 𝑖2
 consist of the letter 2
, and so on. In this notion,
𝑇𝑖=∑𝑖1+⋯+𝑖𝑘=𝑖(𝑖𝑖1,…,𝑖𝑘)∏𝑗=1𝑘(𝑐𝑗−1𝑖𝑗−1),
as we can choose the arrangement of blocks within 𝑖
 blocks in (𝑖𝑖1,…,𝑖𝑘)
 ways, after which for each letter 𝑗
 we can choose how we distribute 𝑐𝑗
 equal letters into 𝑖𝑗
 non-empty blocks in (𝑐𝑗−1𝑖𝑗−1)
 ways (as the last block ends with the last letter, and we can choose any 𝑖𝑗−1
 among first 𝑐𝑗−1
 letters to end first 𝑖𝑗−1
 blocks).

On the other hand, we can rewrite the expression for 𝑇𝑖
 as a convolution:
𝑇𝑖𝑖!=[𝑥𝑖]∏𝑗=1𝑘∑𝑎=1𝑐𝑗(𝑐𝑗−1𝑎−1)𝑥𝑎𝑎!,
which means that 𝑇𝑖
 can be found from the product of 𝑘
 polynomials 𝐴1(𝑥),…,𝐴𝑘(𝑥)
, where
𝐴𝑗(𝑥)=∑𝑎=1𝑐𝑗(𝑐𝑗−1𝑎−1)𝑥𝑎𝑎!
This result can also be interpreted from EGF point of view, but we won't delve into it.

I
104854I - Intelligent Cat Embedding
Let's assume there exists a solution were we apply 𝑘
 words 𝑎1,𝑎2,…𝑎𝑘
. Consider the word 𝑎𝑘
 which is the last one to be applied. For every position 𝑝𝑎𝑘,𝑗
 that we change we must have that 𝑤𝑝𝑎𝑘,𝑗=𝑣𝑎𝑘,𝑗
. Now for 𝑎𝑘−1
 for each position that is not edited by word 𝑎𝑘
 we must have 𝑤𝑝𝑎𝑘−1,𝑗=𝑣𝑎𝑘−1,𝑗
, since positions edited by word 𝑎𝑘
 will be overwritten. This follows on until 𝑎1
.

We can use this to apply the words in reverse order. We start of with a vector where all positions are unfinished. Then we iterate through all words and check if they can be applied. If a word can be applied, we apply it. If at some point we still haven't finished all positions and there is no word that can be applied, then it must be impossible to create this target embedding. Otherwise we reach a position where all positions have been finished, then we write out the words we applied in reverse order.

Time complexity is (𝑤𝑘2)
J
Tutorial is loading...
K
104854K - Kenough Time
This problem can be thought of as two independent problems. How do you calculate the shortest path between two points (𝑥1,𝑦1)
 and (𝑥2,𝑦2)
, and how to calculate the shortest time to rescue all the swimmers.

1. Shortest path between two points (𝑥1,𝑦1)
 and (𝑥2,𝑦2)
.

A shortest path will always be one of three options.

Travel directly between (𝑥1,𝑦1)
 and (𝑥2,𝑦2)
. If they are both on land this will always be the case.
Travel from (𝑥1,𝑦1)
 to (𝑐,0)
 and then from (𝑐,0)
 to (𝑥2,𝑦2)
, for some 𝑐
. Consider the image in the description.
Travel from (𝑥1,𝑦1)
 to (𝑐,0)
, then from (𝑐,0)
 to (𝑑,0)
, then from (𝑑,0)
 to (𝑥2,𝑦2)
. Imagine that the speed on land is very fast, while the speed in water is very slow. Both locations are in the water but very close to 𝑦=0
, and very far apart in 𝑥
. It is then optimal to walk up towards land, walk on land for some time and then go back to the water. Note that |𝑥1−𝑐|
 and |𝑥2−𝑑|
 do not have to be equal.
If you really hate yourself there is a closed formula, maybe it simplifies...

Still need to extend it with 𝑣run
 and 𝑣swim
, ask Konstantin for further explanation.
An easier method is to create a function 𝑓
, that on input (𝑥1,𝑦1)
, (𝑥2,𝑦2)
 and 𝑐
 outputs 𝑑
, by using ternary search, which is possible as this function is a parabola. Then using 𝑓
 you can create a function 𝑓′
 that ternary searches 𝑐
. In this way we can ternary search the shortest path between any two points in log2(106)
. Pre calculating this beforehand for all pairs of points will then be (𝑛2𝑙𝑜𝑔2(106))
.

2. Shortest time to rescue all the swimmers.

This part of the problem can be solved using bitmask dynamic programming. Let 𝑑𝑝(𝑚𝑎𝑠𝑘,𝑎𝑡,ℎ)
 be our state space, where 𝑚𝑎𝑠𝑘
 is a bit set representing the swimmers we have already visited, 𝑎𝑡
 the position of which swimmer we are currently at and ℎ
 be how many people we are currently holding. Then 𝑑𝑝(𝑚𝑎𝑠𝑘,𝑎𝑡,ℎ)
 is the minimum time it takes to save all the swimmers that are left. We also let 𝑎𝑡=𝑛
 be the initial position and 𝑎𝑡=𝑛+1
 be the position of the ice cream store.

For us to be finished we must be at the ice cream store, and we must have visited all the swimmers. So 𝑑𝑝(2𝑛−1,𝑛+1,𝑥)=0
. The transitions are then as follows.

𝑑𝑝(𝑚𝑎𝑠𝑘,𝑎𝑡,ℎ)=min⎧⎩⎨⎪⎪0𝑑𝑝(𝑚𝑎𝑠𝑘,𝑛+1,0)+𝑑𝑖𝑠𝑡(𝑎𝑡,𝑛+1)min0≤𝑖≤𝑛−1∧𝑖 not in mask𝑑𝑝(𝑚𝑎𝑠𝑘|2𝑖,𝑖,ℎ+1)+𝑑𝑖𝑠𝑡(𝑎𝑡,𝑖)𝑚𝑎𝑠𝑘=2𝑛−1∧𝑎𝑡=𝑛+1ℎ>0ℎ<𝑠
The answer is then 𝑑𝑝(0,𝑛,0)
.

The time complexity is (2𝑛𝑛2𝑠+𝑛2𝑙𝑜𝑔2(106))
.

 Announcement of 2023-2024 ICPC, Swiss Subregional
Tags swiss subregional, team selection, gym, switzerland

	Tasks

Perhaps I ought to create a separate file uh a separate file for web log posts and tasks themselves uh just kind of an enumeration a comprehensive enumeration of all of the tasks I have done and seen maybe uh... just write up my own editorial or something.

MITIT Spring 2025 Qualification 2 1 Balls And Bins

Sort based on {BinMax-BinCurrentlyHolds, BinCurrentlyHolds} and then process through starting with a 0 on hand basically if and only if the number on hand is >= the next value.first we may add the next value.second to number on hand essentially BinCurrentlyHolds and this process terminates at the end if and only if it was possible because this greedy strategy is quite easily provable to be optimal.

MITIT Spring 2025 Qualification 2 2 Medians Of Medians

So here it was easy enough casework. In particular it is the case that if the median is one of the medians then in fact the other 2 medians will be one will be greater and one will be lesser. So if it is a median it will be the median of the medians. So that is actually not a true fact for the 5 case say. Anyways then if the median is set basically need to see just like count the number of dudes > and < the median in each of the 3 bins and it ends up something pretty easy combinatorial using combo.cpp and reasoning accurately about how the bins split up with respect to different numbers and factorials like e.g. after picking and ordering the elements inside the bin with the median the rest can be 1 simple factorial from there of all of the other dudes is a useful trick to note for future rounds.

MITIT Spring 2025 Qualification 2 3 Busy Beaver's Colorful Walk

Lovely little task. Pretty easy. Can not believe I missed it should have could have would have been an OnSight. The editorial provided was not quite as intuitive as one I will write up here I think. So basically the motivation is like can we block them from being on a location sure you actually can simply go from left to right so on their 1st move we can certainly block them from positions 0, 1, and 2 by picking the colour excluded from those locations. So then after that it is immediate that we can then pick to exclude the colours from the 1, 2, 3 range now this is interesting because he was blocked previously from being on positions 1 and 2 it means he can not not be on 0 and also can not be on 1, 2, and 3. So OK so then keep on doing this down the chain and we will be done after n - 2 colours selected hmmm. Now this is funny too because maybe if I had bashed it out I would have spotted the pattern by simply examining some more cases from the length 5 or so. Hmmm. And observing other people ace it fast suggested there existed a terse code too. Hmmm.