\Large
Alibaba Global Mathematics Competition 2022 Qualifying Round \\
Lazar Ilic

1 \\
Lol dude I had a hard time with this one frankly was thinking about writing down some actual points in $\mathbb{R}^3$ and trying to hack around the joints, angles, cleave the apparatus but really not a whizz with modeling software, eyeballing, or simply rotating this sort of an object in my head. The pictures, images, depicted did not help me all that much and I just clicked $\sqrt{13}$ and decided to come back to this one later if I thought that was a rational allocation of time when I really was focusing more on writeups or trying to score the points get in there for the Final Round and secure the bag with firms in the mix too and Google Code Jam preparations.

% In review it is perhaps possible to uh solve this one using the right sort of uh equations solvers setting some angles and using coordinates bashing essentially in 3-dimensions with the right sort of uh programmatic help for example there might be super trivial O[polylog] style algorithms here essentially Gradient Descent in Python in to a reasonable approximate configuration to get the answer Output here. I might uh try and write up that code here mock it up prior to the 2023 Qualification Round. Just bashing out around some uh angles values or whatever probably could parametrise in terms of like 4 6 8 relevant uh angles and solve using Python quite easily nowadays in < 60 minutes perhaps.

2 \\
There exists like for example a very natural Settlers Of Catan tier well what is formally known more simply on tilings of the plane $\mathbb{R}^2$ with regular $n$-gons but a hexagonal tiling for example of hexagons which circumscribe circles of radius $1$ and those which inscribe circles of radius $\frac{1}{2}$ now each in the bigger former must contain $1$ dude and each in the latter case can contain at most $1$ dude and thus this gives the needed bounds for the answer of B $c_1 \sqrt{n} \le KP_n \le c_2 \sqrt{n}$.

3 \\
A very simple naive rough density approximation for example would be from there being like $6$ such hexagons on the circle of radius $1$, $12$ on the circle of radius $3$, and so on and so on $30$ at $9$, $36$ at $11$ etc. which then gives like $2 \pi$ as a sum of $\approx 60 \text{arctan} \left( \frac{1}{6 \cdot 9} \right) + 72 \text{arctan} \left( \frac{1}{6 \cdot 11} \right) + 84 \text{arctan} \left( \frac{1}{6 \cdot 13} \right) + 96 \text{arctan} \left( \frac{1}{6 \cdot 15} \right) + 108 \text{arctan} \left( \frac{1}{6 \cdot 17} \right) + 120 \text{arctan} \left( \frac{1}{6 \cdot 19} \right)$ which would correspond with $30+36+42+48+54+60 = 270$ dudes and I am fairly confident that nailing the simplest of $30$ layer followed by a $30$ layer is quite a trivial construction perhaps as simple as just like whatever it is I think a naive equi distributed layer around the initial radius $10$ circle can quite simply lead to an $n$ layer and an $n$ layer for $n \ge 30$ to hit that bound.

Upon further review I do not think that my numerics here quite work out I think they are a little bit too handwaved and that in particular there can be up to $2$ multiplicity overlap on the density so we actually need like... $2 \pi$ as a sum of $\approx \sum_{n=5}^{16} (6 n) \text{arctan} \left( \frac{1}{6 (2 n - 1)} \right)$ which corresponds with like $12 \cdot 63 = 756$ dudes which still puts us in the $[60,800]$ bracket range. Well OK maybe we would need to like replace the $2n-1$ with a $2n$ term and then multiplicity corrections but I think if you actually do a slightly higher resolution more accurate computation based upon the hexagon tiling with a less naive bound, instead actually summing over the explicit induced distances from the set of hexagons rather than simply summing over each layer by that layer's upper bound then you will probably produce a value slightly less than $800$.

4 \\
Usual Markov Chain matrix inversion works $\frac{22}{3}$.

5 \\
It is interesting that they did not simply ask for the setting of $(p,q,r)=\left( p,1-\frac{p}{2},1-\frac{p}{2} \right)$ which would minimise the expected value of the hitting time number of steps in the Markov Chain. We can do the usual matrix inversion, system of linear equations, or naive simulation to produce the optimum. In any case the answer to the task is C $\left( \frac{2}{5},\frac{3}{10},\frac{3}{10} \right)$.

6 \\
Indeed it suffices to show the one way implication direction that being Right Uniform implies being Left Uniform. Set $\epsilon_2 = 4 \epsilon_1$. Then we explicitly exhibit a method of construction for such a set of finitely many elements $a_1,a_2,\dots,a_n$ with the bound $< \epsilon_2$ obtained. Consider the natural partitioning of $X$ into pairwise mutually disjoint subsets $X_1,X_2,\dots,X_m$ given by the indicator function $i(t)$ which effectively partitions the rows into sufficiently similar rows with low error or aggregate difference $\Delta$ between them. Note that for any $t \in X$ we have that it must take on a tight range of values over each of these subsets namely an immediate contradiction would arise if there existed $c,d \in X_j$ with $|f(t,c)-f(t,d)| > 2 \epsilon_1$ by the definition as we are granted that both $|f(t,c)-f(t,b_j)| < \epsilon_1$ and $|f(t,d)-f(t,b_j)| < \epsilon_1$ which implies $|f(t,c)-f(t,d)| < 2 \epsilon_1$. Now then consider a very natural super partitioning of the unit interval $[0,1]$ into pieces $[0,4 \epsilon_1], [\epsilon_1,5 \epsilon_1], \dots, [,1]$ where here there are $M \approx \frac{1}{\epsilon_1}$ pieces and the last one can be terminated early. Then for each $t \in X$ consider the values it takes on each of $X_1,X_2,\dots,X_m$ and arbitrarily classify that range into one of these buckets which is ensured by construction that the for example we may set it to be the earliest minimal bucket which captures the minimal value of that range/codomain will also capture the maximal value. Then this will naturally partition the columns, $t \in X$ values with an $m$-tuple $(e_1,e_2,\dots,e_m)$ with each $e_k \in [1,M]$ referring to that piece. And then from each of these which is hit simply take an arbitrary column $t \in X$ value in that partition as a representative and note by construction the claimed desired $\epsilon_2$ error gap value is obtained attained by construction. And thus we have produced a constructive proof of Left Uniformity with maximal $n \approx \left( \frac{1}{\epsilon_1} \right)^m$ elements.

% Upon further review we can simply replace the $4$ with a $3$ as in $\epsilon_2 = 4 \epsilon_1$ but OK oh well alas it is still a mathematically air tight proof argumentation insofar as I can discern.

% That is to say that for each $\epsilon > 0$ there exists a finite set of ``rows" such that each ``row" is that tightly approximated across the entire row by one of these ``rows" if and only if the same is true for the ``columns". It suffices to show either of $2$ implications of the $1$ way direction e.g. either that being right uniform implies being left uniform or that not being right uniform implies not being left uniform. The like contrapositive. It would strike me that the former is actually quite trivial.

% We may without loss of generality simply consider $Y$ which is a reordering of $X$ such that all of the intervals and values from $X$ which corresponded with $b_1$ come first and then come all of those associated with $b_2$ etc. i.e. we simply consider a simple permutation of the rows such that there are only $m-1$ phase transitions between new row types. Then this changes all columns simultaneously under this permutation operation and namely $f$ on this new set function is left uniform if and only if it was previously because the column definitions and $\epsilon$ gaps will map through this and transfer through to this new setting. But now note that any column can be very simply classified as inside this column we note an internal fluctuation on each of these $m$ bands of at most $2 \epsilon$ which means that for example we may partition the unit interval $[0,1]$ quite naturally into $\approx \frac{1}{2 \epsilon} = A$ indicator bars of size say $4 \epsilon$ for example equally spaced such that each resultant column bar on a row is logically forced to be captured inside and then for example take simply $A^m$ classes of columns based upon which of those bars the actual dude was captured inside of. Then inside each of these there is an internal error of at most $4 \epsilon$ so for example we have that for any $\epsilon > 0$ we can simply partition the actual columns of $X$ into $n \approx \left( \frac{1}{2 \epsilon} \right)^m$ classes which is the desired set of finitely many elements $a_1, a_2, \dots, a_n \in X$ which implies the desired $1$ way implication which goes both ways and thus the claim is shown.

% Indeed I claim that we may choose appropriate $\epsilon_1,\epsilon_2$ and consider the induced partitions of $[0,1]$ via the relevant $\epsilon_1$ at which point each column I think is quite naturally partitioned into the relevant classes on the relevant sub intervals of $X$ e.g. the immediate idea is to use this and note discontinuity can occur and hack together constructed columns from these rows on certain intervals so that the actual underlying columns for example disagree with the natural constructed columns by at most $\epsilon_1$ which means in between eachother they have a maximum gap of $2 \epsilon_1 + \text{discontinuity } \Delta$.

% I ought to write better in the future.

7 \\
1 \\
This follows quite naturally by direct algebraic computation. In particular we note that the function $s_{[W]}(u) = u - 2 \text{proj}_W (u)$ quite simply. Indeed note that $s_v (u) = u - \frac{2(u,v)}{(v,v)}v = u - \frac{2 u \cdot v}{v \cdot v}v = u - \frac{2 ||u|| ||v|| \cos (\theta)}{||v||^2}v$ in which the actual magnitude of the vector $||v||$ cancels so without loss of generality we may take that the $v_i$ are all unit vectors that is $||v_i|| = 1 \forall i$ at which point the expression simply becomes $s_v (u) = u - 2 ||u||v \cos(\theta)$ is of course just a vector of magnitude $2||u|| \cos(\theta)$ in the direction of the vector $v$ which is to say it is simply $2$ times the projection of $u$ onto $v$ and so namely one has that $s_v (u)$ is a function which inverts or negates the component of $u$ with respect to $v$. Now it does not matter which order these components go in or the actual basis used because in the end each of these components are orthogonal to eachother and thus when one is changed and shifted, actually none or $0$ of the others do and so in the end we get the claimed. An alternate way to see this very simply is to simply re parametrise the basis i.e. instead express the coordinisation via a simple basis expansion from $W$ extended to $V$ at which point in that coordinisation it becomes even more directly clear that this is true as the actual literal coordinates of each of those $k$ coordinate positions will simply become negative what they were and so it is even more clear that this is the transformation as claimed. Indeed though one could also in this setting simply coordinatise the actual projection value to see this.

% Upon further review when I was editing and tossing in little terse equations to more legitimise my writeups for non native English speakers to be crystal clear and try to maximise expected value of points granted I maybe ought to have tossed in a little shtick on the coordinisation of the actual projection and note it is the sum of each of those projections onto the orthogonal basis unit vectors from that basis trivially.

% I thought this was an ACceptable writeup in round. Now not too sure really I might want to try try try even harder to writeup better.

2 \\
One of the simplest ways to do this subtask without even comprehending 1 fully would be to simply note that by part 1 we may take that $s_{[W]}^2 = s_{[W]} s_{[W]} = s_{[W]} s_{[W']}$ where the precise choice of the orthonormal basis of $s_{[W']} = s_{[W]}$ is $\{ v_k,\dots,v_1 \}$ that is it is inverted in order. So that then we have that $s_{[W]}^2 = s_{v_1} \dots s_{v_k} s_{v_k} \dots s_{v_1}$ and this telescopes from the inside out. That is it suffices then to note algebraically or from our observations in 1 that $s_{v_k} s_{v_k} = \text{id}$ whence inductively this algebraic expression compresses from the inside out with like terms canceling until we are left simply with the whole thing being $\text{id}$ as claimed. However, in the context of 1 it is also quite simple to note that $s_{[W]}$ is more simply given by $u' = u - 2 \text{proj}_W (u)$ which is to say that irrespective of the order or choice of the orthonormal basis we have that each component of $u$ in the direction of each of the basis vectors for the subspace is inverted. And hence re applying this transformation will of course simply re invert these all back to where they were originally as $\text{proj}_W (u') = - \text{proj}_W (u)$ at which point $u'' = u' - 2 \text{proj}_W (u') = u - 2 \text{proj}_W (u) + 2 \text{proj}_W (u) = u$. See 1 when each for example in the basis expansion coordinisation setting inverts back to its initial coordinate value.

3 \\
The most obvious naive claim is that the maximal cardinality of a nice set would be that of size $\binom{n}{k}$ and would follow by construction for example the simplest set of each subspace spanned by precisely some subset of $k$ elements from the standard $n$ basis set works. That this works is quite immediate indeed all coordinates in a subspace which were initially $0$ will remain $0$ under the transformation by definition and construction at which point we are simply left in the initial subspace on this standard coordinisation. That this is the maximal cardinality of a nice set follows either via a span argumentation or the observation that in the generalised angles setting in $\mathbb{R}^n$ there would be some intermediary values and perhaps an invocation of the intermediate value theorem which would lead to the existence of some $\cos(\theta) \neq 0,1$ for some relevant vectors leading to the existence of some $u \in W'$ in one of the spaces which is transformed through another of the spaces into a $u'$ which is not in the subspace $W'$.

% Possibly 0 or 1 points on Task Part 3 here.

8 \\
This task has quite a lot to do with the more famous and canonical task where one starts ab initio at the origin $(0,0)$. There, generating functions, and recursions to compute probabilities associated with the first hitting time are common, and here similar techniques apply including the $\frac{pi}{2}$ rotation and homothety e.g. spiral similarity homothety argumentation which transforms and isomorphs these notions into those of the $2$-dimensional case emerging quite naturally from $2$ simultaneous copies, random walks, from the $1$-dimensional case. I think that one can imitate the proof of people like Durrett that a simple random walk is recurrent in $2$ dimensions. Perhaps even further readings can be found in the references and beyond like Two-Dimensional Random Walk From Path Counting To Random Interlacements by Serguei Popov. \\
1 \\
In fact this statement re arranges to the distance between the station and the courier is greater than $\frac{n}{2}$ e.g. the courier does not lie inside the disk of center origin $(0,0)$ and radius $\frac{n}{2}$ but in the even stronger case we can extend this to the circumscribing square of like taxicab distance $\approx \frac{n}{\sqrt{2}} = n \cdot \frac{\sqrt{2}}{2}$. So in this new even stronger framing it is like computing the probability that after $n^{1.5}$ steps in the rotational frame normed it is like each of the $2$ separate simultaneous $1$ dimensional random walks would both need to be between $n \cdot \frac{2-\sqrt{2}}{2}$ and $n \cdot \frac{2+\sqrt{2}}{2}$. And this probability of course is given quite simply by $\left( \lim_{n \to \infty} \sum_{i=n \cdot \frac{2-\sqrt{2}}{2}}^{n \cdot \frac{2+\sqrt{2}}{2}} \binom{n^{1.5}}{\frac{n^{1.5}+i}{2}} \cdot \frac{1}{2^{n^{1.5}}} \right)^2 = 0$. Where the floors and ceilings are taken appropriately. More relevantly one can do a simple transformation here to note that the probability of being in the relevant range in $1$ of the $2$ distinct and independent $1$-dimensional simple random walks from Binomial Distribution to Normal Distribution or Gaussian and observe that the probability of being inside of the range $\left[ -n \cdot \frac{2-\sqrt{2}}{2},n \cdot \frac{2-\sqrt{2}}{2} \right]$ after $n^{1.5}$ trials is in the limit like being inside of $\left[ n^{2/3} \cdot \frac{2-\sqrt{2}}{2},n^{2/3} \cdot \frac{2-\sqrt{2}}{2} \right]$ after $n$ trials. But of course in the Binomial Distribution to Normal Gaussian Distribution limiting case the standard deviation is $\approx O(\sqrt{n})$ and so this is such an extremal outlier it goes to being $\infty$ many standard deviations beyond the mean here in this setting. And as $n \to \infty$ the relevant Probability Density Function in the integral i.e. the Survival Function here goes to $0$. What this really tells us is that with probability $1$ we will be quite a lot closer in fact to where we start i.e. in the limiting case with probability $1$ we are nearly guaranteed to be much much much closer to the point $(n,0)$.

% I should have been even clearer in round I think stated something like yadda yadda yadda it suffices to prove for the even larger region yadda at which point rotating by 45 degrees yadda yadda yadda then reduces the 2-Dimensional case in to 2 simultaneous instances of the 1-Dimensional case yadda yadda yadda and frankly think maybe my submission here took a 0 points because "of course in the Binomial Distribution to Normal Gaussian Distribution limiting case" is a pretty wacky writeup and I ought to have written down that limit much much much more clearly in the terms of a literal fraction asymptotically converging on to the right sort of a uh underlying asymptotic really gotten down a formal fractional asymptotic approximation to 1st degree term written down here much more concretely.

2 \\
This part actually follows immediately from the previous part. Argumentation that is the probability that the courier ever reaches the station in the first $n^{1.5}$ steps is low and in fact the even stronger probability that the courier has ever even reached being within $\frac{n}{2}$ of the station within the first $n^{1.5}$ steps is also very low. It suffices to prove the much much much weaker claim here which is that a standard random $1$-dimensional walk starting from $0$ in the first $n$ steps will hit the level $n^{2/3}$ with probability $0$ in the limiting case. But of course this sort of thing can be computed directly like the probability of time $b$ being the first hitting time of level $a$ is given by $\frac{a}{b} \cdot \binom{b}{\frac{b+a}{2}} \cdot \frac{1}{2^n}$ canonically whence this probability becomes the limit $\lim_{n \to \infty} \sum_{i=n^{2/3}}^n \frac{n^{2/3}}{i} \cdot \binom{i}{\frac{i+n^{2/3}}{2}} \cdot \frac{1}{2^i} = 0$. But again I think that there are simpler methods. One does not need a computer algebra system producing a proof for this sort of a fact when Stirling's Approximation suffices. But really this is a valid discrete analog extension of the well known properties of the continuous setting $1$-dimensional Brownian Motion in Stochastic Processes in the First Passage Time setting of the First Hitting Time. In particular this has to do with the Full Width At Half Maximum and more relevantly with the Survival Probability which leads to the Levy Distribution. Again though this task was isomorphic with like a the discrete setting with levels as horizontal $y=$ lines and us hitting the rightward wall at $x=n$ with limiting probability $1$ prior to hitting that level of $y=n^{2/3}$.

3 \\
One can imitate Two-Dimensional Random Walk From Path Counting To Random Interlacements by Serguei Popov, The Range Of Two Dimensional Simple Random Walk by Jian Jiao, the citations in Lattice Green Functions And Calabi-Yau Differential Equations by Anthony J Guttmann, Random Walk: A Modern Introduction by Gregory F. Lawler and Vlada Limic, Principles Of Random Walk by F. Spitzer, Intersections Of Random Walks by G. Lawler, I think we can use similar argumentations for the distribution of the first hitting time as previously with summations of relevant binomials and Stirling's Approximation frankly to get that the limit of the $P = 1$ as desired. Literally just like a joint multiplication of the relevant generating functions in the Levy distribution case example.

% I ought to have worked out these details and fully fleshed it out for points and superior Ranking had I known that the firm would be posting up rankings.

9 \\
1 \\
Well immediately by plugging in the rational number $\theta = 0$ one obtains that $\text{sup}_{N \in \mathbb{N}} \left| \sum_{n=1}^N a_n \right| < \infty$ implies that such a periodic sequence would need to have period $n = 2 m$ and consist of in some order $m$ copies of $+1$ and $m$ copies of $-1$ else an immediate contradiction of a counter example arises right there. So it suffices to demonstrate that for any such periodic sequence then there exists a rational $\theta$ such that in the common mutual repeating component then terms do not cancel and we are left with something non zero that is so that then after every say after $j$ cycles of a fixed length then the offset vector in $\mathbb{C}$ would be something like $jv$. And as $N \to \infty$ of course then $\lim_{j \to \infty} ||jv|| = \infty$ as desired. But in any case we note that say for example in the breakdown of the $n = 2^a \cdot b$ where $b \equiv 1 \pmod{2}$ is an odd number. Then of course by examining upon primitive $2$th, $4$th, $\dots$, $2^a$th root of unity cases one infers there that of course the sum of $a_1+a_3+\dots+a_{n-1} = a_2+a_4+\dots+a_n = 0$ because otherwise one would obtain the immediate desired contradiction by considering that the sum $a_1+a_2+\dots+a_n = 0$ and then in the primitive $2$th root of unity case one obtains that we would have otherwise a $a(-1)-a(1)=-2a \neq 0$ contradiction counterexample case immediately. And then likewise from there we obtain deduce that $a_1+a_5+a_9+\dots = 0 = a_3+a_7+a_{11}+\dots$ because otherwise right there again we would otherwise have an immediate contradiction on the primitive minimal $4$th root of unity case. But in any case we can do this with the powers of $2$ inductively until we end up deducing that in the final step of course due to the odd parity it must be the case that the aggregate coefficient on the final dudes i.e. $a_1+a_{1+2^a}+\dots = -(a_{1+2^{a-1}}+a_{1+2^a+2^{a-1}}+\dots) \neq 0$ but then right here a natural contradiction arises as then for $\theta = \frac{1}{2^b}$ we have a contradiction as for example in On Vanishing Sums Of Roots Of Unity by T. Y. Lam and K. H. Leung and references but more notably the simple logic that we would have a combination of, for example under a natural rotation, we would then need to have a non trivial integer linear combination of the $2^n$th roots of unity except for $\pm 1$ which combined to a positive integer. But of course then by this and the similar observation we obtain a contradiction by examining the resultant polynomial in terms of $\sin \left( 2 \pi \cdot \frac{1}{2^a} \right)$ and the $\cos \left( \frac{x}{2} \right) = \sqrt{ \frac{1+\cos(x)}{2} }$ and the resultant Algebraic Number structure there provides a contradiction on rationality and irrationality.

% Eh writeup.

2 \\
It is interesting to ask for which discrete minimal value or asymptotic of this minimal value $M$ is it ensured that after $a_1, a_2, \dots, a_M$ there is guaranteed to exist such a rational number $\theta$ for which the $\text{sup}_{N \le M}$ is already $> 2022$. One approach is to consider the sequence of these partial sums mapped out as a function in $\mathbb{C}$ over all Real Numbers $\theta \in [0,2 \pi)$. One supposes that in fact Linearity Of Expectation or rather a total sum blow up argumentation in the context of the extension to the Real Numbers in conjunction with arbitrarily tight rational approximations which lead to desired counterexample would work so it suffices to exhibit that for some finite such $M$ there is guaranteed to exist a Real Number $\theta$ at which a contradiction emerges and then take a sufficiently precise approximation which will work there as the error term can be minimised all the way towards $0$. So we just need to exhibit that there exists some set of Real numbers such that there will be an element which after this finite time has blown up. Now I claim that we can start with all Reals and then execute a filtration sort of after each time step to select some subset of non trivial measure which is in particular sufficiently dense everywhere i.e. we may always just select some continuous Probability Distribution Function over $[0,2 \pi)$ at each time step such that the average distance in that subset chosen has increased after each time step by at least some distance. But think that distance which can be ensured has to do with the integral of the distance function say over a disc of radius $1$ from a point at distance $r$ from the origin for example namely from $(r,0)$ the expected value of the distance is for sufficiently large $r$ it will be like $\frac{2(r+1) E \left( \frac{4r}{(r+1)^2} \right)}{\pi}$ where E is the complete Elliptic Integral of the $2$nd kind with parameter $m = k^2$. But namely over time we can capture precisely this sort of increasing distance in our desired target points due to the denseness of the reals under their respective positions and the relevant offsets in the distribution inside of $\mathbb{R}^2$ meaning we can certainly choose to have sufficient mass wherever any such increase does occur and then we can have finally that $2022$ because these Elliptic Integrals canonically it will be via Harmonic Divergence in fact as the increase here goes in $\Delta \approx \Theta \left( \frac{1}{r} \right)$.

% Going off on pointless tangents is unwise. Want to keep writeups terse, simple, to-the-point, mathematical, True, some real concrete algebraic expressions when possible to clear up any ambiguities between logical steps. Re-read Evan Chen's technical documentations on technical writeups [despite his extremely weaky weak mediocre performances across the board in mathematics competitions].

3 \\
An extremely simple example would be a sequence of blocks of $+1$s followed by identically sized blocks of $-1$s where the block lengths are of sizes $2,2^2,2^2 \cdot 3,2^3 \cdot 3,2^3 \cdot 3^2,2^3 \cdot 3^2 \cdot 5,2^4 \cdot 3^2 \cdot 5,\dots$ where we essentially imitate the $\mathbb{Q}$ rationals lattice outwards spiral walking type style of argumentation upon instead the exponents in a prime factorisation. So we have the sequence begins like $1,1,-1,-1,1,1,1,1,-1,-1,-1,-1,1,1,\dots$. Now this ensures that for every rational $\theta \in \mathbb{Q} - \mathbb{Z}$ say in least reduced coprime form the rational is $\theta = \frac{a}{b}$ i.e. a primitive $b$th root of unity is the underlying representation in this schemata. Then in a finite time, number of steps, e.g. for a sufficiently large finite $N$ one obtains that these summations will become cyclic from that point onwards e.g. because the sequence is not periodic modulo $b$ per se but at some point each relevant sub block of length $b$ in a natural partition starting from precisely the first point at which $b | \text{Block Length}$ it will be from that point onwards by definition and construction the case that each block in this natural sub partitioning will be either consisting of all $+1$s or all $-1$s which then ensures that in particular the literal summations just cycle around a couple of chords paths on circles in $\mathbb{C}$ and are inside a bounded disk of finite radius in particular.

% This part seemed OK to me uh.

10 \\
1 \\
This follows by the usual Differential Equations and Partial Differential Equations blow up argumentation with respect to an exponential of a root of magnitude $||a|| > 1$ having $\lim_{t \to \infty} ||a||^{f(t)} = \infty$ as the relevant $\lim_{t \to \infty} f(t) = \infty$.

% I definitely need to formally write out Partial Differential Equations logic this year on the round to impress all of the graders and firms and girls.

2 \\
A proof eludes me.

11 \\
I have been doing maths, and maths competitions for a long while, seriously since the age of 12. But only recently, about 15 months ago when I started my studies here at the University Of Texas At Austin did I really become passionate. I found my passion in Algorithms, Combinatorics, Statistics, and Machine Learning. It is really fun to have ideas, compose code, and build new things which can help people and change lives for the better! Thanks to Alibaba, the task composers, the team, and the graders for running this fun fun fun competition and providing the funding for the prizes. Indeed, together, let us create the maths moment.