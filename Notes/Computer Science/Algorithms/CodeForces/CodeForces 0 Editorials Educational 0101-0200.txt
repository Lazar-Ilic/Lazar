I have ceased posting up solutions here.

----------

Educational Codeforces Round 152 — Editorial

By BledDest, 19 hours ago, In English 1849A - Morning Sandwich

Idea: BledDest

Tutorial 1849A - Morning Sandwich Notice that the type of filling doesn't matter. We can treat both cheese and ham together as one filling, with quantity c+h .

Then let's start building a sandwich layer by layer. Put a piece of bread. Then put a layer of filling and a piece of bread. Then another layer of filling and a piece of bread. Observe that you can add one layer of filling and one piece of bread until one of them runs out. After you've done that k times, you placed k+1 layers of bread and k layers of filling.

Thus, there are two general cases. If bread runs out first, then k=b−1 . Otherwise, k=c+h . The one that runs out first is the smaller of these two values. So the answer is min(b−1,c+h) .

Overall complexity: O(1) per testcase.

Solution (awoo) 1849B - Monsters

Idea: BledDest

Tutorial 1849B - Monsters Let's simulate the game process until the number of health points of each monster becomes k or less. Then we can consider that the i -th monster has aimodk health instead of ai (except for the case when ai is divisible by k , then the remaining health is k , not 0 ).

Now, the health points of all monsters are from 1 to k , so each time we damage a monster, we kill it. Therefore, monsters with k health points will die first, then the ones with k−1 health points, and so on. So, let's sort the monsters by their remaining health points in descending order (don't forget that, if two monsters have the same health, then they should be compared by index). And the order you get after sorting is the answer to the problem.

Solution (Neon) 1849C - Binary String Copying

Idea: BledDest

Tutorial 1849C - Binary String Copying We can see that each modified copy is determined by only two integers lb and rb — the first position at which the character has changed and the last such position. If we can find such numbers for each of the copies, the number of different pairs will be the answer to the problem.

Let lfi be the position of the nearest character 0 at the position i or to the left of it, and rgi be the position of the nearest character 1 at the position i or to the right of it. If the first character of the string s is 1 then lf0=−1 , otherwise lf0=0 . And if the last character of the string s is 0, then rgn−1=n , otherwise rgn−1=n−1 . The values lf and rg can be calculated using simple dynamic programming, lf is calculated from left to right, and rg — from right to left. Then the numbers lb and rb we need are equal to rgl and lfr , respectively. If rgl>lfr , then the changed segment is degenerate (and this means that the string does not change at all). We can define some special segment for this type of strings, for example, (−1,−1) . Otherwise, the segment (rgl,lfr) of the string will change.

Time complexity: O(nlogn) .

Solution (vovuh) 1849D - Array Painting

Idea: BledDest

Tutorial 1849D - Array Painting Suppose we used a second operation as follows: we decreased a red element x , and painted another element y red. Let's then say that x is the parent of y .

Furthermore, let's say that the element x controls the element y if one of the two conditions applies:

x is the parent of y ; x controls the parent of y . Now, suppose we used coins to paint some elements red. For each of those elements, there exists a segment of red elements which it controls. So, the problem can actually be reformulated as follows: we want to split the given array into the minimum number of segments so that each segment can be painted using one coin. Let's call a segment of the array good if it can be painted using only one coin.

To continue with our solution, we need the following property: if a segment is good, then its every subsegment is also good. This is kinda intuitive, but if you are interested in a formal proof, you can read the following paragraph.

Formal proof: there are two main cases we need to consider: either the element that controls the main segment belongs to the subsegment we analyze, or it does not. In the first case, the proof is simple: we can paint all elements of the subsegment red using just one coin in the same way as we painted the whole segment, by starting from the element that controls it. In the second case, it is a bit more difficult, but instead of the element controlling the main segment, we can either start from the leftmost element of the subsegment, or from the rightmost element of the subsegment — depending on whether this subsegment is to the right or to the left of the element controlling the whole segment. This starting element will be painted red by spending a coin, and every other element of the subsegment can be painted red in the same way we painted the whole segment.

Since if a segment is good, its every subsegment is also good, we can use the following greedy approach to solve the problem: start with the segment which contains only the first element of the array, and expand it to the right until it becomes bad. When the segment is no longer good, we need to start a new segment, which we will again expand to the right until it becomes bad, and so on. Then each element of the array will be considered only once. If we design a way to determine if the segment is still good when we add a new element to it in O(1) , our solution will work in O(n) .

All that's left is to analyze how to check if the segment is good. There are multiple ways to do this. The way used in the model solution employs the following ideas:

there cannot be any zeroes in the middle of the segment, since if we start painting red from the left of that zero, we cannot reach the elements to the right of that zero, and vice versa; if there are no zeroes in the middle, and at least one of the endpoints is not zero, the segment is good because we can just start from that endpoint and expand to the other endpoint; if there are no zeroes in the middle, and there is at least one element equal to 2 , we can start by painting that element red and expand to the left and to the right of it until we arrive at the borders of the segment; and if there are zeroes at both endpoints, but all other elements are 1 's, it's easy to see that paining the whole segment using only one coin is impossible: the sum of elements on the segment is k−2 , where k is the length of the segment, but we need to paint at least k−1 elements red without spending coins. All of these ideas allow us to verify that the segment is good using just a couple if-statements.

Solution complexity: O(n) .

Solution (BledDest) 1849E - Max to the Right of Min

Idea: awoo

Tutorial 1849E - Max to the Right of Min The problem was originally prepared as part of the lecture on a monotonic stack. Thus, I will omit its explanation.

First, recall a common technique of counting all segments satisfying some property. You can count the segments that have the same right border at the same time.

Consider all segments [l,r] with a fixed r . How do the minimums and the maximums on them change from each other? If you look at the segments in the order of decreasing l , we can write down the sequence of the following events: the minimum becomes smaller or the maximum becomes larger. In fact, we can maintain both of these types with two monotonic stacks.

Now, which l correspond to good segments with regard to the events? Consider two adjacent events (i1,t1) and (i2,t2) , where ij is the index of the event and tj is the type (0 for min, 1 for max). It's easy to see that if t2=0 , then all segments that have l from i1+1 to i2 are good. It means that, while going from right to left, the last event we encountered was the minimum getting smaller. Thus, the index of the minimum becomes to the left from the index of the maximum.

As for the implementation, we will maintain these events in a set of pairs: (index of the event, type of the event). This way, it's not that hard to maintain the sum of distances from each event of type 0 to the left to the previous event.

When you erase an event, only a few distances can be affected: the distance from the next one to the current one, from the current one to the previous one and the newly created distance from the next one to the previous one. Just check for the types. When you add an event, you only add it to the very end of the set, so it's trivial to recalculate.

That will be O(nlogn) just from the set, the monotonic stacks by themselves are linear.

You can optimize this solution to O(n) by using a doubly-linked list, but it really was not necessary for the problem.

There's also a different solution for which you can maintain the intervals of good values l explicitly. First, compress them in such a way that the segments don't touch at borders. Now, you can notice that by going from r to r+1 we can only affect the rightmost ones of them: possibly remove some, then change the last one and add a new one. So we can actually simulate this behavior with another stack. The details are left as an exercise to a reader. With the correct implementation, this solution will be O(n) .

Solution (awoo) 1849F - XOR Partition

Idea: BledDest

Tutorial 1849F - XOR Partition Disclaimer: the model solution to this problem is a bit more complicated than most of the solutions written by participants, but I will still use it for the editorial so that I can explain some of the classical techniques appearing in it.

Suppose we have built a graph on n vertices, where each pair of vertices is connected by an edge, and the weight of the edge connecting i with j is ai⊕aj . If we treat the partition of the set as a coloring of this graph, then the cost of the partition is equal to the minimum weight of an edge connecting two vertices of the same color. So, suppose we want to check that the answer is at least x . It means that every edge with weight less than x should connect two vertices of different colors; so, the graph where we erase all edges with weight greater than or equal to x should be bipartite. This allows us to write a binary search solution if we somehow understand how to construct the graph and check that it is bipartite implicitly, but this is not the way the model solution goes.

Instead, suppose we do the following greedy. Initially, let the graph be empty. Then, we add all edges with weight 1 , and check if it is bipartite. Then, we add all edges with weight 2 , and check if it is bipartite. And so on, until the graph is no longer bipartite.

Of course, there are O(n2) edges, so this is too slow. But in fact, we don't need all the edges for this graph. Some of the edges we added didn't really affect the coloring of the graph: if, in this process, we add an edge which connects two vertices from the same component, one of the following two things happens:

if it connects two vertices of the same color, then the graph is no longer bipartite; otherwise, this edge actually does not change anything in the coloring. Now, suppose we stop before making the graph non-bipartite. It means if an edge we added was connecting two vertices from the same component, that edge was actually redundant.

Let's take a look again at what we're actually doing. Initially, the graph is empty. Then, we consider all possible edges in ascending order of their weights; if an edge connects two vertices in different components, it should be added to the graph (and it affects the coloring), otherwise, it does not matter (unless it makes us stop the algorithm, since the graph is no longer bipartite). Doesn't it sound familiar? Yup, it is almost like Kruskal's algorithm.

And in fact, the problem can be solved as follows: build any MST of this graph, and use it to form the two-coloring (paint the vertices in such a way that every edge from MST connects two vertices of different colors). The paragraph below contains the formal proof that this coloring is optimal; feel free to skip it if you're not interested in the strict proof.

Formal proof: suppose that, if we color the vertices using the MST, we get an answer equal to x . This means that there are two vertices i and j such that their colors are the same, and ai⊕aj=x . Let us consider the cycle formed by the edge from i to j and the path from j to i in the MST. Since these two vertices have the same colors, this cycle contains an odd number of edges — so, at least one edge on this cycle will connect vertices of the same color no matter how we paint the graph. And there are no edges on this cycle with weight greater than x — otherwise, that edge would be replaced by the edge (i,j) in the MST. So, at least one edge with weight ≤x will be connecting two vertices of the same color, and thus the answer cannot be greater than x .

Okay, now we have to actually build the MST in this graph. XOR-MST is a fairly classical problem (it was even used in one of the previous Educational Rounds several years ago). I know two different solutions to it: the one based on Boruvka's algorithm and the D&C + trie merging method. I will describe the former one.

The classical Boruvka's algorithm is one of the algorithms of finding a minimum spanning tree. It begins with a graph containing no edges, and performs several iterations until it becomes connected. On each iteration, the algorithm considers each component of the graph and adds the minimum edge connecting any vertex from this component with any vertex outside this component (these edges are added at the same time for all components of the graph, so be careful about creating a cycle or adding the same edge twice; different methods can be used to prevent this — in the model solution, I memorize the edges I want to add on each iteration, then use a DSU to actually check which ones should be added without causing a cycle to appear).

During each iteration of the algorithm, the number of components decreases to at least half of the original number, so the algorithm works in O(Ilogn) , where I is the complexity of one iteration. Usually, I is O(m) since we need to consider each edge of the graph twice; but in this problem, we can perform one iteration of Boruvka's algorithm much faster, in O(nlogA) , where A is the constraint on the numbers in the input.

It can be done as follows: maintain a trie data structure which stores all values of ai and allows processing a query "given an integer x , find the value y in the data structure such that x⊕y is the minimum possible" (this can be done with a trie descent). When we consider a component and try to find the minimum edge leading outside of it, we first delete all vertices belonging to this component from the data structure; then, for each vertex from the component, find the best vertex in the data structure; and then, insert all vertices back. That way, a component of size V will be processed in O(VlogA) , and the total time for one iteration is O(nlogA) .

All of this results in a solution with complexity O(nlognlogA) .

Solution (BledDest)

Educational Codeforces Round 151 Editorial

By awoo, history, 118 minutes ago, translation, In English 1845A - Forbidden Integer

Idea: BledDest

Tutorial 1845A - Forbidden Integer The problem is about considering the least amount of cases possible. I propose the following options.

If x≠1 , then you can always print n ones. So the answer is YES.

If k=1 , then no integer is available, so the answer is NO.

If k=2 , then only 2 is available, so you can only collect even n . So if it's odd, the answer is NO.

Otherwise, you can always collect n with the following construction: if n is even then take 2 , otherwise take 3 . Then take ⌊n2⌋ twos. You can see that an even n only uses twos, so it fits the previous check. If it's odd, then k is at least 3 , so 3 is allowed to take.

Overall complexity: O(n) per testcase.

Solution (awoo) 1845B - Come Together

Idea: adedalic

Tutorial 1845B - Come Together Let d(P1,P2) be the Manhattan distance between points P1=(x1,y1) and P2=(x2,y2) . Then d(P1,P2)=|x1−x2|+|y1−y2| .

Note that if you are going from A to B (or to C ) along the shortest path, the Manhattan distance will be decreasing with each move. So Bob and Carol can walk together as along as they find the next cell that is closer to both B and C .

Now note that if they are in the bounding box of cells B and C then there are no such "next cell", since d(X,B)+d(X,C) is constant and equal to d(A,B) if X is in the bounding box. In other words, Bob and Carol can walk together until they reach some cell X inside the bounding box, where they split up.

Finally, let's look at the total distance they will walk: from one side it's d(A,B)+d(A,C) . But from the other side it's 2⋅d(A,X)+d(X,B)+d(X,C) .

So, d(A,X)=d(A,B)+d(A,C)−(d(X,B)+d(X,C))2 But since d(X,B)+d(X,C)=d(B,C) whichever X is chosen we can calculate answer as d(A,B)+d(A,C)−d(B,C)2+1 where +1 is because we are talking about cells A , B and C and not just points.

Solution (adedalic) 1845C - Strong Password

Idea: BledDest

Tutorial 1845C - Strong Password Consider the naive solution. You iterate over all password options that fit the criteria on l and r and check if they appear in s as a subsequence.

That check can be performed greedily: find the first occurrence of the first digit of the password, then find the first occurrence after it of the second digit of the password, and so on. If all digits are found, then it's present. Otherwise, it isn't.

Notice how the checks from the i -th digit onwards only depend on the position of the (i−1) -st digit. Moreover, to have a lower probability to find these digits, we want the (i−1) -st digit to be as much to the right as possible.

That leads us to the greedy solution to the full problem. Iterate over the first digit and choose the one that appears as much to the right as possible. Then the same for the second digit, and so on. If any digit is not found in the string after the starting position of the check, then the password that starts with the chosen digits is strong.

So far, the solution sounds like O(nmD) , where D is the number of digits (equal to 10 ). Which is actually fine under these constraints, but we can do better.

One option is to precalculate something to help us find the next occurrence of each digit. For example, that can be an array of positions of each digit. Then we can use lower_bound on it to find the next one. That would be O(n+D+mDlogn) .

Alternatively, you can calculate an array next[i][j] that stores the next occurrence of digit j from position i . It's possible to calculate next[i] from next[i+1] . Notice that only next[i][s[i]] changes. So you can copy next[i+1] into next[i] and set next[i][s[i]]=i . Now you can just query the array by looking into the corresponding cells. That would be O(nD+mD) .

Finally, let's analyze the complexity of the linear search better. So, for each digit, actually run a while loop that searches the string until it encounters that letter. We proposed that it is O(nmD) . Notice that if some iteration of the loop for the i -th digit passes a position x , then the digits from the (i+1) -st onwards won't look into it. So, such position can only be checked in O(D) loops (all loops for the current digit). Thus, each of n positions can only be accessed O(D) times, making the solution O(m+nD) , which makes it the fastest of all our options. That is basically the same analysis as two pointers. O(m) comes from the outer loop over digits that you can't really get rid of. However, you can say that m≤n (m>n can be solved in O(1) ), and call the solution O(nD) .

Overall complexity: O(m+nD) for each testcase.

Solution (awoo) 1845D - Rating System

Idea: BledDest

Tutorial 1845D - Rating System Let's fix some k and look at the first and the last moment when the rating should fall below k , but doesn't. After such moments, the rating is equal to k . So we can "delete" all changes (array elements) between those moments. And the remaining changes (to the left from the first moment and to the right from the last moment) can be considered without any additional constraints (the value of k doesn't affect those changes).

Using this fact, we can see that the total rating is not greater than the sum of the whole array minus some continuous segment. So the maximum possible final rating doesn't exceed the sum of the array minus the minimum sum segment (this segment can be empty, if all elements are positive). In fact, there is always such k that provides such rating. Let the minimum sum segment be [l;r] , then k is equal to the prefix sum from 1 -st to (l−1) -th positions of the array.

The only remaining question is: why the rating after r -th match is equal to k ? It can't fall below k (since the rating is at least k already); and if it is greater than k , then there is a positive suffix of the segment [l;r] , so we can remove it, and the sum of the segment would decrease. Which means the segment [l;r] is not minimum sum segment, which contradicts the previous statement. So the rating after r -th match is equal to k .

Solution (Neon) 1845E - Boxes and Balls

Idea: BledDest

Tutorial 1845E - Boxes and Balls Consider a harder problem. For each k′ from 0 to k , what's the number of arrangements that have k′ as the smallest number of operations needed that obtain them?

That would help us solve the full problem. You just have to sum up the answer for k′ such that they have the same parity as k (since, once the arrangement is obtained, you can perform two moves on it and change nothing).

Turns out, calculating the smallest number of operations for a fixed arrangement is not that hard. Let the initial arrangement have balls in boxes c1,c2,…,ct for some t ; the fixed arrangement have balls in boxes d1,d2,…,dt . Then the first ball in the fixed arrangement has to come from the box of the first ball in the initial one. And so on. So the answer is at least ∑i=1t|ct−dt| . That estimate can be achieved. Just move the balls one by one from left to right. So that amount is actually the smallest possible.

That can lead to a dynamic programming solution. Following this construction, let dp[i][j][k] be the number of ways to fill the first i boxes with j balls such that the smallest number of operations to move the first j balls of the initial arrangement into their new boxes is k . The transitions are trivial. Either leave the i -th (all 0 -indexed) box empty and go to dp[i+1][j][k] or put a ball into it and go to dp[i+1][j+1][k+|i−cj|] . The answer will be in dp[n][t][k′] for all k′ of the same parity as k . That solution is O(n2k) that is supposedly too much (although unfortunately can be squeezed if you try hard enough).

That solution has surprisingly little to do with the full one but gives us some insight into the problem.

For a faster solution, let's change the way we calculate the smallest number of operations. What is exactly |ct−dt| ? Let ct>dt . Then on its path the ball crosses the spaces between boxes ct and ct−1 , ct−1 and ct−2 , so on until dt+1 and dt . The amount is exactly (ct−dt) .

Thus, we could instead calculate the number of balls that move across each space between the boxes along their paths and add up the values. Now it's some sort of balance. We could also denote balls going to the right as positive values and going to the left as negative values. Notice how if some ball moves from a box i to a box i−1 in the optimal construction, then there is no ball that moves from i−1 to i . Just because the balls never cross each other paths. So the absolute value of the balance is still what we have to add up.

Intuitively, the value for space between boxes i and i+1 is equal to the signed difference between the initial number of balls to the left of it and the one in the current arrangement. If the numbers are different, then we move exactly this amount of ball from one side to another.

Now we can pack it into another dynamic programming. Let dp[i][j][k] be the number of ways to fill the first i boxes, such that the current balance is j and the smallest number of operations to achieve that is k .

The transitions are the following. If we place a ball into box i , the balance changes to j+1−ai (ai is whether there was a ball in box i initially) and |j+1−ai| gets added to k . If we don't place a ball, the balance changes to j−ai and |j−ai| gets added to k .

Notice that at the end the balance will be 0 if and only if we placed as many boxes as there were initially. So, the answer will be in dp[n][0][k′] for all k′ of the same parity as k .

That solution is still O(n2k) and even worse in the way that j can range from −n to n , doubling the runtime.

However, notice how j can't change by more than 1 on each step. At the same time, |j| always gets added to k . Thus, to make j equal to 0 at the end, we would have to add |j|+(|j|−1)+(|j|−2)+⋯+1 to k . And since k can't exceed 1500 , |j| actually can't exceed 55 (more or less O(k−−√) ).

So, that solution can be optimized to O(nk1.5) by reducing the second dimension of the dynamic programming.

In order to store values from −x to x in an array, shift them up by x . So the values become from 0 to 2x .

In order to avoid O(nk1.5) memory, instead of storing all n layers of the dp, only store the current and the next one. That will make it O(k1.5) memory.

Overall complexity: O(nk1.5) .

Solution (awoo) 1845F - Swimmers in the Pool

Idea: adedalic

Tutorial 1845F - Swimmers in the Pool Firstly, note that there are two different situations when some two swimmers meet: they either move in the same direction, or in opposite directions.

Suppose, swimmers i and j meet while moving in the same direction. We can write some easy system of equation and get that they will meet each 2l|vi−vj| seconds. Analogically, if they meet while moving in the opposite directions, they will meet each 2lvi+vj seconds.

Let's create array w that will contain all possible values |vi±vj| exactly once. If V=maxvi then values wi≤2V and we can calculate all of them using FFT fast multiplication two times: for sums vi+vj and for differences vi−vj .

Okay, we got all possible wi , how to calculate the answer. For a fixed value wi meeting moments are 2l⋅kwi for all k in segment [1,ki] . ki is the upper bound and can be calculated as ki=⌊t⋅wi2l⌋ .

We found that for each wi there are exactly ki meeting points, but since in one meeting moment more than two swimmers may meet we need to calculate each value 2l⋅kwi exactly once.

Note that 2l⋅k1wi=2l⋅k2wj iff k1wi=k2wj . And we can rephrase our task as following: calculate the number of unique fractions kwi where 1≤k≤ki .

The key idea here is to calculate only irreducible fractions. Suppose we have fractions 1wi,2wi,…,kiwi . Let's add to the answer only irreducible fractions among them (we will discuss how to do it later). For any other fraction k′wi (k′,wi)=d>1 and d is a divisor of wi .

If we fix some divisor d of wi there will be exactly ⌊kid⌋ fractions k′wi with d|(k′,wi) . Moreover, numerators will also form a segment [1,kid] . So, instead of calculating them now, we will just "pass" that task to wj=wid .

In total, we iterate wi in decreasing order, add only the number of irreducible fractions kwi to the answer. Then iterate over all divisors d of wi and "update" value kj for wj=wid with value kid .

How to calculate the number of irreducible fractions kwi with 1≤k≤ki ? With Möbius function, of course: iri=∑d|wiμ(d)⌊kid⌋ since for each divisor d of wi there are exactly ⌊kid⌋ fractions where (k′,wi) is divisible by d .

Both passing calculations and Möbius inversion works in O(number of divisors(wi)) . And since we iterate over all wi≤2V , the total complexity is O(VlogV) .

Both FFT and next part works in O(VlogV) , so the total complexity is O(n+VlogV) .

P.S.: If you note that if wj=wid then kj is always equal to kid then you can not only simplify the part with "passing down calculations" but get rid of Möbius inversion at all, replacing it with Sieve-like two for-s iterations.

Solution (adedalic) Tutorial of Educational Codeforces Round 151 (Rated for Div. 2)

Educational Codeforces Round 150 Editorial

By awoo, history, 32 minutes ago, translation, In English 1841A - Game with Board

Idea: BledDest

Tutorial 1841A - Game with Board Let's try to find a winning strategy for Alice. We can force Bob into a situation where his only action will be to make a move that leaves Alice without any legal moves, so she will win. For example, Alice can start by merging n−2 ones, so the board will be {1,1,n−2} after her move. The only possible move for Bob is to merge the remaining 1 's, and the board becomes {2,n−2} , and Alice wins.

Unfortunately, this works only for n≥5 . Cases n=2 , n=3 , n=4 should be analyzed separately:

if n=2 , the only possible first move for Alice is to merge two 1 's, and the board becomes {2} , so Bob wins; if n=3 , Alice can transform the board either to {3} or to {1,2} ; in both cases, Bob instantly wins; if n=4 , Alice can transform the board to {4} , {1,3} , or {1,1,2} . In the first two cases, Bob instantly wins. In the third case, the only possible response for Bob is to merge two 1 's; the board becomes {2,2} . It's easy to see that after the only possible move for Alice, the board becomes {4} , and Bob wins. So, when n≥5 , Alice wins, and when n≤4 , Bob wins.

Solution (BledDest) 1841B - Keep it Beautiful

Idea: BledDest

Tutorial 1841B - Keep it Beautiful First, notice that the given operation is a cyclic shift of the array. So we can treat the array as cyclic, meaning element n is a neighbor of element 1 .

Let's try to rephrase the condition for the beautiful array. What does it mean for the array to be sorted? For all j from 1 to n−1 , aj≤aj+1 should hold. If they do, then you can choose i=0 (leave the array as is).

What if there are such j that aj>aj+1 ? If there is only one such j , then we might still be able to fix the array: choose i=j . However, that will make a pair an and a1 cyclically shift into the array. So an≤a1 should hold.

If there are at least two such j or just one but an>a1 , then we can show that it's impossible to make the array sorted. Since there are at least two pairs of neighboring elements that are not sorted, at least one of them will still be in the array after any cyclic shift.

Thus, we can maintain the number of such j that aj>aj+1 and check if an>a1 every time if the count is exactly 1 .

Overall complexity: O(q) per testcase.

Solution (awoo) 1841C - Ranom Numbers

Idea: BledDest

Tutorial 1841C - Ranom Numbers There are two main solutions to this problem: dynamic programming and greedy.

DP approach

Reverse the string we were given, so that the sign of each digit depends on the maximum digit to the left of it (not to the right). Then, run the following dynamic programming: dpi,j,k is the maximum value of the number if we considered i first characters, applied k changes to them (k is either 0 or 1 ), and the maximum character we encountered so far was j (j can have 5 possible values). The transitions are fairly simple: when we consider the state dpi,j,k , we can either leave the current character as it is, or, if k=0 , iterate on the replacement for the current character and use the replacement instead (then we go to the state with k=1 ). Note that this solution can also work if we can make more than one operation. This dynamic programming has O(n⋅A⋅m) states (where A is the number of different characters, and m is the maximum number of changes we can make), and each state has no more than A+1 outgoing transitions. In this problem, A=5 and m=1 , so this solution easily passes.

Greedy approach

Of course, we can try to iterate on every character and check all possible replacements for it, but quickly calculating the answer after replacement can be a bit difficult. Instead, we claim the following: it is optimal to consider at most 10 positions to replace — the first and the last position of A, the first and the last position of B, and so on. That way, we have only 50 different candidates for the answer, and we can check each of them in O(n) .

How to prove that this is enough? When we replace a character, we either increase or decrease it. If we increase a character, it's easy to see why it's optimal to try only the first occurrence of each character — increasing a character may affect some characters to the left of it (turn them from positive to negative), and by picking the first occurrence of a character, we make sure that the number of characters transformed from positive to negative is as small as possible. Note that if the string has at least one character different from E, we can replace the first such character with E and increase the answer by at least 9000 (this will be useful in the second part of the proof).

Now suppose it's optimal to decrease a character, let's show that it's always optimal to choose the last occurrence of a character to decrease. Suppose we decreased a character, and it was not the last occurrence. This means that this character will be negative after replacement, so it should be negative before the replacement. The maximum value we can add to the answer by replacing a negative character with another negative character is 999 (changing negative D to negative A), and we have already shown that we can add at least 9000 by replacing the first non-E character in the string with E. So, if we decrease a character and it was not the last occurrence of that character, it's suboptimal.

Solution 1 (Neon) Solution 2 (BledDest) 1841D - Pairs of Segments

Idea: BledDest

Tutorial 1841D - Pairs of Segments The resulting array should consist of pairs of intersecting segments, and no segments from different pairs should intersect.

Let's suppose that segments [l1,r1] and [l2,r2] intersect, and segments [l3,r3] and [l4,r4] intersect. How to check that no other pair of these four segments intersects? Instead of pairs of segments, let's work with the union of segments in each pair; no point should belong to more than one of these unions. Since segments [l1,r1] and [l2,r2] intersect, their union is also a segment, and its endpoints are [min(l1,l2),max(r1,r2)] . So, if we transform each pair into a union, then the union-segments we got should not intersect.

Thus, we can solve the problem as follows: for each pair of intersecting segments in the input, generate their union segment, and pick the maximum number of unions we can (these unions represent the pairs in the answer). It's quite easy to see that we won't pick a segment in two pairs simultaneously (the corresponding unions will intersect).

Okay, now we have O(n2) union segments, how to pick the maximum number of them without having an intersection? This is a somewhat classical problem, there are many methods to solve it. The one used in the model solution is a greedy algorithm: sort the union segments according to their right border (in non-descending order), and pick segments greedily in sorted order, maintaining the right border of the last segment we picked. If the union segment we consider is fully after the right border of the last union we picked, then we add it to the answer; otherwise, we skip it. This approach works in O(n2logn) .

Solution (BledDest) 1841E - Fill the Matrix

Idea: BledDest

Tutorial 1841E - Fill the Matrix Notice that the rows of the matrix are basically independent. When we fill the matrix with integers, the values from the rows are just added together. Moreover, in a single row, the segments of white cells separated by black cells are also independent in the same way.

And how to solve the problem for one segment of white cells? Obviously, put the numbers one after another. Placing a non-zero amount of integers k will yield the result of k−1 . Thus, the answer is ∑i=1ski−1 , where s is the number of used segments. We know that the sum of ki is just m . So it becomes m+∑i=1s−1=m−s . So, the problem asks us to minimize the number of used segments.

In order to use the smallest number of segments, we should pick the longest ones. We only have to find them.

In the worst case, there are O(n2) segments of white cells in the matrix. However, we can store them in a compressed form. Since their lengths are from 1 to n , we can calculate how many segments of each length there are.

Look at the matrix from the bottom to the top. First, there are some fully white rows. Then some black cell appears and splits the segment of length n into two segments. Maybe more if there are more black cells in that row. After that the split segments behave independently of each other.

Let's record these intervals of rows each segment exists at. So, let some segment exist from row l to row r . What does that mean? This segment appeared because of some split at row r . At row l , some black cell appeared that split the segment in parts. If we knew the r for the segment, we could've saved the information about it during the event at row l .

So, we need a data structure that can support the following operations:

find a segment that covers cell x ; erase a segment; insert a segment. Additionally, that data structure should store a value associated with a segment. Thus, let's use a map. For a segment, which is a pair of integers, store another integer — the bottommost row this segment exists.

Make events (ai,i) for each column i . Sort them and process in a non-increasing order. During the processing of the event, find the segment this black cell splits. Save the information about this segment. Then remove it and add two new segments (or less if some are empty): the one to the left and the one to the right.

At the end, for each length from 1 to n , we will have the number of segments of such length. In order to fill them with m integers, start with the longest segments and use as many of each length as possible. So, while having m more integers to place and cnt segments of length len , you can use min(⌊mlen⌋,cnt) of segments.

Overall complexity: O(nlogn) per testcase.

Solution (awoo) 1841F - Monocarp and a Strategic Game

Idea: TheWayISteppedOutTheCar and BledDest

Tutorial 1841F - Monocarp and a Strategic Game Let's denote a,b,c,d as the total number of humans, orcs, elves, and dwarves taken respectively. We calculate the contribution of humans and orcs to the final score. It will be a(a−1)+b(b−1)−2ab+a+b , where a(a−1)+b(b−1) is the contribution to the total happiness (and thus to the final score) for an increase of 1 happiness point per inhabitant of the same race, −2ab is the contribution to the total happiness decrease by 1 point for each inhabitant of the hostile race, and the last two terms a+b is the total number of humans and orcs, which contributes to the total score. Notice that a(a−1)+b(b−1)−2ab+a+b=(a−b)2 .

Similarly for the contribution of elves and dwarves. So we need to maximize the expression (a−b)2+(c−d)2 . Let's say we have n vectors, the i -th of which is equal to (xi;yi)=(ai−bi;ci−di) . Then note that we want to select some subset of vectors among the n vectors (xi;yi) , a subset A such that the value of the function x2+y2 is maximum for the vector (x,y) equal to the sum of the vectors of subset A .

For each subset of vectors, we get a vector equal to the sum of the elements of the subset (vectors), let's denote the set of all such vectors as S .

Then consider the n line segments at the vertices (0;0) and (xi;yi) and calculate their Minkowski sum. We get a convex polygon. Notice that all elements of S are in it by the definition of the Minkowski sum. Also, each vertex of the resulting convex polygon is in S , which is also known from the properties of the sum. Note that due to the upward convexity of the function we want to maximize, it is enough for us to only consider the vertices of the resulting polygon, not all vectors from S .

Thus, using a standard algorithm for calculating the Minkowski sum, we obtain the polygon of interest and calculate the values of the function at its vertices. We take the maximum from them. The total asymptotic time complexity is O(nlnn) .

Solution (TheWayISteppedOutTheCar) Tutorial of Educational Codeforces Round 150 (Rated for Div. 2)

Educational Codeforces Round 149 Editorial

By awoo, history, 16 minutes ago, translation, In English 1837A - Grasshopper on a Line

Idea: BledDest

Tutorial 1837A - Grasshopper on a Line When x is not divisible by k , the grasshopper can reach x in just one jump.

Otherwise, you can show that two jumps are always enough. For example, jumps 1 and x−1 . 1 is not divisible by any k>1 . Also, x and x−1 can't be divisible by any k>1 at the same time.

Solution (awoo) 1837B - Comparison String

Idea: BledDest

Tutorial 1837B - Comparison String Suppose there is a segment of length k that consists of equal characters in s . This segment implies that there are at least k+1 distinct values in the answer: for example, if the segment consists of < signs, the first element should be less than the second element, the second element should be less than the third element, and so on, so the corresponding segment of the array a contains at least k+1 different elements.

So, the answer is at least m+1 , where m is the length of the longest segment of the string that consists of equal characters. Can we construct the array a which will contain exactly m+1 distinct values? It turns out we can do it with the following greedy algorithm: let's use integers from 0 to m for our array a , and let's construct it from left to right; every time we place an element, we choose either the largest possible integer we can use (if the next character is >) or the smallest possible integer we can use (if the next character is <). For example, for the string <><<<>, the first 6 elements of the array will be [0,3,0,1,2,3] (and we can use any integer from 0 to 2 in the last position). That way, whenever a segment of equal characters begins, the current value in the array will be either m or 0 , and we will be able to decrease or increase it m times, so we won't arrive at a situation where, for example, the current value is 0 and we have to find a smaller integer.

So, the problem basically reduces to finding the longest contiguous subsegment of equal characters in s .

Solution (BledDest) 1837C - Best Binary String

Idea: BledDest

Tutorial 1837C - Best Binary String First of all, let's try solving an easier problem — suppose we have a binary string, how many operations of the form "reverse a substring" do we have to perform so that it is sorted? To solve this problem, we need to consider substrings of the form 10 in the string (i. e. situations when a zero immediately follows a one). Sorted binary strings should not contain any such substrings, so our goal is to reduce the number of such strings to zero.

Let's try to analyze how can we reduce the number of substrings equal to 10 by reversing a substring. Suppose that we want to "remove" a particular substring 10 from the string without causing any new ones to appear. We can reverse the substring that consists of the block of ones and the block of zeroes adjacent to each other; that way, after reversing the substring, these two blocks will be swapped; the block of zeroes will either merge with the block of zeroes to the left, or move to the beginning of the string; the block of ones will either merge with the block of ones to the right, or move to the end of the string. For example, if, in the string 0011101, you reverse the substring from the 3 -rd to the 6 -th position, you get 0001111, and you reduce the number of substrings 10 by one.

What if we want to reduce this number by more than one in just one operation? Unfortunately, this is impossible. Suppose we want to affect two substrings 10 with one reverse operation. There will be a substring 01 between them, so, after reversing, it will turn into 10, and we'll reduce the number of substrings 10 only by one. The same when we try to affect three, four or more substrings 10. So, we can reduce the number of substrings 10 only by one in one operation. So, the answer to the problem "count the number of operations required to sort the binary string" is just the number of substrings 10.

Okay, back to the original problem. Now we want to replace every question mark so that the resulting string contains as few substrings 10 as possible. You can use dynamic programming of the form dpi,j — the minimum number of substrings if we considered i first characters of the string and the last of them was j . Or you can try the following greedy instead: go through the string from left to right, and whenever you encounter a question mark, replace it with the same character as the previous character in the string (if the string begins with a question mark, it should be replaced with 0). That way, you will create as few "blocks" of characters as possible, so the number of times when a block of 1's changes into a block of 0's will be as small as possible.

Solution (Neon) 1837D - Bracket Coloring

Idea: BledDest

Tutorial 1837D - Bracket Coloring What properties do beautiful bracket sequences have? Well, each beautiful sequence is either an RBS (regular bracket sequence) or a reversed RBS. For RBS, the balance (the difference between the number of opening and closing brackets) is non-negative for every its prefix, and equal to zero at the end of the string. For a reversed RBS, the balance is non-positive for every prefix, and zero at the end of the string. So, every beautiful string has balance equal to 0 , and if the string s has non-zero balance, it is impossible to color it. Let's consider the case when the balance of s is 0 .

Suppose we calculated the balance on every prefix of s , and split it into parts by cutting it along the positions where the balance is 0 . For example, the string (()())())( will be split into (()()), () and )(. For every part of the string we obtain, the balance in the end is equal to 0 , and the balance in the middle of the part is never equal to 0 (since positions with balance equal to 0 were the positions where we split the string). So, the balance is either positive in all positions of the part, or negative in all positions of the part; and every string we obtain from cutting s into parts will be beautiful.

A concatenation of two RBS'es is always an RBS. The same can be said about the strings which become RBS after reversing. So, for every part we obtain after cutting s into parts, we can determine whether it is an RBS or a reversed RBS, concatenate all RBS'es into one big RBS (by coloring them into color 1 ), and concatenate all reversed RBS'es into one string (by coloring them into color 2 ). This construction shows that the maximum number of colors is 2 and allows to obtain the coloring into two colors; so, all that's left to solve the problem is to check whether it's possible to use just one color (it is the case if and only if the given string s is beautiful).

Solution (BledDest) 1837E - Playoff Fixing

Idea: BledDest

Tutorial 1837E - Playoff Fixing Let's investigate the structure of the tournament, starting from the first round.

We know that teams from 2k−1+1 to 2k have to lose during this round. At the same time, there are exactly 2k−1 losers in this round. So, every pairing has to look like this: a team from 1 to 2k−1 versus a team from 2k−1+1 to 2k .

Let's try to solve for zero reserved seeds — all ai=−1 . So, we only have to count the number of valid tournaments.

Let's try to do that starting from the top. The winner is team 1 . The final is team 1 against team 2 , but we have an option to choose their order in the finals. Next, the semifinal. One of teams 3 and 4 have to play against team 1 and another one against team 2 . So we have the following options: choose a permutation of the losers on this stage, then choose the order of teams in every game.

If we extrapolate that argument further, we will see that the stage with 2i teams multiplies the answer by 2i−1!⋅22i−1 (arrange the losers, then fix the order in pairs). The product over all stages is the answer.

In order to add the reserved seeds, we have to go back to solving bottom-up.

Consider the first stage. We want to calculate its contribution to the answer, then promote the winners of each pair to the next stage and continue solving for k=k−1 .

Some pairs have both teams reserved. If both or neither of the teams are from 1 to 2k , then the answer is immediately 0 . Otherwise, the winner is known, so we can promote it further.

Some pairs have no teams reserved. Here, we can pick the order of the teams (multiply the answer by 2 ). And we basically know the winner as well — this is marked as −1 . The loser is yet to be determined.

Some pairs have one team reserved, and that team is from 1 to 2k−1 . That team has to win in this pair, so we know who to promote. We will determine the loser later as well.

Finally, some pairs have one team reserved, and the team is from 2k−1+1 to 2k . We know the loser here, but the winner is marked −1 . Still, we can promote this −1 further and deal with it on the later stages.

How do we arrange the losers? Well, it's almost the same as in the case with zero reserved teams. There are some pairs that are missing losers, and there are loser teams yet to be placed. We only have to choose which team goes where. So, it's (the number of pairs with no reserved loser)!.

Why can we promote the "winner" −1 's furthers without dealing with them immediately? Basically, that factorial term is the only place where we assign the numbers to teams. Obviously, we want to assign each team a number exactly once. And that happens only when the team loses. Once we assign some number to it, we can uniquely trace back to where this team started, since we fixed the order of each pair beforehand.

We decrease k until it's equal to 0 . The answer is still the product of the combinations for each stage.

Overall complexity: O(2k) .

Solution (awoo) 1837F - Editorial for Two

Idea: BledDest

Tutorial 1837F - Editorial for Two In the problem, we are asked to first choose k problems, then fix a split into a prefix and a suffix. But nothing stops us from doing that in reverse. Let's fix the split of an entire problemset first, then choose some l (0≤l≤k ) and l problems to the left of the split and the remaining k−l problems to the right of it.

That allows us to proceed with some polynomial solution already. Having fixed the split and l , we only have to find l shortest editorials to the left and k−l shortest editorials to the right. It's trivial to show that it's optimal.

That can be done in O(n2logn) easily or O(n2) if you think a bit more. That solution doesn't really help with the full one, so I won't elaborate.

Next, when we see something along the lines of "minimize the maximum", we think about binary search.

Let's try to apply it here. Binary search over the answer — now we want the sum of the left and the right parts of the split to be less than or equal to some fixed x .

More specifically, there should exist some l that the sum of l smallest elements to the left is ≤x and the sum of k−l smallest elements to the right is ≤x . Let's say it differently. There should be at least l elements to the left with their sum being ≤x , same to the right. And once more. The largest set with the sum ≤x to the left is of size at least l , same to the right.

But that doesn't really require that l now, does it? Find the largest set with the sum ≤x to the left and to the right. The sum of their sizes should be at least k .

Let's calculate the size of this largest set for all prefixes and all suffixes of the array. Then we will be able to check the condition in O(n) .

You can precalculate the sizes for all prefixes in O(nlogn) . The idea is the following. Take the set for some prefix i . Add the (i+1) -st element to it. If its sum is ≤x , then it's the new set. Otherwise, keep removing the largest element from it until the sum becomes ≤x .

That solution is O(nlogAlogn) , where A is the sum of the array, so you should be careful with the constant factor. For example, doing it with a multiset or with a segment tree probably won't pass. priority_queue is fast enough, though.

However, we can do it faster. Imagine a solution with the multiset. Let's replace it with a doubly-linked list.

In particular, we want the following operations:

insert an element in it; check and remove the last element from it. If we never removed any element, we would be able to determine where to insert each element. We can just precalculate that before the binary search in O(nlogn) . For each element, find the largest element less than or equal to it and to the left of it. That would be the previous element when this one is inserted.

When we remove elements, it can happen that this previous element was already removed when we attempt to insert this one. We can use the specificity of the problem to avoid that issue. Notice that if we removed an element less than or equal to the current one, then the current one could never be inside an optimal multiset. So we can just skip this element.

To implement such a list, we can renumerate all elements into values from 0 to n−1 (in order of (value,i) ). Then, for each i store the value of the previous and the next elements existing in the list. For convenience, you can also add nodes n and n+1 , denoting the tail and the head of the list.

Then insert and remove is just rearranging some links to the previous and the next elements.

Overall complexity: O(nlogA) per testcase, where A is the sum of the array.

Solution 1 (awoo) Solution 2 (awoo) Tutorial of Educational Codeforces Round 149 (Rated for Div. 2)

Educational Codeforces Round 148 Editorial

By awoo, history, 50 minutes ago, translation, In English 1832A - New Palindrome

Idea: BledDest

Tutorial 1832A - New Palindrome Let's look at the first ⌊|s|2⌋ characters. If all these characters are equal, then there is no way to get another palindrome. Otherwise, there are at least two different characters, you can swap them (and the characters symmetrical to them on the right half of the string), and get a palindrome different from the given one.

Solution (Neon) 1832B - Maximum Sum

Idea: BledDest

Tutorial 1832B - Maximum Sum The first instinct is to implement a greedy solution which removes either two minimums or one maximum, according to which of the options removes elements with smaller sum. Unfortunately, this doesn't work even on the examples (hint to the participants of the round: if your solution gives a different answer on the examples, most probably you should try to find an error in your solution, not send messages to the author that their answers are definitely wrong).

Okay, we need something other. Let's notice that the order of operations does not matter: deleting two minimums and then maximum is the same as choosing the other way around. So, we can iterate on the number of operations m when we remove the two minimums; then the resulting array is the array a from which we removed 2m minimums and (k−m) maximums.

How to quickly calculate the sum of remaining elements? First of all, sorting the original array a won't hurt, since the minimums then will always be in the beginning of the array, and the maximums will be in the end. After the array a is sorted, every operation either deletes two leftmost elements, or one rightmost element. So, if we remove 2m minimums and (k−m) maximums, the elements that remain form the segment from the position (2m+1) to the position (n−(k−m)) in the sorted array; and the sum on it can be computed in O(1) with prefix sums.

Time complexity: O(nlogn) .

Solution (awoo) 1832C - Contrast Value

Idea: BledDest

Tutorial 1832C - Contrast Value Let's rephrase the problem in the following form: let the elements of the array be points on a coordinate line. Then the absolute difference between two adjacent elements of the array can be represented as the distance between two points, and the contrast of the entire array is equal to the total distance to visit all points in the given order.

In this interpretation, it becomes obvious that removing any set of points does not increase contrast. Since the resulting contrast should be equal to the original one, we can only remove elements from the array that do not decrease the contrast.

First of all, let's look at consecutive equal elements, it is obvious that you can delete all of them except one, and the contrast of the array will not change. In some languages, you can use a standard function to do this — for example, in C++ you can use unique.

After that, let's look at such positions i that ai−1<ai<ai+1 ; you can delete the i -th element, because |ai−1−ai|+|ai−ai+1|=|ai−1−ai+1| . Similarly, for positions i , where ai−1>ai>ai+1 , the element can be removed. In all other cases, removing the element will decrease the contrast.

Solution (Neon) 1832D1 - Red-Blue Operations (Easy Version)

Idea: BledDest

Tutorial 1832D1 - Красно-синие операции (простая версия) Let's try applying the operation to a single element multiple times. You can see that when you apply an operation an odd number of times, the element always gets larger. Alternatively, applying an operation an even amount of times always makes it smaller (or equal in case of 0 ). Thus, generally, we would want to apply an odd number of operations to as many elements as we can.

Let's start by investigating an easy case: k≤n . Since there are so few operations, you can avoid subtracting from any element at all. Just one operation to k elements. Which elements should get this operation? Obviously, k smallest elements. If you don't pick any of them, it'll remain as the minimum of the array. If you do, the minimum will get larger than any of them. Another greedy idea is where to apply each operation to. We should add k to the minimum element, k−1 to the second minimum and so on. You can show that it is always optimal by contradiction.

What if k>n ? Then we have to subtract from some elements. First, recognize that there's no more than one element that will become smaller in the optimal answer. Consider an answer where at least two elements get smaller and move the later operation from one to another. Now both of them become larger.

Moreover, for parity reasons, if kmod2=nmod2 , you can increase all elements. Otherwise, you have to leave one out.

Another greedy idea. You somewhat want the pair of operations (increase, decrease) to change the element is small as possible. In particular, you can always make it −1 . To show it, you can look at some decreasing operation such that an increasing operation goes right before it and they are applied to different elements. You can swap them. They remain the same types. However, both elements get only larger. Thus, in the end, all adjacent increasing and decreasing operations go to the same element.

Now, let's think of the answer as the following sequence: adjacent pairs of (increase, decrease) and the separate increase operations that are applied no more than once to each element. Since all blocks do −1 , only the values in the lone increase operations affect the answer value. Obviously, we would want these lone increases to be k,k−1,…,k−t , where t is the number of changed elements (n−1 or n ). Once again, you can show that you can always rearrange the answer to obtain that.

Finally, the answer looks like this: add k to the minimum, k−1 to the second minimum, k−(n−(n+k)mod2)−1 to the maximum or the second maximum depending on the parity. Before that, add −1 to some elements. Since we know where to apply the last operations, let's apply them first. Then we want to apply −1 's to make the minimum as large as possible. Well, we should always apply it to the current maximum. First, the minimum would remain as the original minimum. Then all elements will become equal to it at some point. Then we will be spreading the −1 's equally among the elements. So every n additional operations, the minimum will be decreasing by 1 .

For the easy version, you can simulate this process. Sort the array, apply the last increase operations, find the minimum and the sum of the array, then calculate the result based on that. Let a′=[a1+k,a2+k−1,…] . Let s=∑ni=1(a′−mina′) . Then s operations of −1 will not change the minimum. The rest k−(n−(n−k)mod2)2−s operations will decrease it by 1 every n operations. So it will get decreased by ⌈max(0,k−(n−(n−k)mod2)2−s)n⌉ .

Overall complexity: O(n⋅q) .

1832D2 - Red-Blue Operations (Hard Version)

Idea: BledDest

Tutorial 1832D2 - Red-Blue Operations (Hard Version) Read the editorial to the easy version to get the general idea of the solution.

Now, we should only optimize the calculations.

First, the sorting can obviously be done beforehand. Now we want to get the minimum and the sum after applying the last increase operations. Consider (n+k)mod2=0 and k>n , the other cases are similar. The minimum is that: minni=1ai−(i−1)+k=k+minni=1ai−(i−1) . Notice how the second part doesn't depend on k and precalculate it. This also helps with the case of k<n . You can calculate prefix minimum array over this.

The sum is similar: ∑ni=1ai−(i−1)+k=n⋅k+∑ni=1ai−(i−1) . As easy to precalculate.

The rest is the same.

Overall complexity: O(n+q) .

Solution (awoo) 1832E - Combinatorics Problem

Idea: BledDest

Tutorial 1832E - Combinatorics Problem Unfortunately, it looks like the constraints were insufficient to cut the convolution solutions off. So it's possible to solve this problem using a very fast convolution implementation, but the model approach is different from that.

One of the properties of Pascal's triangle states that (xy)=(x−1y)+(x−1y−1) . Using it, we can rewrite the formula for bi as follows:

bi=(∑j=1i(i−j+1k)⋅aj) bi=(∑j=1i(i−jk)⋅aj)+(∑j=1i(i−jk−1)⋅aj) Now, the first sum is almost the same as bi , but with i decreased by 1 . So, it is just bi−1 .

What does the second sum stand for? It's actually bi−1 , but calculated with k−1 instead of k .

Now, let ci,j be equal to the value of bi if we solve this problem with k=j . The formula transformations we used show us that ci,j=ci−1,j+ci−1,j−1 , so we can use dynamic programming to calculate ci,k in O(nk) . But we need some base values for our dynamic programming. It's quite easy to see that c0,j=0 ; but what about ci,0 ?

ci,0=∑j=1i(i−j+10)⋅aj And since (i−j+10)=1 , then

ci,0=∑j=1iaj So, in order to obtain ci,0 , we can just build prefix sums on the array a .

In fact, it's possible to show that transitioning from the j -th layer of dynamic programming to the (j+1) -th is also just applying prefix sums; then the solution would become just replacing a with prefix sums of a exactly k+1 times. This observation was not needed to get AC, but it allows us to write a much shorter code.

Solution complexity: O(nk) .

Solution (BledDest) 1832F - Zombies

Idea: BledDest

Tutorial 1832F - Zombies First of all, let's rephrase the problem a bit. For each entrance, we will have two time segments: the segment when it is guarded, and the segment when the corresponding generator works. The zombies will arrive through that entrance at every moment not belonging to these two segments; so, if we want to maximize the number of zombies, we want to minimize the length of the unions of these pairs of segments for the entrances; and since the lengths of the segments are fixed, minimizing the union means maximizing the intersection. So, we need to choose the time segments for the generators and assign entrances to generators so that the sum of intersections of segment pairs for each entrance is the maximum possible.

Okay, now we work with the sum of intersections. Suppose there are multiple generators (for which we have already chosen their time segments) and an entrance; is there an easy method how to choose the optimal generator for the entrance? In fact, there is. We need to look at the centers of all segments (both for the generators and the entrance), and choose the generator for which the distance from the center of the entrance-segment to the center of the segment from that generator is the minimum possible (this only works because all generators have the same working time). Mathematically, if the segments for the generators are denoted as [Lj,Rj) , and the segment borders for the entrance are [lj,rj) , we need to choose the generator that minimizes the value of |(Lj+Rj)−(lj+rj)| , since it also minimizes the distance between the centers of the segments.

Do you see where this is going? In fact, this observation allows us to use exchange argument DP. Suppose the segments for the generators are already chosen; we can sort the entrances according to the values (lj+rj) for each entrance, and then split the sorted order of entrances into several groups: all entrances in the first group are optimal to hook up to the first generator, all entrances in the second group are optimal to hook up to the second generator, and so on. But since the segments of the generators are not fixed, we will instead split the sorted order of the entrances into k groups, and for each group, optimally choose the segment for the respective generator. So, this leads us to the following dynamic programming: dpi,j — the maximum possible sum of intersections if we have split the first i entrances in sorted order into j groups. The transitions in this dynamic programming are quite simple: we can just iterate on the next group, transitioning from dpi,j to dpi′,j+1 .

Unfortunately, there are still two issues with this solution:

How to reduce the number of transitions from O(n3) to something like O(n2logn) or O(n2) ? How to choose the best generator placement for a group of entrances, preferably either in O(1) or O(logn) per one segment of entrances? Okay, the solution to the first issue is not that difficult. Intuitively, it looks like the running time of this dynamic programming can be improved with some of the well-known DP optimizations. The two that probably come to mind first are divide-and-conquer optimization and aliens trick. Both of them seem to work, but unfortunately, we can prove only one of them (the proof is in the paragraph below). You can choose any of these two optimizations.

Proof that D&C optimization works

We can prove this via quadrangle inequality: let cost[l..r] be the total intersection for the entrances from l to r if the generator for them is chosen optimally, and opt[l..r] be the optimal starting moment of the generator for entrances from l to r . We have to show that cost[a..c]+cost[b..d]≥cost[a..d]+cost[b..c] , where a≤b≤c≤d .

Suppose opt[a..d]=opt[b..c] . Then, if we take all entrances [c+1..d] from the first group and add them to the second group, then choosing opt[a..d] as the starting point for these two groups gives us the total intersection equal to exactly cost[a..d]+cost[b..c] . So, in this case, cost[a..c]+cost[b..d]≥cost[a..d]+cost[b..c] .

Now suppose opt[a..d]<opt[b..c] (the case opt[a..d]>opt[b..c] is similar). Let's again try to move all entrances [c+1..d] from the first group to the second group. If the resulting sum of intersections (without shifting the starting points for the generators) did not decrease, we have shown that cost[a..c]+cost[b..d]≥cost[a..d]+cost[b..c] . Otherwise, at least one entrance from [c+1..d] is closer to opt[a..d] than to opt[b..c] (in terms of distance between the segment centers). This means that since the centers of the segments in [b..c] are not greater than the center of the segments in [c+1..d] , then the segments from [b..c] are also closer to opt[a..d] than to opt[b..c] . So, the optimal starting moment for [b..c] can be shifted to opt[a..d] , and we arrive at the case we analyzed in the previous paragraph.

End of proof

The solution to the second issue is a bit more complex. First of all, notice that the only possible starting moments for generators we are interested in are of the form li and ri−m , so there are only 2n of them. Then let's try to understand how to evaluate the sum of intersections for the generator starting at some fixed moment and a segment of entrances. The model solution does some very scary stuff with logarithmic data structures, but the participants of the round showed us a much easier way: create a 2n×n matrix, where the number in the cell (i,j) is the intersection of the segment for the i -th starting moment of the generator, and the segment when the j -th entrance (in sorted order) is guarded; then, for a segment of entrances and a fixed starting moment of the generator, the total intersection can be calculated in O(1) using prefix sums on this matrix.

Unfortunately, trying each starting moment for every group of segments is still O(n3) . But it can be improved using something like Knuth optimization: let optl,r be the optimal starting point of the generator for the group of entrances [l..r] ; then optl,r is between optl,r−1 and optl+1,r , so calculating these optimal starting points in the style of Knuth optimization gives us O(n2) . However, there's one last nasty surprise waiting for us: if we are not careful about choosing optimal starting moments, it's possible that optl,l>optl+1,l+1 (for example, if the segment for the entrance l includes the segment for the generator l+1 ), which breaks the initialization of Knuth optimization. To resolve this issue, we can initialize the values of optl,l in a monotonic way, choosing optl+1,l+1 only from values not less than optl,l .

Implementing all of this results in a solution that works in O(n2logn) .

Solution (awoo)

Educational Codeforces Round 147 — Editorial

By BledDest, 2 hours ago, In English 1821A - Matching

Idea: BledDest

Tutorial 1821A - Matching In a positive integer, the first digit is from 1 to 9 , and every next digit can be any. This allows us to implement the following combinatorial approach:

calculate the number of different values for the first digit, which is 0 if the first character of s is 0, 1 if the first character of s is any other digit, or 9 if the first character of s is ?; calculate the number of different values for each of the other digits, which is 1 if the corresponding character of s is a digit, or 10 if it is ?; multiply all these values. Solution (BledDest) 1821B - Sort the Subarray

Idea: BledDest

Tutorial 1821B - Sort the Subarray Let's find the leftmost and the rightmost position in which the arrays a and a′ differ. Since only the elements in the chosen subsegment might change, the subarray we sorted should contain these two positions.

Let's start with the subarray from the leftmost "different" position to the rightmost one, and then expand it to get the longest subarray which meets the conditions. Suppose we want to expand it to the left. Let the current left border be L ; how to decide if we can make it L−1 or less? If a′L<aL−1 , then we cannot include L−1 in the subarray we sort, since otherwise the order of these two elements would have changed. Otherwise, aL−1 is not greater than any element in the subarray we have chosen, so we can include it and reduce L by 1 . We do this until it's impossible to reduce L further.

The same goes for the right border: we expand it to the right until we find an element which is less than the previous element, and we cannot include this element in the subarray.

Solution (BledDest) 1821C - Tear It Apart

Idea: BledDest

Tutorial 1821C - Tear It Apart The resulting string looks like a single letter repeated a number of times. That sounds too vague. Let's fix the exact letter c that will be left. Since the size of the alphabet is only 26 , we can afford that.

The letters now separate into letters c and other letters. And all other letters can be treated as indistinguishable from each other. Let's make letter c into a binary 1 and any other letter into a binary 0 . Our goal is to remove all zeros from the string with the given operations.

First, notice that it doesn't help you to removes ones. If some operation contains both ones and zeros, then taking ones out of it doesn't make any zeros in it adjacent. At the same time, these ones can only help you separate adjacent zeros later.

Thus, we have some blocks of zeros, separated by the blocks of ones. We want to remove only zeros. Notice how these blocks can be solved completely independently of each other. If you solve block 1 in cnt1 operations, block 2 in cnt2 operations, ..., block k in cntk operations, then you can solve the entire string in max(cnt1,cnt2,…,cntk) operations. Since the blocks are separated by the blocks of ones, you can combine the first operations for all blocks into one big operation and so on.

The only thing left is to calculate the number of operations for a single block. Let it have length l . Basically, in one operation, you can decrease its length to ⌊l2⌋ . You can see that the longer the block, the greater answer it has. So you can find the longest block first, then calculate the answer for it. You can either use this iterative formula or notice that it's a logarithm of l in base 2 and calculate that however you want.

To find the lengths of the blocks of zeros, you can use two pointers.

Overall complexity: O(|AL|⋅n) per testcase.

This problem can also be solved in O(nlogn) on an arbitrarily large alphabet. Basically, when you fix a letter, you can tell the lengths of the blocks of other letters by looking at the occurrences of the letter. For occurrences i1,i2,…,ik , the lengths of the blocks are i1−1,i2−i1,…,ik−ik−1,n−ik . So we can calculate the answer for a letter in O(number of its occurrences) . The total of that for all letters is O(n) .

Solution (awoo) 1821D - Black Cells

Idea: adedalic

Tutorial 1821D - Black Cells Let's look at what's happening in the task: in the end the pointer will move into some position p and some segments on the prefix [0,p] will be colored. Note that it's optimal to stop only inside some segment (or li≤p≤ri for some i ), and if we colored x segments (including the last segment [li,p] that may be colored partially) the answer will be equal to p+2⋅x .

Let's prove that it's not optimal to skip segments with length len=r[i]−l[i]+1≥2 . By contradiction, suppose the optimal answer a has a skipped segment [li,ri] . If we color that segment instead, we will make 2 more moves for pressing and releasing Shift, but we can make at least len right move less. So the new answer a′=a+2−len≤a — the contradiction.

Now we are ready to write a solution. Let's iterate over i — the last segment we will color (and therefore the segment where we stop). At first, let's imagine we colored the whole segment [li,ri] as well. Let s be the total length of all segments on prefix [0,ri] that are longer than 1 and c be the number of segments of length 1 on this prefix.

There are three cases:

if s+c<k , stopping inside the i -th segment is not enough; if s<k but s+c≥k , we will color all "long" segments plus several short ones. The current answer will be equal to ri+2((i−c)+(k−s)) , where ri is where we stop, (i−c) is the number of long segments and (k−s) is the number of short segments we need; if s≥k , then we don't need any short segments. More over, we can stop even before reaching ri . So, the current answer will be equal to ri−(s−k)+2(i−c) , where ri−(s−k) is the exact position to stop to get exactly k black cells and (i−c) is the number of long segments. Note that i is 1 -indexed in calculations above, and we can stop the first moment we met the situation s≥k . The answer is the minimum among the answers we've got in the process. Since it's easy to update values s and c when we move from i to i+1 , the total complexity is O(n) .

Solution (adedalic) 1821E - Rearrange Brackets

Idea: BledDest

Tutorial 1821E - Rearrange Brackets First, let's define the cost of an RBS a bit clearer. The absolute smallest cost of removing each pair of brackets is the number of bracket pairs it's inside of. That can actually be achieved — just remove the pairs right to left (according to the positions of the opening brackets in pairs). So you can instead say that the total cost is the sum of balance values after all closing brackets. Or before all opening brackets — these are actually the same.

From that, we can code a classic dp. Imagine we are not moving brackets, but instead doing that in two separate movements: put a bracket in some buffer and place it in the string. We'd love to use dp[pos][open][close][moves] — the smallest answer if we processed pos brackets, open opening brackets are in the buffer, close closing brackets in the buffer and moves are performed. Sadly, that doesn't really allow moving brackets to the left, since you would have to first place the bracket, then put in it the buffer. Does that actually break anything? Apparently, no. You can make these buffer states from −k to k , and think of negative values as taking a loan. These states are enough to determine the current balance of the string. Thus, enough to both update the states and check if the string stops being an RBS after placing a closing bracket.

Overall complexity: O(nk3) .

We can do it faster, but our proof isn't that convincing.

Start by showing that there exists an optimal answer such that each move leaves the sequence an RBS. Consider a sequence of moves that ends up being an RBS. First, you can basically rearrange the moves (maybe adjusting the exact positions is required). Second, there exists a move that, performed first, leaves an RBS. Make it and propagate the proof. You can show that such a move exists by studying some cases.

Then I found it more intuitive to switch to another representation — you can look at the forest induced by the bracket sequence. The roots of the trees in it are the topmost opening and closing brackets of the RBS. Their children are the inner topmost brackets for each of them, and so on. With that representation, the answer is actually the sum of depths of all vertices.

Now for the moves. Let's move an opening bracket to the right. We won't move it after its corresponding closing bracket to not break an RBS. How will it change the tree? It will turn some children of the corresponding vertex into the children of its parent. Thus, it will decrease their depths by one, and the depths of their descendants as well. How about to the left? That will turn some children of its parent into its own children, increasing their depths (and the depths of their descendants) by one. Similar analysis can be performed for the closing brackets.

The claim is that, in the optimal answer, you should only move opening brackets and only to the right. Then they decrease the answer independently of each other. It's pretty clear that the best position to move each bracket to is as much to the right as possible — place it next to its respective closing bracket. That will decrease the answer by the size of the subtree (excluding the vertex itself).

Finally, we want to choose k vertices that have the largest sum of their subtrees. That can be just done greedily — pick k largest ones.

You don't have to build the tree explicitly for that — the size of the subtree is half of the number of brackets between an opening bracket and a corresponding closing one. So, everything can be processed with a stack.

Overall complexity: O(n) or O(nlogn) .

Solution 1 (awoo) Solution 2 (awoo) 1821F - Timber

Idea: BledDest

Tutorial 1821F - Timber People say that this problem can be bashed with generating functions. Sadly, I know nothing about them, so I'll explain the more adhoc solution I managed to come up.

Let's investigate the general form of some placement of trees. Imagine we fixed some position x1,x2,…,xm for them. How to check if it's possible to cut them all down? Well, it's a simple greedy problem. Process trees left to right. If a tree can fall down to the left, make it fall to the left. Otherwise, if it can fall down to the right, make it fall to the right. Otherwise, no answer.

Try a harder problem. Consider the spots the fallen trees take. How many constructions can these spots be induced by after the greedy algorithm is performed? First, these taken spots are actually segments of length k+1 . Each tree can fall either from the left or the right side of the segment. To determine the answer, one has to look at the number of free spots to the left of each fallen tree. If there're at least k free spots, then the tree can only fall from the left side of that segment. Otherwise, it can fall both ways. Thus, for x trees that have less than k free spots to the left of them, the answer is 2x .

We can't really iterate over the constructions, so let's try yet another related problem. Would be cool if we could fix the exact number of trees that have more than k free cells to the left of them. I don't really know how to do that. But I know how to fix at least the amount. Let that be at least x trees. For x trees, make their segments of length 2k+1 — k+1 for the tree itself and k extra cells to the left of it. For the rest m−x trees, make their segments of length k+1 . Then we have to place them on the given n spots. The path is as follows. Rearrange the long and the short segments among each other — (mx) . Then use stars and bars to choose the amount of space between the segments — (n−x⋅(2k+1)−(m−x)⋅(k+1)+mm) . Finally, multiply by 2m−x to fix the side for the short segments.

Initially, we thought that we could just use PIE to calculate the exact answer from that — ∑x=0mf(x)⋅(−1)x . And the results of this formula coincided with the naive solution, so we thought that everything should be fine. But unfortunately, even though the formula is right, using PIE in the described way is incorrect. Let us show you the correct way.

Let F(x) be the number of ways to choose the segments for the trees in such a way that x fixed segments are "long" (i. e. at least of length 2k+1 ). To calculate F(x) , we use the familiar stars and bars concept: (n−x⋅(2k+1)−(m−x)⋅(k+1)+mm) (we already had this formula earlier).

Now, let G(x) be the number of ways to choose the segments for the trees in such a way that x fixed segments are "long", and all other segments are "short". The formula for G(x) is a straightforward application of PIE: G(x)=∑y=0m−x(−1)y(m−xy)F(x+y) , where we iterate on y — the number of segments that should be short, but are definitely long.

The answer to the problem can be calculated as ∑x=0m2m−x(mx)G(x) — we choose which x segments are "long", and all others should be "short". Expanding G(x) gives us the following formula:

∑x=0m∑y=0m−x(−1)y2m−x(mx)(m−xy)F(x+y) which, after expanding binomial coefficients into factorials and getting rid of (m−x)! , becomes

∑x=0m∑y=0m−x(−1)y2m−xm!x!y!(m−x−y)!F(x+y) We introduce the substitution variable z=x+y , and the formula becomes

∑z=0m∑y=0z(−1)y2m−z+ym!(z−y)!y!(m−z)!F(z) By multiplying both the numerator and the denominator by z! , we then get

∑z=0m∑y=0z(−1)y2m−z+y(mz)(zy)F(z) ∑z=0m(mz)2m−zF(z)∑y=0z(−2)y(zy)F(z) And ∑y=0z(−2)y(zy) is just (1−2)z . Thus, the answer is equal to ∑z=0m(−1)z(mz)2m−zF(z) .

Overall complexity: O(n) .

Solution (awoo)

Educational Codeforces Round 146 Editorial

By awoo, history, 11 minutes ago, translation, In English 1814A - Coins

Idea: BledDest

Tutorial 1814A - Coins Note that 2 coins with denomination k can be replaced with k coins with denomination 2 . So, if the answer exists, then there is also such a set of coins, where there is no more than one coin with denomination k . Therefore, it is enough to iterate through the number of coins with denomination k (from 0 to 1 ) and check that the remaining number is non-negative and even (i. e. it can be represented as some number of coins with denomination 2 ).

Solution (awoo) 1814B - Long Legs

Idea: BledDest

Tutorial 1814B - Long Legs Let's fix the number of leg length increases we do. Let the final length be k . Notice that for all i from 1 to k there is some time when the length is exactly i . Thus, we can perform jumps of form (x,y)→(x+i,y) or (x,y)→(x,y+i) .

What's the jumping strategy, then? Obviously, we can solve the problem independently for n and m . Consider n . We would love to just make jumps of length k as that's the maximum possible length. Unfortunately, that only works when n is divisible by k . Otherwise, we are left with some remainder which is smaller than k . But we have already figured out how to jump to any value from 1 to k . So, that only adds another jump. You can say that the total number of jumps is ⌈nk⌉ .

Same for m . Finally, for a fixed k , the answer is ⌈nk⌉+⌈mk⌉+(k−1) .

The constraints tell us that we are not allowed to iterate over all k from 1 to max(n,m) . It feels like huge k will never be optimal, but let's try to base our intuition on something.

Try to limit the options by studying the formula. Let's simplify. Assume n=m and also get rid of the ceil. Not like that changes the formula a lot. Now it becomes 2nk+(k−1) . We can see that when we increase k , 2nk becomes smaller and (k−1) becomes larger. However, we care more about how fast they become smaller and larger. You can just guess or write down the derivative explicitly and figure out that the first term shrinks faster than the second term grows until around n−−√⋅c for some constant c (apparently, c=2–√ ). Thus, their sum decreases until then, then increases.

Thus, you can search for the best k around n−−√ or m−−√ or max(n,m)−−−−−−−−−√ . It doesn't really matter, since, for implementation, you can basically try all k until around 105 , which is safely enough.

Solution (awoo) 1814C - Search in Parallel

Idea: BledDest

Tutorial 1814C - Search in Parallel If the ball of color x is present in the first list on position i , then it takes i⋅t1 seconds to find it. The same for the second list: if color x is on position j , it takes j⋅t2 seconds to find it. So, for each position, we have a coefficient which will be multiplied by the number of times it is requested, and the total search time is the sum of these products for all positions.

There is a classical problem of the form "you are given two arrays ai and bi , both of length m , consisting of non-negative integers; permute the elements of a in such a way that ∑i=1mai⋅bi is the minimum possible". To solve this problem, you have to pair the maximum element of a with the minimum element of b , the second maximum of a with the second minimum element of b , and so on.

We can reduce our problem to this one. For each of 2n positions in the lists, there is a coefficient; you have to assign the boxes from 1 to n to the positions so that the sum of ri multiplied by the coefficients for the positions is the minimum possible. This looks similar, but there are 2n positions and only n boxes.

To resolve this issue, we can try a lot of different approaches. I believe the easiest one is the following: initially, both lists are empty, and when want to add an element to one of these two lists, we choose the list such that the coefficient for the new position (which is si⋅(1+cnti) , where cnti is the number of elements we already added to the i -th list) is smaller. If for both lists, adding a new element has the same coefficient — it doesn't matter which one we choose.

This greedy approach works because every time we add an element to the list, next time we'll add another one into the same list, the coefficient for that element will be greater.

So, the problem can be solved in O(nlogn) : first, we sort the boxes by the number of times they are requested (in non-ascending order), and then we put them into the two lists greedily, every time choosing the list such that the coefficient for the next element is smaller.

Solution (BledDest) 1814D - Balancing Weapons

Idea: BledDest

Tutorial 1814D - Balancing Weapons Note that the answer n is always possible: for example, we can set di=∏fjfi , then p1=⋯=pn=∏fj and maxpi−minpi=0 .

If the answer is less than n then there is at least one gun id we won't change. It means that all other guns' firepower should be "around" pid , i. e. |pi−pid|≤k . So we can look at segment [pid−k,pid+k] and, for each gun i , find what values d′i we should set to get into this segment. After that we can rephrase our task into the next one: we should choose segment [l,l+k]⊂[pid−k,pid+k] such that each gun occurs in [l,l+k] at least once and the number of corresponding d′i that are equal to di is maximum possible.

It can be solved with two pointers technique. Note that there are at most three interesting values d′i we should consider: v1=⌊pidfi⌋ , v2=v1+1 and v3=di . For each unique value vj such that vj⋅fi∈[pid−k,pid+k] we can add an event (i,cj) in position vjfi , where cj is 1 if vj=di or 0 otherwise.

Now, with two pointers technique, we can iterate over all subsegments of length k+1 of segment [pid−k,pid+k] . To get the desired answer we should maintain the number of unique i from events that are present in the subsegment and the sum s of cj from that events. Since there is only one cj=1 for each gun i then the sum s of cj we have is equal exactly to the number of guns we shouldn't change. Then we take the maximum mx over sums s of all subsegments where all guns occur, and the answer for a fixed pid is n−mx .

Let's iterate over all "fixed" id and take the minimum from all n−mx : that will be the answer for the initial task.

Checking answer for a fixed id involves creating 3n events and two pointers over segment [pid−k,pid+k] , so it takes O(n+k) time and O(n+k) space. So, the total complexity is O(n2+nk) time and O(n+k) space.

Solution (adedalic) 1814E - Chain Chips

Idea: BledDest

Tutorial 1814E - Chain Chips Let's try to analyze how many times we traverse each edge, in the style of "Contribution to the Sum" technique. For each edge, the number of times it is traversed must be even, since for every chip that goes from the part of the graph [1..i] to the part [i+1..n] , there should be a chip that goes in the opposite direction (the number of chips on vertices [1..n] should be unchanged).

For each vertex, at least one incident edge should be traversed at least twice — otherwise, the chip from this vertex cannot be moved to any other vertex.

We would also like to traverse the edges as rarely as possible. Is it possible to find an answer where, if we traverse any edge, we traverse it only twice? It turns out it is possible. Let's "split" the graph into several parts by removing the edges we don't traverse. If we don't break the constraint that each vertex has at least one incident edge which is traversed by some chip, then each part of the graph will contain at least two vertices. And in each part, we can make sure that each edge is traversed only twice as follows: let the part represent the segment [l,r] of vertices; if we move the chip r to the vertex l , the chip l to the vertex l+1 , the chip l+1 to the vertex l+2 , ..., the chip r−1 to the vertex r , then every edge in that part will be traversed exactly twice.

So, we have shown that if we pick a subset of edges which we traverse that meets the constraint on each vertex having an incident traversed edge, then it is enough to traverse each chosen edge only twice. Now the problem becomes the following: choose a subset of edges in such a way that every vertex has at least one incident chosen edge, minimize the total weight of this subset, and print the integer which is double that total weight.

Since the structure of the graph is specific, we can run dynamic programming of the form dpi,f — the minimum total weight of the subset, if we considered the first i edges, and f is 0 if we haven't taken the last edge, or 1 if we have. Obviously, this dynamic programming works in O(n) , which is too slow because we have to process queries. We will employ a classical technique of storing the dynamic programming in segment tree: build a segment tree on n−1 leaves; in every vertex of the segment tree, we store a 2×2 matrix d ; if the segment represented by the node of the segment tree is [l..r] , then the value of d[f1][f2] is the minimum total weight of the subset of edges between l and r such that among every pair of adjacent edges, at least one is chosen; f1 and f2 represent the status of the first/last edge on the segment, respectively. And when some element changes, we need to recalculate only O(logn) nodes of the segment tree, so this solution works in O(nlogn+qlogn) , albeit with a very big constant factor.

Implementation note: don't use dynamic-size arrays (like std::vector in C++) to store the values in the matrices, it might slow your solution very seriously. Instead, use static-size arrays.

Solution (BledDest) 1814F - Communication Towers

Idea: Neon

Tutorial 1814F - Communication Towers Let's consider the sweep line approach by the value of the variable x ; the vertex i is active from the moment li to the moment ri . And we have to find vertices that are reachable in the graph of active vertices from the vertex 1 . So, we rephrased the problem as follows: there are vertices that are active at some moments, and we want to get some information about connectivity during each moment of time. This is a standard offline dynamic connectivity problem which can be solved with a divide-and-conquer approach described here.

Now we are able to find the connectivity component of the 1 -th vertex for each value of x . It remains to understand how to combine answers for all values of x fast enough. Let's try to visualize the components as vertices of a directed graph. We assign a vertex to each component, and when two components merge, we add two directed edges from the new vertex to the vertices corresponding to the components; and now we can use the reachability information in this graph. Each vertex of the original graph corresponds to one of the sinks in this graph; and sinks that correspond to the vertices of some component are reachable from the vertex corresponding to that component. To restore all the vertex indices later, we will mark all components containing the vertex 1 while we run our dynamic connectivity approach. Then the vertex v (of the original graph) is included in the answer if the vertex representing the component containing only the vertex v is reachable from any of the marked vertices. Now, all you need to do is run DFS or BFS from all the marked vertices in the component graph.

Solution (BledDest)

Educational Codeforces Round 145 Editorial

By awoo, history, 7 hours ago, translation, In English 1809A - Garland

Idea: BledDest

Tutorial 1809A - Garland Note that there are only a few configuration classes: 1111, 1112, 1122, 1123 and 1234. Let's discuss each of them.

If all 4 bulbs are of the same color, then it is impossible to turn all the bulbs on, because after you switch one light bulb, it is impossible to turn the others on.

If there is a color with 3 bulbs, then it is impossible to turn all the bulbs on in 4 operations, which means there is a bulb that turns on, turns off and then turns on again, i.e. the answer is at least 6 operations. And there is a sequence of exactly 6 operations (such an example was shown in the problem notes).

For configurations like 1122 and 1123, it is enough to turn on the 1 color bulbs not in a row (i.e. in order [1,2,1,2] for the first case and [1,2,1,3] for the second one). So the answer for such configurations is 4 .

If all the bulbs are of different colors, then nothing prevents you from turning them all on in 4 operations.

Solution (Neon) 1809B - Points on Plane

Idea: BledDest

Tutorial 1809B - Points on Plane Suppose, the answer is k . What's the maximum number of chips we can place? Firstly, the allowed points (x,y) to place chips are such that |x|+|y|≤k . We can group them by x -coordinate: for x=k there is only one y=0 , for x=k−1 possible y are −1,0,1 ; for x=k−2 possible y are in segment [−2,…,2] and so on. For x=0 possible y are in [−k,…,k] . The negative x -s are the same.

Let's calculate the maximum number of chips we can place at each "row": for x=k it's 1 ; for x=k−1 there are three y -s, but since we can't place chips at the neighboring y -s, we can place at most 2 chips; for x=k−2 we have 5 places, but can place only 3 chips; for x=0 we have 2k+1 places, but can occupy only k+1 points.

In total, for x∈[0,…,k] we can place at most 1+2+⋯+(k+1)=(k+1)(k+2)2 chips. Analogically, for x∈[−k,…,−1] we can place at most 1+2+⋯+k=k(k+1)2 chips.

In total, we can place at most (k+1)(k+2)2+k(k+1)2=(k+1)2 chips with cost at most k . Note that (k+1)2 can actually be reached since the distance between chips on the different rows is greater than 1 .

So, to solve our task, it's enough to find minimum k such that (k+1)2≥n that can be done with Binary Search.

Or we can calculate k=⌈n−−√⌉−1 . Note that n−−√ can lose precision, since n is cast to double before taking the square root (for example, 975461057789971042 transforms to 9.754610577899711⋅1017=975461057789971100 when converted to double). So you should either cast long long to long double (that consists of 80 bits in some C++ compilers) or check value k+1 as a possible answer.

Solution 1 (adedalic) Solution 2 (adedalic) 1809C - Sum on Subarrays

Idea: BledDest

Tutorial 1809C - Sum on Subarrays There are many ways to solve this problem. I will describe the following recursive solution:

if k<n , let's compose an array where every segment ending with the k -th element is positive, and every other segment is negative. This array can be [−1,−1,−1,…,200,−400,−1,−1,−1] , where 200 is the k -th element of the array (note that when k=0 , 200 doesn't belong to the array, so it consists of only negative numbers). but if k≥n , solve the same problem with n−1 and k−n recursively, get an array of length n−1 with k−n positive subarrays, and append 1000 to it to make all n segments ending with the last element positive. Solution (BledDest) 1809D - Binary String Sorting

Idea: BledDest

Tutorial 1809D - Binary String Sorting Note that the price of operations is much greater than the difference between them. Therefore, first of all, we have to minimize the number of operations, and then maximize the number of operations of the first type.

Swapping two elements if at least one of them will be deleted later is not optimal. Therefore, first let's delete some elements of the string, and then sort the remaining elements using swaps. The number of swaps for sorting is equal to the number of inversions (i. e. the number of pairs such that sj>si and j<i ). From here we can notice that if the number of inversions is greater than 1 , then there is an element that produces at least 2 inversions. So it is more profitable for us to remove it, to minimize the number of operations.

From the above we get that the number of operations of the first type is at most 1 . If all operations are only of the second type, then we need to find a subsequence of the maximum length of the form 0000111111. To do this, we can iterate over the number of zeros that we include in the final string, and then add the number of ones from the remaining suffix of the string (that goes after the fixed number of zeros). If there is an operation of the first type, then it is enough to iterate over the pair that creates the inversion, to the left of it take all zeros, and to the right of it take all ones (you can notice that in fact it is enough to iterate over only a pair of neighboring elements of the string).

Solution (Neon) 1809E - Two Tanks

Idea: BledDest

Tutorial 1809E - Two Tanks Consider a naive solution. Iterate over all pairs (c,d) and apply all operations. The complexity is O(a⋅b⋅n) . The constraints obviously imply that it's too much. What can we cut from it? Well, surely O(n) will still remain there. Both of a and b also should. So we can probably only hope to turn this O(ab) into O(a+b) . Let's try that.

Notice that no matter what operations are applied, c+d never changes. You can also peek at the examples and see that the patterns are suspiciously diagonal-shaped in the matrix. Let's try to solve the problem by fixing c+d and calculating the answer for all values of c .

I will call the fixed c+d variable cd . Consider case where cd≤a and cd≤b . Here, all cd can fit into both a and b , so we can avoid caring about one restriction on the operations. We'll think what to do with large volumes later.

If there are no operations, the answer for each initial c is c for all c from 0 to cd . Now consider an operation x for some x>0 . For c=0 , nothing changes. Actually, for all c≤x the result of the operation is the same as for c=0 . Hmm, but if the result is the same, it will remain the same until the end. Same from the other side. The answers for x<0 and d≤−x also get merged together. To me it kind of looks like a primitive form of DSU on these volume states: you merge some prefix of the answers together and merge some suffix of the answers together.

If the state was merged to either c=0 or d=0 , then it's easy to calculate the actual answer for that state. What happens to the remaining states? Well, since they weren't merged anywhere, the operation for them was applied fully: if x was requested, all x was poured.

How to deal with multiple operations then? I propose the following idea. When applying an operation, we only want to know which of the previously non-merged states become merged. Basically, we can squish all previous operations into one: just sum up the signed amounts of water. Since they all were applied fully to the non-merged states, it's completely valid. After the squish, check for the new merges.

You can actually study the structure of the answers and see that they go like that: [l,l,…,l,l+1,l+2,…,r−1,r,…,r,r] for some values of l and r such that l≤r . It isn't that important, but it makes the code easier. You can basically calculate the length of the merged prefix, the length of the merged suffix, then calculate the answer at the end of the prefix in O(n) and restore all answers from it.

We neglected larger values of cd earlier, time to return to them. Another kind of limit to each operation is added: when x extra water doesn't fit in another tank. Well, it doesn't change that much. It only makes more prefix/suffix merges. To come up with the exact formulas, I followed these points. Something merges on an operation x , when any of these holds:

c<x (not enough water in the first tank); b−d<x (not enough space in the second tank); d<−x (not enough water in the second tank); a−c<−x (not enough space in the first tank). Replace all d with cd−c , and you get the constraints for prefix and suffix merges.

Overall complexity: O(n⋅(a+b)) .

Solution (awoo) 1809F - Traveling in Berland

Idea: BledDest

Tutorial (Neon) 1809F - Traveling in Berland The problem has a rather obvious naive solution in O(n) for each starting city, but it's too slow. So we have to speed up this solution somehow. Binary lifting is one of the options, but here we have a problem that it is difficult to connect two consecutive groups of steps, because after the first group there is a certain amount of fuel left. Therefore, one of the solutions is to switch to such steps that 0 liters of fuel remains after it.

Let's consider one of such "greedy" steps. Suppose we are in the city i with 0 fuel, then the following situations are possible:

bi=2 , let's buy exactly ai liters of fuel to reach the next city, then the step length is 1 and the cost is 2ai ; bi=1 and cnt=0 (where cnt is the maximum number such that bi+1=2 , bi+2=2 , ..., bi+cnt=2 , i.e. the number of consecutive cities with the cost 2 ), let's buy exactly ai liters of fuel to reach the next city, then the step length is 1 and the cost is ai ; bi=1 and cnt>0 , let's find a minimum j such that sum=ai+ai+1+⋯+aj≥k and j≤i+cnt (i.e. such j that you can reach it by spending all k of liters): sum≤k , let's buy exactly sum liters with the cost 1 in the city i , then the step length is j−i+1 and the cost is sum ; sum>k , let's buy k liters with the cost 1 in the city i , and the remainder of sum−k liters with the cost 2 in the city j , then the step length is j−i+1 and the cost is k+2(sum−k) . Now using these types of steps, we maintain an important invariant — after each step, the amount of fuel is 0 . So we can easily calculate the total distance and cost for several consecutive steps. Which leads us to a solution using binary lifting: for each city i calculate the length and cost of the path with 2pw (for all pw up to logn ) greedy steps. And then, using this data, we can calculate the answer for each starting city in O(logn) .

Solution (awoo) 1809G - Prediction

Idea: BledDest

Tutorial 1809G - Prediction We need some sort of better criterion other than "all matches can be predicted" first. Suppose the ratings of the participants are r1,r2,…,rn in the order of their indices. Then, if all games are predictable, the i -th game should be won by the participant with the rating equal to maxj=1i+1rj ; and in the (i+1) -th game, they will play against the participant with rating ri+2 . So, in order for each game to be predictable, the maximum on each prefix should be different from the next element by at least k+1 . This is the criterion we will use.

So, we will try to count the number of orderings meeting this condition. One very important observation we need to make is that, if we remove several participants with the lowest ratings from the ordering, that ordering still satisfies the condition (for each element, either the prefix before it is removed completely, or the maximum on it is unchanged). So, this allows us to construct the correct ordering by placing the sportsmen from the maximum rating to the minimum rating, and making sure that on every step, the order stays correct.

Okay. Let's reverse the ratings array, and try to write the following dynamic programming: dpi is the number of correct orderings of the first i sportsmen (the i highest-rated sportsmen, since we reversed the ratings array). Let's try to place the next sportsman. We run into the following issue: for some orderings of the first i sportsmen, it is possible to place the next one anywhere (these orderings are where the first sportsman in the ordering doesn't conflict with the sportsman we are trying to place); but for other orderings, some positions might be forbidden. And to keep track of which positions are forbidden, and for which sportsmen, we probably need some additional states for the dynamic programming, which we don't really want to since O(n) states is probably the most we can allow.

Okay, so let's avoid this issue entirely. We don't like the orderings where the next sportsman can't be placed anywhere, so let's find a way to "ignore" them:

discard the previous definition of dpi . Now, let dpi is the number of correct orderings of the i highest-rated sportsmen where the first element in the ordering doesn't conflict with any of the elements we haven't placed yet; when we place the next sportsman, in case it becomes the first element and conflicts with some of the elements we haven't placed yet, we place those conflicting elements as well. So, this leads to the following transitions in the dynamic programming:

if we place the (i+1) -th sportsman on any position other than the first one, there are i ways to do it, and we transition from dpi to dpi+1 ; otherwise, if we place the (i+1) -th sportsman on the first position, let f(i+1) be the last sportsman "conflicting" with the sportsman i+1 . Let's try placing all sportsmen from i+2 to f(i+1) before placing the sportsman i+1 . They cannot be placed on the first position (otherwise they will conflict either with each other or with the sportsman i+1 ), so the first one can be placed in i ways, the second one — in (i+1) ways, and so on; this product can be easily calculated in O(1) by preparing factorials and inverse factorials. So, then we transition from dpi to dpf(i+1) . There is a special case in our dynamic programming. It should start with dp1=1 , but what if the 1 -st sportsman conflicts with someone? Then the ordering of the first i=1 sportsmen is incorrect. In this case, the answer is 0 since the 1 -st and the 2 -nd sportsmen are conflicting.

Overall complexity of this solution is O(nlogMOD) or O(n+logMOD) depending on your implementation.

Solution (BledDest)

Educational Codeforces Round 144 Editorial

By awoo, history, 45 hours ago, translation, In English1796A - Typical Interview Problem

Idea: BledDest

Tutorial1796A - Typical Interview ProblemIt's easy to see that the FB-string repeats every 8 characters: after processing every 15 numbers, we will get the same remainders modulo 3 and 5 as 15 numbers ago, and when we process 15 consecutive numbers, we get 8 characters. So, fi+8=fi.

This means that if we want to find a substring no longer than 10 characters in the FB-string, we don't need to consider more than 17 first characters of the FB-string: the substring of length 10 starting with the 8-th character ends with the 17-th character, and we don't need to consider substrings starting on positions greater than 8.

So, the solution is to generate at least 17 first characters of the FB-string, and then check if the substring occurs in the generated string using a standard function like find.

Solution (BledDest)1796B - Asterisk-Minor Template

Idea: BledDest

Tutorial1796B - Asterisk-Minor TemplateWhat's the reason behind authors specifically asking for templates that have less or equal asterisks than letters? Well, without that the problem would be kind of trivial. A template "*" is matched by every string, so it would always work.

Hmm, let's try to make something similar to that template then. We basically have to find some part of that occurs in both strings that we can use letters on to get some freedom to use asterisks.

There are some easy cases. If the first letters of both strings are the same, then the template can be that letter followed by an asterisk. There's a symmetrical case for the last letter.

By studying the examples, you can also notice the final case: a common substring of both strings of length at least two surrounded by two asterisks. Moreover, since we only use two asterisks, we can find a substring of length exactly two (which always exists if a longer common substring exists).

Turns out, that's it. If a template exists, one of these three kinds also exists.

This is not that hard to show. If the first two kinds don't work, then you have to use asterisks on both sides of the template. In order for the template with asterisks on both sides to work, there have to be adjacent letters in it at least once (otherwise, it's like "*a*a*a*", and there are more asterisks than letters). And since at least one such substring exists, we can just remove everything other than this substring and the asterisks on the sides.

Overall complexity: O(|a| ⋅ |b|) per testcase.

Solution (awoo)1796C - Maximum Set

Idea: BledDest

Tutorial1796C - Maximum Set

Every beautiful set can be represented as a sequence of its elements in sorted order. Let these elements for some set be a1,a2,…,am; also, let di=ai+1ai. When the set is beautiful, every di is an integer greater than 1.

It's easy to see that if a1 and am belong to [l,r], the whole set belongs to [l,r]. Since am=a1⋅∏i=1m−1di, in order to maximize m, we need to choose a1 and di as small as possible. So, why don't we choose a1=l and every di=2? This will allow us to calculate the maximum possible size of a beautiful set (let m be this maximum possible size).

Okay, what about counting those sets? The claims a1=l and that every di=2 are no longer true by default. However, there are some constraints on di.

Firstly, every di≤3. If we had some value of di≥4, we could replace it with two values of di=2, and the size of the set would increase.

Secondly, there is at most one di=3. If there are two values di=3, we could replace them with three di=2, and the size of the set would increase as well.

So, the sequence di contains at most one value 3, and the rest of the values are 2.

We will divide the sets we want to count into two categories: the ones with all di=2, and the ones with one value di=3.

To count the sets in the first category, we simply need to count the number of different minimum values in those sets. Those minimum values have to be such that multiplying them by 2m−1 wouldn't make them greater than r, so these are all integers from the segment [l,⌊r2m−1⌋]. For every such integer, there exists exactly one set of the first category.

To count the sets in the second category, we do a similar thing. The minimum value in the set should be from the segment [l,⌊r2m−2⋅3⌋; but for every integer from this segment, there are m−1 different sets of the second category since there are m−1 ways to choose which di is equal to 3.

Solution (BledDest)1796D - Maximum Subarray

Idea: BledDest

Tutorial1796D - Maximum SubarrayThere are greedy and dynamic programming solutions. We will describe dynamic programming solution.

The main task is to choose some segment that is the answer to the problem, while choosing k positions to increase by x. To do this, we can use dynamic programming dpi,j,t, where i is the number of positions that have already been considered (from 0 to n), j is the number of elements that have already been increased by x (from 0 to k), t is the flag showing the current state (whether we are before the chosen segment, inside the segment, or after the segment). Transitions in such dynamic programming are quite simple: we have a choice either to increase j by 1, then the value of the i-th element is ai+x, or not to increase, then the value of the i-th element is ai−x; we can also change the state of the flag (note that you can only switch from the current state to the subsequent ones, i.e., for example, you cannot switch from the state "the segment has already ended" to the state "inside the segment"). If the current state of the flag is "inside the segment", then ai+x or ai−x (depending on the selected transition) should be added to the dynamic programming value itself.

So, we got a solution in O(nk).

Solution (Neon)1796E - Colored Subgraphs

Idea: BledDest

Tutorial1796E - Colored SubgraphsLet's start by choosing a vertex r naively. Iterate over all vertices and try each of them.

Root the tree by r and observe what the conditions become. dv for each v is just the depth of each vertex. Well, then the only case, when the connected subgraph of vertices of the same color has all values of d distinct, is when they form a vertical path in the tree.

So the problem becomes the following. Split the tree into some vertical paths in such a way that the shortest path is as long as possible.

Let's try greedy, I guess. Start the paths from the leaves and propagate them up. Consider some vertex v with at least two children. All children have some paths leading up to them. We'd love to continue them all with v, but we can't do that. We can only continue one path and cut the rest. Pretty easy to see that the path to continue is the shortest path available. It's at least as optimal as any other path.

Do that from the lowest vertices up, and you got yourself a working greedy. Also don't forget to stop all paths in root, since you can't continue any of them further up.

Let's make this greedy more formal. Every time we update the answer is with a path that is:

the shortest in every vertex lower than the current one;not the shortest in the current one.So we want to propagate the shortest child up and update the answer with the remaining children. Updating the answer means just taking the minimum of values. Thus, we can actually ignore all children except the second shortest in each vertex. Just don't forget to treat the root properly.

Now we can actually solve the problem in O(n) for a fixed r. You can just find two minimums in each vertex.

Well, now that we can solve the problem for a single root, let's try rerooting to solve for all of them. There are solutions in O(n) but I found the solution in O(nlogn) the neatest. The constraints are low enough to allow it.

For each vertex, maintain a multiset of lengths of vertical paths from its children. I chose to store nothing in the leaves — that only makes the implementation cleaner. In order to update the vertex from its child, you can take the minimum element in the child's set and add 1 to it. If it's empty (the child is a leaf), return 1.

Additionally, store a multiset of the second minimums of all vertices that have at least two children.

In order to update the answer with the current root, find the minimum of that multiset and the shortest path from the root.

To achieve O(n), you will probably have to either store prefix and suffix second minimums over children of each vertex or store three shortest paths in it. It is kind of messy but it should still perform better.

Overall complexity: O(nlogn) or O(n) per testcase.

Solution (awoo)1796F - Strange Triples

Idea: Neon and adedalic

Tutorial1796F - Strange TriplesLet |n| be the length of number n. Thenannb=ab⇔(a⋅10|n|+n)b=a(n⋅10|b|+b)Let g=gcd(a,b) then a=a′g and b=b′g and we get(a⋅10|n|+n)b=a(n⋅10|b|+b)⇔g2a′b′⋅10|n|+gnb′=ga′n⋅10|b|+g2a′b′⇔⇔ga′b′⋅10|n|+nb′=a′n⋅10|b|+ga′b′Note that the right part is divisible by a′, so the left part should as well. Then we can see that n should be divisible by a′, since gcd(b′,a′)=1. Let's say n=n′a′ then divide both sides by a′. We getga′b′⋅10|n|+n′a′b′=a′n′a′⋅10|b|+ga′b′⇔gb′⋅10|n|+n′b′=a′n′⋅10|b|+gb′Let's rearrange it and get the following:n′b′=a′n′⋅10|b|−b(10|n|−1)b′=a′⋅10|b|−b(10|n|−1)n′Since b′ is integer, then b(10|n|−1)n′ should be integer. In other words, we can define n′=k1k2 such that b is divisible by k1 and (10|n|−1) is divisible by k2 and k1 is minimum possible.

Since (10|n|−1) has a very special structure, let's iterate over all lengths |n| (1≤|n|≤9) and all divisors k2 of (10|n|−1) for a fixed |n|. Let's say d=bk1 and r=(10|n|−1)k2. Thenb′=a′⋅10|b|−d⋅rFor a fixed |n| and k2 we know r, but don't know d. So, let's just iterate over all possible d (1≤d≤105) and let's also iterate over all |b|, since |b| is small (|d|≤|b|≤5, since d is a divisor of b). Next step is following: let's look at previous equation, but modulo 10|b|:b′≡−d⋅r(mod10|b|)Since b′≤b<10|b| then there is a unique solution to the previous module equation, or:b′=10|b|−(d⋅r)mod10|b|Now we know exact value b′ and |b|, so now the time to guess g (let's recall that b=b′g). Since we fixed |b|, then 10|b|−1≤b′g<10|b|, or⌈10|b|−1b′⌉≤g≤⌊10|b|−1b′⌋Let's name the left border as lb and right border rb, so inequality earlier is lb≤g≤rb.

But we can make constrains even tighter: note that b=b′g, but lately, we said that d=bk1 or b=dk1. So, b′g=dk1, or g should be divisible by dd=dgcd(d,b′).

In total, we can iterate g in range [⌈lbdd⌉⋅dd,…,⌊rbdd⌋⋅dd] with step dd, since we are interested only in g divisible by dd.

Now we have enough variables to construct a triple: we know b′ and g, so b=b′g. If b is already big (b≥B), we can skip that candidate. Also, we can calculate k1=bd and check that pair (k1,k2) is valid, i. e. k1 is really minimum possible. We can understand it by checking that gcd(k1,r)=1 (otherwise, we can reduce k1 by gcd(k1,r)).

Value a′ can be calculated from one of the formulas above as a′=d⋅r+b′10|b|. After that, we calculate a=a′g and check that a is not too big. Value n can be calculated as n=k1k2a′=bdk2a′.

At last, we should check that the given triple satisfy all remaining assumptions we made: n is not too big, gcd(a′,b′) is really 1 and length of calculated n is exactly |n| we fixed. If it's all fine, then we found a correct triple. It looks like, thanks to all previous checks, the triple we found is unique, but, just for safety, let's push them all in one set to get rid of copies.

Calculating complexity is not trivial, but let's note something: the total number of divisors k2 of 10|n|−1 for all |n| is around 180. For a fixed pair (|n|,k2), we iterate over all d from 1 to B and for each d we iterate |b| from |d| to 5, but it's easy to prove that the total number of pairs (d,|b|) is at most 1.3⋅B. Now the last cycle: iteration of g with step dd where dd=dgcd(d,b′). If we assume that gcd(d,b′) is quite small then dd is proportional to d, and pairs (d,g) are something like harmonic series with O(BlogB) complexity. In total, the complexity is around O(200BlogB).

Solution (Neon)

Educational Codeforces Round 143 Editorial

By awoo, history, 27 minutes ago, In English

1795A - Two Towers

Idea: BledDest

Tutorial1795A - Two TowersNote that it does not make sense to move several blocks first from the left tower to the right, and then from the right to the left, since this is similar to canceling the last actions.

Using the fact described above and small restrictions on the input data, one of the possible solutions is the following: choose which tower will be the one where we take blocks from (try both options), iterate over the number of operations, and then check that both towers are beautiful after that number of operations.

There is a faster solution: move all the blocks to the left tower, and then check that there is no more than one pair of adjacent blocks of the same color. If there are no such pairs, then we can divide the tower into two in an arbitrary way, and if there is exactly one pair, then we need to make a "cut" exactly between two blocks of the same color. Otherwise, there will always be a pair of adjacent blocks of the same color in one of the towers.

Solution (Neon) 1Solution (Neon) 21795B - Ideal Point

Idea: BledDest

Tutorial1795B - Ideal Point

First of all, let's delete all segments that do not cover the point k (because they increase the value of the function f at points other than k). If there are no segments left, then the answer is NO. Otherwise, all segments cover the point k. And it remains to check whether the point k is the only point which is covered by all segments. Note that it does not make sense to delete any of the remaining segments, because if there are several points with maximum value of f, then deleting segments can only increase their number.

To check the number of points with the maximum value of f, you can iterate over x from 1 to 50 and calculate f(x), because of the small number of segments in the problem. A faster way is to check the size of the intersection of all segments. The left boundary of the intersection is L=maxi=1nli, and the right boundary is R=mini=1nri; if L=R, then the point k is ideal, otherwise it is not.

Solution (Neon)1795C - Tea Tasting

Idea: BledDest

Tutorial1795C - Tea TastingConsider how each sort of tea affects the tasters.

The i-th sort makes testers i,i+1,…,j−1, for some j, drink to their limit bi,bi+1,…,bj−1, and the j-th taster drink the remaining tea. Sometimes there is no such j-th taster, but we'll explore the general case.

Let's add the remaining tea straight to the j-th taster answer addj. And for each taster k from i to j−1 we'll add 1 into the value cntk denoting how many times they drank at their limit bk.

If we have these calculated, we can obtain the answer by adding addi and cnti⋅bi.

In order to find j, we can use prefix sums. Build pref over the sequence b. Now you want to find the largest j such that prefj−1−prefi≤ai. Rewrite it as prefj−1≤ai+prefi. You can do this with a binary search. In particular, with an upper_bound call. The amount of the remaining tea can also be calculated from prefix sums.

To add 1 on a range [i,j−1], you can use a technique called delta encoding. Add 1 to cnti. Subtract 1 from cntj. After everything is added, propagate this values via a prefix sum. This way, if both +1 and −1 happened non-strictly to the left or strictly to the right of i, it doesn't affect i at all (the segment either closes before i or opens after i). Otherwise, it adds exactly 1 to cnti.

Overall complexity: O(nlogn) per testcase.

Solution (Neon)1795D - Triangle Coloring

Idea: BledDest

Tutorial1795D - Triangle ColoringLet's ignore the constraint on the number of red/blue vertices for a moment. What is the maximum possible weight of a coloring? From any triple, we can have any two edges connect vertices of different colors. So, the maximum possible weight of a coloring (not necessarily a valid one) is the sum of all edge weights except for the minimum weight in each triple.

Let's show that it is always possible to choose a valid coloring to achieve this weight. In each triple, we should make sure that the two maximum edges connect vertices with different colors; to do this, we can color the vertex incident to both of these edges in one color, and the two other vertices will be painted in the other color. So, for each triple of vertices, there will be either one red vertex and two blue ones, or two red ones and one blue. Let's suppose the first n6 triples have one red vertex and two blue vertices each, and the other n6 triples have one blue vertex and two red vertices each. That way, we obtain a valid coloring with maximum possible weight.

Okay, now let's try to find out how do we calculate the number of valid colorings with the maximum possible weight. Each triple of vertices will be either "red" (two red vertices, one blue), or "blue" (the other way around). Since exactly half of the vertices should be red, then exactly half of the triples should be red, so the number of ways to choose a "color" for all triples is (n/3n/6).

After choosing the color of each triple, let's choose how we actually color them. The triples are independent, so for each triple, we can introduce the coefficient ci, which is the number of ways to color it so that its weight is maximized, and the triple has some specific type (either red or blue, doesn't matter since these are symmetric). Choosing the vertex which will be different from its neighbors is equivalent to choosing the edge which will not be included in the weight of the coloring (this is the edge which is not incident to the chosen vertex). So, ci is equal to the number of ways to choose that vertex in the i-th triple so that the weight is maximized; i. e. the weight of the edge not incident to the chosen vertex should be minimized. Thus, ci is just the number of minimum edge weights in the i-th triple.

The formula for the final answer is (n/3n/6)∏i=1n/3ci.

Solution (BledDest)1795E - Explosions?

Idea: BledDest

Tutorial1795E - Взрывы?Note that each unit of damage dealt by explosions save us from using one more basic spell. In other words, the more the damage from explosions, the better. So, the answer will be equal to ∑hi−(maximum total damage from explosions).

Note that in order to kill all remaining monsters with the last spell, the array h should have the following structure: there is a monster p we cast the spell onto it and h is strictly increasing in [0..p] and strictly decreasing in [p..n) (ignoring prefix and suffix of 0-s).

Let's focus on the left part of array h (segment [0...p]), since solving the right part is exactly the same. Maximizing the total damage is equivalent to maximizing the sum of hi right before the final spell. Note that we can use the straight greedy strategy: to kill the chosen monster p we should use "Explosion" spell of power exactly hp — it's not optimal to make it either more or less powerful.

After that, monster p will create an explosion of power hp−1. If hp−1>hp−1 we must decrease it to exactly hp−1 to continue the chain of explosions of maximum total damage. If hp−2>hp−2 we also decrease it to exactly hp−2 and so on. (The general formula is hp−i>hp−i).

This series will stop either if hp−i≤0 (or i≥hp), or there are no monsters left (p−i<0), or we met the monster with hp−i≤hp−i. The two first cases are easy to check in constant time, so let's look at the last case. Suppose that monster position is equal to j=p−i, then i=p−j or hj≤hp−(p−j) ↔ hj−j≤hp−p.

That monster j is interesting to us because after death it creates an explosion of damage hj−1 that already doesn't depend on p and next calculation is practically the same task: what chain of explosion we can have if we start from j. That idea drives us to dp: let d[i] be the maximum damage of chaining explosion we can deal if we start from i and move to the left. For simplicity, let's include hi into that total damage.

Calculating d[i] is next: let's find the "first" j such that hj−j≤hi−i. If there are no such j (or if that j is too far from i, i. e. i−j≥hi), we will set j=max(0,i−hi).

Now we know that on interval (j,i] the damage dealt is the arithmetic progression: for i it's hi, for i−1 it's hi−1, ..., for j+1 it's hi−(i−j)+1. In total, d[i]=(i−j)hi−(i−j)(i−j−1)2. And if such j exists and not too far away, we increase d[i] by d[j] as well.

The last question is finding for each i the closest j<i such that aj−i≤ai−i. Note that if we define a′i=ai−i, we need just need to find last a′j≤a′i and that's quite standard task that can be solved with stack.

Let's iterate over i and maintain a stack of previous a′i. When we need to find j for the current a′i let's just look at the top of the stack: if a′j≤a′i we found j we wanted, otherwise just pop it and check the new top again and so on, until either we find j or stack becomes empty that would mean that there are no a′j≤a′i. After processing the i-th element, push a′i on top of the stack.

Why it works? Consider some i. The element on top of the stack is a′j (firstly, it's a′i−1 but we are talking about general case). If a′j≤a′i we found what we want. Otherwise, a′j>a′i but it also means that previous elements a′k, that was popped on previous iteration j, was greater than a′j. So, a′k is bigger than a′i as well, and there were no need to even consider them, i. e. popping them out earlier doesn't break anything.

Since each element is pushed in the stack once and popped out once, then the complexity is O(n) for all i for 1 to n, or O(1) amortized.

The answer for the chosen position p then is ∑hi−dL[p]−dR[p]+2hp where dL[p] is dp we discussed above, dR[p] is the same dp but on reversed array h and 2hp because we included hp into both dL[p] and dR[p].

Both dL and dR are calculated in O(n), so the total comlpexity is O(n).

Solution (adedalic)1795F - Blocking Chips

Idea: BledDest

Tutorial1795F - Blocking ChipsThe constraints tell us that the solution should be linear or pretty close to it. Well, in particular, that implies that the solution almost certainly isn't dynamic programming, since we have both n and k to care about. Thus, we'll think about something greedy.

When we know the number of move the game will last, we can tell how many steps each chip should make. Well, since the more moves the game last, the more steps each ship makes, the answer is a monotonic function. Let's apply binary search and think if we can check if each chip can make some known number of steps.

A common idea in the problems where you have to do something greedily on a tree is to root the tree arbitrarily and process everything bottom up.

Consider the bottommost chip. If it can move its number of moves downwards, it's always optimal to do that. Since it's the bottommost chip, it can only make things worse for chips above it. And any of them can't pass through the initial vertex of this chip anyway.

If it can't, it has to move to its parent vertex. Let's move it there and deal with this chip later — when it becomes the bottommost again.

If it can't move to its parent, it can't move at all. Thus, the game can't last for this many steps.

Since we only apply either the move which is guaranteed to not interrupt any other moves or the move which is forced, the greedy strategy is correct.

As for implementation details, it's not too tricky. Basically, for each vertex, we should maintain these values:

if this vertex has been visited;the number of steps the chip in this vertex still has to make (if any chip is in this vertex);the longest path downwards from this vertex via non-visited vertices.The second value can be initialized beforehand and pushed to the parent when needed. The rest of them are easily maintained with a single dfs.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1795G - Removal Sequences

Idea: BledDest

Tutorial1795G - Removal SequencesLet's consider what the sequence of removals looks like in general. We will base some intuition on a fact that at least one valid sequence is guaranteed to exist.

Remove all vertices that have their degree correct from the start at once. There surely be such vertices, since a valid sequence would have to start with some of them.

Notice that there can't be any adjacent vertices among them. If there were, we wouldn't be able to remove such a pair regardless of the order we choose, since removing one of them makes another one's degree too low.

Now remove the vertices that just got their degrees correct from removing the first layer. Once again, these must exist (if the graph is not empty yet), because otherwise any valid sequence would get stuck.

Process until nothing is left.

This algorithm is basically a bfs, and you can implement it like one.

Note that each vertex becomes available to be removed only after a certain subset of its neighbours is removed. No matter what order you choose to remove the vertices in, these vertices will always be the same.

Huh, so for each vertex, some of its neighbours have to be removed before it, and the rest have to be removed after it (since otherwise, the degree of that vertex will become too low). That actually means that our graph is not as undirected as it seemed. We can direct each edge from a vertex that is removed before the other.

This makes a valid sequence of removals just a topological sort of that directed graph. So a pair is nice if there exist two topological orders such that x and y go one before another in them.

We can make a bold but perfectly reasonable guess about all nice pairs. A pair is nice if neither of x and y are reachable from each other.

The necessity of this condition is obvious. Let's show sufficiency.

Let's show the construction such that x goes before y. To remove x, we first have to remove all vertices that have edges to x. To remove them, we have to remove vertices with edge to them. And so on. Basically, to remove x, we have to remove all vertices that are reachable from x on the transposed directed graph. Since x is not reachable from y, it doesn't have to be removed before x. So we can first remove all the required vertices, then remove x, then continue removing vertices until we are able to remove y.

By switching x and y in the description of that construction, we can obtain the construction for y before x.

Thus, we reduced the problem to a rather well-known one. Calculate the number of reachable pairs of vertices in a directed graph.

As far as I know, it's not known to be solvable in sub-quadratic time. And we are not given a specific graph. Yes, it's obviously acyclic, but turns out every acyclic graph can be made into a test for this problem. You just have to make dv equal to the number of the outgoing edges for each v.

Somehow we are still given 105 vertices and edges.

If you are more familiar with that problem, you might know that you can use bitset to solve it. In particular, let reach[v] be a bitset such that reach[v][u]=1 if u if reachable from v. Then you can initialize reach[v][v]=1 for all vertices and propagate the knowledge in reverse topological order by applying reach[v]=reach[v]|reach[u] for all edges (v,u).

Unfortunately, that requires O(n2) memory, and 1010 bits is over a gigabyte.

Let's use of my favorite tricks to make a solution with O(n) memory and the same complexity. Man, I love that trick.

Process vertices in batches of 64. Let's calculate which vertices can reach vertices from 1 to 64. The algorithm is basically the same. For each vertex, store a smaller bitset of size 64 (also known as an unsigned long long). Initialize the bitset for 64 vertices from the batch and propagate the same way for all n vertices. Now just add up the number of ones in each bitset (__builtin_popcountll). Proceed to the next batch.

That makes it n64 iterations of a O(n+m) algorithm. This might require some constant optimizations. In particular, I suggest not to use dfs inside the iteration, since the recursion makes it really slow. You might iterate over a vertex in reverse topological order and its outgoing edges. Or, which is way faster, unroll that graph into a list of edges and iterate over it directly.

Educational Codeforces Round 142 Editorial

By awoo, history, 3 weeks ago, translation, In English1792A - GamingForces

Idea: BledDest

Tutorial1792A - GamingForcesThe first spell looks pretty weak compared to the second spell. Feels like you almost always replace one with another. Let's show that you can totally avoid casting the spell of the first type twice or more on one monster.

Let the two first spell casts be (i,j) and (i,k) for some monsters i,j and k. You can replace them by a cast of the second spell on i and a cast of the first spell on (j,k). That would deal even more damage to i and the same amount to j and k. The number of casts doesn't change.

Thus, it only makes sense to use the first spell on monsters with 1 health. Calculate the number of them, kill the full pairs of them with the first spell, and use the second spell on the remaining monsters.

Overall complexity: O(n) per testcase.

Solution (Neon)1792B - Stand-up Comedian

Idea: BledDest

Tutorial1792B - Stand-up ComedianFirst, let Eve tell the jokes of the first type — they will never do any harm. At the same time, let her tell the jokes of the fourth time at the very end — they will not do any good.

Types two and three are kind of opposites of each other. If you tell jokes of each of them one after another, then the moods of both spectators don't change. Let's use that to our advantage. Tell the jokes of these types in pairs until one of them runs out. There's a little corner case here, though. If there were no jokes of the first type, then you can't use a single pair because of the spectators leaves after one joke.

Finally, try to tell the remaining jokes of the same type before the fourth type. So the construction looks like 1,1,…,1,2,3,2,3,…,2,3,2,2,2,…,2,4,4,4,…,4 with 2 and 3 possibly swapped with each other.

Let's recover the answer from that construction. After the first type, both moods are a1. After the alternating jokes, the moods are still the same. After that, one of the spectators will have his/her mood only decreasing until the end. Once it reaches −1, the show ends.

Thus, Eve can tell a1+min(a2,a3)⋅2+min(a1+1,abs(a2−a3)+a4) jokes if a1≠0. Otherwise, it's always 1.

Overall complexity: O(1).

Solution (awoo)1792C - Min Max Sort

Idea: BledDest

Tutorial1792C - Min Max SortIf the array is already sorted, then the answer is 0. Otherwise, there is a last operation, after which the permutation takes the form 1,2,…,n. Which means that the elements 1 and n are selected as the last operation (because they are at the first and last positions after the operation). Now we know that the last operation is (1,n) and it doesn't matter where exactly these numbers are in the permutation, i. e. we can assume that the answer has increased by 1, and consider only the numbers 2,3,…,n−2,n−1. Similarly, for the "remaining" permutation, there are two options, either it is sorted, and then the answer is 1, or there is a last operation and the numbers 2 and n−1 are used in it. And so on until the "remaining" permutation is sorted or empty.

It remains to find out how to quickly check whether the numbers in the segment [k,n−k+1] are sorted (they go in the correct order in the initial permutation). Note that this segment corresponds to values of elements, not to positions in the permutation. If this segment is sorted for some k, then the answer does not exceed k−1.

There are several ways to check, let's consider one of them. Note that if the segment [k,n−k+1] is sorted for some value k, then it will be sorted for large values as well. So we can start with the maximum value of k (which is equal to ⌊n+12⌋) and decrease it until the segment remains sorted. Now for each k we need only two checks that posk < posk+1 and posn−k+1 > posn−(k+1)+1, where posi is the position of the element i in the permutation.

Thus, we got the solution in linear time.

Another way is to run binary search on k since if the numbers in [k,n−k+1] appear in the permutation in sorted order, the same holds for k+1. This approach yields a solution in O(nlogn).

Solution (Neon)1792D - Fixed Prefix Permutations

Idea: BledDest

Tutorial1792D - Fixed Prefix PermutationsLet's try to solve for one of the given permutations. Let it be some p. How to make the answer for it at least 1? Well, we have to find another permutation q such that p⋅q=(1,r2,r3,…,rm). How about at least k? Well, the same: p⋅q=(12…,k,rk+1,…,rm).

Push q to the right side of the equation. p=(12…,k,rk+1,…,rm)⋅q−1. Now think. What does it actually mean for some permutation to be multiplied by (1,2,…,k)? It stays the same. So the first k elements of p will be equal to the first k elements of q−1.

Thus, you have to find a permumtation such that its inverse has the longest common prefix with p. This can be done in multiple ways. For example, you can store all inverses in a trie and traverse it with p until you reach a dead end. Or simply push all prefixes of each inverse into a set and iterate over k. Alternatively, you can just sort inverses and do lower_bound for p in this list — the permutation with longest common prefix will be either the result or the one before it.

Overall complexity: O(nm)/O(nmlogn)/O(nm2logn) per testcase.

Solution (awoo)1792E - Divisors and Table

Idea: adedalic

Tutorial1792E - Divisors and TableFirstly, let's factorize m. Since m=m1⋅m2 we can factorize m1 and m2 separately and then "unite" divisors. For example, use can get canonical representations of m1=pf11pf22…pfkk and m2=pg11pg22…pgkk to get canonical representation of m=pf1+g11pf2+g22…pfk+gkk and then use it to generate all divisors of m.

Let's estimate the number of divisors divs(m). It's convenient for our purposes to estimate it as O(m13). More precisely, there are at most 105000 divisors for m≤1018 (search "Highly Composite Numbers" for more info).

How to calculate the answer ai for each divisor d? There are two ways.

The intended solution: for each d we are searching for the minimum x that d=xy and y≤n. Since d is fixed, the minimum x means the maximum y≤n. So let's find y instead. In other words, for each d we need to find the maximum y such that y divides d and y≤n. We can do it efficiently with dp on divisors.

Let dp[d] be the maximum y that is a divisor of d and y≤n. If d≤n then, obviously, dp[d]=d. Otherwise, we know that we are searching y<d.

Let say that p1,p2,…,pk are the prime divisors of the initial number m. Since y is a divisor of d and y<d then exists some pi among the set of prime divisors such that y is a divisor of dpi as well. So, instead of brute search, it's enough to take a value dp[dpi].

In other words, if d>n we can calculate dp[d]=maxpi|ddp[dpi].

Ok, now we know value dp[d] for each divisor d. Since we found the maximum y≤n, the last step is to calculate the desired x=ddp[d] and if x≤n we found the answer ai, otherwise (x>n) it means that d is not presented n×n table and ai=0.

The total complexity is O(m1+m2−−−−−−−√+divs(m)⋅z(m)⋅log(divs(m))) per test, where divs(m) is the number of divisors of m (divs(m)≤105000) and z(m) is the number of prime divisor of m (z(m)≤15). Note that complexity is quite high, so you should write it at least a little accurate, for example store dp[d] in an array, not map, and search position of dp[dpi] with lower_bound().

There is also a way to get rid of extra log(divs(m)) factor if you iterate through dp is a smart way.

The alternative solution (faster, easier, unproven): Let's generate a list of all divisors of m as d1,d2,…,dl in the increasing order. For some divisor di we are searching the minimum x that is a divisor of di and dix≤n. It means that x≥⌈din⌉.

So let's just find the first position j such that dj≥⌈din⌉ with lower_bound and start iterating from j onward searching the first dj that is a divisor of di. The found dj would be the minimum x we need.

It looks like, in average, we will find the correct dj quite fast, or we'll break when dj>n.

Solution (adedalic)1792F1 - Graph Coloring (easy version)

Idea: BledDest

Tutorial1792F1 - Graph Coloring (easy version)Lemma: if an undirected graph is disconnected, then its complement is connected. Similarly, if its complement is disconnected, then the graph itself is connected.

Proof: suppose a graph is disconnected. Pick two vertices x and y from different components. Every vertex outside of x's component is connected to x in the complement, and every vertex outside of y's component is connected to y in the complement; the complement also contains the edge from x to y, so all vertices in the complement graph belong to the single component.

Why do we need this lemma at all? We can treat the graph formed by blue edges as the complement to the graph formed by red edges. So, if the "red" graph is disconnected, then the "blue" graph is connected, so we don't need to consider the case when some set of vertices is connected by neither color. We only need to make sure that no set of vertices is connected by both colors.

Let An be the answer for n. Every graph counted in An is either red-disconnected or blue-disconnected; since there is a bijection between red-disconnected and blue-disconnected graphs (you can flip the colors of all edges to transform one type into the other), we will count only red-disconnected graphs and multiply it by 2.

Let Bn be the number of blue-connected graphs with n vertices meeting the properties of the problem statement. It's easy to see that An=2⋅Bn if n>1, otherwise An=Bn (the case n=1 is special because a graph on one vertex is both red-connected and blue-connected). To calculate An, let's iterate on k — the number of vertices which are in the same "red" component as 1. This component must be a red-connected graph which meets the problem statement, so the number of ways to build the graph on these k vertices is Bk; there are (n−1)!(k−1)!(n−k)! ways to choose the vertices in the same component as 1, and the remaining graph can be either red-connected or blue-connected, so the number of ways to build the remaining graph is An−k.

Thus, we get the following two relations:

Bn=∑k=1n−1BkAn−k(n−1)!(k−1)!(n−k)!An=2⋅Bn if n>1, otherwise BnWe can calculate all values with dynamic programming using these formulas in O(n2).

Solution (BledDest)1792F2 - Graph Coloring (hard version)

Idea: BledDest

Tutorial1792F2 - Graph Coloring (hard version)Please read the tutorial for the easy version first, since this tutorial uses some definitions from it.

Okay, we need more definitions. Here they come:

C0=0,Ci=Aii! if i>0D0=0,Di=Bi(i−1)! if i>0This way, we can transform the formula for Bn to the following:

Bn=(n−1)!⋅∑k=1n−1Cn−kDk.

Or even this, since C0=D0=0:

Bn=(n−1)!⋅∑k=0nCn−kDk.

This is almost the convolution of the sequences C and D (with a bit extra additional operations after the convolution), so, to compute the sequence B, we just need to compute the sequences C and D, and then calculate their convolution with NTT. All that's left is to multiply every element by the corresponding factorial.

But wait, that's not so easy. In order to calculate Ci and Di, we need to know Bi. Note that we can ignore the fact that Ci and Di appear in the formula for Bi, since they are multiplied by 0, so at least we don't have a dependency cycle. Unfortunately, we cannot just straightforwardly use convolution if we don't know the sequences Ci and Di.

The model solution handles it using the following approach. Let's generate A, B, C and D in parallel: on the i-th iteration, calculate Bi, then calculate Ai, Ci and Di using it. And sometimes we will calculate the convolution of the sequences C and D.

Suppose we want to calculate Bi, and the last time we calculated the convolution of C and D was after the iteration t. Back then, we knew all elements from C0 to Ct and from D0 to Dt. So, the i-th term in the convolution of C and D contained the sum of Ci−kDk over all k such that k≤t and i−k≤t. So, in order to calculate Bi, we have to pick this value from the convolution and then add the sum of Ci−kDk over all k such that k>t or k≤i−t, and there are 2(i−t) such values.

Suppose we compute the convolution every K iterations. Then the maximum value of i−t is K, and every value of Bi is calculated in O(K). We also make nK convolutions, so the total complexity of this solution will be O(n2lognK+nK), which can be transformed into O(nnlogn−−−−−√) if we pick K=nlogn−−−−−√.

Educational Codeforces Round 141 Editorial

By awoo, history, 5 weeks ago, translation, In English1783A - Make it Beautiful

Idea: BledDest

Tutorial1783A - Make it BeautifulIf we put the maximum in the array on the first position, then for every element, starting from the third one, the sum of elements before it will be greater than it (since that sum is greater than the maximum value in the array). So, the only element that can make our array ugly is the second element. We need to make sure that it is not equal to the first element.

Let's put the maximum element on the first position, the minimum element on the second position, and then fill the rest of the array arbitrarily. The only case when it fails is when the maximum element is equal to the minimum element — and it's easy to see that if the maximum is equal to the minimum, then the first element of the array will be equal to the second element no matter what, and the array cannot become beautiful.

So, the solution is to check if the maximum is different from the minimum, and if it is so, put them on the first two positions, and the order of remaining elements does not matter. Note that the given array is sorted, so the minimum is the first element, the maximum is the last element.

Solution (BledDest)1783B - Matrix of Differences

Idea: BledDest

Tutorial1783B - Matrix of DifferencesThe first step is to notice that beauty doesn't exceed n2−1, because the minimum difference between two elements is at least 1, and the maximum difference does not exceed n2−1 (the difference between the maximum element n2 and the minimum element 1).

At first, finding a matrix with maximum beauty seems to be a quite difficult task. So let's try to find an array of n2 elements of maximum beauty. In this case, it is not difficult to come up with an array of the form [n2,1,n2−1,2,n2−2,3,…]. In such an array, there are all possible differences from 1 to n2−1. So we found an array with the maximum possible beauty.

It remains to find a way to "convert" the array to the matrix, i.e. to find such a sequence of matrix cells that each two adjacent cells in it are side-adjacent. One of the ways is the following: traverse the first row of the matrix from left to right, go down to the second row, traverse it from right to left, go down to the third row, traverse it from left to right, and so on.

Thus, we constructed a matrix with the maximum possible beauty n2−1.

Solution (Neon)1783C - Yet Another Tournament

Idea: BledDest

Tutorial1783C - Yet Another TournamentSuppose, at the end, you won x matches, what can be your final place? Look at each opponent i with i<x (0-indexed). Since the i-th opponent (0-indexed) won i games against the other opponents, even if they win against you, they'll gain i+1≤x wins in total and can't affect your place (since your place is decided by only opponents who won strictly more matches than you).

From the other side, let's look at each opponent i with i>x (0-indexed). Even if they lose to you, they still have i>x wins (you have only x), so all of them have strictly more wins than you.

As a result, there is only one opponent i=x, whose match against you can affect your final place: if you won against them, your place will be n−x, otherwise your place will be n−x+1.

Now, let's compare your possible places if you win x games with places for winning only x−1 games: x wins gives you places n−x or n−x+1, while winning x−1 leads you to places n−x+1 or n−x+2 that objectively worse.

In other words, it's always optimal to win as many matches as possible.

How to win the most number of games? It's to choose the easiest opponents. Let's sort array a and find the maximum prefix [0,x) with a0+a1+⋯+ax−1≤m. So, we found maximum number of games x we can win. The last is to check: can we get place n−x, or only n−x+1.

If ax contains among x smallest values, then we'll take place n−x. Otherwise, let's try to "insert" ax in this set, i. e. let's erase the biggest among them and insert ax. If the sum is still lower or equal to m, it's success and we get place n−x. Otherwise, our place is n−x+1.

The total complexity is O(nlogn) because of sorting.

Solution (Neon)1783D - Different Arrays

Idea: BledDest

Tutorial1783D - Different ArraysOne of the key observations to this problem is that, after the first i operations, the first i elements of the array are fixed and cannot be changed afterwards. Also, after the i-th operation, the elements on positions from i+3 to n are the same as they were before applying the operations.

This allows us to write the following dynamic programming: dpi,x,y — the number of different prefixes our array can have, if we have performed i operations, the (i+1)-th element is x, and the (i+2)-th element is y. The elements after i+2 are the same as in the original array, and the elements before i+1 won't be changed anymore, so we are interested only in these two elements.

Let's analyze the transitions in this dynamic programming. We apply the operation i+1 to the elements ai+1, ai+2 and ai+3. If we add ai+2 to ai+1, then we subtract it from ai+3, so we transition into state dpi+1,y,ai+3−y. Otherwise, we transition into state dpi+1,y,ai+3+y. The element we leave behind is either x−y or x+y, and if y≠0, these two transitions give us different prefixes. But if y=0, we need to make only one of these transitions, because adding or subtracting 0 actually makes no difference.

Okay, now we've got a solution with dynamic programming in O(n3A2), where n is up to 300 and A is up to 300. This is too slow. But we can notice that the value of ai+1 actually does not affect our transitions at all; we can just discard it, so our dynamic programming becomes O(n2A), which easily fits into TL.

Small implementation note: elements can become negative, and in order to store dynamic programming with negative states in an array, we need to do something about that. I don't recommend using maps (neither ordered nor unordered): you either get an extra log factor, or make your solution susceptible to hacking. Instead, let's say that the value of dpi,y, where y can be a negative number, will be stored as dp[i][y+M] in the array, where M is some constant which is greater than the maximum possible |y| (for example, 105 in this problem). That way, all array indices will be non-negative.

Solution complexity: O(n2A).

Solution (BledDest)1783E - Game of the Year

Idea: BledDest

Tutorial1783E - Game of the YearConsider some value of k. When is it included in the answer? When Monocarp spends a lower or an equal amount of "blocks" of attempts than Polycarp for killing every boss.

Formally, ⌈aik⌉≤⌈bik⌉ for all i from 1 to n.

Let's reverse this condition. k is not in the answer if there exists such i from 1 to n that ⌈bik⌉<⌈aik⌉. So, there exists at least one value between ⌈bik⌉ and ⌈aik⌉. Let's call it x. Now it's ⌈bik⌉<x≤⌈aik⌉. I set the ≤ and < signs arbitrarily, just so that it shows that such a value exists. You can't put both ≤ or both <, because that will accept 0 values or at least 2 values, respectively.

Would be cool if we could multiply everything by k and it still worked. Is it completely impossible, though? Take a look at bi<xk≤ai. What it says is that there exists a multiple of k between bi and ai. A multiple of k is a number that's the last in each "block" of attempts (the block of value that are rounded up the same). Turns out, this is what we are looking for already. Right after the multiple of k, the new block starts. Thus, we are wrong we our signs. It should be bi≤xk<ai — ai is in the block after bi, so it requires more blocks of attempts.

So for k to not be included in the answer, there should exist at least one i such that there exists a multiple of k in the half-interval [bi;ai).

That is pretty easy to implement. For each x, calculate the number of half-intervals that cover x. I think this is called delta-encoding. Iterate over all half-intervals and make two updates for each one: increment by 1 on position bi and decrement by 1 on position ai. Then make a prefix sum over these updates. Now the value in the x-th position tells you the number of half-intervals that cover x.

To check a particular value of k, iterate over all multiples of k and check that none are covered by half-intervals. It's known that the total number of multiples over all numbers from 1 to n is n+n2+n3+⋯+nn=O(nlogn).

Overall complexity: O(nlogn) per testcase.

Solution (BledDest)1783F - Double Sort II

Idea: BledDest

Tutorial1783F - Double Sort IIThe solution to this problem uses cyclic decomposition of permutations. A cyclic decomposition of a permutation is formulated as follows: you treat a permutation as a directed graph on n vertices, where each vertex i has an outgoing arc i→pi. This graph consists of several cycles, and the properties of this graph can be helpful when solving permutation-based problems.

First of all, how does the cyclic decomposition of a sorted permutation look? Every vertex belongs to its own cycle formed by a self-loop going from that vertex to itself. We will try to bring the cyclic decompositions of the given permutations to this form.

What does an operation with integer i do to the cyclic decomposition of the permutation? If i is in its own separate cycle, the operation does nothing (pi=i, so we swap an element with itself).

Otherwise, let's suppose that x is the element before i in the same cycle (px=i), and y is the element after i in the same cycle (pi=y). Note that this can be the same element. When we apply an operation on i, we swap px with pi, so after the operation, pi=i, and px=y. So, i leaves the cycle and forms its separate cycle, and y becomes the next vertex in the cycle after x. So, using the operation, we exclude the vertex i from the cycle.

Suppose we want to sort one permutation. Then each cycle having length ≥2 must be broken down: for a cycle of length c, we need to exclude c−1 vertices from it to break it down. The vertex we don't touch can be any vertex from the cycle, and all other vertices from the cycle will be extracted using one operation directed at them. It's easy to see now that if we want to sort a permutation, we don't need to apply the same operation twice, and the order of operations does not matter.

Okay, then what about sorting two permutations in parallel? Let's change the problem a bit: instead of calculating the minimum number of operations, we will try to maximize the number of integers i such that we don't perform operations with them. So, an integer i can be left untouched if it is the only untouched vertex in its cycles in both permutations... Can you see where this is going?

Suppose we want to leave the vertex i untouched. It means that in its cycles in both permutations, every other vertex has to be extracted with an operation. So, if two cycles from different permutations have a vertex in common, we can leave this vertex untouched, as long as there are no other vertices left untouched in both of these cycles. Let's build a bipartite graph, where each vertex in the left part represents a cycle in the first permutation, and each vertex in the right part represents a cycle in the second permutation. We will treat each integer i as an edge between two respective vertices in the bipartite graph. If the edge corresponding to i is "used" (i is left untouched), we cannot "use" any edges incident to the same vertex in left or right part. So, maximizing the number of untouched numbers is actually the same as finding the maximum matching in this bipartite graph.

After you find the maximum matching, restoring the actual answer is easy. Remember that the edges saturated by the matching correspond to the integers we don't touch with our operations, the order of operations does not matter, and each integer has to be used in an operation only once. So, the actual answer is the set of all integers without those which correspond to the edges from the matching.

This solution runs in O(n2) even with a straightforward implementation of bipartite matching, since the bipartite graph has at most O(n) vertices and O(n) edges.

Solution (BledDest)1783G - Weighed Tree Radius

Idea: BledDest

Tutorial1783G - Weighed Tree RadiusFirstly, let's define the weight of path (u,v) as wp(u,v)=au+du(v)+av. On contrary to weighted distances, wp(u,v)=wp(v,u) and also wp(v,v)=2av.

Now, let's define the diameter of a tree as path (u,v) with maximum wp(u,v). It's okay if diameter may be explicit case (v,v). The useful part of such definition is next: our diameter still holds most properties of the usual diameter. Let's look at two of them:

There is a vertex on diameter path (x,y) with wv(x)=⌈wp(x,y)2⌉ and wv(y)=wp(x,y)−wv(x). It's easy to prove after noting the fact that ax≤dx(y)+ay and ay≤dy(x)+ax (otherwise, you could choose diameter (x,x) or (y,y)).For any vertex v eccentricity e(v)=max(wv(x),wv(y)). In other words, either x or y has the maximum distance from v. (You can also prove it by contradiction). It also means that e(v)≥⌈wp(x,y)2⌉.The two properties above give us an easy way to calculate the radius: just maintain diameter (x,y), and the answer is a half of it.Now let's look how the diameter changes when we change the weight av. If av is increasing it's quite easy. The only paths that change weights are the paths ending at v. Denote such path as (v,u) and note that either v=u or wp(v,u)=av+wv(u)≤av+e(v) = av+max(wv(x),wv(y)). In other words, there will be only three candidates for a new diameter:

path (v,v) with wp(v,v)=2av;path (v,x) with wp(v,x)=av+dv(x)+ax;path (v,y) with wp(v,y)=av+dv(y)+ay.The only thing you need to calculate fast enough is the two distances dv(x) and dv(y). And since dv(x)=depth(v)+depth(x)−2⋅depth(lca(v,x)), your task is to calculate lca.

Finally, how to handle decreasing av's? Let's get rid of them using DCP (dynamic connectivity problem) technique. Keep track of each value av: each possible value av for some vertex v will be "active" on some segment of queries [l,r)∈[0,m). Since there are only m queries, there will be exactly n+m such segments for all vertices v in total.

Now, all queries becomes "assign av=x on some segment of queries [l,r)". Note that in that case, the previous value of av was 0, so you are dealing with only "increasing value" queries.

Finally, to handle all range queries efficiently, you build a Segment Tree on queries, set all queries and then traverse your Segment Tree while maintaining the current diameter in order to calculate answers for all queries.

Each of n+m queries transforms in O(logm) queries to segment tree vertices, and preforming each query asks you to calculate lca two times.

If you use the usual binary lifting, then your complexity becomes O((n+m)logmlogn) what is okay. But if you use Sparse Table on Euler tour, you can take lca in O(1) and your complexity will be O(nlogn+(n+m)logm).

Educational Codeforces Round 140 Editorial

By awoo, history, 2 months ago, translation, In English1767A - Cut the Triangle

Idea: BledDest

Tutorial1767A - Cut the TriangleThe line we draw must go through a triangle's vertex; otherwise, two sides of the triangle are split, and one of the resulting parts becomes a quadrilateral.

So we need to check if it is possible to make a horizontal or vertical cut through a vertex. A horizontal cut is possible if all y-coordinates are different (we can draw it through a vertex with the median y-coordinate); a vertical cut is possible if all x-coordinates are different (we can draw it through a vertex with the median x-coordinate).

So, all we need to check is the following pair of conditions:

all xi are different;all yi are different.Solution (BledDest)1767B - Block Towers

Idea: BledDest

Tutorial1767B - Block TowersNotice that it never makes sense to move blocks between the towers such that neither of them is tower 1 as that can only decrease the heights. Moreover, it never makes sense to move blocks away from the tower 1. Thus, all operations will be moving blocks from some towers to tower 1.

At the start, which towers can move at least one block to tower 1? Well, only such i that ai>a1. What happens after you move a block? Tower 1 becomes higher, some tower becomes lower. Thus, the set of towers that can share a block can't become larger.

Let's order the towers by the number of blocks in them. At the start, the towers that can share a block are at the end (on some suffix) in this order. After one move is made, the towers get reordered, and the suffix can only shrink.

Ok, but if that suffix shrinks, what's the first tower that will become too low? The leftmost one that was available before. So, regardless of what the move is, the first tower that might become unavailable is the leftmost available tower. Thus, let's attempt using it until it's not too late.

The algorithm then is the following. Find the lowest tower that can move the block to tower 1, move a block, repeat. When there are no more towers higher than tower 1, the process stops.

However, the constraints don't allow us to do exactly that. We'll have to make at most 109 moves per testcase.

Ok, let's move the blocks in bulk every time. Since the lowest available tower will remain the lowest until you can't use it anymore, make all the moves from it at the same time. If the current number of blocks in tower 1 is x and the current number of blocks in that tower is y, ⌈y−x2⌉ blocks can be moved.

You can also avoid maintaining the available towers by just iterating over the towers in the increasing order of their height.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1767C - Count Binary Strings

Idea: BledDest

Tutorial1767C - Count Binary StringsSuppose we build the string from left to right, and when we place the i-th character, we ensure that all substrings ending with the i-th character are valid. What do we need to know in order to calculate the number of different characters in the string ending with the i-th character?

Suppose the character si is 0. Let's try going to the left of it. The string from i to i will have the same characters; but if there is at least one character 1 before the i-th position, the string s1s2s3…si will have two different characters. What about the strings in the middle? The string sjsj+1…si will contain different characters if and only if there is at least one 1 in [j,i) (since si is 0), so we are actually interested in the position of the last character 1 before i.

The same logic applies if the character si is 1: we are only interested in the position of the last 0 before i, and it is enough to check if all substrings ending with the i-th character are violated.

What if when we choose the i-th character, we violate some substring that doesn't end in the i-th position? Well, you could also check that... or you could just ignore it. Actually, it doesn't matter if this happens because it means that the substring that is violated ends in some position k>i; and we will check it when placing the k-th character.

So, the solution can be formulated with the following dynamic programming: let dpi,j be the number of ways to choose the first i characters of the string so that the last character different from si was sj (or j=0 if there was no such character), and all the constraints on the substrings ending no later than position i are satisfied. The transitions are simple: you either place the same character as the last one (going from dpi,j to dpi+1,j), or a different character (going from dpi,j to dpi+1,i); and when you place a character, you check all the constraints on the substrings ending with the i-th position. Note that the state dp1,0 is actually represented by two strings: 0 and 1.

This solution works in O(n3), although O(n4) or O(n2) implementations are also possible.

Solution (BledDest)1767D - Playoff

Idea: Neon

Tutorial1767D - PlayoffFirstly, let's prove that the order of characters in s is interchangeable.

Suppose we have a tournament of four teams with skills a, b, c and d such that a<b<c<d; and this tournament has the form 01 or 10. It's easy to see that a and d cannot be winners, since a will be eliminated in the round with type 1, and d will be eliminated in the round with type 0. However, it's easy to show that both with s=10 and with s=01, b and c can be winners.

Using this argument to matches that go during phases i and i+1 (a group of two matches during phase i and a match during phase i+1 between the winners of those matches can be considered a tournament with n=2), we can show that swapping si and si+1 does not affect the possible winners of the tournament.

So, suppose all phases of type 1 happen before phases of type 0, there are x phases of type 1 and y phases of type 0 (x+y=n). 2x+y−2y teams will be eliminated in the first part (phases of type 1), and the team with the lowest skill that wasn't eliminated in the first half will win the second half.

It's easy to see that the teams with skills [1..2x−1] cannot pass through the first part of the tournament, since to pass the first part, a team has to be the strongest in its "subtree" of size 2x.

Furthermore, since the minimum of 2y teams passing through the first half wins, the winner should have skill not greater than 2x+y−2y+1 — the winner should have lower skill than at least 2y−1 teams, so teams with skills higher than 2x+y−2y+1 cannot win.

Okay, now all possible winners belong to the segment [2x,2n−2y+1]. Let's show that any integer from this segment can be winning.

Suppose k∈[2x,2n−2y+1], let's construct the tournament in such a way that only team with skill k and 2y−1 teams with the highest skill pass through the first part of the tournament (obviously, then team k wins). There are 2y independent tournaments of size 2x in the first part; let's assign teams with skills from 1 to 2x−1, and also the team k to one of those tournaments; for all other 2y−1 tournaments, let's assign the teams in such a way that exactly one team from the 2y−1 highest ones competes in each of them. It's easy to see that the team k will win its tournament, and every team from the 2y−1 highest ones will win its tournament as well, so the second half will contain only teams with skills k and [2n−2y+2..2n] (and, obviously, k will be the winner of this tournament).

So, the answer to the problem is the segment of integers [2x,2n−2y+1].

Solution (Neon)1767E - Algebra Flash

Idea: BledDest

Tutorial1767E - Algebra FlashImagine we bought some subset of colors. How to check if there exists a path from 1 to n?

Well, we could write an easy dp. However, it's not immediately obvious where to proceed from that. You can't really implement buying colors inside the dp, because you should somehow know if you bought the current color before, and that's not really viable without storing a lot of information.

Let's find another approach. Let's try to deduce when the subset is bad — the path doesn't exist. Trivial cases: c1 or cn aren't bought. Now, if there are two consecutive platforms such that their colors aren't bought, the path doesn't exist. Otherwise, if there are no such platforms, you can show that the path always exists.

In particular, that implies that among all pairs of consecutive platforms, at least one color of the pair have to be bought. If the colors of the pair are the same, then it's just that this color have to be bought.

The next step is probably hard to get without prior experience. Notice how the condition is similar to a well-known graph problem called "vertex cover". That problem is about finding a set of vertices in an undirected graph such that all graph edges have at least one of their endpoints in the set. In particular, our problem would be to find a vertex cover of minimum cost.

That problem is known to be NP-hard, thus the constraints. We can't solve it in polynomial time but we'll attempt to it faster than the naive approach in O(2m⋅m2).

Let's start with this approach anyway. We can iterate over a mask of taken vertices and check if that mask is ok. In order to do that, we iterate over edges and check if at least vertex is taken for each of them.

Again, having a bit of prior experience, one could tell from the constraints that the intended solution involves meet-in-the-middle technique.

Let's iterate over the mask of taken vertices among vertices from 1 to m2. Then over the mask of taken vertices from m2+1 to m. The conditions on edges split them into three groups: the edges that are completely in mask1, the edges that are completely in mask2 and the edges that have one endpoint in mask1 and another endpoint in mask2.

First two types are easy to check, but how to force the third type to be all good? Consider the vertices that are not taken into mask1. All edges that have them as one of the endpoints will turn out bad if we don't take their other endpoints into mask2. That gives us a minimal set of constraints for each mask1: a mask con that includes all vertices from the second half that have edges to at least one of non-taken vertex in mask1.

Then mask2 is good if it has con as its submask. Thus, we would want to update the answer with the mask1 of the minimum cost such that its con is a submask of mask2.

Finally, let dp[mask] store the minimum cost of some mask1 such that its con is a submask of mask. Initialize the dp with the exact con for each mask1. Then push the values of dp up by adding any new non-taken bit to each mask.

When iterating over mask2, check if it's good for edges of the second kind and update the answer with dp[mask2].

Overall complexity: O(2m/2⋅m2).

Solution (awoo)1767F - Two Subtrees

Idea: shnirelman

Tutorial1767F - Two SubtreesOriginal solution (by shnirelman)

First, let's solve the following problem: we need to maintain a multiset of numbers and process queries of 3-th types: add a number to the multiset, remove one occurrence of a number from the multiset (it is guaranteed that it exists), calculate the mode on this multiset. To do this, we will maintain the array cnti — the frequency of i in the multiset. Now the mode is the position of the leftmost maximum in this array. There are many ways to search for this position, we will use the following: we will build a sqrt-decomposition on the array cnt: for a block we will maintain a maximum on this block and an array ci — the number of positions j in this block, such that cntj=i. Since in each of the initial requests cnti changes by no more than 1, the maximum in the block also changes by no more than 1 and, using the c array, it is easy to update it after each query. Now, to find the mode (the position of the leftmost maximum in the cnt array), you first need to go through all the blocks to find the value of the maximum and the leftmost block in which this maximum occurs, then iterate over the desired position in this block. Thus, queries to add and remove an element run in O(1), and a mode search query runs in O(A−−√), where A is the number of possible distinct values, in a given problem A=2⋅105.

Now let's get back to the problem itself. Let's build a Preorder traversal of our tree. Let tinv be the position in the 0-indexing of the vertex v in the Preorder traversal, toutv be the size of the Preorder traversal after leaving the vertex v. Then the half-interval [tinv,toutv) of the Preorder traversal represents the set of vertices of the subtree of the vertex v. For the i-th query, we will consider tinvi≤tinui.

Let szv=toutv−tinv be the size of the subtree of v, B be some integer, then v will be called light if szv<B, and heavy otherwise. A query i is called light (heavy) if vi is a light (heavy) vertex. We will solve the problem for light and heavy queries independently.

Light queries. Let's use the small-to-large technique and maintain the multiset described at the beginning of the solution. Let at the moment we have this multiset for the vertex w. Let's answer all light queries for which ui=w. To do this, take all the vertices from the subtree of vi and add the numbers written on them, calculate the mode on the current multiset — this will be the answer to the query, and then delete the newly added vertices. In the standard implementation of small-to-large, you need to maintain several structures at the same time, which in this case is impossible due to the fact that each of them takes up O(AA−−√) of memory. This problem can be avoided, for example, as follows: before constructing the Preorder traversal for each vertex v, put its heaviest son at the head of the adjacency list. Then it will be possible to iterate over the vertices in the order of the Preorder traversal, preserving the asymptotics. This part of the solution runs in O(nlogn+qB+qA−−√).

Heavy queries. Let's divide all heavy vertices into non-intersecting vertical paths, so that two vertices from the same path have subtrees that differ by no more than B vertices, and the number of the paths themselves is O(nB). To do this, let's take the deepest of the unused heavy vertices and build one of the desired paths, going up to the parent, while the first of these conditions is met. Then we mark all the vertices in this path as used, and start over. We will continue to do this while there are still unused heavy vertices. It is easy to see that the resulting paths are vertical and the subtrees of two vertices from the same path differ by no more than B by construction. Let's prove that there are not very many of these paths. To do this, we will understand in which cases the path breaks:

If the current path contains a root, then since the root has no parent, the path will terminate. Obviously, this path is only 1.If the parent of the last vertex of the path has only one heavy child (this last vertex itself). From the construction, a break means that the number of vertices in this path plus the number of children outside the heaviest son subtree of the parent of the last vertex and each vertex of the path, except for the initial one, is more than B in total, but each of the counted vertices can be counted in only one of such cases, that is, the number of paths that terminate in this way does not exceed nB.If the parent of the last node has more than one heavy child. Let's leave only heavy vertices in the tree (since the parent of a heavy vertex is a heavy vertex too, it will indeed be a tree (or an empty graph)). This tree contains at most nB leafs. Calculating the total degree of the vertices of this tree, we can see that there are at most nB additional sons (all sons of a vertex except one). This means that the number of paths terminating in this way is at most nB.We got that the paths are O(nB).Let's divide the heavy queries according to the paths where the vi is situated. We will answer queries with vertices v from the same path together. We will do it similarly to the case with light queries, with minor differences: at the very beginning, we add to the multiset all the vertices of the subtree of the initial vertex of the path and mentally remove these vertices from the subtrees of vi vertices. Everything else is preserved. Let's calculate how long it takes: add all vertices from one subtree: O(n), small-to-large: O(nlogn), to answer one query due to condition on vertices from one path we have to add at most B vertices. Since there are only O(nB) paths, the whole solution will take O(n2lognB+qB+qA−−√). We take B=n2lognq−−−−−√ and, counting n≈q, we get B=nlogn−−−−−√ and total running time O(nnlogn−−−−−√+nA−−√).

Implementation details. As already mentioned, a subtree corresponds to a segment of the Preorder traversal, so 2 subtrees are 2 segments. We will maintain the data structure described at the beginning on the sum of 2 segments. By moving the boundaries of these segments, you can move from one query to another, as in Mo's algorithm. It remains only to sort the queries. Heavy queries are sorted first by path number of vi, then by tinui. Light queries are sorted only by tinui, but here you can't just move the segment of the v subtree, you need to rebuild it for each query.

Bonus. Solve this problem for two subtrees and a path connecting the roots of these subtrees.

Alternative solution (by BledDest)

This solution partially intersects with the one described by the problem author. We will use the same data structure for maintaining the mode; and we will also use DFS order of the tree (but before constructing it, we will reorder the children of each vertex so that the heaviest child is the first one).

Let tinv be the moment we enter the vertex v in DFS, and toutv be the moment we leave the vertex. As usual, the segment [tinv,toutv] represents the subtree of vertex v, and we can change the state of the structure from the subtree of the vertex x to the subtree of the vertex y in |tinx−tiny|+|toutx+touty| operations. Let this number of operations be cost(x,y).

Let v1,v2,…,vn be the DFS order of the tree. We can prove that cost(v1,v2)+cost(v2,v3)+⋯+cost(vn−1,vn) is estimated as O(nlogn) if we order the children of each vertex in such a way that the first of them is the heaviest one.

Proof. Let's analyze how many times some vertex v is added when we go in DFS order and maintain the current set of vertices. When some vertex is added to the current subtree, this means that the previous vertex in DFS order was not an ancestor of the current vertex, so the current vertex is not the first son of its parent. So, the size of the subtree of the parent is at least 2x the size of the current vertex. Since the path from v to root can have at most O(logn) such vertices, then the vertex v is added at most O(logn) times.

Okay, how do we use it to process queries efficiently? Let's say that the vertex vi (the i-th vertex in DFS order) has coordinate equal to cost(v1,v2)+cost(v2,v3)+⋯+cost(vi−1,vi). Let this coordinate be cvi. Then, if we have the data structure for the query (x1,y1) and we want to change it so it meets the query (x2,y2), we can do it in at most |cx1−cx2|+|cy1−cy2| operations, which can be treated as the Manhattan distance between points (cx1,cy1) and (cx2,cy2).

Do you see where this is going? We can map each query (x,y) to the point (cx,cy), and then order them in such a way that the total distance we need to travel between them is not too large. We can use Mo's algorithm to do this. Since the coordinates are up to O(nlogn), but there are only q points, some alternative sorting orders for Mo (like the one that uses Hilbert's curve) may work better than the usual one.

Educational Codeforces Round 139 Editorial

By awoo, history, 2 months ago, translation, In English1766A - Extremely Round

Idea: BledDest

Tutorial1766A - Extremely RoundThere are many ways to solve this problem.

The most naive one (iterating through all numbers from 1 to n in each test case and checking if they are extremely round) fails, since it is O(tn), but you can optimize it by noticing that extremely round numbers are rare. So, for example, we can iterate through all numbers from 1 to 999999 once, remember which ones are extremely round, store them into an array, and while answering the test case, only check the numbers from the array we have created.

There is also a solution in O(1) per test case with a formula, try to invent it yourself.

Solution (BledDest)1766B - Notepad#

Idea: BledDest

Tutorial1766B - Notepad#Why does the problem ask us only to check if we can do less than n operations instead of just asking the minimum amount? That must be making the problem easier, so let's focus our attention on that.

What if it was ≤n instead of <n? Well, then the problem would be trivial. You can type the word letter by letter and be done in n operations. So we only have to save one operation. In order to save at least one operation, we have to use the copy operation and copy more than one character in that.

Let's take a closer look at any of the copy operations we do. Basically, it has to be a substring that has at least two non-intersection occurrences in the string. Thus, if the string has any substring that has length at least two that appears at least twice in the string, we can copy it, and the answer will be "YES".

That's still not enough to solve the problem — we'd have to check all substrings, which is O(n2).

Let's think further. Imagine we found a substring that works. Let it have length k. Notice how you can remove its last character, obtaining a substring of length k−1, and it will still occure in the same set of positions (possibly, even more occurrences will be found). Remove characters until the substring has length 2. Thus, if any appropriate substring exists, an appropriate substring of length 2 also exists.

Finally, we can check if there exists a substring of length 2 that appears at least twice in the string so that the occurrences are at least 2 apart. That can be done with a set/hashset or a map/hashmap. Some implementations might require careful handling of the substrings of kind "aa", "bb" and similar.

Overall complexity: O(n) or O(nlogn) per testcase.

Solution (awoo)1766C - Hamiltonian Wall

Idea: BledDest

Tutorial1766C - Hamiltonian WallWhy is there a constraint of each column having at least one black cell? Does the problem change a lot if there were white columns? Well, if such a column was inbetween some black cells, then the answer would be "NO". If it was on the side of the grid, you could remove it and proceed to solve without it. So, that doesn't really change the problem other than removing some casework.

Let's try to fix a start. Find a column that has only one black cell in it. If there are no such columns, the answer is immediately "YES". Otherwise, the path will always go through it in known directions: to the left and to the right (if both of them exist). Let's solve the problem separately for the left part of the path and for the right one — find a path that starts to the left of it and covers everything to the left and the same for the right part.

Consider the right part.

If the next column also has one black cell, then we can determine where to go uniquely. If this cell is on the opposite row, then the answer is "NO". Otherwise, go there and proceed further.

Let it have two black cells now. Find the entire two black row rectangle of maximum size that starts there. If there's nothing after it, you can easily traverse it any way you like. Otherwise, you have to traverse it in such a way that you end up in its last column, then go to the right from there. Turns out, there's only one way to achieve that. Go up/down to another row, go right, up/down to another row, right and so on. Now you just have to check if you end up in the correct row.

Thus, you can simulate the path to the left and to the right and check if you never get stuck.

Overall comlexity: O(n) per testcase.

Solution (awoo)1766D - Lucky Chains

Idea: BledDest

Tutorial1766D - Lucky ChainsSuppose, gcd(x+k,y+k)=g. It means that (y+k)−(x+k)=(y−x) is also divisible by g, or gcd(x+k,y−x)=h is divisible by g. And backward: if gcd(x+k,y−x)=h, then (x+k)+(y−x)=(y+k) is also divisible by h, or gcd(x+k,y+k)=g is divisible by h.

Since h is divisible by g and g is divisible by h, so h=g. In other words, we proved that gcd(x+k,y+k)=gcd(x+k,y−x).

Now, knowing the equivalence above, we can understand that we are looking for the smallest k≥0 such that gcd(x+k,y−x)>1. In other words, we are searching k such that x+k is divisible by some d>1, where d is some divisor of (y−x).

The problem is that there are a handful of divisors for some (y−x). But we can note that we can consider only prime divisors of (y−x): if d|(y−x) and d is composite then there is some prime p|d, thus p|(y−x).

It's easy to prove that there are no more than log2n prime divisors of some n. Now the question is how to find all these prime divisors.

Note that if you know only one prime divisor for each value from 1 to n, then you can find all prime divisors for all k≤n in O(logk). The prime divisors pi are next:

p1=minD[k], k1=kminD[k];p2=minD[k1], k2=k1minD[k1];p3=minD[k2], k3=k2minD[k2];and so on until ki>1.The final step is to calculate a prime divisor minD[i] for each value from 1 to A, where A≥max(yi) or A≥107. We can do it by slight modifications of Sieve of Eratosthenes: at the step, where you have some prime p and want to "throw out" all values k⋅p, set minD[kp]=p for each kp (plus set minD[p]=p).

As a result, we, firstly, calculate Sieve in O(NloglogN) and, secondly, calculate answer for each pair (xi,yi) in O(logN).

Note that the input and output is large, so you should you tricks to speed up your input and output.

Solution (adedalic)1766E - Decomposition

Idea: BledDest

Tutorial1766E - DecompositionLet's assume that we don't have any zeroes in our array. We'll deal with them later.

The key observation is that the number of sequences in the decomposition is not more than 3. To prove this, we can use the fact that each element 3 will be appended to the first subsequence in the decomposition; so, if the second/third subsequence in the decomposition ends with the number 2 or 1, all such numbers can be appended to that subsequence, thus they won't create any new subsequences. So, if we consider the combination of the last elements in the subsequences of the decomposition, there are only 33+32+31+30=40 such combinations (even less in practice).

Okay, now let's try to use the fact that the number of such combinations is small. There are many ways to abuse it, but, in my opinion, the most straightforward one (and also a bit slow, but fast enough to easily pass the time limit) is to run the following dynamic programming: dpi,c, where i is the index of the element we are processing, and c is the vector representing the combination of last elements of subsequences in the decomposition.

But it's not clear what do we store in this dynamic programming. The model solution stores the total number of subsequences added to the decomposition, if right now the state of decomposition is c, we process the i-th element, and we consider all possible stopping points (i. e. we will consider the number of subsequences added while processing the elements a[i..i],a[i..i+1],a[i..i+2],…,a[i..n]). So, our dynamic programming automatically sums up the answers for all possible right borders of the segment we decompose. Transitions in this dynamic programming is easy: we need to see how does the element ai alter the state of decomposition c (let it change it to c′), take the value of dpi+1,c′, and if the element ai forms a new subsequence, let's account for it by increasing dpi,c by n−i+1, because this increase will affect n−i+1 different right endpoints of the segment we decompose.

And now it's easy to see how to add zeroes to our solution. We can just assume they don't change the state of decomposition, they simply add a new subsequence which won't take any other elements. So, in our transitions, processing 0 means that c′=c, but the size of decomposition increases.

To actually get the answer to the problem, we need to consider all possible starting points of the segment, so we sum up dpi,o (where o is the empty vector) for all i∈[1,n].

Solution (BledDest)1766F - MCF

Idea: BledDest

Tutorial1766F - MCFThis problem is solved using minimum cost flows (duh).

Suppose all arcs have even capacity. Then we can just divide each arc's capacity by 2 and solve a usual minimum cost flow problem. However, when we have arcs with odd capacity, it's not that simple. We will deal with them as follows: split an arc with capacity 2k+1 into two arcs: one with capacity 2k, the other with capacity 1, and somehow enforce that the second arc must be saturated. We cannot divide all arcs by 2 now, because that would lead to non-integer capacities; instead, we will exclude these arcs with capacity 1 and somehow handle the fact that they must be saturated, and only then divide all capacities by 2.

Okay, how do we handle the edges we deleted? For each vertex, let's check if the number of such arcs connected to it is even. If it is not — the total flow for this vertex cannot be 0, so it's impossible to find the answer (the only case when it might be possible is if this vertex is the source or the sink; in this case, we need to check that both of these vertices have an odd number of arcs we want to delete connected to them, and consider an additional arc 1→n with capacity 1 and weight 0 to make it even).

If for each vertex, the number of odd arcs connected to it is even, let's consider how much excess flow these arcs bring into the vertices. For example, if a vertex has 4 ingoing odd arcs, it has 4 units of flow going into it, which will be lost if we remove the edges we want to ignore. To handle this, add a new source and a new sink to our network (let's call them s and t), and process excess flow going into the vertex using an arc from s to that vertex (in the previous example, we can add an arc from s to the vertex with capacity 2 — not 4 since we divide all capacities by 2). Similarly, excess flow going outside the vertex can be processed with an arc from that vertex to t. We need to make sure that all these edges must be saturated.

Okay, what about actually running the flow from 1 to n? We can do it as in "flow with lower bounds" problem by adding an arc n→1 with infinite capacity... Wait a minute, this may cause a negative cycle to appear! If your implementation of mincost flow handles them, you can use this approach; but if you don't want to mess with negative cycles, instead do the following:

add an arc s→1 and an arc n→t, both with infinite capacities, to make sure that flow can go from 1 to n;since these arcs don't have to be saturated, but other arcs going from s or into t must be saturated, set the costs of these "other" arcs to −109.Okay, that's it — we just need to find the minimum cost flow in the resulting network. The constraints are low enough so any minimum cost flow algorith can pass.

Educational Codeforces Round 138 Editorial

By awoo, history, 4 months ago, translation, In English1749A - Cowardly Rooks

Idea: BledDest

Tutorial1749A - Cowardly RooksFirst, note that m is always less than or equal to n. If there were at least n+1 rooks on the board, at least two of them would share a row or a column (by pigeonhole principle).

If m<n, then there is always at least one free row and at least one free column. You can move any rook into that row or column.

Otherwise, all rows and columns are taken, so any move will make two rooks share a row or a column, which is prohibited.

Thus, if m=n, then it's "NO". Otherwise, it's "YES".

Overall complexity: O(1) per testcase.

Alternatively, you could check every rook and every possible move.

Overall complexity: O(m2⋅n2) per testcase.

Solution 1 (awoo)Solution 2 (awoo)1749B - Death's Blessing

Idea: BledDest

Tutorial1749B - Death's BlessingNote that whichever order you choose, the total time will always contain all initial health ai, in other words, any answer will contain ∑ni=1ai as its part. So the lower the sum of bi you will add to the answer — the better.

Look at some monster i. If you kill it while it has both left and right neighbor, it will add 2⋅bi to the answer. If it is the first or the last in the row, it will add just bi. And if it is the last monster, it will add 0.

There can be only one last monster, so any other will add at least bi to the answer. And for any chosen last monster l you can find the order that gives exactly bi for all other monsters. For example, you can firstly kill monsters 1,2,…,(l−1), then n,(n−1),…,(l+1) and, finally, moster l.

In other words, if the last monster is the l-th one, the total answer will be equal to ∑ni=1ai+∑ni=1bi−bl. Since we need to minimize answer, we can choose monster with maximum bl.

So, the answer is ∑ni=1ai+∑ni=1bi−maxni=1bi.

Solution (adedalic)1749C - Number Game

Idea: BledDest

Tutorial1749C - Number GameNote that if Bob has increased some element, then Alice can't remove it on the next stages. Obviously, it is more profitable for Bob to "prohibit" the smallest element of the array. Using this fact, we can iterate over the value of k, and then simulate the game process. To simulate the game, we can maintain the set of elements that Alice can remove. On the i-th stage, Alice removes the maximum element x, such that x≤k−i+1, if there are no such elements, then Alice lost. Bob always removes the minimum element of the set.

Thus, the complexity of the solution is O(n2logn) for each test case.

There is another possible solution: we can notice that, if Alice wins, Bob will "prohibit" the elements on positions 1,2,…,k−1 of the sorted array. So, Alice has to delete the next k elements. So, if the segment [k…2k−1] of the sorted array can be deleted by Alice during the game phases, she wins with this value of k.

Solution (Neon)1749D - Counting Arrays

Idea: BledDest

Tutorial1749D - Counting ArraysWe will calculate the answer by subtracting the number of arrays which have only one removal sequence from the total number of arrays. The latter is very simple — it's just m1+m2+⋯+mn.

How do we calculate the number of unambiguous arrays? We can always delete the 1-st element of an array; so, [1,1,1,…,1] is a removal sequence for each array. So, we have to calculate the number of arrays which have no other removal sequences.

How do we check if the array has no removal sequences other than [1,1,…,1]? If, at any time, it's possible to remove some element other than the 1-st from the array, it creates another removal sequence since we can always complete that sequence.

Let's analyze the constraints on each element of the array. a1 can be any integer from 1 to m. a2 should be divisible by 2 (otherwise, we can remove it on the first step). a3 should be divisible by 3 (otherwise, we can remove it on the first step) and by 2 (otherwise, we can remove it on the second step). a4 should be divisible by 2 and 3, but not necessarily by 4 since an element which is divisible by 2 already has a common divisor with 4. And so on — using induction, we can show that the i-th element should be divisible by p1⋅p2⋅p3⋅⋯⋅pk, where p1,p2,…,pk are all of the primes in [1,i]. Obviously, the number of such elements is mp1⋅p2⋅p3⋅⋯⋅pk.

So, we can easily calculate the number of possible elements for each index of the array, and that allows us to count all unambiguous arrays.

Solution (BledDest)1749E - Cactus Wall

Idea: BledDest

Tutorial1749E - Cactus WallIn order to block any path from the top row to the bottom row, you have to build a path from the left side to the right side consisting of '#'. Since two consecutive cacti in a path cannot be placed side by side, they should be placed diagonally (i.e (x,y) should be followed by (x±1,y±1) on the path). So we can rephrase the task as a shortest path problem. The edge weight is 0 if cactus is already in the cell that corresponds to the end of the edge, and 1 otherwise. Don't forget that some cells can't contain a cactus, thus be part of a path, because of the cacti initially placed. The shortest path can be found using Dijkstra's or 0-1 BFS algorithm.

Solution (Neon)1749F - Distance to the Path

Idea: BledDest и adedalic

Tutorial1749F - Distance to the PathFor the purpose of solving the task, let's choose some root in the tree and introduce another operation to the tree: add k to all vertices that are in the subtree of the given vertex v and on the distance d from v. For example, if d=0, it's v itself or if d=1 then it's all children of v.

Let's p[v] be the parent of vertex v, p2[v]=p[p[v]], p3[v]=p[p[p[v]]] and so on (p0[v]=v). So, how to perform this operation? Instead of adding k to all vertices in the subtree, we can add k only to the vertex v. And when we need to get the answer for some vertex u, we will get it from ans[pd[u]].

Of course, since there are different d-s, we'll create different arrays ansd for each possible d. So, the answer for the vertex will be equal to ∑di=0ansi[pi[d]].

Now, let's discuss how to use the introduced operation to perform the given one. We can make the given operation "u v k d" using ours in the following way:

Let's find l=lca(u,v) using any standard algorithm (binary lifting, for example).Let's split all affected vertices in three groups: subtrees of path [v,l) (v inclusive, l exclusive), subtrees of path [u,l) and subtrees of path l,p[l],p2[l],…,pd[l]. Note that in such way all affected vertices belong to at least one group.Let's look at group of path [v,l). The lowest vertices are on distance d from v, the next "level" are on distance d from p[v], the next "level" are on distance d from p2[v] and so on. The last "level" we'll consider in this group is the vertices in the subtree of the child of l on distance d from it. In such a way, all we need to do is add k to all ansd on the path from [v,l).The group of the path [u,l) is handled in the same way.What's left? It's verticesin subtree of l on distances d,(d−1),…,0;in subtree of p[l] on distances (d−1),(d−2),…,0;in subtree of pi[l] on distances (d−i),(d−i−1),…,0;in subtree of pd[l] on distance 0.Note that vertices in subtree of l on distance d−2 are included in vertices in subtree of p[l] on distance d−1. Analogically, vertices on distance d−3 from l are included in vertices on distance d−2 from p[l].Moreover, vertices on distance d−4 from l are included in "d−3 from p[l]" that are included in "d−2 from p2[l]" and so on. In other words, all we need to proccess are vertices:

in subtree of l on distances d and (d−1),in subtree of p[l] on distances (d−1) and (d−2),in subtree of pi[l] on distances (d−i) and (d−i−1).In total, it's at most 2d operations: "add k to some vertex x".As a result, all we need to do is

add k on path from v to some ancestor l of v;add k in some vertex v (can be done as operation 1 on path [v,p[v]));ask value in some vertex v.We can do all of these operations in O(logn) using Fenwick tree (BIT) on tin-s and tout-s (we can get from binary lifting). So the first statement operation will work in O(dlogn) time and the second one — also in O(dlogn).In total, complexity is O(nlogn+mdlogn) time and O(n(logn+d)) space.

P.S.: the second operation can be further optimized to O(d+logn), but it's not really necessary.

Educational Codeforces Round 137 Editorial

By awoo, history, 4 months ago, translation, In English1743A - Password

Idea: fcspartakm

Tutorial1743A - PasswordThere are two possible solutions for the problem.

The first solution is basically brute force. Each password can be obtained from an integer from 0 to 9999. If the number is from 1000 to 9999, then it's already a password of length 4. Otherwise, you have to prepend it with enough zeros so that it becomes length 4.

Then you have to check if the password is valid. First, check if it consists of exactly two different digits: make a set of all its characters (set<char> in case of C++, for example) and check its size. Then check if the first digit of the password appears exactly twice. It would mean that the other digits appears exactly twice as well. Finally, check if neither of the found digits are forbidden.

The second solution is based on combinatorics. First, choose the two digits that will appear in the password: C(10−n,2). Since n digits are prohibited, the remaining 10−n are allowed. Second, choose the positions that will be taken by the first one: C(4,2). The answer is the product of these two values.

Solution 1 (awoo)Solution 2 (fcspartakm)1743B - Permutation Value

Idea: BledDest

Tutorial1743B - Permutation ValueThe subsegment [1], as well as the whole permutation, will always be a permutation, so the value is at least 2. Let's try to find a way to generate a permutation of n elements with value equal to 2.

Every permutation must contain the number 1. Let's try to construct the answer in such a way that if a subsegment contains the number 1, then it also contains the number n (if it is so, it can only be a permutation if it contains all n numbers). If we begin our permutation with the numbers 1 and n, we will reach our goal: the only subsegment which does not contain n but contains 1 is [1], and the only subsegment which contains n and also a permutation is the whole permutation itself. So, any permutation that begins with [1,n…] can be the answer.

Solution (BledDest)1743C - Save the Magazines

Idea: fcspartakm

Tutorial1743C - Save the MagazinesLet's process the boxes from left to right.

Consider the first box. If it has a lid, then you can just add the number of magazines in it to the answer and forget about this box. To be exact, proceed to solve the problem with the first box removed.

If it doesn't have a lid, then look at the next box. If it doesn't have a lid too, then this box can never be covered. Remove it and proceed further.

If the next box has a lid, then look at the next one. Again, if it doesn't have a lid, then these two first boxes are solved independently of everything else. You can cover exactly one of them. Choose the bigger one and remove them both.

To propagate the argument, let's derive a pattern. First, there's a box without a lid. Then some number of boxes with lids in a row. Then a box without a lid again. Among the first box and the box with lids, you can choose exactly one to not be covered. However, that can be any one of them. The best box to be left uncovered is the one with the smallest number of magazines in it.

Thus, the solution is the following. As long as the first box has a lid, keep removing the first box and adding it to the answer. Then, as long as there are boxes left, take the first box and the largest number of consecutive boxes with lids after it (that number might be zero). On that segment, find the minimum value and the sum. Add the sum minus the minimum to the answer, remove the entire segment.

The removals can be done explicitly with a queue or just a reversed vector or implicitly with maintaining a pointer to the first non-removed box.

Overall complexity: O(n).

Solution (awoo)1743D - Problem with Random Tests

Idea: BledDest

Tutorial1743D - Problem with Random TestsThe first observation we need is that we can choose two prefixes of s as the substrings used in forming the results. This can be proved easily: suppose we chose a substring which does not contain the leftmost character of s; if we expand it to the left, the answer won't become worse. So, it is optimal to choose two prefixes of s as the substrings.

Furthermore, one of these prefixes must be s itself: if the leftmost index of 1 is i, the length of the answer won't exceed n−i+1, but the only way to have a 1 in the (n−i+1)-th bit of the answer is to choose a prefix of s where the (n−i+1)-th character (from the right) is 1; and there is only one such prefix of s, which is s itself.

So, now we can solve the problem in O(n2) — try to combine all prefixes of s with s itself, and choose the one that yields the best answer. To speed this up, we need to somehow cut down on the number of prefixes of s we check.

Let's look at the first block of 1's in s. The next character after this block is 0; since we take s as one of the substring, in order to get 1 instead of 0 in the corresponding position of the answer, we need to choose a prefix which has 1 in that position. This 1 represents one of the 1's from the first block of 1's, since only one of them can shift to that position. So, we need to check only the prefixes such that, by using them, we shift some character 1 from the first block to the position of the first 0 after this block. Since the tests are random, the expected length of the first block of 1's is O(1) (furthermore, even the probabiliy that its length is 20 or bigger is about 10−6), so the expected number of prefixes we need to check is also O(1). Thus, the expected runtime of our solution is O(n).

Solution (BledDest)1743E - FTL

Idea: BledDest

Tutorial1743E - FTLAt any time, we have three possible choices: wait and shoot the first laser, the second laser and both lasers. Sometimes it makes sense to wait to both because you can deal s more damage than you would do by shooting both lasers separately.

The first claim: greedy won't work. Maybe there is a sufficiently smart greedy, we weren't able to come up with it. The second claim: bruteforce won't work. The funny thing is that it actually worked on the constraints up to 2000, but again, we couldn't code any sufficiently fast one for 5000.

Thus, let's try some dynamic programming. Since all the times are huge, we'd want to avoid having them as the states. What is small, however, is the durability of the enemy ship and the number of shots we have to make to destroy it.

Ideally, we'd like to have some dp[i] — the smallest time to deal i damage to the enemy ship. This way, dp[n] would be the answer. Sadly, it's not immediately clear how to get rid of reload times completely. There might be states with different times until the charge with the same damage dealt, and we don't know which of those we want to keep.

Thus, let's make the dp state more complicated. Let dp[i] be the smallest time it takes to deal i damage if the last shot was from both lasers at the same time. This way we know the reload times of both lasers — they are full t1 and t2.

dp[0]=0, as moment 0 has both lasers zero charged as if after a shot.

What are the transitions? Well, now we have to shoot each laser multiple times, then wait until both are charged and shoot both. Both lasers can now be considered independent of each other.

Let the time between the previous double shot and the next one be some value t. During this time, it never made sense to wait until shooting each laser. So we waited t1, shot the first laser, waited another t1, shot again, until we couldn't shoot anymore, since the laser wouldn't recharge in time before the double shot. Same for the second laser. Notice that if both tmodt1≠0 and tmodt2≠0, then you could just decrease t by 1 and shoot each laser the same number of times. Thus, only t that are multiples of either t1 or t2 are optimal.

Thus, we can iterate over all possible waiting times t. Just iterate over i⋅t1 and i⋅t2 for all i from 1 to h. Having a fixed t, calculate the number of shots of each laser, calculate the damage, go into the corresponding dp state.

It could also happen that the last shot before destroying the ship wasn't a double one. However, it still follows the same ideas. It means that each laser was shooting non-stop until the ship was destroyed. Thus, the destruction time is still a multiple of either of the reload times.

Overall complexity: O(h2).

Solution (awoo)1743F - Intersection and Union

Idea: BledDest

Tutorial1743F - Intersection and UnionWe will use the Contribution to the Sum technique to solve this problem: for every integer from 0 to 300000, let's calculate the number of ways to choose the operators so it belongs to the result, and add all of the results.

For a fixed integer x, the number of ways to choose the operators so that x belongs to the result can be done as follows: let dpi,f be the number of ways to choose the first i operators so that, after applying them, the resulting set contains x if f=1, and does not contain x if f=0. The transitions from dpi to dpi+1 depend on whether the number x belongs to the segment i+1.

Obviously, this is too slow if we compute the dynamic programming from scratch for every integer x. Instead, we can notice that the transitions from dpi to dpi+1 are linear combinations: both dpi+1,0 and dpi+1,1 are linear combinations of dpi,0 and dpi,1 with coefficients depending on whether the element x belongs to the set or not. So, transitioning from dpi to dpi+1 can be written in terms of multiplying by a 2×2 matrix.

Let's build a segment tree where each vertex stores a transition matrix, and operations are "calculate the product of matrices on a segment" and "replace a matrix at some index". We can build a sequence of these transition matrices for x=0 and store them in the segment tree; for x=1, this sequence of transition matrices will change only in positions j such that either 0 belongs to [lj,rj] and 1 does not belong to it, or vice versa. So, we can go from x=0 to x=1 by replacing these transition matrices in the segment tree. For x=2, the only changes from x=0 are in positions j such that either 1 belongs to [lj,rj] and 2 does not belong to it, or vice versa — and we can replace the matrices in these positions as well. In total, there will be only O(n) such replacements; so, we solve the problem in O(M+nlogM), where M is the constraint on the numbers belonging to the sets.

Solution (BledDest)1743G - Antifibonacci Cut

Idea: BledDest

Tutorial1743G - Antifibonacci CutThe first idea that comes to mind is running some sort of dynamic programming: dpi — the number of ways to cut the string consisting of the first i characters. When we calculate dpi, we need to take the sum of the previous values of dp, and then subtract dpj for every j such that the string from the j-th character (inclusive) to the i-th character (non-inclusive) is a Fibonacci string. Unfortunately, there are two main issues with this solution: firstly, we cannot store the array dp in memory; and secondly, we have to search for the Fibonacci strings ending in a certain index quickly (something like Aho-Corasick could work with a less strict memory limit, but right now we cannot use it).

We will try to resolve both of these issues with the following approach: while we process the characters, we will maintain the list of tuples (j,dpj) such that the string from the j-th character to the current one is a prefix of some Fibonacci string. How do we maintain them?

Every Fibonacci string fi (except for f0) is a prefix of fi+1. So, all Fibonacci strings we are interested in (except for f0 again) are prefixes of the same long Fibonacci string. Suppose a tuple (j,dpj) represents some index j such that the string from the j-th character to the current one is a prefix of that long Fibonacci string. Each time we append a character, we filter this list of tuples by trying to check if this new character matches the next character in the prefix (if it does not, the tuple is discarded). For the tuples that represent the prefixes equal to Fibonacci strings, we need to subtract the value of dpj from the new dp value we are trying to calculate (checking if a prefix is a Fibonacci string is easy, we just need to check its length). How do we check that if we add a character 1 or 0, it is still a prefix? There are two ways to do this:

either generate the first 3⋅106 characters of the long Fibonacci string;or represent the current prefix as the sum of Fibonacci strings fi1+fi2+⋯+fik such that for every j∈[1,k−1], the condition fij>fij+1+1 holds (i. e. the Fibonacci strings we split the current prefix into are arranged in descending order, and there is no pair of equal or adjacent (by index) Fibonacci strings in the split). This representation is very similar to writing an integer in Zeckendorf system. The next character in the prefix depends on whether f1 belongs to this split: if it belongs, it is the last string in the split, so we need to append 0 to transform f1 into f2; otherwise, we need to append 1.Okay, so now we can solve the problem in O(NM) time (where N is the total length of the strings in the input, and M is the size of the list of tuples (j,dpj) we discussed earlier). This actually works since it looks like the size of the list of tuples is bounded as O(logN). Unfortunately, we don't have a strict mathematical proof of this; we checked this by brute force with N up to 3⋅106, so it definitely works under the constraints of the problem.

Educational Codeforces Round 136 Editorial

By awoo, history, 5 months ago, translation, In English1739A - Immobile Knight

Idea: BledDest

Tutorial1739A - Immobile KnightLet's consider some cases.

If at least one of n or m are 1, then all cells are isolated. A knight can't move one in a perpendicular direction.

If at least one of n or m are at least 4, then the knight always has at least one move. No matter where you place it, it can move two cells along the greater of the dimensions and move one in a perpendicular direction, because it's at least 2.

Three cases are left. (2,2), (2,3) and (3,3). For all of these cases, the middle cell is isolated. That cell is (⌊n2⌋+1,⌊m2⌋+1).

Since it doesn't matter which cell you print in the first two cases, you can always print (⌊n2⌋+1,⌊m2⌋+1).

Overall complexity: O(1) per testcase.

Alternatively, you can check every possible cell. Iterate over a cell and check all eight possible knight moves from it. If none are inside the board, the cell is isolated.

Overall complexity: O(nm) per testcase.

Solution 1 (awoo)Solution 2 (awoo)1739B - Array Recovery

Idea: BledDest

Tutorial1739B - Array RecoveryNote that ai=ai−1+di or ai=ai−1−di. Since there is no upper bound for the values of ai, the case where ai=ai−1+di for all i always exists. It remains to check if there are other ways. To do this, it is enough to check whether there is such a position pos that:

pos>1;dpos≠0;the change apos=apos−1+dpos to apos=apos−1−dpos doesn't result in a negative value of apos.The reason for dpos≠0 is that for dpos=0 no matter the plus or minus we choose, the array a doesn't change. If you could change at least one sign to minus, that would be another answer.

Solution (Neon)1739C - Card Game

Idea: BledDest

Tutorial1739C - Card GameThe example tests suggest that there is only one possible distribution with a draw. Let's find out why it is so. We will use a process similar to induction/recursion to distribute the cards between the two players so that the game ends in a draw:

suppose Alex receives the card n. Then he wins since he can play it immediately. So, for the game to result in a draw, Boris must receive the card n.suppose Boris receives the card n−1. Then he wins since he also has the card n, he can use it to answer any first move of Alex, and then win the game by playing n−1. So, for the game to result in a draw, Alex must receive the card n−1.suppose Boris receives the card n−2. Then he wins since he also has the card n: if Alex plays the card n−1, Boris responds with n and then plays n−2; if Alex plays some other card, Boris responds with n−2 and the plays n. So, for the game to result in a draw, Alex must receive the card n−2.and so on.In fact, if Alex receives the card n−1 and Boris receives the card n, Alex must play the card n−1 or something equivalent to it on the first move, and Boris must respond with the card n, so we can consider the game without these two cards with the roles swapped.

So, if we consider the distribution of cards as a string with characters A and B, where A denotes the card belonging to Alex, and B denotes the card belonging to Boris, and the i-th character of the string represents the card n−i+1, the only possible distribution for the draw is BAABBAAB... But there's more to this string representation of the distribution of cards: the first character that is different from this pattern denotes the winner; if the first different character is A in the draw distribution and B in the distribution we consider, the winner is Boris; otherwise, the winner is Alex.

This may lead us to the following ways to count the number of possible distributions which win/lose for Alex:

we can use dynamic programming of the form dpx,y,t, where x is the number of characters A we used, y is the number of characters B we used, and t is 0, 1 or 2 depending on whether our string coincides with the draw string (t=0), differs from it in a way that Alex wins (t=1), or differs from it in a way that Boris wins (t=2); the actual value of dpx,y,t must be the number of ways to reach this state of dynamic programming. The answer then is stored in the states of the form dpn2,n2,t.or we can use combinatorics: let's iterate on the length of the prefix that is common in the draw string and in the string representing the distribution of cards, and then count the number of ways to distribute the remaining characters with a binomial coefficient. To calculate the binomial coefficients, we can use one of the following methods: Pascal's triangle, precalculating factorials and modular inverses to then, or calculating factorials with big integers in Java or Python.Solution 1 (BledDest)Solution 2 (BledDest)1739D - Reset K Edges

Idea: BledDest

Tutorial1739D - Reset K EdgesStart with the following. Let's look at the input format and consider what the operation actually does to it. Since it only changes the parent of some vertex, it modifies only one value in it. Moreover, it just assigns it to 1. Thus, the goal is to assign at most k values of parents to 1 to minimize the resulting height of the tree.

In particular, that implies that we can freely rearrange the operations, since the assignments don't depend on each other.

One more conclusion. Imagine we have already built some answer. One by one, we moved some subtrees to be children of the root. It could happen that we first moved some subtree of a vertex u and then applied the operation to an edge inside the subtree of u. Let's show that it's always possible to rearrange the operations in the answer to avoid that. Just apply the operations in order of decreasing the depth of the vertex u.

If we knew what height h we want to get, we could have been making sure that cut subtree u has height at most h−1 (since it gets increased by 1 when glueing it to the root), then pretending that that subtree doesn't exist anymore.

Moreover, it's always required to cut subtrees with height at most h−1. If you cut a higher subtree, then the answer can't be smaller than h+1, since we rearranged the operation to not touch that subtree anymore.

Well, let's fix that height h if we wanted that. Let's try the solve the opposite problem. How many operations will it require to make the tree height at most h? Obviously, the values for this problem are non-increasing — the greater we allow the height to be, the less operations it will require. Thus, we will be able to apply binary search to it to find the smallest height we can achieve with at most k operations.

Now we want to be choosing the subtrees of height at most h−1 repeatedly and cutting them off until the height of the tree becomes at most h.

Let's think greedily. If the height of the tree is not at most h yet, then there exists a vertex with the depth greater than h. Let's look at the deepest of them. That leaf has to be cut in some subtree. Otherwise, the tree won't become any less higher. What subtree is the best for it? What options do we have? That vertex itself and all its parents up until h−1 above. It's always optimal to cut the highest of them — the (h−1)-st parent, since it will remove at least all the vertices of any other cut and some other vertices along with them. It's also always possible to remove the (h−1)-st parent, since it will always have height exactly h−1. The vertex we are looking at is the deepest in the entire tree — there are no deeper vertices in the subtree of the (h−1)-st parent.

Thus, the strategy is to keep cutting the (h−1)-st parent of the deepest vertex until the tree becomes at most h height.

Now about the implementation details.

First, we can process the vertices from the deepest upwards in their order in the original tree. The operation only removes some vertices but doesn't change the depth of the remaining ones. For example, you can do a bfs from the root to find the order.

Now the (h−1)-st parent. Let's find it for each vertex before starting the process. Run a dfs and maintain the stack of the ascendants. When going down the child, append it to the stack. What exiting, pop from the stack. Now you can just look at the (h−1)-st element from the top of the stack. To be able to do that, simulate the stack with a vector (C++) or a list (Python).

Finally, we would have to determine if the current vertex in the order is removed or not. For that, we could maintain a boolean array used for the removed vertices. Once you apply the operation, run the dfs from the removed vertex and mark all the newly removed descendants of it in used. If you don't go into already marked vertices, there will be no more than n calls of the dfs.

The number of cut vertices is the answer for the fixed height h.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1739E - Cleaning Robot

Idea: BledDest

Tutorial1739E - Cleaning RobotWhy did the author choose the width of the hallway to be only 2? Well, in that case you can show that the robot will never move to the left while cleaning. That is not true on width 3 already.

When does the robot break? Let the robot currently be in the cell (j,i) (0-indexed) and the next column with a dirty cell be nxti (possibly, nxti=i). The robot breaks only if both (1−j,nxti) and (j,nxti) are dirty.

That helps us to do a dynamic programming solution. Since we can only care about O(1) next columns, we would want to have some dp[i][j] — the largest number of dirty cells we can leave to the robot if we processed the first i columns of the hallway and are currently standing in the j-th row of the i-th column. Maybe with some additional states of the current or the next columns.

We want the dp to maintain the invariant that everything to the left of the i-th column is cleaned in such a way the robot can reach the cell (j,i). We can choose when to fix the i-th column: either maintain it being correct prior to entering the state or handling it in the transition to the next one. I chose the former option. There probably exists a million different dps that work, I'll describe the one I did.

Let dp[i][j][f] be the largest number of dirty cells that we can leave to the robot if:

we fixed which of the dirty cells in the first i columns, inclusive, are cleaned by hand;the robot reaches the cell (j,i) from the left;f is true if the cell in the opposite row of the i-th column is dirty.The transitions handle what to do with the dirty cells in the (i+1)-st column and where the robot goes based on that.

In particular, there are the following transitions:

if f is true, then we have to clean the cell (j,i+1), and the robot will move into (1−j,i+1) — otherwise the robot breaks from having two options;if f is false, then let's say that the robot doesn't break immediately but moves into the next column in a unique way: it moves horizontally first, then possibly vertically;we can leave the next column as is, and the robot will move into (j,i+1) if the cell (1−j,i+1) is clean, or (1−j,i+1) if it's dirty;if f is false, then we can clean the cell (1−j,i+1), and the robot will move into (j,i+1).Since we maintained the invariant that the i-th column is valid, we can update the answer from all four states in the last column.

Overall complexity: O(n).

Solution (awoo)1739F - Keyboard Design

Idea: BledDest

Tutorial1739F - Keyboard DesignFor each word, let's consider a graph on 12 vertices where the i-th and the j-th vertices are connected by an edge iff the i-th character of the alphabet is adjacent to the j-th character of the alphabet in this string. Obviously, this graph is connected (except for the isolated vertices). If there is a vertex of degree 3 or more in this graph, or if there is a cycle in this graph, it is impossible to design a keyboard to type the word easily: in the first case, the letter represented by that vertex must have at least three neighbors on the keyboard, but can have only at most two; in the second case, the keyboard must be cyclic (and it is not). So, the word can be typed easily only if the graph representing it consists of one path and several isolated vertices.

Let's write the letters along the path we constructed for the word in a single string. For example, for the word abacabacd, we get edges ab, ac and cd in the graph, so the letters along the path are either dcab or bacd (and, obviously, one can be obtained from the other by reversing the string). Let f(s) and f′(s) be the two strings we obtain from the word s using this method. Now, we claim that the word s can be typed easily if and only if one of these two strings (f(s) and f′(s)) is a substring of the keyboard — this would mean that every pair of letters that should be on adjacent positions are actually on adjacent positions.

Okay, now we construct f(si) and f′(si) for each word, and our goal is to find the permutation of the first 12 characters of Latin alphabet such that the sum of ci over all words having either f(si) or f′(si) as a substring is the maximum possible. There are two key observations that allow us to solve this problem:

f(si) and f′(si) cannot be the substrings of the same keyboard (the proof is simple: if f(si) is a substring, its first character must be before its second character; and if f′(si) is a substring, its second-to-last character (which is the second character of f(si)) must be before its last character (which is the first character of f(si));neither f(si) nor f′(si) can appear in the keyboard twice (it's obvious since the keyboard is a permutation).So, we can reformulate the problem as follows: let ci be the cost of the string f(si) and the cost of the string f′(si) as well; find the permutation of the first 12 characters of the Latin alphabet so that its cost (which is the sum of costs of its substrings) is the maximum possible. To solve this problem, we can store the strings in an Aho-Corasick automaton, and for every state of the automaton, precalculate the total cost of all string ending in this state (that is, the cost of this state and all states reachable from it via the suffix links). Then run a dynamic programming of the form dpmask,v — the maximum possible cost of a partial keyboard if we used a mask of characters and the Aho-Corasick automaton is currently in the state v. This dynamic programming runs in O(2K⋅K⋅A), where K is the size of the alphabet (12), and A is the size of the automaton (up to 4000).

Educational Codeforces Round 135 Editorial

By awoo, history, 5 months ago, translation, In English1728A - Colored Balls: Revisited

Idea: BledDest

Tutorial1728A - Colored Balls: RevisitedLet's prove that the color with the maximum value of cnt is one of the possible answers.

Let the color x have the maximum value of cnt; if there are several such colors, choose any of them. Let's keep taking the balls of two different colors out of the bag without touching the balls of color x for as long as possible.

After such operations, two cases exist. In one case, only balls of color x are left — then everything is fine. In other case, there are balls of color x and some color y (let cnty be the remaining number of balls of this color). Since initially cntx was one of the maximums, cnty≤cntx. However, the number of remaining balls is odd, which means cnty≠cntx and cnty<cntx. Therefore, we can keep taking the balls of colors y and x until only balls of color x are left.

Solution (Neon)1728B - Best Permutation

Idea: BledDest

Tutorial1728B - Best PermutationLet xi be the value of the variable x after i steps. Note that xn−1 should be less than pn for xn to be not equal to 0. It means that xn does not exceed 2pn−1. It turns out that for n≥4 there is always a permutation such that xn is equal to 2n−1.

The only thing left is to find out how to build such a permutation. There are many suitable permutations, let's consider one of the possible options. For an even n, a suitable permutation is [2,1,4,3, dots,n−2,n−3,n−1,n]. You can see that x in such a permutation changes as follows: [0,2,0,4,0,…,n−2,0,n−1,2n−1]. For an odd n, there is a similar permutation [1,3,2,5,4,…,n−2,n−3,n−1,n], where x changes as follows: [0,1,4,0,5,0,…,n−2,0,n−1,2n−1].

Solution (Neon)1728C - Digital Logarithm

Idea: BledDest

Tutorial1728C - Digital LogarithmFirst, why can you always make the arrays similar? Applying a digital logarithm to any number will eventually make it equal to 1. Thus, you can at least make all numbers into 1s in both arrays.

Then notice the most improtant thing — applying the digital logarithm to a number greater than 1 always makes this number smaller.

Thus, if a number appears in only one of the arrays, you will have to do one of the followings two things:

decrease some greater number to make it equal to this one;decrease this number.What if there is no greater number at all? This is the case for the largest number in both arrays altogether. If it appears in only one of the arrays, you must always decrease. If it appears in both, though, why decrease it further? Worst case, you will decrease it in one array, then you'll have to decrease it in the other array as well. This is never more optimal than just matching one occurrence in both arrays to each other and removing them from the arrays.

So, the proposed solution is the following. Consider the largest element in each array. If they are equal, remove both. If not, apply digital logarithm to the larger of them. Continue until the arrays are empty.

What's the estimated complexity of this algorithm? Each number in the first array will be considered at most the number of times you can decrease it with a digital logarithm operation plus one. That is at most 2+1 — a number greater than 9 always becomes a single digit and a single digit always becomes 1. Same goes for the second array. So the complexity is basically linear.

To implement it efficiently, you will have to use some data structure that provides three operations:

peek at the maximum;remove the maximum;insert a new element.The perfect one is a heap — priority_queue in C++.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1728D - Letter Picking

Idea: BledDest

Tutorial1728D - Letter PickingWhat do we do, when the array loses elements only from the left or from the right and the constraints obviously imply some quadratic solution? Well, apply dynamic programming, of course.

The classic dp[l][r] — what is the outcome if only the letters from positions l to r (non-inclusive) are left. dp[0][n] is the answer. dp[i][i] is the base case — the draw (both strings are empty). Let −1 mean that Alice wins, 0 be a draw and 1 mean that Bob wins.

How to recalculate it? Let's consider a move of both players at the same time. From some state [l;r), first, Alice goes, then Bob. The new state becomes [l′,r′), Alice picked some letter c, Bob picked some letter d. What's that pick exactly? So, they both got a letter, prepended it to their own string. Then continued the game on a smaller string s and prepended even more letters to the string. Thus, if we want to calculate [l,r) from [l′,r′), we say that we append letters c and d. Now it's easy. If dp[l′][r′] is not a draw, then the new letters change nothing — the result is still the same. Otherwise, the result of the game is the same as the comparison of letters c and d.

How to perform both moves at once? First, we iterate over the Alice's move: whether she picks from l or from r. After that we iterate over the Bob's move: whether he picks from l or from r. Since we want dp[l][r] to be the best outcome for Alice, we do the following. For any Alice move, we choose the worse of the Bob moves — the maximum of dp[l′][r′]. Among the Alice's moves we choose the better one — the minimum one.

Overall complexity: O(n2) per testcase.

Solution (awoo)1728E - Red-Black Pepper

Idea: BledDest

Tutorial1728E - Red-Black PepperLet's start by learning how to answer a query (1,1) — all red pepper and black pepper options are available.

Let's iterate over all options to put the peppers and choose the maximum of them. First, let's use the red pepper for all dishes. Now we want to select some k of them to use black pepper instead of red pepper. Which ones do we choose? When we switch from the red pepper to the black pepper, the total tastiness changes by −ai+bi for the i-th dish. They are completely independent of each other, so we want to choose k largest of these values.

Let d1,d2,…,dn be the sequence of values of −ai+bi in a non-increasing order.

Thus, k black peppers will yield the result of ∑i=1nai+∑i=1kdi. We can answer a query (1,1) by looking for a maximum in the sequence.

Now consider an arbitrary query. Let p1,p2,…,pt be all options for the amount of available black peppers for the query. Naively, we could iterate over all of them and choose the maximum one.

However, notice an interesting thing about the sequence of the answers. By definition, it is non-strictly convex. In particular, one idea that can be extracted from this is the following. Find the position of an arbitrary maximum in this sequence. Then everything to the left of is is non-increasing. Everything to the right of it is non-increasing.

Thus, for a query, it's enough to consider only two options: the one closest to the maximum from the left and from the right.

Now we only have to learn how to get these options fast enough. For a query (x,y) we want to solve what's called a diophantine equation ax+by=n. An arbitrary solution can be obtained by using extended Euclid algorithm. Let it be some (a,b). Then we would want to check the answer for ax black peppers. The amount of solutions to the equation is either infinite or zero. If it's infinite, all solutions will be of the form ax+k⋅lcm(x,y) for any integer k. Remember that not all the solutions will be in a range [0,n].

Finally, find the two solutions that are the closest to the maximum, check that they are in the range [0,n] and print the best answer of them.

Overall complexity: O(nlogn+qlogX).

Solution (awoo)1728F - Fishermen

Idea: BledDest

Tutorial1728F - FishermenSuppose we have fixed some order of fishermen and calculated the values of bi. Then, we have the following constraints on bi:

all values of bi are pairwise distinct;for every i, ai divides bi.Not every possible array b meeting these constraints can be achieved with some order of fishermen, but we can show that if we choose an array b with the minimum possible sum among the arrays meeting these two constraints, there exists an ordering of fishermen which yields this array b. The proof is simple — suppose the ordering of fishermen is the following one: the first fisherman is the one with minimum bi, the second one — the one with the second minimum bi, and so on. It's obvious that if we generate the values of b according to this order, they won't be greater than the values in the array we have chosen. And if some value is less than the value in the chosen array b, it means that we haven't chosen the array with the minimum possible sum. So, we can rephrase the problem as the following one: for each ai, choose the value of bi so that it is divisible by ai, all bi are distinct, and their sum is minimized.

Using the pigeonhole principle, we can show that for every ai, we need to consider only the values of bi among [ai,2⋅ai,3⋅ai,…,n⋅ai]. So, we can formulate the problem as an instance of the weighted bipartite matching: build a graph with two parts, where the left part contains n nodes representing the values of ai, the right part represents the values of the form k⋅ai where 1≤k≤n, and there exists an edge between a vertex in the left part representing the number x and a vertex in the right part representing the number y with cost y if and only if y=k⋅x for some integer k∈[1,n]. Note that we don't add the edge if k>n because we need to ensure that the size of the graph is O(n2).

Okay, now we need to solve this weighted matching problem, but how? The number of vertices is O(n2), and the number of edges is O(n2) as well, so mincost flow will run in O(n4) or O(n3logn), which is too much. Instead, we can notice that the cost of the edges incident to the same vertex in the right part is the same, so we can swap the parts of the graph, sort the vertices of the new left part (representing the numbers k⋅ai) according to their costs, and run the classical Kuhn's algorithm in sorted order. Kuhn's algorithm in its original implementation will always match a vertex if it is possible, so it obtains the minimum total cost for the matching if we do it in sorted order.

But this is still O(n4)! What should we do? Well, there are some implementations of Kuhn's algorithm which can run on graphs of size about 105 (sometimes even 106). Why can't we use one of these? Unfortunately, not all optimizations that can be used in Kuhn's algorithm go together well with the fact that the vertices of the left part have their weights. For example, greedy initialization of matching won't work. So we need to choose optimizations carefully.

The model solution uses the following optimization of Kuhn's algorithm: if you haven't found an augmenting path, don't reset the values representing which vertices were visited by the algorithm. With this optimization, Kuhn's algorithm works in O(M(E+V)), where M is the size of the maximum matching, E is the number of edges, and V is the number of vertices. So, this results in a solution with complexity of O(n3).

I think it's possible to show that some other optimizations of Kuhn can also work, but the one I described is enough.

Solution (BledDest)1728G - Illumination

Idea: BledDest

Tutorial1728G - IlluminationLet's start without the queries. How to calculate the number of ways for the given n lanterns?

First, it's much easier to calculate the number of bad ways — some point of interest is not illuminated. If at least one point of interest is not illuminated, then all lanterns have power lower than the distance from them to this point of interest. More importantly, it's less than d. Thus, the number of good ways is (d+1)n minus the number of bad ways.

Let's use inclusion-exclusion. For a mask of non-illuminated points of interest, let's calculate the number of ways to assign the powers to the lanterns in such a way that at least these points of interest are not illuminated. All other points can be either illuminated or not. Let's call it ways[mask]. With the values for all masks, the answer is the sum of ways[mask]⋅(−1)popcount(mask) over all masks.

How to calculate the value for the mask? First, let's do it in O(nm) for each mask. Each lantern can have any power from 0 to the distance to the closest point of interest inside the mask non-inclusive. Thus, we can iterate over the lanterns and find the closest point to each of them, then multiply the number of ways for all lanterns.

Let's calculate it the other way around. Initialize the answers for the masks with 1. Then iterate over the lantern and the point of interest that will be the closest non-illuminated one to this lantern. Let the distance between them be some value d. Which masks will this pair affect? Let the lantern be to the right of that point of interest. The opposite can be handled similarly.

All points to the left of the chosen point can be in either state. All points between the chosen one and the lantern must be illuminated. All points to the right of the lantern and with distance smaller than d must also be illumunated. All point to the right of these can be in either state. Thus, the masks look like "**..**1000..000**..**", where 1 denotes the chosen non-illuminated point.

All masks that correspond to this template will be multiplied by d. You have to be careful when there are two points of interest with the same distance d to some lantern — one to the left of it and one to the right of it. In particular, in one case, you should force illumination on all points with distance <d. In another case, you should force illumination on all points with distance ≤d.

How to multiply fast enough? We'll use a technique called sum-over-subsets. Let's express the template in terms of submasks. For a template "***100000***", all submasks of "111100000111" will be multiplied by d. However, we accidentally multiplied masks of form "***000000***" too. Let's cancel them by dividing the submasks of "111000000111" by d. Record all multiplications for all pairs, them force push them into submasks with sum-over-subsets (well, product-over-subsets in this case :)).

Now we have the values of ways[mask] for all masks in basically O(nm+2m⋅m), give or take the time to find the points that must be forced illuminated (extra O(logm) from lower_bound or two pointers, which is not really faster).

Now for the queries. How does the answer change after an extra lantern is added? Again, let's iterate over the closest point of interest and find the mask template. All masks corresponding to this template will get multiplied by d. Thus, the answer will change by the sum of values of these masks, multiplied by d, including the inclusion-exclusion coefficient. How to handle that? Well, yet another sum-over-subsets. Just collect the sum of values over the submasks beforehand and use these during the query. That gives us an O(m) per query.

Overall complexity: O(nm+qm+2m⋅m).

Educational Codeforces Round 134 Editorial

By awoo, history, 6 months ago, translation, In English1721A - Image

Idea: BledDest

Tutorial1721A - ImageThere are some solutions based on case analysis, but in my opinion, the most elegant one is the following:

Let's pick a color with the maximum possible number of pixels and repaint all other pixels into it. We will try to pick all pixels of some other color and repaint them in one operation, and we can ignore the constraint that we can repaint no more than 2 pixels, since we will never need to repaint 3 or 4 pixels in one operation. So, the number of operations is just the number of colors other than the one we chosen, or just d−1, where d is the number of different colors in the image. To calculate this, we can use a set or an array of size 26, where we mark which colors are present.

Solution (BledDest)1721B - Deadly Laser

Idea: BledDest

Tutorial1721B - Deadly LaserFirst, let's determine if it's possible to reach the end at all. If the laser's field doesn't span until any wall, then it's surely possible — just stick to the wall yourself.

If it touches at most one wall, it's still possible. If it's the bottom wall or the left wall, then take the path close to the top and the right wall. Vice versa, if it's the top wall or the right wall, then take the path close to the bottom and the left wall.

What if both of these paths are locked? That means that the laser touches at least two walls at the same time: the top one and the left one, or the bottom one and the right one. Turns out, it's completely impossible to reach the end in either of these two cases. Just draw a picture and see for yourself.

Thus, we can always take at least one of the path sticking to the walls. The distance from the start to the end is |n−1|+|m−1|, and both of these paths are exactly this long. So the answer is always either -1 or n+m−2.

To check if the laser touches a wall with its field, you can either use a formula or check every cell adjacent to a wall.

Overall complexity: O(1) or O(n+m) per testcase.

Solution (awoo)1721C - Min-Max Array Transformation

Idea: BledDest

Tutorial1721C - Min-Max Array TransformationFor the start, let's note that ai≤bi for each i. Otherwise, there is no way to get b from a.

Firstly, let's calculate dmini for each i. Since all di≥0 then bj is always greater or equal than ai you get it from. So, the minimum di would come from lowest bj that still ≥ai. Since b is sorted, we can find such bj with lower_bound in O(logn).

Let's prove that we can build such d that transforms ai to bj we found earlier. Let's just make dk=bk−ak for k∈[1..j)∪(i..n]; dk=bk+1−ak for k∈[j..i) and di=bj−ai. It's easy to see that all di are non-negative, so such d is valid.

Now, let's calculate dmaxi. Suppose, we transform ai to bj for some j≥i. It's not hard to prove that the "proving" array d may be constructed in the similar way: dk=bk−ak for k∈[1..i)∪(j..n]; dk=bk−1−ak for k∈(i..j] and di=bj−ai.

In order to build such array d, you need bk−1≥ak for each i∈(i..j]. In other words, if there is some position l such that l>i and bl−1<al you can't choose j such that j≥l. It means that we can iterate i in descending order and just keep track of leftmost l≥i with bl−1<al. Then, dmaxi is equal to b[l−1]−a[i] (or b[n]−a[i] if there are no such l).

The resulting complexity is O(nlogn) because of the first part. But it can be optimized to O(n) if we use two pointers instead of lower_bound.

Solution (adedalic)1721D - Maximum AND

Idea: BledDest

Tutorial1721D - Maximum ANDWe will build the answer greedily, from the highest significant bit to the lowest one. Let's analyze how to check if the answer can have the highest bit equal to 1. It means that every value in c should have its highest bit equal to 1, so for every i, exactly one of the numbers {ai,bi} should have this bit equal to 1. For both of the given arrays, we can calculate how many elements have which value of this bit, and then the number of elements with 1 in this bit in the array a should be equal to the number of elements with 0 in the array b (and the same for elements with 0 in a and elements with 1 in b). If these values are equal, it means that the elements of a and b can be matched in such a way that in every pair, the XOR of them has 1 in this bit. If it is so, then the highest bit of the answer is 1, otherwise it is 0.

Okay, then let's proceed to the next bit. Should we just do the same to check if this bit can be equal to 1 in the answer? Unfortunately, that's not enough. Let's look at the case: a=[3,0], b=[2,1]. We can get the value 1 in the 0-th bit or in the 1-st bit, but not in both simultaneously. So, for the next bit, we need to make sure that not only we can get 1 in the result, but we can also do this without transforming some of the 1-s to 0-s in the higher bits. If it is impossible, it doesn't matter if we can get 1 in the current bit since it will be suboptimal, so we have to use an ordering that gets 0 in this bit.

In general case, it means that we have to solve the following subproblem: check if we can obtain 1 in several bits of the answer; let these bits be {x1,x2,…,xk} (x1 to xk−1 are the bits that we have already checked; xk is the new bit we are trying to check). Let mask be the number that has 1 in every bit xi and 0 in every other bit. The elements should be matched in such a way that (ai&mask)⊕(bi&mask)=mask. If we group all numbers from a and from b according to the value of ai&mask (or bi&mask), then for every group of elements from a, there is a corresponding group in b such that we can match the elements from the first group only with the elements from the second group. So, if for every such group, its size in a is equal to the size of the corresponding group in b, then we can set all bits from {x1,x2,…,xk} to 1 simultaneously.

Some implementation notes: if the number of bits we need to check is big, the number of groups can become too large to handle all of them (since it is 2k). So, to store the number of elements in each group, we should use some associative data structure, like, for example, std::map in C++. If you use a map, splitting elements into groups will be done in O(nlogn), so in total, you will get complexity of O(nlognlogA), where A is the maximum possible value in the input.

Solution (Neon)1721E - Prefix Function Queries

Idea: BledDest

Tutorial1721E - Prefix Function QueriesWhat's the issue with calculating the prefix function on the string s and then appending the string t with an extra |t| recalculations? Calculating prefix function is linear anyway. Well, it's linear, but it's also amortized. So while it will make O(n) operations for a string in total, it can take up to O(n) on every particular letter. These particular letters can appear in string t, making the algorithm work in O(q⋅(|s|+|t|)).

Let's analyze the classic way to calculate the prefix function. To append a character to the string and calculate the new value of the prefix function, you have to do the following:

take the longest proper prefix of a string before appending the letter, which is also a suffix;if the letter right after it is the same as the new one, then the new value is length of it plus one;if it's empty, then the new value is 0;otherwise, take its longest proper prefix and return to step 2.Basically, from having the value of the prefix function of the string and the new letter, you can determine the new value of the prefix function.

If |t| was always equal to 1, then you would only want to try all options for the next letter after a string.

That should remind you of a structure known as prefix function automaton. Its states are the values of the prefix function, and the transitions are appending a letter to a string with a certain value of the prefix function.

So you can append a letter in O(1) if you have an automaton built on the string s. However, you can't just append more letters after one — you don't have the automaton built this far.

You can follow two paths.

The first one is to jump with a regular way of calculating the prefix function until you reach the state of the automaton which exists.

The second one is to continue building the automaton onto the string t, calculating the prefix function along the way. Appending a layer to the automaton takes O(AL) non-amortized. After you calculated everything you needed, pop the states back to the original.

Overall complexity: O(|s|⋅AL+q⋅|t|) or O(|s|⋅AL+q⋅|t|⋅AL).

Solution (awoo)1721F - Matching Reduction

Idea: BledDest

Tutorial1721F - Matching ReductionLet's start by finding the maximum matching in the given graph. Since the constraints are pretty big, you need something fast. The model solution converts the matching problem into a flow network and uses Dinic to find the matching in O(m1.5), but something like heavily optimized Kuhn's algorithm can also work.

Okay, then what about finding the minimum possible number of vertices to delete in order to reduce the maximum matching? We claim that it is always enough to remove one vertex, and the proof will also provide a way to quickly search for such vertices.

Let's recall that the size of the maximum matching is equal to the size of the minimum vertex cover (this only works in bipartite graphs). So, we will try to find a way to reduce the minimum vertex cover by 1, and it's actually pretty easy — just remove any vertex belonging to the vertex cover; it's obvious that it reduces the vertex cover by 1, and the maximum matching by 1 as well. So, we can find the minimum vertex cover in the graph using the standard algorithm to convert the MM into MVC (or, if you're using Dinic to find the maximum matching, you can represent the minimum vertex cover as the minimum cut problem), and for each query of type 1, just take a vertex from the vertex cover we found.

Now the only thing that's left is discussing how to maintain the structure of the maximum matching in the graph. In fact, it's quite easy:

on the one hand, since we remove the vertices belonging to the minimum vertex cover, every edge (including the edges from the matching) will be incident to one of the vertices we will remove;on the other hand, due to the definition of the maximum matching, there is no vertex that is incident to two or more edges from the maximum matching;so, every vertex from the vertex cover has exactly one edge from the maximum matching that is incident to it, and when we remove a vertex, we can simply remove the corresponding edge from the maximum matching.So, the only thing we need to do is to maintain which edge from the matching corresponds to which vertex from the minimum vertex cover, and it will allow us to maintain the structure of the maximum matching — and since these "pairs" don't change when we remove a vertex, it is enough to get this information right after we have constructed the maximum matching in the given graph; we won't need to rebuild it.

Educational Codeforces Round 133 Editorial

By awoo, history, 6 months ago, translation, In English1716A - 2-3 Moves

Idea: vovuh

Tutorial1716A - 2-3 MovesIf n=1, the answer is 2 (we can't get 1, so we can move by 3 to the right and by 2 to the left). If n=2 or n=3, the answer is obviously 1. Otherwise, the answer is always ⌈n3⌉. We can't get the answer less than this value (because we need at least ⌈n3⌉ moves to get to the point greater than or equal to n) and we can always get this answer by the recurrence.

Solution (vovuh)1716B - Permutation Chain

Idea: BledDest

Tutorial1716B - Permutation ChainIdeally, we would want the fixedness values to be n,n−1,n−2,…,0. That would make a chain of length n+1.

However, it's impossible to have fixedness of n−1 after one swap. The first swap always makes a permutation with fixedness n−2.

Okay, how about n,n−2,n−3,…,0 then? That turns out to always be achievable.

For example, swap elements 1 and 2, then elements 2 and 3, then 3 and 4 and so on.

Overall complexity: O(n2) per testcase.

Solution (awoo)1716C - Robot in a Hallway

Idea: BledDest

Tutorial1716C - Robot in a HallwayLet's first consider the possible paths across the grid that visit all cells. You can immediately think of two of them. The first one is: go right to the wall, turn into the other row and return. Let's call it a hook. The second one is: go down, go right, go up, go right and so on. Let's call it a snake.

Turns out, these two are basically the two extremes of all paths. You can start with a snake and turn into a hook when you wish. You can see that once you move right twice in a row, you can only continue with a hook. And as long as you didn't move right twice, you are just doing a snake.

Let's fix some path across the grid. What will its minimum time be? Calculate it iteratively. If you want to enter the next cell, and it's still locked, wait until it isn't. So there are some seconds of waiting (possibly, zero) before each cell.

However, why not instead do the following. Let's calculate the sum of waiting time required and wait for that amount of seconds before starting to move. All cells will be visited at the same time as before or even later. Thus, they will surely be unlocked if they were in the original plan.

So the goal is to calculate the minimum amount of time required to wait in the start, then add the movement time to it.

Once again, the path is fixed. Let the k-th cell of the path be (xk,yk). If you start after waiting for t seconds, then you reach the k-th cell at time t+k (k is 1-indexed). Thus, the k-th cell should have axk,yk≤t+k−1. If all cells satisfy this condition, then the path can be done after waiting for t seconds at the start.

Let's rewrite it into t≥axk,yk−k+1. So, the condition tells us that t should be greater or equal than this value for all cells. In other words, t should be greater or equal than the maximum of the values over all cells.

Study the formula. Imagine we have some path with a known length and want to append a cell to it. That's pretty simple. Just update the maximum with the value with the corresponding cell and increase the length.

What if we wanted to prepend a cell to it? Turns out, it's not that hard as well. Every cell in the path gets its value k increased by 1. From the formula, you can see that this actually decreases the value of each cell by 1. So the maximum decreases by 1 as well. The only thing left is to update the maximum with the value of the new first cell. Well, and increase the length again.

Finally, let's learn how to choose the best path. We can iterate over the length of the snake part. The hook part is determined uniquely.

It's easy to maintain the maximum on the snake. Just append the new cell to the path.

How to glue up the hook part to that?

Well, actually, realize that the formula allows us to glue up two paths into one. Let path 1 have length n1 and maximum mx1 and path 2 have length n2 and maximum mx2. To make path 2 start after path 1, we just decrease its maximum by n1. The resulting path has length n1+n2 and maximum max(mx1,mx2−n1).

Let's look closer into what the hooks look like. They start in some column j, traverse all the way right, then left up to the same column j. If the snake part took both cells in its last column, then that's it. Otherwise, the hook has to take the final cell in the last column — column j−1.

If we manage to precalculate something for hooks that start in some column j and end in column j, then we will be able to use that. Appending the final cell is not a hard task, since we know its index in the path (k=2⋅m).

Let sui,j be the waiting time required for a hook that starts in cell (i,j) and ends in a cell (3−i,j) as if the path started with the hook (cell (i,j) is the first one).

sui,j can be calculated from sui,j+1. Prepend it with a cell (i,j) and append it with a cell (3−i,j).

The only thing left is to find the best answer. I found the most convenient to start with a snake of length 1 (only cell (1,1)) and progress it two steps at the time:

update the answer;progress the snake to the other cell of the current column;update the answer;progress the snake into the next column.Overall complexity: O(m) per testcase.

Solution (awoo)1716D - Chip Move

Idea: BledDest

Tutorial1716D - Chip MoveLet's calculate dynamic programming dps,i — the number of ways to achieve i in s moves. From the state (s,i), you can make a transition to the states (s+1,j), where i<j and j−i is divisible by k+s.

Let's try to estimate the maximum number of moves, because it seems that there can't be very many of them. For m moves, the minimum distance by which a chip can be moved is k+(k+1)+⋯+(k+m−1) or (k+k+m−1)⋅m2. From here one can see that the maximum number of transitions does not exceed 2n−−√ (maximum at k=1). So it remains to make transitions in dynamic programming faster than O(n) from a single state for a fast enough solution. Let's use the fact that j≡i(modk+s). Let's iterate over the value of j and maintain the sum of dynamic programming values with smaller indices for each remainder modulo k+s in a separate array.

The final complexity of such a solution is O(nn−−√).

It remains to solve the memory problem, because with the existing limits, it is impossible to save the entire dp matrix of size n32. However, this is easy to solve if you notice that only the previous layer is used for transitions in dp, i.e. it is enough to store dps to calculate dps+1.

Solution (Neon)1716E - Swap and Maximum Block

Idea: BledDest

Tutorial1716E - Swap and Maximum BlockLet's carefully analyze the operation denoted in the query. Since the length of the array is always divisible by 2k+1, every element will be swapped with some other element. The elements can be split into two groups — the ones whose positions increase by 2k, and the ones whose positions decrease by 2k.

Let's find some trait of the elements which will allow us to distinguish the elements of one group from the elements of the other group. The first 2k elements will be shifted to the right, the next 2k elements will be shifted to the left, the next 2k elements will be shifted to the right, etc. If we look at the binary representations of integers 0,1,…,n−1, then we can see that the first 2k elements have 0 in the k-th bit, the next 2k elements have 1 in the k-th bit, the next 2k elements have 0 in the k-th bit, and so on. So, if we consider the positions of elements as 0-indexed, then the operation can be described as follows: "Let the position of the element be i. If the k-th bit in i is 0, i gets increased by 2k, otherwise i gets decreased by 2k". What does it look like? Actually, it is just i⊕2k (where ⊕ denotes XOR).

So, each query can be represented as "swap ai with ai⊕x for some integer x". The combination of two queries can also be represented with a single query; in fact, the state of the array can be denoted as the XOR of all 2k from the previous queries.

Now, let's try to solve the following problem: for every x∈[0,2n−1], calculate the maximum sum of subsegment if every element ai is swapped with ai⊕x. To solve this problem, we can use a segment tree.

First of all, we need to understand how to solve the problem of finding the maximum sum on subsegment using a segment tree. To do this, we should store the following four values in each vertex of the segment tree:

sum — the sum of elements on the segment denoted by the vertex;pref — the maximum sum of elements on the prefix of the segment denoted by the vertex;suff — the maximum sum of elements on the suffix of the segment denoted by the vertex;ans — the answer on the segment.If some vertex of the segment tree has two children, these values for it can be easily calculated using the values from the children. So, we can "glue" two segments represented by the vertices together, creating a new vertex representing the concatenation of these segments.

Okay, but how do we apply XOR to this? For every vertex of the segment tree, let's create several versions; the x-th version of the vertex v represents the segment corresponding to this vertex if we apply swapping query with x to it. For a vertex v representing the segment of length 2k, we can use the following relation to get all its versions (here, we denote t(v,x) as the x-th version of v, and vl and vr as the children of v):

if x≥2k−1, then t(v,x)=combine(t(vr,x−2k−1),t(vl,x−2k−1));else t(v,x)=combine(t(vl,x),t(vr,x));The function combine here denotes the "glueing together" of two vertices we described above.

Now let's try to analyze how many versions of each vertex we need. For the root, we will need all 2n versions. For its children, we need only 2n−1 versions. For the children of the children of the root, we need only 2n−2 versions, and so on; so, overall, the total number of versions is only O(n2n), and each version can be constructed in O(1), so the solution works in O(n2n).

Solution (BledDest)1716F - Bags with Balls

Idea: BledDest

Tutorial1716F - Bags with BallsThe main idea of this problem is to use a technique similar to "contribution to the sum". We will model the value of Fk as the number of tuples (i1,i2,…,ik), where each element is an index of a bag from which we have taken an odd ball. Let G(t) be the number of ways to take balls from bags so that all elements from tuple t are indices of bags with odd balls; then, the answer to the problem can be calculated as the sum of G(t) over all possible tuples t.

First of all, let's obtain a solution in O(k2) per test case. We need to answer the following questions while designing a solution to the problem:

How do we calculate G(t) for a given tuple?How do we group tuples and iterate through them?The first question is not that difficult. Every element from the tuple (i1,i2,…,ik) should be an index of a bag from which we have taken an odd ball; so, for every bag appearing in the tuple, we can take only a ball with odd number; but for every bag not appearing in the tuple, we can choose any ball. So, if the number of distinct elements in a tuple is d, then G(t) for the tuple can be calculated as ⌈m2⌉d⋅mn−d.

This actually gives as a hint for the answer to the second question: since G(t) depends on the number of distinct elements in the tuple, let's try to group the tuples according to the number of distinct elements in them. So, the answer will be calculated as ∑i=1kH(i)⌈m2⌉i⋅mn−i, where H(i) is the number of tuples with exactly i different elements.

How do we calculate H(i)? First of all, if i>n, then H(i) is obviously 0. Otherwise, we can use the following recurrence: let dpi,j be the number of tuples of i elements with j distinct ones; then:

if i=1 and j=1, dpi,j=n (for a tuple with one element, there are n ways to choose it);if i=1 and j≠1, dpi,j=0;if i>1 and j=1, dpi,j=dpi−1,j (there is only one distinct element, and it was already chosen);if i>1 and j>1, dpi,j=dpi−1,j⋅j+dpi−1,j−1⋅(n−j+1) (we either add an element which did not belong to the tuple, and there are n−j+1 ways to choose it, or we add an already existing element, and there are j ways to choose it).Obviously, this recurrence can be calculated in O(k2) with dynamic programming, so we get a solution in O(k2) per test case.

How do we speed this up? Let's change the way we calculate H(i). Instead of considering tuples with values from 1 to n, we will consider only tuples where values are from 1 to k, and the first appearance of a value i is only after the first appearance of the value i−1. So, these tuples actually represent a way to split a set of integers {1,2,…,n} into several subsets; so they are the Stirling numbers of the second kind, and we can calculate them in O(k2) with dynamic programming outside of processing the test cases.

How do we calculate H(i) using these values? If we use i distinct integers as the elements of the tuple, there are n ways to choose the first one, n−1 ways to choose the second one, etc. — so H(i)=S(k,i)⋅∏j=0i−1(n−j), where S(k,i) is the Stirling number of the second kind for the parameters k and i. We can maintain the values of ∏j=0i−1(n−j) and ⌈m2⌉i⋅mn−i while iterating on i from 1 to k, and that gives us a way to solve the problem in O(k) per test case.

Overall complexity: O(k2) for precalculation and O(k) per test case.

Educational Codeforces Round 132 Editorial

By awoo, history, 7 months ago, translation, In English1709A - Three Doors

Idea: BledDest

Tutorial1709A - Three DoorsNote that we never have a choice in what door should we open. First, we open the door with the same number as the key in our hand. Then, the door with the same number as the key behind the first opened door. Finally, the door with the same number as the key behind the second opened door.

If any of the first and second opened doors didn't have a key behind it, then it's impossible. Otherwise, we open every door.

Let a1,a2,a3 be the keys behind the corresponding doors. Then we should check if a[x] is not zero and a[a[x]] is not zero.

Overall complexity: O(1) per testcase.

Solution (awoo)1709B - Also Try Minecraft

Idea: BledDest

Tutorial1709B - Also Try MinecraftSo, the first idea that is coming into mind is prefix sums. Let's define two values li=max(0,ai−ai+1) and ri=max(0,ai+1−ai). The value li means the amount of fall damage when we are going to the right from the column i to the column i+1, and ri means the amount of fall damage when we are going to the left from the column i+1 to the column i. Then let's build prefix sums on these two arrays. Now let pli be the sum of all li on a prefix [0;i) (i. e. pl0=0), and pri be the sum of all ri on a prefix [0;i). Then if s<t in a query, the answer is plt−1−pls−1, otherwise (if s>t) the answer is prs−1−prt−1.

Time complexity: O(n).

Solution (vovuh)1709C - Recover an RBS

Idea: BledDest

Tutorial1709C - Recover an RBSThere are many different approaches to this problem, but I think the model solution has the most elegant one.

First of all, let's construct an RBS from the given string (it always exists, so it is always possible). By calculating the number of opening brackets, closing brackets and questions in the given string, we can compute the number of question marks that should be replaced with opening brackets (it is easy since exactly half of the characters in each RBS are opening brackets). Then, let's form the RBS greedily: replace the first several question marks with opening brackets, and all remaining ones with closed brackets.

Okay, then what about finding a second RBS? Recall that a bracket sequence is an RBS when for each of its positions, the number of closing brackets before it is not greater than the number of opening brackets before it (and these two values should be equal at the end of the sequence, but it is less important now). Consider the segment between the last question mark replaced with an opening bracket, and the first question mark replaced by the closing bracket. If we try to change the order of characters corresponding to question marks, the balance on this segment will decrease at least by 2 (since at least one opening bracket to the left of it will become a closing bracket). Is there a way to affect only this segment, and change the balance on it only by 2? Yes — just swap the endpoints of this segment (i. e. the last opening bracket that was a question mark and the first closing bracket that was also a question mark). If it yields an RBS, then the answer is NO. Otherwise, the answer is YES since any other permutation of characters that were replacing question marks will also decrease the balance on this segment by at least 2.

Solution (Neon)1709D - Rorororobot

Idea: BledDest

Tutorial1709D - RorororobotWhat if there were no blocked cells? Then the movement is easy. From cell (x,y) we can go to cells (x+k,y), (x,y+k), (x−k,y) or (x,y−k). Thus, we can visit all cells that have the same remainder modulo k over both dimensions. The answer would be "YES" if xsmodk=xfmodk and ysmodk=yfmodk.

Let's choose the following path from start to finish. Let xs be less or equal to xf. If that isn't the case, swap the cells. First, move up until the row is the same, then move to the side until the column is the same.

What stops us from doing the same on a grid with blocked cells? The first part of the part can remain the same — we can always move up from the cell. Only cells below the start cell can be blocked. The second part is trickier. If there is a column with too many blocked cells between the start and the finish column, then we won't be able to pass through it.

Let's adjust the path for that. Move up as high as possible — to the highest cell with the same remainder modulo k in this column. Then move to the finish column and go down to the finish cell.

If there still exists a column with too many blocked cells, then the answer is "NO". No matter what we do, we won't be able to go around that column. Otherwise, the answer is "YES".

Thus, the solution is to check for remainders, then find the largest number of blocked cells between the query columns and compare it to the highest row with the same remainder modulo k as the start or the finish. You can use any RMQ data structure you want.

Overall complexity: O(nlogn+q) with sparse table for RMQ, for example.

Solution (awoo)1709E - XOR Tree

Idea: BledDest

Tutorial1709E - XOR TreeTo begin with, we note that there are no restrictions on the values that can be written on the vertices, so we can use numbers of the form 230+x for the x-th replacement. Then, if we replaced the value of a vertex, then no path passing through this vertex has weight 0.

Let's root the tree at the vertex number 1. We can use a greedy approach: consider some vertex v such that it is the LCA for two vertices x and y, the path between which has XOR equal to 0. Among such vertices v, pick one with the maximum distance from the root. We need to change at least one vertex on the path (x,y). It turns out that changing the vertex v is always no worse than changing any other vertex u on this path, because all the remaining bad paths that pass through the vertex u also pass through the vertex v (that's why we have chosen the deepest LCA). This means that in order to solve the problem, it is necessary to quickly find the deepest LCA of some bad path.

For the convenience of solving the problem, let's denote the XOR on the path (x,y) as bx⊕by⊕aLCA(x,y), where bv is XOR on the path from the root to the vertex v. For all vertices v, let's maintain a set of values bx, such that x belongs to the subtree of v. Let's use the small-to-large method to obtain such sets. Also, during the union of sets, we can check if there is a bad path in this subtree, i. e. if two values in the sets we merge have the same XOR as the value written on the current vertex (because that's when the XOR on path is 0). If such a path exists, then we have to change the value of the vertex v and mark that the vertices of this subtree cannot be the ends of a bad path anymore — that means we just clear the set instead of pulling it up the tree.

This solution works in O(nlog2n).

Solution (Neon)1709F - Multiset of Strings

Idea: BledDest

Tutorial1709F - Multiset of StringsFirst of all, let's visualize the problem in a different way. We have to set some constraints on the number of strings which have some kind of prefix. Let's think about a data structure that would allow us to understand it better. One of the most common data structures to store strings which works with their prefixes and maintains the number of strings with some prefix is a trie; so, we can reformulate this problem using tries.

Now the problem is the following one: we have a binary trie of depth n; the leaves of this trie may store strings, and for each vertex except for the root, we can set a constraint on the number of strings stored in the subtree; what is the number of ways to choose these constraints so that the maximum number of strings (possibly with copies) the trie can store is exactly f? To handle it, we can use dynamic programming of the form dpv,i — the number of ways to choose the constraints for the vertex v and its subtree so that the maximum number of strings which can be stored in the subtree is exactly i. When calculating dpv,i, we can iterate on the constraint for the vertex v (let it be a), and the maximum number of strings in the subtrees of v1 and v2 (let these be b and c), and make updates of the form "add dpv1,b⋅dpv2,c to the value of dpv,min(a,b+c)". This dynamic programming will work in O(2nk2) or O(2nk3) depending on the implementation, which is too slow.

However, we can use the following optimizations to improve the complexity of the solution:

all vertices on the same depth can be treated as equivalent, so we can actually calculate this dynamic programming not for O(2n) vertices, but just for O(n);when handling transitions from some node's children to that node, let's split these transitions into two steps. The first step is iterating on the number of strings which fit into the subtrees of the children; the second step is iterating on the constraint for the subtree of the node. The first step is actually a convolution: if we don't consider the constraint for the node itself, then the transitions would be something like "add dpv1,b⋅dpv2,c to the value of dpv,b+c)"; so it can be improved to O(klogk) with FFT. The second step can be improved to O(k) as well, if we iterate on the minimum between the constraint for the node and the total number of strings which can be stored in the children, and maintain the sum on suffix for the values of dynamic programming.Overall, these optimizations lead to a solution with complexity O(nklogk).

Educational Codeforces Round 132 Editorial

By awoo, history, 7 months ago, translation, In English1709A - Three Doors

Idea: BledDest

Tutorial1709A - Three DoorsNote that we never have a choice in what door should we open. First, we open the door with the same number as the key in our hand. Then, the door with the same number as the key behind the first opened door. Finally, the door with the same number as the key behind the second opened door.

If any of the first and second opened doors didn't have a key behind it, then it's impossible. Otherwise, we open every door.

Let a1,a2,a3 be the keys behind the corresponding doors. Then we should check if a[x] is not zero and a[a[x]] is not zero.

Overall complexity: O(1) per testcase.

Solution (awoo)1709B - Also Try Minecraft

Idea: BledDest

Tutorial1709B - Also Try MinecraftSo, the first idea that is coming into mind is prefix sums. Let's define two values li=max(0,ai−ai+1) and ri=max(0,ai+1−ai). The value li means the amount of fall damage when we are going to the right from the column i to the column i+1, and ri means the amount of fall damage when we are going to the left from the column i+1 to the column i. Then let's build prefix sums on these two arrays. Now let pli be the sum of all li on a prefix [0;i) (i. e. pl0=0), and pri be the sum of all ri on a prefix [0;i). Then if s<t in a query, the answer is plt−1−pls−1, otherwise (if s>t) the answer is prs−1−prt−1.

Time complexity: O(n).

Solution (vovuh)1709C - Recover an RBS

Idea: BledDest

Tutorial1709C - Recover an RBSThere are many different approaches to this problem, but I think the model solution has the most elegant one.

First of all, let's construct an RBS from the given string (it always exists, so it is always possible). By calculating the number of opening brackets, closing brackets and questions in the given string, we can compute the number of question marks that should be replaced with opening brackets (it is easy since exactly half of the characters in each RBS are opening brackets). Then, let's form the RBS greedily: replace the first several question marks with opening brackets, and all remaining ones with closed brackets.

Okay, then what about finding a second RBS? Recall that a bracket sequence is an RBS when for each of its positions, the number of closing brackets before it is not greater than the number of opening brackets before it (and these two values should be equal at the end of the sequence, but it is less important now). Consider the segment between the last question mark replaced with an opening bracket, and the first question mark replaced by the closing bracket. If we try to change the order of characters corresponding to question marks, the balance on this segment will decrease at least by 2 (since at least one opening bracket to the left of it will become a closing bracket). Is there a way to affect only this segment, and change the balance on it only by 2? Yes — just swap the endpoints of this segment (i. e. the last opening bracket that was a question mark and the first closing bracket that was also a question mark). If it yields an RBS, then the answer is NO. Otherwise, the answer is YES since any other permutation of characters that were replacing question marks will also decrease the balance on this segment by at least 2.

Solution (Neon)1709D - Rorororobot

Idea: BledDest

Tutorial1709D - RorororobotWhat if there were no blocked cells? Then the movement is easy. From cell (x,y) we can go to cells (x+k,y), (x,y+k), (x−k,y) or (x,y−k). Thus, we can visit all cells that have the same remainder modulo k over both dimensions. The answer would be "YES" if xsmodk=xfmodk and ysmodk=yfmodk.

Let's choose the following path from start to finish. Let xs be less or equal to xf. If that isn't the case, swap the cells. First, move up until the row is the same, then move to the side until the column is the same.

What stops us from doing the same on a grid with blocked cells? The first part of the part can remain the same — we can always move up from the cell. Only cells below the start cell can be blocked. The second part is trickier. If there is a column with too many blocked cells between the start and the finish column, then we won't be able to pass through it.

Let's adjust the path for that. Move up as high as possible — to the highest cell with the same remainder modulo k in this column. Then move to the finish column and go down to the finish cell.

If there still exists a column with too many blocked cells, then the answer is "NO". No matter what we do, we won't be able to go around that column. Otherwise, the answer is "YES".

Thus, the solution is to check for remainders, then find the largest number of blocked cells between the query columns and compare it to the highest row with the same remainder modulo k as the start or the finish. You can use any RMQ data structure you want.

Overall complexity: O(nlogn+q) with sparse table for RMQ, for example.

Solution (awoo)1709E - XOR Tree

Idea: BledDest

Tutorial1709E - XOR TreeTo begin with, we note that there are no restrictions on the values that can be written on the vertices, so we can use numbers of the form 230+x for the x-th replacement. Then, if we replaced the value of a vertex, then no path passing through this vertex has weight 0.

Let's root the tree at the vertex number 1. We can use a greedy approach: consider some vertex v such that it is the LCA for two vertices x and y, the path between which has XOR equal to 0. Among such vertices v, pick one with the maximum distance from the root. We need to change at least one vertex on the path (x,y). It turns out that changing the vertex v is always no worse than changing any other vertex u on this path, because all the remaining bad paths that pass through the vertex u also pass through the vertex v (that's why we have chosen the deepest LCA). This means that in order to solve the problem, it is necessary to quickly find the deepest LCA of some bad path.

For the convenience of solving the problem, let's denote the XOR on the path (x,y) as bx⊕by⊕aLCA(x,y), where bv is XOR on the path from the root to the vertex v. For all vertices v, let's maintain a set of values bx, such that x belongs to the subtree of v. Let's use the small-to-large method to obtain such sets. Also, during the union of sets, we can check if there is a bad path in this subtree, i. e. if two values in the sets we merge have the same XOR as the value written on the current vertex (because that's when the XOR on path is 0). If such a path exists, then we have to change the value of the vertex v and mark that the vertices of this subtree cannot be the ends of a bad path anymore — that means we just clear the set instead of pulling it up the tree.

This solution works in O(nlog2n).

Solution (Neon)1709F - Multiset of Strings

Idea: BledDest

Tutorial1709F - Multiset of StringsFirst of all, let's visualize the problem in a different way. We have to set some constraints on the number of strings which have some kind of prefix. Let's think about a data structure that would allow us to understand it better. One of the most common data structures to store strings which works with their prefixes and maintains the number of strings with some prefix is a trie; so, we can reformulate this problem using tries.

Now the problem is the following one: we have a binary trie of depth n; the leaves of this trie may store strings, and for each vertex except for the root, we can set a constraint on the number of strings stored in the subtree; what is the number of ways to choose these constraints so that the maximum number of strings (possibly with copies) the trie can store is exactly f? To handle it, we can use dynamic programming of the form dpv,i — the number of ways to choose the constraints for the vertex v and its subtree so that the maximum number of strings which can be stored in the subtree is exactly i. When calculating dpv,i, we can iterate on the constraint for the vertex v (let it be a), and the maximum number of strings in the subtrees of v1 and v2 (let these be b and c), and make updates of the form "add dpv1,b⋅dpv2,c to the value of dpv,min(a,b+c)". This dynamic programming will work in O(2nk2) or O(2nk3) depending on the implementation, which is too slow.

However, we can use the following optimizations to improve the complexity of the solution:

all vertices on the same depth can be treated as equivalent, so we can actually calculate this dynamic programming not for O(2n) vertices, but just for O(n);when handling transitions from some node's children to that node, let's split these transitions into two steps. The first step is iterating on the number of strings which fit into the subtrees of the children; the second step is iterating on the constraint for the subtree of the node. The first step is actually a convolution: if we don't consider the constraint for the node itself, then the transitions would be something like "add dpv1,b⋅dpv2,c to the value of dpv,b+c)"; so it can be improved to O(klogk) with FFT. The second step can be improved to O(k) as well, if we iterate on the minimum between the constraint for the node and the total number of strings which can be stored in the children, and maintain the sum on suffix for the values of dynamic programming.Overall, these optimizations lead to a solution with complexity O(nklogk).

Educational Codeforces Round 130 Editorial

By awoo, history, 8 months ago, translation, In English1697A - Parkway Walk

Idea: vovuh

Tutorial1697A - Parkway WalkIf you have at least sum(ai) units of energy, then the answer is 0, because you can just walk to the end. Otherwise, the answer is sum(ai)−m, because you can just sit on the first bench and then just go.

Time complexity: O(n).

Solution (vovuh)1697B - Promo

Idea: BledDest

Tutorial1697B - PromoFirst of all, there is an answer with exactly x items bought. Suppose items worth p1≤p2≤⋯≤pm (x<m) were purchased. Then by removing p1 from this set, the sum of y the cheapest items in the set will change by py+1−p1≥0, which means the answer will not decrease.

The second fact that is necessary to solve the problem — x of the most expensive items should be chosen. Otherwise, one can remove the minimum price item from the set and add an item with a higher price (it can always be found), which means the answer will not decrease.

Using these two facts, it is enough to sort the array and use prefix sums.

Solution (Neon)1697C - awoo's Favorite Problem

Idea: BledDest

Tutorial1697C - awoo's Favorite ProblemFirst, check that the counts of all letters are the same in both strings.

Then consider the following restatement of the moves. The letters 'b' in the string s are stationary. Letters 'a' and 'c', however, move around the string. The move of the first type moves a letter 'a' to the right. The move of the second type moves a letter 'c' to the left.

Notice that letters 'a' and 'c' can never swap with each other. Thus, if you remove all letters 'b' from both strings, the remaining strings should be the same.

Again, since letters 'a' and 'c' can never swap with each other, you can deduce where each of these letters should end up after the swaps. The first letter '{a}' in s should be on the position of the first letter 'a' in t and so on.

After that, we recall that 'a's can only move to the right and 'c's can only move to the left. Thus, we check that the i-th occurrence of 'a' in s is to the left or equal to the i-th occurrences of 'a' in t and vice versa for 'c's.

Finally, we can see that this is a sufficient condition. Easy to show by construction: you can just fix the positions one after another left to right.

Overall complexity: O(n) per testcase.

Solution (awoo)1697D - Guess The String

Idea: BledDest

Tutorial1697D - Guess The StringThere are several ways to solve this problem. The model solution does it as follows:

Restore the characters of s from left to right. The first character is restored by query ? 1 1. For each of the next characters, let's ask if this character is new (by querying ? 2 1 i and comparing the result with the number of different characters on the segment [1,i−1]). If it's new, ask ? 1 i to obtain the i-th character (there will be at most 26 such queries).

Otherwise, we can find the previous occurrence of the i-th character with binary search. Let f(x,y) be the number of different characters from position x to position y. If we want to find the previous occurrence of the i-th character, we need to find the last index j such that f(j,i)=f(j,i−1). Since the value f(j,i)−f(j,i−1) does not decrease when we increase j, we can find the last j such that f(j,i)−f(j,i−1)=0, with binary search.

Unfortunately, the number of queries of type 2 will be too large if we just use binary search over the whole segment [1,i−1]. To decrease the number of queries, we can use the fact that the value of j we are interested in is the last occurrence of some character we already met; there are at most 26 such values, and binary search among them will need only 5 iterations.

Solution (BledDest)1697E - Coloring

Idea: BledDest

Tutorial1697E - ColoringLet's call a point i isolated if its color does not match the color of any other point. If a point is not isolated, then it has the same color as the points with minimum distance to it (and only these points should have this color).

Let's build a directed graph where the arc i→j means that the point j is one of the closest to the point i (i. e. d(i,j)=mink=1,k≠ind(i,k)). If there is a path from the vertex i to the vertex j, it means that if the vertex i is not isolated, the vertex j should have the same color as vertex i.

Suppose the set of vertices reachable from i (including i itself) is S(i). Finding S(i) is easy — just run DFS from the vertex i. Let's analyze two cases:

there exists a pair of vertices (j,k) such that j∈S(i), k∈S(i), and there is no arc from j to k;for every pair of vertices (j,k) such that j∈S(i) and k∈S(i), there is an arc j→k.Why do we need to analyze these two cases? In the first case, the vertex i must be isolated, because painting it and some other vertex into the same color means that every vertex from S(i) will have this color, and it will break the condition in the statement. In the second case, the vertex i may be isolated, or it may have the same color as all vertices in S(i) — and if it is isolated, then the whole set S(i) should consist of isolated vertices.

Let's find all such set of vertices that meet the second case. Each vertex will belong to at most one of these sets; if it doesn't belong to any, it must be isolated, otherwise either the whole its set consists of isolated vertices, or the whole set has the same color. So, for each set, we either use 1 color or |S(i)| colors. This allows us to implement a knapsack-like dynamic programming: let dpi,j be the number of ways to paint i first sets into j colors, such that the colors are not ordered. After running this dynamic programming, we can get the answer by simple combinatorics: iterate on the number of colors we use in these sets in total, multiply the dynamic programming for it by the (ordered) number of ways to choose these colors from n, and then by the number of ways to choose the colors for points that must be isolated.

This dynamic programming can even be implemented a bit easier if we treat every vertex that must be isolated as a set of size 1, and this is the way it's written in the model solution.

Solution (BledDest)1697F - Too Many Constraints

Idea: BledDest

Tutorial1697F - Too Many ConstraintsImagine there were no constraints of the second or the third types. Then it would be possible to solve the problem with some greedy algorithm. Unfortunately, when both these constraints are present, it's not immediately clear how to adapt the greedy.

Dynamic programming is probably also out of question, because you can't maintain all possible cuts between equal values on each prefix.

Thus, let's try to make a graph problem out of this. Who knows, maybe a flow or something else could work.

Create k nodes for each position. Let the x-th of them on the i-th position represent the condition of kind "is ai equal to x?". Then all constraints can be described as edges on this graph. Binary variables, restrictive edges. Surely, this is 2-SAT.

Connect the pairs of values that satisfy each constraint. Add the edges between the adjacent positions to enforce the restriction on the non-decreasing order. Prohibit each position to be assigned to multiple values. Force each position to be assigned at least one value. Huh, it's not that easy. That's where the 2-SAT idea fails. We want the conditions of form (ai=1∨ai=2∨⋯∨ai=k). But that is not allowed, since 2-SAT has to have two variables in a clause.

That's where the main idea of the problem comes up. Instead of making our nodes (i,x) represent ai=x, let's make them ai≥x and try building the graph again.

If ai=x, then all nodes (i,y) for y≤x will be true, and the rest will be false. So if (i,x) is false, then (i,x+1) is false. That will enforce the validity of the nodes themselves.

First, the order. If (i,x) is true, then (i+1,x) is true.

The first type of constraints. ai≠x is basically the same as (ai<x or ai>x). For our conditions, it's rather ((not ai≥x) or ai≥x+1).

The second type of constraints. ai+aj≤x. Let ai be greater than or equal to some y. Then, for this constraint to hold, aj should be no greater than x−y. Thus, if (ai≥y) is true, then (aj≥x−y+1) should be false. Same for i and j swapped.

The third type of constraints is similar. ai+aj≥x. Let ai be less than or equal to some y. Then, for this constraint to hold, aj should be greater than or equal to x−y. Thus, if (ai≥y+1) is false, then (aj≥x−y) should be true. Same for i and j swapped.

And that's it. Solve the 2-SAT and restore the answer. I can advise making not k but actually k+2 nodes for ai≥0,1,…,k+1 and force the values to be between 1 and k. That will simplify the checks while adding the constraints.

Overall complexity: O((n+m)k).

Educational Codeforces Round 129 Editorial

By awoo, history, 9 months ago, translation, In English1681A - Game with Cards

Idea: BledDest

Tutorial1681A - Game with CardsLet the maximum card among all n+m cards be x.

If only one player has a card of value of x, then he/she can win by playing it on the first turn or on the second turn; the opponent won't be able to respond with any of their cards.

Otherwise (if both players have a card with value x), the player who plays this card earlier wins the game. So, in this case, the winner is the player who makes the first turn.

Solution (BledDest)1681B - Card Trick

Idea: BledDest

Tutorial1681B - Card TrickThe easiest way to solve to problem is probably to see the resemblense of a shuffle operation to an std::rotate function. So you can obtain the final deck by applying cyclic shifts of the deck by b1, then b2 and so on.

Since the shifts are cyclic, it doesn't matter if you shift by x or by x+n or by x+k⋅x for any non-negative k. The result will be the same. Thus, you can calculate the sum of rotations you apply, and subtract n, until it becomes less than n. That is taking it modulo n.

Finally, after rotating a sequence by some x, the x-th element of it (0-indexed) becomes the first one. Thus, you just want to print the (summodn)-th element of a.

Overall complexity: O(n+m) per testcase.

Solution (awoo)1681C - Double Sort

Idea: BledDest

Tutorial1681C - Double SortImagine that all elements of a are distinct. This way, sorting a in increasing order will fix the order of b.

If b turns out sorted in a non-decreasing order, then the answer exists. Otherwise, it doesn't. To obtain the sequence of swaps, you can sort a with any comparison-based sorting algorithm you want: even bubble sort will not exceed the allowed number of swaps.

What changes if a has repeated elements? Distinct elements are still ordered among themselves, but now there are also blocks of equal elements. For each block, look into the corresponding values in b. Obviously, these have to be sorted in a non-decreasing order. Rearrange them as they should be.

In fact, this is exactly the same as sorting the sequence of pairs (ai,bi) with a default comparator — first by ai, then by bi.

Since we fixed the wanted order, we can proceed with the same steps we made in a distinct case.

Overall complexity: O(nlogn) or O(n2) per testcase.

Solution (awoo)1681D - Required Length

Idea: BledDest

Tutorial1681D - Required LengthOne of the possible approaches to this problem is to try multiplying x only by the largest digit in it. Unfortunately, this doesn't work quite well, since it gives WA on one of the examples. That example is too big to consider, but a smaller version of it can prove that this is an incorrect solution: let n=5, x=403. If we multiply 403 by 4, we get 1612, and there's no way to obtain a number with 5 digits using the next action. But, if we multiply 403 by 3, we get 1209, which can then be multiplied by 9 to obtain a 5-digit number. So, considering only the largest digit is not enough.

This implies that we somehow need to consider the options that are not optimal locally, but optimal globally (i. e. choose a lower digit right now to obtain a higher digit in the future).

Let's try to estimate the number of possible integers that can be obtained using these operations to see if we can consider all possible options. The key observation is that each integer we obtain will have the form x⋅2a⋅3b⋅5c⋅7d, since only one-digit primes can be added to the factorization. Since we consider only numbers less than 1019, a is not greater than 63, b is not greater than 39, c is not greater than 27, and d is not greater than 22, and the number of reachable integers is about 1.5 million (note that this is a very generous bound since not all combinations of (a,b,c,d) yield an integer less than 1019, and not all such integers can be reached with the operations).

This allows us to use BFS or dynamic programming to calculate the answer.

Solution (BledDest)1681E - Labyrinth Adventures

Idea: BledDest

Tutorial1681E - Labyrinth AdventuresWLOG, assume all queries ask to move from a lower layer to a higher layer. The first thing to notice in the problem is that it is always optimal to never go down a layer.

You have an optimal path that is going down some layers, and then returning to the same layer. So it leaves a layer in some its cell and returns to it in some other cell (or the same one). The best distance it can achieve is the Manhattan distance between these two cells. However, we can also achieve the Manhattan distance by just going along this layer, and the answer will be at least as optimal.

If the query asks about the cells of the same layer, just answer with the Manhattan distance. Otherwise, we can describe the path as follows: go from the first cell to some door on its layer, enter the door and go to another door on the next layer, so on until the layer of the second cell, where you go from a door to the second cell.

Thus, we could potentially write dpi,j — the shortest distance from the start to the j-th door of the i-th layer. Initialize both doors of the first layer, take the best answer from the both doors of the last layer. That would be O(n) per query, which is too slow.

Let's optimize it with some precalculations. In particular, we want to know the shortest distance between one door of some layer and one door of another layer.

We can use the technique similar to binary lifting. Calculate the distance between a pair of doors on layers which are 2x apart for all x up to logn. Let dpi,x,d1,d2 be the distance from door d1 of layer i to door d2 of layer i+2x. dpi,0,d1,d2 can be initialized straightforwardly. Then, to calculate dpi,x,d1,d2, we can use the values for x−1: dpi,x−1,d1,t and dpi+2x−1,x−1,t,d2 for some intermediate door t on layer i+2x−1.

To obtain the answer, use O(logn) jumps to reach the layer one before the last one. Then iterate over the last door.

Alternatively, you could pack this dynamic programming into a segment tree, use divide and conquer on queries or do square root decomposition.

Overall complexity: O((n+m)logn).

Solution (awoo)1681F - Unique Occurrences

Idea: BledDest

Tutorial1681F - Unique OccurrencesLet's use contribution to the sum technique to simplify the problem. Instead of counting the number of colors that occure only once for each path, let's, for each color, count the number of paths that contain this color exactly once. Now we can solve the problem independently for each color, and sum up the answers.

The first intended solution was the following. So we want to calculate the answer for some color c. Mark all edges of color c as good, the rest are bad. Then we can calculate dpv,i — the number of paths up to vertex v such that they contain either 0 or 1 good edges. The transitions should be pretty easy, and the answer should be updated when you consider gluing up paths from different children in each vertex. Obviously, this is O(n) per color, so O(n2) overall.

However, we can only calculate this dynamic programming as easily on a virtual tree of vertices adjacent to all good edges. How to calculate the dp for some vertex v? First, push the paths from all virtual children to v. That was enough in the dp for the entire tree but now there are also removed vertices that could also have paths starting in them. All these paths contain 0 good edges (otherwise, they would have had virtual vertices on them). Their amount is the following: the size of the real subtree of v minus the sizes of real subtrees of all its virtual children. The rest is exactly the same as in the dp on the real tree.

A little fun trick. Usually, you want to add lca of adjacent vertices to the virtual tree. But that's actually not needed here: you can just add the root of the tree and link the vertices without a parent to them. That won't change the result of the dp.

That solution works in O(nlogn) or O(n).

The second intended solution is slower complexity-wise but not time-wise. In the first solution we wanted to leave only the good edges in the tree. Here, we want to remove only them. Consider the resulting connected components. What's the number of paths that contain only one of the good edges? It's actually the product of sizes of the connected components this edge connects.

So we want to remove edges, add edges and maintain the sizes of the connected components of the tree. That's basically the same problem as dynamic connectivity. The O(nlog2n) implementation works well enough.

Educational Codeforces Round 128 Editorial

By awoo, history, 9 months ago, translation, In English1680A - Minimums and Maximums

Idea: BledDest

Tutorial1680A - Minimums and MaximumsFirstly, since we are interested in minimum possible size of the array, we don't need any elements other than minimums and maximums. So, the array has at most 2 distinct elements.

Now there are many possible solutions. The simplest one is to iterate on the number of minimums (let this be i) and maximums (let this be j). If the number of minimums is equal to the number of maximums, then the array should have all elements as both its minimums and maximums, so its length should be i; otherwise, it should be i+j. We can iterate on all possible pairs (i,j) and find the best result over all of them.

A solution in O(1) is possible if you see that you only have to consider l1 and l2 as the number of minimums/maximums, or check if the segments [l1,r1] and [l2,r2] intersect in O(1).

Solution (BledDest)1680B - Robots

Idea: BledDest

Tutorial1680B - RobotsLet's assume that the rows are numbered from 0 to n−1 from top to bottom, and columns are numbered from 0 to m−1 from left to right.

If there is no robot in the cell (0,0) initially, we have to perform several moves up and/or left. If the first row with at least one robot is the i-th row, then we can make at most i steps up (and we should do at least i steps up, since otherwise there will me no robot in the upper row). Similarly, if the first column with at least one robot is the j-th column, then we can make at most j steps to the left (and we should do at least j steps to the left, since otherwise there will me no robot in the leftmost column).

Now there are two possible solutions, both starting with finding i and j: we afterwards either simulate i moves up and j moves to the left and check that everything is fine, or just check that there is a robot in the cell (i,j) (since only this robot can end up in (0,0)).

Solution (BledDest)1680C - Binary String

Idea: BledDest

Tutorial1680C - Binary StringThere are many different approaches to this problem: dynamic programming, binary search, greedy, two pointers, anything you want. The model solution uses an approach based on binary search, so I'll describe it.

First of all, why does binary search work? Let's say that the number of 1's is c. If the cost of deletion is k, then we have deleted at most k characters 1, and have left at most k characters 0. Let's increase the number of characters we delete from the prefix of the string until the number of deleted 1's becomes k+1: if c≥k+1, it's always possible. So, if we consider the segment of values [0,c], the fact that we can get cost k implies that we can get cost k+1, so we can use binary search on segment [0,c] to find the minimum achievable cost.

Now, how to check if we can obtain the cost of deletion equal to k? One possible way to do this is to form an array pos, where posi is the position of the i-th character 1 in the string, and find the minimum value of posi+c−k−1−posi in this array: the string that should remain has to contain at least c−k characters 1, and the minimum value of posi+c−k−1−posi is the minimum possible length of such string. Then we can find the number of 0's in this string and check if it is greater than k or not.

Solution (BledDest)1680D - Dog Walking

Idea: vovuh and BledDest

Tutorial1680D - Dog WalkingConsider every cyclic shift of the array a. Suppose that now the array a starts from the position i (the first element is a[i] and the last element is a[(i+n−1)%n]). Assume that before the position i our dog reached her minimum possible position and now the minimum position will not change. So our problem is to fill all zeros in the array a in such a way that the maximum prefix sum of a is the maximum possible and the total sum of a is zero.

For simplicity, consider the array b which is the i-th cyclic shift of a (i. e. the first element b[0] is a[i], the second element b[1] is a[(i+1)%n], and so on). Let's iterate from left to right and maintain the current sum of the array b. Let this variable be s. Now, when we meet bj=0, we should replace it with the maximum possible value we can (because in such a way we will increase the maximum number of prefix sums). Let x be the number of zeros in b starting from the position j+1. This value can be calculated in advance in O(n) for every cyclic shift using suffix sums. Then the segment of positions we can have at the end is [s−xk;s+xk] and we want to place the maximum possible value in b[j] in such a way that this remaining segment (with addition of our current element) will cover 0. This maximum value equals b[j]=min(k,xk−s). If b[j] becomes less than −k then this cyclic shift is invalid, and we should skip it. Otherwise, let's add b[j] to s and proceed. If there are no values b[j]<−k, then we placed anything correctly.

Now can just simulate the movements of our dog to find the answer for the current cyclic shift. But there are cases when a do not contain zeros, so these cases should be handled somehow (I just checked that after simulation we returned to 0). If we returned to 0, we can update the answer as the difference between the maximum and the minimum positions plus one. If there is no valid cyclic shift, then the answer is -1.

Time complexity: O(n2).

Solution (vovuh)1680E - Moving Chips

Idea: vovuh

Tutorial1680E - Moving ChipsFirstly, I want to say a few words about the difficulty of this problem. Till the last moment, we didn't know easy to prove (and easy to write) solutions, so we decided that this is a good problem E. But now we realized it is a lot easier than we expected.

Now, let's talk about the solution. At the beginning, let's remove redundant columns from the beginning and from the end (i. e. columns without chips) and change the value n correspondingly. Now, let costi,j be 1 if sj,i is '*', and 0 otherwise. This array needed to make the implementation easier.

Let's calculate the dynamic programming dpi,j, where i is the index of the last processed column and j is the number of the row where our chip is standing. This seems a bit suspicious why we can calculate such a dynamic programming, so let's explain some things about it.

It can be shown that in the optimal answer there will be some column where the last move happens. And if the number of this column is j then all chips to the left of j will move only to the right and all chips to the right of j will move only to the left.Actually, we can always consider that j is the last column. Consider paths of two chips that will survive till the last move. The first chip is to the left of j and will move only to the right, and the second one is to the right of j and will move only to the left. Then we can replicate the path of the second chip in the reverse order using the first chip. So the second chip can stay still until the last move.In the optimal answer, it is always better to have exactly one chip in the current column, because moving two chips to the right is always worse than just eat one of them and move the remaining one.Initial states of dp are +∞ except the values of the first column. For the first column, dp0,0=cost0,1 and dp0,1=cost0,0. The answer will be min(dpn−1,0,dpn−1,1).

Okay, how to make transitions from dpi,j? For all i from 0 to n−2, let's consider four cases:

dpi,0→dpi+1,0 — here we need one move to go to the next column and, probably, one more move to delete the figure in the second row in the column i+1. So the transition seems like dpi+1,0=min(dpi+1,0,dpi,0+1+costi+1,1);dpi,1→dpi+1,1 — same as the previous transition, dpi+1,1=min(dpi+1,1,dpi,1+1+costi+1,0);dpi,0→dpi+1,1 — because the cost of this transition is always 2 (the distance between these cells is 2), we just go firstly to the right and then down (to ensure that we eat the figure in the first row). So the transition is dpi+1,1=min(dpi+1,1,dpi,0+2);dpi,1→dpi+1,1 — same as the previous transition, dpi+1,0=min(dpi+1,0,dpi,1+2).Time complexity: O(n).

Solution (vovuh)1680F - Lenient Vertex Cover

Idea: BledDest

Tutorial1680F - Lenient Vertex CoverLet's think about why we can't always make a perfect vertex cover — such a vertex cover that each edge has exactly one endpoint in it. Or why the answer can not exist at all.

Consider a bamboo. It's always possible to find a perfect vertex cover. Just choose every other vertex in it and account for parity.

Make a bamboo into a loop. Now you can see that an even length loop has a perfect vertex cover. An odd length doesn't.

That tells us that each odd length loop in a graph will have a bad edge on it. Odd length loops should instantly make you think about bipartite colorings.

So we can see that a bipartite graph always has a perfect vertex cover. Just choose one of the parts into a cover, and each edge will have exactly one endpoint in it. At the same time, a non-bipartite graph never has a perfect cover.

So our general goal is to remove (basically, mark as bad) at most one edge in such a way that the remaining graph is bipartite.

Consider a dfs tree of the graph, colored bipartitely. Every edge in the tree is good (has endpoints in different parts). Every edge outside the tree can be either good or bad. What happens to the tree if we remove an edge?

If we remove an edge outside the dfs tree, then nothing happens to it. So if there is no more than one bad edge outside the tree, then we found the answer. That was the easy part.

Now what happens if we remove an edge from the tree? The back edges from the subtree of the edge can force the subtree to either remain colored the same or flip all its colors. We don't really care if it remains the same, because we already took care of it in the first part. So let's pretend it always flips the colors.

Thus, all edges that go from the subtree upwards above the removed edge, have only one of their endpoints colors changed. Good edges turn bad, bad edges turn good.

All other edges don't change.

So you should choose such an edge to remove that all bad edges in the graph go from its subtree upwards above that edge and no good edges go from its subtree upwards above that edge.

That can be calculated with a dfs. Since all non-tree edges in the dfs tree are back edges, you can simply increment a counter on the bottom vertex, decrement the counter on the top vertex and collect sums from the bottom. The sum in the vertex will tell you the number of edges that start below or in the vertex and end above the vertex.

Do this for both kinds of edge and check the conditions for all vertices. Finally, choose such a part to be a vertex cover that the removed edge has both ends in it (if you choose the other part, that edge won't be covered at all).

The solution is linear, but the problem still requires a massive time and memory limit only because of recursion in the dfs.

Overall complexity: O(n+m) per testcase.

Educational Codeforces Round 127 Editorial

By awoo, history, 10 months ago, translation, In English1671A - String Building

Idea: BledDest

Tutorial1671A - String BuildingEvery character in strings aa, aaa, bb and bbb has at least one character adjacent to it that is the same. So, if there is an isolated character in our string (a character that has no neighbors equal to it), we cannot build it.

It's easy to see that in the other case, we can build the string: we can split it into blocks of consecutive equal characters, and since there are no isolated characters, each block will have at least 2 characters, so it can be formed from strings of length 2 and/or 3 consisting of equal characters.

So, the problem is reduced to checking if each character has a neighbor equal to it.

Solution (BledDest)1671B - Consecutive Points Segment

Idea: vovuh

Tutorial1671B - Consecutive Points SegmentWe can see that the answer is YES if and only if there are no more than two gaps of length 1 between the given points. If there is no gap, the answer is obviously YES. If there is only one gap of length 1, we can just move the left (or the right) part of the set to this gap. When there are two gaps, we can move the part before the first gap to the right and the part after the second gap to the left. Of course, if there is a gap of length at least 3 (or multiple gaps with the total length 3), we can't move the points from the left and the right part to satisfy the middle gap.

Time complexity: O(n).

Solution (vovuh)1671C - Dolce Vita

Idea: vovuh and BledDest

Tutorial1671C - Dolce VitaFirstly, note that if we want to buy as many packs as possible, then it's optimal to buy the cheapest packs. In other words, if we sort all packs, we'll always buy a prefix of array a.

Next, note that each day we buy some number of packs i∈[1,n], so, instead of iterating through the days, we can iterate through the number of packs i and for each i calculate the number of days we'll buy exactly i packs.

Since the prices increasing and at day k+1 the price is ai+k, then exists last day ki+1 such that as days 1,2,…,ki+1 we could buy i packs and at days ki+2,ki+3,… we can't. And we can find ki as maximum possible integer solution to inequation (a1+ki)+⋯+(ai+ki)≤x or ki=⌊x−(a1+⋯+ai)i⌋.

We can calculate all ki using prefix sums a1+⋯+ai in linear time. As a result, we buy

n packs in days (0,k1+1]; n⋅(k1+1) in total;n−1 packs in days (k1+1,k2+1]; (n−1)⋅(k2−k1) in total;n−2 packs in days (k2+1,k3+1]; (n−2)⋅(k3−k2) in total and so on.The resulting complexity is O(nlogn) because of sort.

Solution (adedalic)1671D - Insert a Progression

Idea: vovuh and BledDest

Tutorial1671D - Insert a ProgressionObserve the cost of inserting a single element. Notice that inserting any value between the minimum of the sequence and the maximum of the sequence is free.

Why is this true? The argument is similar to the algorithm of finding some x such that f(x)=0 for a continous function f if you know some x1 such that f(x1)<0 and x2 such that f(x2)>0.

As a more general idea, it's free to insert some value s into a segment [l;r] such that al≤s and s≤ar (WLOG assume al≤ar). Let's find the position that is free. If r−l=1, then you can insert s between al and ar, since it's free. Otherwise, you can choose an arbitrary position l<i<r. s will be either between ai and al or between ai and ar (or both of them). Descend into the one that holds to continue the search. Since the lenght decreases, at some point you will reach the segment of length 1.

How does that help? Well, you can insert 1 somewhere, then insert x somewhere. The rest of insertions will be free.

Now it's an algorithmic problem. First, consider all options to insert both 1 and x between the same pair of elements. Next, assume you insert 1 somewhere before x. Iterate from left to right, maintaning the lowest price to insert 1. Try to insert x at the current position and 1 into the cheapest position before it. Then update the lowest price for inserting 1. After you finish, reverse the sequence and solve the problem again — that will be the same as inserting x before 1.

Overall complexity: O(n) per testcase.

Solution (awoo)1671E - Preorder

Idea: BledDest

Tutorial1671E - PreorderIn terms of preorder strings, the operation "swap two children of some vertex" means "swap two substrings of equal length in some specific location". This operation can be inverted by applying it an additional time, so for every positive integer k, all of the strings of length 2k−1 are split into equivalence classes in such a way that two strings from the same class can be transformed into each other, and two strings from different classes cannot. For each vertex, the set of its possible preorder strings is one of these classes.

Let's calculate the answer for the problem recursively: let dpv be the number of preorder strings for the vertex v. For a leaf, the number of its preorder strings is 1. For a vertex x with children y and z, one of the two holds:

if the equivalence class for vertex y is different from the equivalence class for vertex z, then we have to pick a string from the class of vertex y, pick a string from the class of vertex z, and choose the order in which we take them. So, dpx=dpy⋅dpz+dpz⋅dpy=2⋅dpy⋅dpz;if the equivalence class for y is the same as the equivalence class for z, then swapping y and z doesn't do anything, so we pick a string from the equivalence class of y, and then a string from the equivalence class of z. So, dpx=dpy⋅dpz=dp2y.The only thing we don't know is how to determine if two vertices represent the same equivalence class. The model solution uses hashing for this, but there's a much simpler method: for each vertex v, let tv be the lexicographically smallest string that can be a preorder string of v. If a vertex x has children y and z, then tx=min(ty+sx+tz,tz+sx+ty), and we can calculate these strings recursively since the total length is O(n2n) — each of 2n−1 characters will be present in O(n) strings.

Solution (BledDest)1671F - Permutation Counting

Idea: BledDest

Tutorial1671F - Permutation CountingA lot of solutions which were written during the contest use Berlekamp-Messey or some other algorithms related to analyzing linear recurrences, but the model solution is based on other principles.

First of all, if the number of inversions is at most 11, it means that most elements of the permutation will stay at their own places, and those which don't stay at their places can't be too far away from them.

Let's denote a block [l,r] in a permutation as a segment of indices [l,r] such that:

all elements less than l are to the left of the block;all elements greater than r are to the right of the block;all elements from [l,r] belong to the block.Let's say that a block is non-trivial if it contains at least two elements.

Suppose we split a permutation into the maximum number of blocks. Then, for each block, we can see that:

if its length is b, it has at least b−1 inversions (to prove it, you can use the fact that the number of inversions is equal to the number of swaps of adjacent elements required to sort the permutation; and if we cannot split the block into other blocks, it means that we have to swap each pair of adjacent elements in it at least once to sort it)if the block is non-trivial, it has at least one i such that pi>pi+1.From these two facts, we can see that:

there will be at most 11 non-trivial blocks;there will be at most 22 elements in total belonging to non-trivial blocks;the maximum possible length of a block is 12.The main idea of the solution is to calculate the following dynamic programming: dpi,j,a,b is the number of ways to split j elements into i non-trivial blocks such that there are exactly b inversions in them and exactly a pairs pi>pi+1. Then, to get the answer for the test case "n k x", we can iterate on the number of non-trivial blocks and the number of elements in them, and choose the elements belonging to that blocks with a binomial coefficient.

The only thing that's left is how to calculate this dynamic programming efficiently. There are a few ways to do it, but the model solution uses a table cnta,b,c — the number of different non-trivial blocks of length a with b elements pi>pi+1 and c inversions — to handle transitions. This table is not very big, so you can run an exhaustive search for 2-3 minutes to calculate it and then just paste its results into the source code of your program. Note that you have to make sure that you consider only the blocks which cannot be split any further.

Educational Codeforces Round 126 Editorial

By awoo, history, 10 months ago, translation, In English1661A - Array Balancing

Idea: BledDest

Tutorial1661A - Array BalancingLet's look at our arrays a and b. Note that for any position p such that |ap−1−ap|+|bp−1−bp|>|ap−1−bp|+|bp−1−ap| we can always "fix it" by swapping all positions i from p to n. In that case, contribution from all i<p won't change, contribution of pair (p−1,p) will decrease and contribution from all i>p won't change again, since we swapped all of them.

It means that we already can use the following algorithm: while exists such p that |ap−1−ap|+|bp−1−bp|>|ap−1−bp|+|bp−1−ap| just swap all i from p to n. This solution works for O(n2) per test, that should be enough.

But we can optimize our approach by realizing that we can (instead of searching p each time) just go from 2 to n and fix pairs one by one: if |a1−a2|+|b1−b2|>|a1−b2|+|b1−a2| then swap a2 with b2; next, if |a2−a3|+|b2−b3|>|a2−b3|+|b2−a3| then swap a3 with b3 and so on. In such way, solution works in O(n).

Solution (adedalic)1661B - Getting Zero

Idea: adedalic

Tutorial1661B - Getting ZeroNote that 32768=215, so you can make any value equal to 0 by multiplying it by two 15 times, since (v⋅215)mod215=0. So, the answer for each value ai is at most 15.

Now, let's note that there is always an optimal answer that consists of: at first, add one cntAdd times, then multiply by two cntMul times — and cntAdd+cntMul is the minimum answer. In other words, let's just iterate over all cntAdd≤15 and cntMul≤15 and check that (v+cntAdd)⋅2cntMulmod32768=0. The answer is minimum cntAdd+cntMul among them.

To prove that it's optimal to add at first and only then to multiply, note that it's not optimal to add more than once after muptiplying (v→2v→2v+2 can be replaced by v→v+1→2(v+1)). So there is at most one +1 between two ⋅2, but it's not optimal to make even one +1 since we need to make v divisible by 215 and +1 break divisibility.

There are many other approaches to this task except this one: for example, since ai<32768 you can write bfs to find the shortest paths from 0 to all ai.

Solution (adedalic)1661C - Water the Trees

Idea: vovuh

Tutorial1661C - Water the TreesThe first observation we need to solve this problem: the required height is either max or max+1, where max is the maximum initial height of some tree. We don't need heights greater than max+1, because, for example, if the height is max+2, we can remove some moves and get the answer for the height max. The same thing applies to all heights greater than max+1. Why do we even need the height max+1? In some cases (like [1,1,1,1,1,1,2]) the answer for the height max+1 is better than the answer for the height max (in this particular case, it is 9 vs 11).

Now, we have two ways to solve the problem: either use some gross formulas, or just write a binary search on the answer. I won't consider the solution with formulas (but we have one), so let's assume we use binary search. Let the current answer be mid. Then let cnt1=⌈mid2⌉ be the number of +1 operations we can do and cnt2=⌊mid2⌋ be the number of +2 operations we can do. We can use +2 operations greedily and then just check if the number of +1 operations is sufficient to grow up the remaining heights.

Time complexity: O(nlogn) per test case.

Solution 1 (vovuh)Solution 2 (awoo)1661D - Progressions Covering

Idea: vovuh

Tutorial1661D - Progressions CoveringLet's solve the problem greedily. But not from the beginning, because if we solve it from the beginning, we can't be sure what option is more optimal for the next elements (e.g. for the second element it is not clear if we need to add 2 to it starting our segment from the first position or add 1 to it starting our segment from the second position). So, let's solve the problem from right to left, then anything becomes clearer.

Actually, let's operate with the array b and decrease its elements instead of using some other array. Let's carry some variables: sum, cnt and the array closed of length n (along with the answer). The variable sum means the value we need to subtract from the current element from currently existing progressions, cnt is the number of currently existing progressions, and closedi means the number of progressions that will end at the position i+1 (i.e. will not add anything from the position i and further to the left).

When we consider the element i, firstly let's fix sum (decrease it by cnt). Then, let's fix cnt (decrease it by closedi). Then, let's decrease bi by sum, and if it becomes less than or equal to zero, just proceed. Otherwise, the number by which we can decrease the i-th element with one progression, equals to el=min(k,i+1) (zero-indexed). Then the number of progressions we need to satisfy this element is need=⌈biel⌉. Let's add this number to the answer, increase sum by el⋅need, increase cnt by need, and if i−el≥0 then we need to end these progressions somewhere, so let's add need to closedi−el.

Time complexity: O(n).

Solution (vovuh)1661E - Narrow Components

Idea: BledDest

Tutorial1661E - Narrow ComponentsConsider the naive approach to the problem.

Cut off the columns directly and count the connected components. There are two main solutions to this problem: either DFS (or BFS) or DSU. I personally found the DSU method easier to adjust to the full problem.

So, to count connected components with DSU, you should do the following. Initialize the structure without edges: every free cell is its own connected component. Then add edges one by one. Each edge connects two cells either vertically or horizontally. When an edge connects different components, they merge, and the number of components decreases by one.

Thus, the number of components on a range of columns is the number of free cells on it minus the number of meaningful edges on it (the ones that will merge components if the algorithm is performed only on these columns — the spanning forest edges).

Let's try to adjust this algorithm to the full problem. It would be great if we could just calculate the spanning forest of the entire matrix, and then print the number of free cells minus the number of its edges on the segment. Unfortunately, it's not as easy as that. For components that lie fully in the segment, it works. However, if a component is split by a border of a segment, it can both stay connected or fall apart. If we determine its outcome, we can fix the answer.

There are probably a lot of ways to adjust for that, but I'll tell you the one I found the neatest to code. Let's add the edges into DSU in the following order. Go column by column left to right. First add all vertical edges in any order, then all horizontal edges to the previous column in any order.

If you start this algorithm at the first column, you will be able to answer all queries with l=1. Since the algorithm adds columns iteratively, the spanning forest it's building is correct after every column. So the answer for each query is indeed the number of cells minus the number of edges on the range.

Let's investigate the difference between starting at the first column and an arbitrary column l.

Look at the column l. If it contains 1 or 3 free cells or 2 that are adjacent, then the cells are always in the same component, regardless of what has been before column l. If there are no free cells, nothing to the left matters, too. This tells us that the spanning forest that the first algorithm has built, is correct for any queries that start in this l.

The only non-trivial case is when only rows 1 and 3 of the l-th column contain a free cell. Then we can't tell if the algorithm is correct or not, because these two cells can be in the same component already or not. Let's call this a "101" column.

Imagine you started processing from the leftmost column of the query, left to right to the rightmost column. Our previous observations tell us that once we encounter a column that is not a "101", the algorithm onwards will be correct. Until then, we only have some "101" columns to deal with.

We can add the part from the first non-"101" column onwards to the answer (the number of cells minus the number of edges). And then handle the prefix with some easy casework:

if the leftmost column is not "101", then add nothing;if all columns in the query are "101", then the answer is 2;if the first non-"101" column is "111", then add nothing (since the "101"s get merged into the component of this column);if the first non-"101" column is "000" or "010", then add 2 components (since neither row 1 nor row 3 is merged anywhere);otherwise, add 1 component.The number of free cells and edges on a segment can be precalculated with some prefix sums. The closest non-"101" column can also be precalculated with a linear algorithm.

Overall complexity: O(n⋅α(n)+q).

Solution (awoo)1661F - Teleporters

Idea: vovuh

Tutorial1661F - TeleportersInitial n+1 portals divide the path from 0 to an into n separate sections. If we place a new portal between two given ones, it only affects the section between these two portals.

Let's suppose we want to place k new portals into a section of length x. This will divide it into (k+1) sections, and it's quite easy to prove that these sections should be roughly equal in size (to prove it, we can show that if the sizes of two sections differ by more than 1, the longer one can be shortened and the shorter one can be elongated so the sum of squares of their lengths decreases). So, a section of length x should be divided into xmod(k+1) sections of length ⌈xk+1⌉ and (k+1)−xmod(k+1) sections of length ⌊xk+1⌋. Let's denote the total energy cost of a section of length x divided by k new portals as f(x,k); since we divide it in roughly equal parts, it's easy to see that

f(x,k)=(xmod(k+1))⋅(⌈xk+1⌉)2+((k+1)−xmod(k+1))⋅(⌊xk+1⌋)2The key observation that we need to make now is that f(x,k)−f(x,k+1)≥f(x,k+1)−f(x,k+2); i. e. if we add more portals to the same section, the energy cost change from adding a new portal doesn't go up. Unfortunately, we can't give a simple, strict proof of this fact, but we have faith and stress (this would be easy to prove if it was possible to place portals in non-integer points, we could just analyze the derivative, but in integer case, it's way more difficult).

Okay, what should we do with the fact that f(x,k)−f(x,k+1)≥f(x,k+1)−f(x,k+2) for a section of length x? The main idea of the solution is binary search over the value of f(x,k)−f(x,k+1); i. e., we use binary search to find the minimum possible change that a new portal would give us. Let's say that we want to check that using the portals that give the cost change ≥c is enough; then, for each section, we want to find the number of new portals k such that f(x,k−1)−f(x,k)≥c, but f(x,k)−f(x,k+1)<c; we can use another binary search to do that. For a fixed integer c, we can calculate not only the number of new portals that we can add if the cost change for each portal should be at least c, but also the total cost of the path after these changes; let's denote g(c) as the total cost of the path if we place new portals until the cost change is less than c, and h(c) is the number of portals we will place in that case.

We have to find the minimum value of c such that g(c)≤m. Now, it looks like h(c) is the answer, but this solution gives WA on one of the sample tests. The key observation we are missing is that, for the value c, we don't have to add all of the portals that change the answer by c; we might need only some of them. To calculate the answer, let's compute four values:

g(c+1);h(c+1);g(c);h(c).If we place h(c+1) portals and add new portals one by one, until the total cost becomes not greater than m, the cost change from each new portal will be equal to g(c+1)−g(c)h(c)−h(c+1) (or just c if we consider the fact that we start using the portals which change the cost by c). So, we can easily calculate how many more additional portals we need to add if we start from h(c+1) portals and cost g(c+1).

The total complexity of our solution is O(nlog2A): we have a binary search over the cost change for each new portal; and for a fixed cost change, to determine the number of portals we place in each section, we run another binary search in every section separately.

Educational Codeforces Round 125 Editorial

By awoo, history, 11 months ago, translation, In English1657A - Integer Moves

Idea: BledDest

Tutorial1657A - Integer MovesNote that the answer does not exceed 2, because the chip can be moved as follows: (0,0)→(x,0)→(x,y). Obviously, in this case, both operation are valid. It remains to check the cases when the answer is 0 or 1. The answer is 0 only if the destination point is (0,0), and the answer is 1 if x2+y2−−−−−−√ is integer.

Solution (Neon)1657B - XY Sequence

Idea: adedalic

Tutorial1657B - XY SequenceStrategy is quite easy: we go from a1 to an and if ai−1+x≤B we take this variant (we set ai=ai−1+x); otherwise we set ai=ai−1−y. Note that all ai are in range [−(x+y),B] so there won't be any overflow/underflow.

It's also not hard to prove that this strategy maximizes the sum. By contradiction: suppose the optimal answer has some index i where ai−1+x≤B but ai=ai−1−y. Let's find first position j≥i where aj=aj−1+x and swap operations between i and j. As a result, B≥ai>ai+1>⋯>aj, all ai from [i,j−1] were increased while aj remained the same, i. e. there is no violation of the rules and the total sum increased — contradiction.

Solution (adedalic)1657C - Bracket Sequence Deletion

Idea: BledDest

Tutorial1657C - Bracket Sequence DeletionConsider the first character of the string. If it is '(', then we can remove the first two characters of the string and continue (because the prefix of length 2 will be either a palindrome or a regular bracket sequence). If the first character of the string is ')' then this is a bad case. Of course, the regular bracket sequence can't start with '(', so this prefix should be a palindrome. And what is the shortest palindrome we can get with the first character ')'? It is the closing bracket ')', then some (possibly, zero) amount of opening brackets '(', and another one closing bracket. We can see that we can't find a palindrome shorter than this one because we have to find a pair for the first character. So, if the first character of the string is ')', then we just remove anything until the next character ')' inclusive. To not remove any characters explicitly, we can just use pointers instead. And the last thing is to carefully handle cases when we can't do any operations.

Solution (vovuh)1657D - For Gamers. By Gamers.

Idea: BledDest

Tutorial1657D - For Gamers. By Gamers.Imagine you are fighting the j-th monster, and you fixed the type of units i and their amount x.

What's the win condition? Hjdi⋅x<hiDj. Rewrite it as Hj⋅Dj<di⋅x⋅hi. Notice how we only care about d⋅h for both the units and the monster, but not about d and h on their own.

Let's call d⋅h⋅x and D⋅H the power of the squad and the monster.

You can see that for each cost c we can only leave one unit type of that price that has the largest value of d⋅h. Let's call it bstc.

Now let's learn to determine the maximum power we can obtain for cost exactly c. We can iterate over the cost c of one unit and the count x of units in the squad. Since c⋅x should not exceed C, that will take C1+C2+⋯+CC=O(ClogC). Propagate bstc to be the maximum power for cost exactly c.

We have the knowledge about cost exactly c, but we actually want no more than c. Calculate prefix maximums over bst — that will be the maximum power we can obtain with no more than c coins.

For each monster, we just have to find the smallest c such that bstc>D⋅H. Since the array is monotone, we can use binary search.

Overall complexity: O(n+(C+m)logC).

Solution (awoo)1657E - Star MST

Idea: BledDest

Tutorial1657E - Star MSTLet the weight of the edge between the vertex x to the vertex y be wx,y.

Suppose there exists a pair of vertices x and y (with indices greater than 2) such that wx,y<w1,x or wx,y<w1,y. Then, if we choose the spanning tree with all vertices connected to 1, it won't be an MST: we can remove either the edge (1,x) or the edge (1,y), add the edge (x,y) instead, and the cost of the spanning tree will decrease. So, we should have wx,y≥max(w1,x,w1,y) for every pair (x,y).

It can be shown that this condition is not only necessary, but sufficient as well: if for every pair (x,y) the condition wx,y≥max(w1,x,w1,y) holds, the MST can't have the weight less than ∑i=2nw1,i. We can prove this by induction (suppose that w1,2≤w1,3≤…≤w1,n for simplicity):

in the spanning tree, there should be at least one edge incident to vertex n, and its weight is at least w1,n;there should be at least two edges incident to vertices n and n−1, and their weights are at least w1,n−1+w1,n;...;there should be at least n−1 edges incident to vertices from 2 to n, and their weights are at least ∑i=2nw1,i.Okay, now let's show how to calculate the number of such graphs. We can run the following dynamic programming: let dpi,j be the number of graphs where we have already connected i vertices to the vertex 1, and the maximum weight we have used is j. We start with dp0,0, and for each transition from dpi,j, we will iterate on the number of vertices we connect to the vertex 1 with edges with weight (j+1) (let the number of those vertices be t), choose them with a binomial coefficient (n−1−i)!t!(n−1−i−t)!, and also choose the weights for the edges that connect one of the chosen vertices with one of the vertices already connected to 1 (since for each of those edges, we know that their weights should be in [j+1,k]) — so, we need to multiply the value in transition by (k−j)e, where e is the number of such edges.

Implementing this dynamic programming can be done in O(n2k) or O(n2klogn), both are sufficient.

Solution (awoo)1657F - Words on Tree

Idea: BledDest

Tutorial1657F - Words on TreeLet's design a naive solution first. For each of the given triples, we have two options: either write the string on the tree in the order from xi to yi, or in reverse order. Some options conflict with each other. So, we can treat this problem as an instance of 2-SAT: create a variable for each of the given strings, which is true if the string is not reversed, and false if it is reversed; find all conflicting pairs of options and then run the usual algorithm for solving 2-SAT.

Unfortunately, the number of conflicting pairs can be up to O(n2), so we need to improve this solution. Let's introduce a variable for each vertex of the tree which will define the character we write on it. At first, it looks like we can't use these variables in 2-SAT, since the number of possible characters is 26, not 2. But if a vertex is covered by at least one path in a triple, then there are only two possible characters we can write in this vertex: either the character which will land on this position if we write the string from xi to yi, or the character on the opposite position in the string si. And, obviously, if a vertex is not covered by any triple, we can write any character on it.

Okay, now for each vertex i, we have two options for a character: ci,1 and ci,2. Let the variable pi be true if we write ci,1 on vertex i, and false if we write ci,2. Also, for each triple j, let's introduce a variable wj which is true if the string sj is written from xj to yj, and false if it is written in reversed order. If the vertex i is the k-th one on the path from xj to yj, then we should add the following constraints in our 2-SAT:

if sj,k≠ci,1, we need a constraint "NOT pi OR NOT wj";if sj,k≠ci,2, we need a constraint "pi OR NOT wj";if sj,|sj|−k+1≠ci,1, we need a constraint "NOT pi OR wj";if sj,|sj|−k+1≠ci,2, we need a constraint "pi OR wj".Thus, we add at most 16⋅105 constraints in our 2-SAT. The only thing we haven't discussed is how to actually restore each path from xj to yj; this can be done either with any fast algorithm that finds LCA, or by searching for LCA "naively" by ascending from one of those vertices until we arrive at the ancestor of another vertex; this approach will visit at most ∑j=1q|sj| vertices.

Overall, this solution runs in O(n+q+∑j=1q|sj|).

Educational Codeforces Round 124 Editorial

By awoo, history, 11 months ago, translation, In English1651A - Playoff

Idea: BledDest

Tutorial1651A - PlayoffDuring the first stage, every player with an even index competes against a player with an odd index, so in each match during the first stage, the player whose index is smaller wins. The pairs are formed in such a way that, in each pair, the player with an odd index has smaller index, so all players with even indices get eliminated, and all players with odd indices advance to the next stage.

All of the remaining matches are between players with odd indices, so the winner of each match is the player with the larger index. So, the overall winner of the tournament is the player with the greatest odd index, which is 2n−1.

Note: in some languages (for example, C++), standard power functions work with floating-point numbers instead of integers, so they will produce the answer as a floating-point number (which may lead to wrong formatting of the output and/or calculation errors). You might have to implement your own power function that works with integers, or compute 2n using a loop.

Solution (BledDest)1651B - Prove Him Wrong

Idea: adedalic

Tutorial1651B - Prove Him WrongSuppose the initial sum of a is equal to S. If we perform the operation, the new sum will be equal to S′=S−(ai+aj)+2|ai−aj|. We want the sum not to decrease, or S′≥S. If ai≥aj, we will get:S′≥S,S−(ai+aj)+2(ai−aj)≥S,ai−3aj≥0,ai≥3aj.If ai≤aj we'll get 3ai≤aj analogically.

In other words, array a you need (if sorted) will have a2≥3a1, a3≥3a2 and so on. And one of the variants (and, obviously, an optimal one) is just [1,3,9,27,…,3n−1].

As a result, since ai≤109, we just need to check: if 3n−1≤109 then we found an answer, otherwise there is no counterexample.

Solution (awoo)1651C - Fault-tolerant Network

Idea: adedalic

Tutorial1651C - Fault-tolerant NetworkThere is a criterion when the given network becomes fault-tolerant: the network becomes fault-tolerant if and only if each of corner computers (let's name them A1, An, B1 and Bn) is connected to the other row.

From the one side: if, WLOG, A1 is not connected to other row then if A2 is broken — A1 loses connection to the other network (since A1 is connected only with A2).

From the other side: suppose, WLOG, Ai is broken, then the row A is falling in at most two parts: A1−⋯−Ai−1 and Ai+1−⋯−An. But since both A1 and An are connected to row B and B is still connected, then the resulting network is still connected.

Now the question is: how to connect all corner computers? Because sometimes it's optimal not to connect corners directly. One of the approaches is described below.

Let's look at A1. Essentially, there are three ways to connect it to row B: to B1, Bn or bestB(A1) (where bestB(A1) is Bj with minimum possible |ai−bj|). The same applies to An.

So, let's just iterate over all these 3×3 variants. For each of these variants,

if we didn't cover B1 then we should also add one more connection between B1 and bestA(B1);if we didn't cover Bn then we should also add one more connection between Bn and bestA(Bn);As a result, we choose the best variant.

Solution (adedalic)1651D - Nearest Excluded Points

Idea: BledDest

Tutorial1651D - Nearest Excluded PointsFirstly, we can find answers for all points that are adjacent to at least one point not from the set. The distance for such points is obviously 1 (and this is the smallest possible answer we can get). On the next iteration, we can set answers for all points that are adjacent to points with found answers (because they don't have neighbors not from the set, the distance for them is at least 2). It doesn't matter which point we will take, so if the point i is adjacent to some point j that have the answer 1, we can set the answer for the point i as the answer for the point j. We can repeat this process until we find answers for all points. In terms of the code, this can be done by breadth first search (BFS). In other words, we set answers for the points that have the distance 1 and then push these answers to all adjacent points from the set in order of the increasing distance until we find all the answers.

Time complexity: O(nlogn).

Solution (vovuh)1651E - Sum of Matchings

Idea: BledDest

Tutorial1651E - Sum of MatchingsInstead of counting the edges belonging to the maximum matching, it is easier to count the vertices. So, we will calculate the total number of vertices saturated by the maximum matching over all possible tuples (l,r,L,R), and then divide the answer by 2. Furthermore, it's easier to calculate the number of unsaturated vertices than the number of saturated vertices, so we can subtract it from the total number of vertices in all graphs we consider and obtain the answer.

Let's analyze how to calculate the total number of unsaturated vertices. Each graph G′(l,r,L,R) is a subgraph of the given graph, so it is still bipartite, and the degree of each vertex is still not greater than 2. A bipartite graph where the degree of each vertex is at most 2 can be represented as a set of cycles and paths, and the maximum matching over each of these cycles/paths can be considered independently. Each cycle has an even number of vertices (since otherwise the graph would not be bipartite), so we can saturate all vertices on a cycle with the matching. For a path, the number of unsaturated vertices depends on its length: if the number of vertices in a path is even, we can match all vertices on it; otherwise, one vertex will be unsaturated. So the problem reduces to counting paths with odd number of vertices in all possible graphs G′(l,r,L,R).

Every path with an odd number of vertices has a center (the vertex which is exactly in the middle of the path). Let's iterate on the center of the path and its length, and calculate the number of times this path occurs in all graphs we consider. Suppose the center of the path is the vertex x, and the number of vertices in it is 2k+1. Then, for this path to exist, two conditions must hold:

every vertex y such that the distance from x to y is not greater than k should be present in the graph;every vertex z such that the distance from x to z is exactly k+1 should be excluded from the graph.It means that, for each of the two parts of the graph, there are several vertices that should be present in the graph, and zero or two vertices that should be excluded from the graph. It's easy to see that among the vertices we have to include, we are only interested in the minimum one and the maximum one (all vertices between them will be included as well if these two are included). So, we need to implement some kind of function that allows us to calculate the number of segments that cover the minimum and the maximum vertex we need, and don't cover any of the vertices that we have to exclude — this can be easily done in O(1). Note that the segments should be considered independently for both parts of the graph.

Overall, for each vertex we have to consider at most O(n) different lengths of odd paths with the center in this vertex. The minimum/maximum indices of vertices in both parts we have to include in the graph can be maintained while we increase the length of the path, so the whole solution works in O(n2).

Solution (BledDest)1651F - Tower Defense

Idea: BledDest

Tutorial1651F - Tower DefenseLet's start thinking about the problem from the easy cases.

How to solve the problem fast if all towers have full mana? We can store prefix sums of their capacities and find the first tower that doesn't get drained completely with a binary search.

Let's try the opposite. How to solve the problem fast if all towers were drained completely in the previous second? It's the same but the prefix sums are calculated over regeneration rates.

What if all towers were drained at the same second, earlier than the previous second, and no tower is fully restored yet? It's also the same but the regeneration rates are multiplied by the time passed since the drain.

What if we drop the condition about the towers not being fully restored? How would a data structure that can answer prefix sum queries work? It should store the total mana capacity of all towers that are full. Then mana regeneration rates for all towers that aren't. If these are kept separately, then it's easy to obtain the prefix sum by providing the time passed. This will be total capacity plus total regeneration rate, multiplied by the time passed.

How to determine if the tower is fully restored since the drain or not? That's easy. For each tower, we can calculate the number of seconds it takes it to get restored from zero. That is ⌈cr⌉. Thus, all towers that have this value smaller than the time passed won't get restored. All the rest will.

Unfortunately, in the actual problem, not all towers were last drained at the same time. However, it's possible to reduce the problem to that. Store the segments of towers that were drained at same time. There are also towers that weren't drained completely, but they can be stored as segments of length 1 too. When a monster comes, it drains some prefix of the towers completely and possibly one more tower partially. In terms of segments, it removes some prefix of the them and possibly cuts one. Then it creates a segment that covers the prefix and possibly a segment of length 1 (with a partially drained tower). So each monster creates O(1) segments and removes no more segments than were created. Thus, if we were to process each creation and removal in some O(T), then the complexity will be O(qT).

All towers on each segment have the same time passed since the drain. We want to query the sum on the entire segment. If it is greater than the remaining health of the monster, we want to find the largest prefix of this segment that has a smaller or equal sum than the monster health.

Given time passed, let's learn to query the range sum. If we knew the queries beforehand, it would be easy. Initialize a segment tree as if all towers are completely restored. Then make events of two kinds: a tower with restore time ⌈cr⌉ and a query with time t. Sort them in the decreasing order and start processing one by one. When a tower event happens, update a single position in the segment tree from capacity to regeneration rate. When a query event happens, find the sum.

Since the queries are not known beforehand, make that segment tree persistent and ask specific versions of it. If a segment of towers was last drained at time tlst, and the query is at time t, then you should query the segment tree in version t−tlst. Obviously, you can store not all versions but only ones that have some tower change. Moreover, it's more convenient to make one version responsible for one tower update. Then you can lower_bound the array of sorted ⌈cr⌉ to find the version you want to ask at.

To determine the largest prefix of this segment that has a smaller or equal sum than the monster health, you can either binary search for O(log2n) or traverse the segment tree for O(logn). The time limit might be a little tight for the first approach, but it can still pass.

Overall complexity: O((n+q)logn).

Educational Codeforces Round 123 Editorial

By awoo, history, 12 months ago, translation, In English1644A - Doors and Keys

Idea: BledDest

Tutorial1644A - Doors and KeysThe necessary and sufficient condition is the following: for each color the key should appear before the door.

Necessary is easy to show: if there is a key after a door, this door can't be opened.

Sufficient can be shown the following way. If there are no closed doors left, the knight has reached the princess. Otherwise, consider the first door the knight encounters. He has a key for this door, so he opens it. We remove both the key and the door from the string and proceed to the case with one less door.

Overall complexity: O(1).

Solution (awoo)1644B - Anti-Fibonacci Permutation

Idea: BledDest

Tutorial1644B - Anti-Fibonacci PermutationLet's consider one of the possible solutions. Let's put the first element in the x-th permutation equal to x, and sort all the other elements in descending order. Thus, we get permutations of the form: [1,n,n−1,…,2], [2,n,n−1,…,1], ..., [n,n−1,n−2,…,1]. In such a construction pi−1>pi for all i (3≤i≤n), and hence pi−2+pi−1>pi.

Solution (Neon)1644C - Increase Subarray Sums

Idea: BledDest

Tutorial1644C - Increase Subarray SumsConsider the naive solution.

Iterate over k. Then iterate over the segment that will have the maximum sum. Let its length be l. Since x is non-negative, it's always optimal to increase the elements inside the segment. So if k≤l, then the sum of the segment increases by k⋅x. Otherwise, only the elements inside the segment will affect the sum, thus, it will increase by l⋅x. That can be written as min(k,l)⋅x.

Notice that we only care about two parameters for each segment. Its length and its sum. Moreover, if there are several segments with the same length, we only care about the one with the greatest sum.

Thus, the idea of the solution is the following. For each length, find the segment of this length with the greatest sum. Then calculate f(k) in O(n) by iterating over the length of the segment.

Overall complexity: O(n2) per testcase.

Solution (awoo)1644D - Cross Coloring

Idea: BledDest

Tutorial1644D - Cross ColoringLet's take a look at a final coloring. Each cell has some color. There exist cells such that there were no operation in their row and their column. They are left white, and they don't affect the answer.

All other cells are colored in one of k colors. For each cell (x,y) there is a query that has been the last one to color this cell (it covered row x, column y or both of them). So all cells that have the same query as the last one will have the same color. Since the color for each query is chosen independently, the number of colorings will be k to the power of the number of queries that have at least one cell belong to them.

How to determine if a query has at least one cell. This is true unless one of these things happen afterwards:

both its row and its column are recolored;all rows are recolored;all columns are recolored.So the solution is to process the queries backwards. Maintain the set of colored rows and colored columns. For each query, check the conditions. If none hold, multiply the answer by k.

Overall complexity: O(qlog(n+m)) or O(q) per testcase.

Solution 1 (awoo)Solution 2 (awoo)1644E - Expand the Path

Idea: BledDest

Tutorial1644E - Expand the PathFirst, get rid of the corner cases. If the string doesn't contain either of the letters, the answer is n.

The general solution to the problem is to consider every single way to modify the path, then find the union of them. Well, every single path is too much, let's learn to reduce the number of different sequences of modifications that we have to consider.

The main observation is that all cells that the robot can visit are enclosed in the space formed by the following two paths:

the first 'R' is duplicated the maximum number of times, then the last 'D' is duplicated the maximum number of times;the first 'D' is duplicated the maximum number of times, then the last 'R' is duplicated the maximum number of times.You can realize that by drawing the visited cells for some large test.

To show that more formally, you can consider the visited cells row by row. Let's show that for every two different visited cells in the same row, all cells in-between them can also be visited.

In general case, we want to show that we can take the prefix of the path to the left one of these cells and duplicate any 'R' on it to reach the right cell. The suffixes of the paths will remain the same as in the initial path.

If there exists an 'R' on the prefix, then we are good. Otherwise, the reason that it doesn't exist is that we duplicated 'D' too many times. Reduce that and there will be 'R' immediately after reaching the cell or earlier.

We should also show that the number of 'R's on the path to the left cell won't reach the maximum allowed amount until reaching the right cell. Use the fact that the number of 'D's on both prefixes of the paths is the same.

The other non-obvious part is that you can't reach cells outside this space. However, that can also be shown by analyzing each row independently.

Finally, about the way to calculate the area of this space. The main idea is to calculate the total number of cells outside this area and subtract it from n2.

Notice that non-visited cells form two separate parts: the one above the first path and the one to the left of the second path. These are pretty similar to each other. Moreover, you can calculate them with a same function. If we replace all 'D's in the string with 'R' and vice versa, then these parts swap places. So we can calculate the upper part, swap them and calculate it again.

I think the algorithm is best described with a picture. Consider test n=15, s= DDDRRDRRDDRRR, for example.

First, there are some rows that only have one cell visited. Then the first 'R' in the string appears. Since we duplicate it the maximum amount of times, it produces a long row of visited cells. The remaining part of the part becomes the outline of the area.

Note that the row that marks the end of the string, always ends at the last column. Thus, only at most first |s| rows matter. To be exact, the amount of rows that matter is equal to the number of letters 'D' in the string.

For each letter 'D', let's calculate the number of non-visited cells in a row it goes down to.

I found the most convenient way is to go over the string backwards. We start from the row corresponding to the number of letters 'D' in the string. It has zero non-visited cells. We can maintain the number of non-visited cells in the current row. If we encounter an 'R' in the string, we add 1 to this number. If we encounter a 'D', we add the number to the answer.

We have to stop after the first 'R' in the string. The later (well, earlier, since we are going backwards) part corresponds to the prefix of letters 'D' — the starting column on the picture.

Each of these rows have 1 visited cell, so n−1 non-visited. So we can easily calculate this part as well.

Overall complexity: O(|s|) per testcase.

Solution (awoo)1644F - Basis

Idea: BledDest

Tutorial1644F - BasisFirst of all, since the second operation changes all occurrences of some number x to other number y and vice versa, then, by using it, we can convert an array into another array if there exists a bijection between elements in the first array and elements in the second array. It can also be shown that F(G(a,x,y),m)=G(F(a,m),x,y), so we can consider that if we want to transform an array into another array, then we first apply the function F, then the function G.

Another relation that helps us is that G(G(a,x,y),x,y)=a, it means that every time we apply the function G, we can easily rollback the changes. Considering that we have already shown that a sequence of transformations can be reordered so that we apply G only after we've made all operations with the function F, let's try to "rollback" the second part of transformations, i. e. for each array, find some canonical form which can be obtained by using the function G.

Since applying the second operation several times is equal to applying some bijective function to the array, we can treat each array as a partition of the set {1,2,…,n} into several subsets. So, if we are not allowed to perform the first operation, the answer to the problem is equal to ∑i=1min(n,k)S(n,i), where S(n,i) is the number of ways to partition a set of n objects into i non-empty sets (these are known as Stirling numbers of the second kind). There are many ways to calculate Stirling numbers of the second kind, but in this problem, we will have to use some FFT-related approach which allows getting all Stirling numbers for some value of n in time O(nlogn).

For example, you can use the following relation:

S(n,k)=1k!∑i=0k(−1)i(ki)(k−i)nS(n,k)=∑i=0k(−1)i⋅k!⋅(k−i)nk!⋅i!⋅(k−i)!S(n,k)=∑i=0k(−1)ii!⋅(k−i)n(k−i)!If we substitute pi=(−1)ii! and qj=jnj!, we can see that the sequence of Stirling numbers for some fixed n is just the convolution of sequences p and q.

For simplicity in the following formulas, let's denote Ai=∑j=1min(i,k)S(i,j). We now know that this value can be calculated in O(ilogi).

Okay, now back to the original problem. Unfortunately, we didn't take the operation F into account. Let's analyze it.

The result of function F(a,m) consists of several blocks of equal elements, and it's easy to see that the lengths of these blocks (except for maybe the last one) should be divisible by m. The opposite is also true — if the lengths of all blocks (except maybe for the last one) are divisible by some integer m, then the array can be produced as F(a,m) for some array a.

What does it mean? If the greatest common divisor of the lengths of the blocks (except for the last one) is not 1, the array that we consider can be obtained by applying the function F to some other array. Otherwise, it cannot be obtained in such a way. Now, inclusion-exclusion principle comes to the rescue.

Let's define Bi as the number of arrays that we consider which have the lengths of all their blocks (except maybe for the last one) divisible by i. It's easy to see that Bi=A⌈ni⌉ (we can compress every i consecutive elements into one). Then, using inclusion exclusion principle, we can see that the answer is

∑i=1nμ(i)Bi=∑i=1nμ(i)A⌈ni⌉,

where μ(i) is the Mobius function. Using this formula, we can calculate the answer in O(nlog2n).

Note 1. This inclusion-exclusion principle handles the arrays according to the GCD of the blocks that they consist of, except for the last one. But what if the array consists only of one block? These arrays can be counted wrongly, so we should exclude them — i. e. use Ai−S(i,1) instead of just Ai and count the arrays consisting of the same element (if we need any of them in the answer separately).

Note 2. Depending on the way you implement this, n=1 or k=1 (or both) may be a corner case.

Educational Codeforces Round 122 — Editorial

By BledDest, 12 months ago, In English1633A - Div. 7

Idea: BledDest, preparation: BledDest

Tutorial1633A - Div. 7A lot of different solutions can be written in this problem. The model solution relies on the fact that every 7-th integer is divisible by 7, and it means that there is always a way to change the last digit of n (or leave it unchanged) so that the result is divisible by 7. So, if n is already divisible by 7, we just print it, otherwise we change its last digit using some formulas or iteration on its value from 0 to 9.

Solution (BledDest)1633B - Minority

Idea: BledDest, preparation: awoo and Neon

Tutorial1633B - MinorityLet's try to estimate the maximum possible answer. Best case, you will be able to remove either all zeros or all ones from the entire string. Whichever has the least occurrences, can be the answer.

If the amounts of zeros and ones in the string are different, this bound is actually easy to reach: just choose the substring that is the entire string.

If the amounts are the same, the bound is impossible to reach. Choosing the entire string will do nothing, and asking a smaller substring will decrease the answer.

The smallest we can decrease the answer by is 1. If you choose the substring that is the string without the last character, you will decrease one of the amounts by one. That will make the amounts different, and the bound will be reached.

Overall complexity: O(|s|) per testcase.

Solution (awoo)1633C - Kill the Monster

Idea: BledDest, preparation: Neon

Tutorial1633C - Kill the MonsterFirst of all, let's understand how to solve the problem without upgrades. To do this, it is enough to compare two numbers: ⌈hMdC⌉ and ⌈hCdM⌉ — the number of attacks that the character needs to kill the monster and the number of attacks that the monster needs to kill the character, respectively. So, if the first number is not greater than the second number, then the character wins.

Note that the number of coins is not very large, which means we can iterate over the number of coins that we will spend on weapon upgrades, and the remaining coins will be spent on armor upgrades. After that, we can use the formula described above to check whether the character will win.

The complexity of the solution is O(k).

Solution (awoo)1633D - Make Them Equal

Idea: BledDest, preparation: Neon

Tutorial1633D - Make Them EqualLet's calculate di — the minimum number of operations to get the number i from 1. To do this, it is enough to use BFS or dynamic programming. Edges in the graph (transitions in dynamic programming) have the form (i,i+⌊ix⌋) for all 1≤x≤i.

Now the problem itself can be reduced to a knapsack problem: there are n items, i-th item weighs dbi and costs ci, you have to find a set of items with the total weight of no more than k of the maximum cost. This is a standard problem that can be solved in O(nk), but it is too slow (although some participants passed all the tests with such a solution). However, we can notice that the values of di should not grow too fast, namely, the maximum value of di for 1≤i≤103 does not exceed 12. This means that the maximum possible weight is no more than 12n, and we can limit k to this number (i. e. make k=min(k,12n)).

Solution (Neon)1633E - Spanning Tree Queries

Idea: BledDest, preparation: awoo

Tutorial1633E - Spanning Tree QueriesConsider a naive solution using Kruskal's algorithm for finding MST. Given some x, you arrange the edges in the increasing order of |wi−x| and process them one by one.

Look closely at the arrangements. At x=0 the edges are sorted by wi. How does the arrangement change when x increases? Well, some edges swap places.

Consider a pair of edges with different weights w1 and w2 (w1<w2). Edge 1 will go before edge 2 in the arrangement as long as x is closer to w1 than w2. So for all x up to w1+w22, edge 1 goes before edge 2. And for all x from w1+w22 onwards, edge 2 goes before edge 1.

This tells us that every pair of edge with different weights will swap exactly once. So there will be at most O(m2) swaps. Which is at most O(m2) different arrangements. Each of them corresponds to some range of x's.

We can extract the ranges of x's for all arrangements and calculate MST at the start of each range. We can also find the arrangement that corresponds to some x from a query with a binary search.

However, only knowing the weight of the MST at the start of the range is not enough. The weights of edges change later in the range, and we can't predict how. Some edges have their weight increasing, some decreasing.

First, let's add more ranges. We want each edge to behave the same way on the entire range: either increase all the way or decrease all the way. If we also add x=wi for all i into the MST calculation, this will hold.

Second, let's store another value for each range: the number of edges that have their weight increasing on it. With that, we can easily recalculate the change in the cost of the spanning tree.

The TL should be free enough for you to sort the edges for each MST calculation, resulting in O(m2(mlogm+nlogn)+klogm) solution. You can also optimize the first part to O(m3).

Solution (awoo)1633F - Perfect Matching

Idea: BledDest, preparation: BledDest

Tutorial1633F - Perfect MatchingLet's root the tree at vertex 1 and try to analyze when a tree contains a perfect matching. If we want to find the maximum matching in a tree, we can use some greedy approaches like "take a leaf of the tree, match it with its parent and remove both vertices, repeat this process until only isolated vertices remain". If we are interested in a perfect matching, then this process should eliminate all of the vertices.

Let's modify this process a bit by always picking the deepest leaf. If there exists a perfect matching, picking the deepest leaf will ensure that the tree always remains a tree and doesn't fall apart, i. e. there will always be one connected component. It means that when we remove the leaf with its parent, this leaf is the only descendant of its parent.

It's easy to see that whenever we remove a pair of vertices in this process, for each remaining vertex, the number of its descendants is either left unchanged or decreased by 2. It means that if a vertex has an even number of descendants, it will have an even number of descendants until it is removed, and the same for odd number of descendants.

Let's call the vertices with even number of descendants (including the vertex itself) even vertices, and all the other vertices — odd vertices. A vertex cannot change its status in the process of building the perfect matching. Each leaf is and odd vertex, and if its parent has only one child, this parent is an even vertex. So, when we remove a pair of vertices, one of them (the child) is odd, and the other of them (the parent) is even.

This leads us to another way of building the perfect matching: match each odd vertex with its parent, and make sure that everything is correct. Unfortunately, implementing it is O(n) per query, so we need something faster. We can see that each even vertex has at least one odd child (because if all children of a vertex are even, then the number of its descendants, including the vertex itself is odd). In order to find a perfect matching, we have to make sure that:

each even vertex has exactly one odd child;each odd vertex has an even vertex as its parent.All this means is that the number of even vertices should be equal to the number of odd vertices: it cannot be greater since each even vertex has at least one odd child, and if it is smaller, it's impossible to match the vertices. The perfect matching itself consists of edges that connect odd vertices with their parents.

Okay, now we need some sort of data structure to maintain the status of each vertex (and the sum of edges that lead to an odd vertex if directed from top to bottom). In our problem, we have to add new leaves to the tree (it happens when a vertex is activated), and this increases the number of descendants for every vertex on the path from the root to this new leaf. So, we need some sort of data structure that supports the operations "add a new leaf" and "flip the status of all vertices on a path". One of the structures that allow this is the Link/Cut Tree, but we can use the fact that the whole tree is given in advance to build a Heavy-Light Decomposition on it, which is much easier to code. Operations on segments of paths can be done with a lazy segment tree, and each vertex then will be added in O(log2n).

Educational Codeforces Round 121 Editorial

By awoo, history, 13 months ago, translation, In English1626A - Equidistant Letters

Idea: BledDest

Tutorial1626A - Equidistant LettersLet's consider a very special case of equal distances. What if all distances were equal to 1? It implies that if some letter appears exactly twice, both occurrences are placed right next to each other.

That construction can be achieved if you sort the string, for example: first right down all letters 'a', then all letters 'b' and so on. If a letter appears multiple times, all its occurrences will be next to each other, just as we wanted.

Overall complexity: O(|s|log|s|) or O(|s|) per testcase.

Solution (awoo)1626B - Minor Reduction

Idea: BledDest

Tutorial1626B - Minor ReductionLet's think how a reduction changes the length of x. There are two cases. If two adjacent letters sum up to 10 or greater, then the length doesn't change. Otherwise, the length decreases by one.

Obviously, if there exists a reduction that doesn't change the length, then it's better to use it. Which among such reduction should you choose? Well, notice that such a reduction always makes the number strictly smaller (easy to see with some case analysis). Thus, the logical conclusion is to leave the longest possible prefix of x untouched. So, the rightmost such reduction will change the number as little as possible.

If all reductions decrease the length, then a similar argument can be applied. The sum will be a single digit, but a digit that is greater than or equal to the left one of the adjacent pair. If it was just greater, it's easy to see that the leftmost such reduction will make the number the largest possible. The equal case adds more case analysis on top of the proof, but the conclusion remains the same: the leftmost reduction is the best one.

As an implementation note, since all the reductions are of the same type, the leftmost reduction always includes the first and the second digits.

Overall complexity: O(|x|) per testcase.

Solution (awoo)1626C - Monsters And Spells

Idea: BledDest

Tutorial1626C - Monsters And SpellsConsider the problem with n=1. There is a single monster with some health h that appears at some second k. In order to kill it, we have to wind up our spell until it has damage h. So we have to use it from second k−h+1 to second k. Look at it as a segment [k−h+1;k] on a timeline.

Actually, to avoid handling zero length segments, let's instead say that a segment covers the time from k−h non-inclusive to k inclusive, producing a half-interval (k−h;k]. This way, the total mana cost will be len(len+1)2, where len is the length of the half-interval.

Now n=2. There are two time segments.

If they don't intersect (segments (1;2] and (2;3] don't intersect, since they are half-intervals), then it's always better to wind up the spell for the monsters separately instead of saving the damage.

However, if they intersect, then we don't have the choice other than to save the damage from the earlier one to the later one. Otherwise, there won't be enough time to wind up the spell.

What that means in a mathematic sense? The answer is the union of two half-intervals. If they don't intersect, they are left as is. Otherwise, they become one half-interval that covers them both.

Now add the third monster into the construction. The same argument applies. While there exists a pair of intersecting half-intervals, keep uniting them.

The union of all half-intervals can be found in O(nlogn), but the constraints allowed slower approaches as well.

Solution (awoo)1626D - Martial Arts Tournament

Idea: BledDest

Tutorial1626D - Martial Arts TournamentSort the weights, now choosing x and y will split the array into three consecutive segments.

Consider a naive solution to the problem. You can iterate over the length of the first segment and the second segment. The third segment will include everyone remaining.

Now you have to check if there exist some x and y that produce such segment. x can be equal to the first element of the second segment (since only all elements of the first segment are smaller than it). Similarly, y can be equal to the first element of the third segment.

However, if the last element of some segment is equal to the first element of the next segment, no x or y can split the array like that.

Otherwise, you can split an array like that. So you can iterate over the lengths, check the correctness and choose the best answer.

Now let's optimize it using the condition about powers of two.

First, iterate over the size of the middle division (which is a power of two). Then over the length of the first segment (which can be not a power of two). Check if the first segment is valid.

So we fixed the length of the first segment and some value which is greater or equal than the length of the second segment. That value isn't necessarily equal to the length of the second segment because the produced segment might be invalid.

So there is a greedy idea that the second segment should be as long as possible under the constraint that it doesn't exceed the fixed value. The intuition is the following. Consider the longest possible valid segment. Now take the last element away from it. We will have to invite one more participant to the middle division. And that element will also get added to the third segment, increasing its length. So potentially, you can only increase the required number of participants to invite.

This can be implemented in the following fashion. For each position i precalculate lefti — the closest possible segment border from the left. Iterate over the size of the middle division mid as a power of two. Iterate over the length of the first segment len1. Find the closest border to the left of len1+mid=left[len1+mid]. Get the lengths of the second and the third segments. Find the closest powers of two to each length and update the answer.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1626E - Black and White Tree

Idea: BledDest

Tutorial1626E - Black and White TreeI think there are some ways to solve this problem with casework, but let's try to come up with an intuitive and easy-to-implement approach.

It's always possible to move closer to some black vertex, no matter in which vertex you are currently and which black vertex was used in the previous operation. However, sometimes if you try to move along an edge, you immediately get forced back. Let's analyze when we can move without being forced back.

We can move along the edge x→y so that our next action is not moving back if:

either y is black (there is no next action);or, if we remove the edge between x and y, the number of black vertices in y's component is at least 2 (we can use one of them to go from x to y, and another one to continue our path).Note that the cases x→y and y→x may be different (sometimes it will be possible to move in one direction, and impossible to move in the opposite direction).

Let's treat this possible move x→y as an arc in a directed graph. We can find all such arcs if we can answer the queries of the type "count black vertices in a subtree of some vertex", and this can be done by rooting the tree and calculating this information for each subtree with DFS.

Now, if there is a way from some vertex i to some black vertex along these arcs, the answer for the vertex i is 1. How can we find all such vertices? Let's transpose the graph (change the direction of each arc to opposite), now we need to find all vertices reachable from black ones — which is easily done with multisource BFS or DFS.

The complexity of this solution is O(n).

Solution (BledDest)1626F - A Random Code Problem

Idea: BledDest

Tutorial1626F - A Random Code ProblemI think it's easier to approach this problem using combinatorics instead of probability theory methods, so we'll calculate the answer as "the sum of values of ans over all ways to choose the index on each iteration of the loop".

If a number aidx is chosen on the iteration i of the loop, then it is reduced to the maximum number divisible by i that doesn't exceed the initial value. So, if a number is divisible by all integers from 1 to k, i. e. divisible by L=LCM(1,2,…,k), it won't be changed in the operation. Furthermore, if ⌊aidxL⌋=x, then the value of this element won't become less than x⋅L.

It means that we can interpret each number ai as ai=x⋅L+y, where x=⌊aiL⌋ and y=aimodL. The part with x⋅L will always be added to the variable ans when this element is chosen, so let's add k⋅nk−1⋅x⋅L to the answer (which is the contribution of x⋅L over all ways to choose the indices in the operations), and work with aimodL instead of ai.

Now all elements of the array are less than L. We can use this constraint by writing the following dynamic programming to solve the problem: dpi,j is the number of appearances of the integer i in the array a over all ways to choose the indices for the first j iterations.

For j=0, dp is just the number of occurrences of each integer in the array a. The transitions from dpi,j are the following ones:

if this element is chosen in the operation, then it becomes i′=i−(imod(j+1)), and we transition to the state dpi′,j+1;otherwise, the element is unchanged, and we transition to the state dpi,j+1, multiplying the current value by n−1, which is the number of ways to choose some other element in the operation.How can we use this dynamic programming to get the answer? On the (j+1)-th iteration, the number of times we choose the integer i is exactly dpi,j, and the number of ways to use the integers in the next operations is nk−j−1, so we add i⋅dpi,j⋅nk−j−1 to the answer for every such state dpi,j.

This solution runs in time O(n+LCM(1,2,…,k)⋅k), which may be too slow if not implemented carefully. Fortunately, we have an easy way to optimize it: use L=LCM(1,2,…,k−1) instead of L=LCM(1,2,…,k), which divides L by 17 in the worst case scenario for our solution. We can do this because even if an integer is changed on the k-th operation, we are not interested in this change since this is the last operation.

Educational Codeforces Round 120 Editorial

By awoo, history, 14 months ago, translation, In English1622A - Construct a Rectangle

Idea: BledDest

Tutorial1622A - Construct a RectangleFirst, the condition about being able to construct a rectangle is the same as having two pairs of sticks of equal length.

Let's fix the stick that we are going to break into two parts. Now there are two cases.

The remaining two sticks can be the same. In that case, you can break the chosen stick into equal parts to make the second equal pair of sticks. Note, however, that the stick should have an even length, because otherwise the length of the resulting parts won't be integer.

The remaining two sticks can be different. In that case, the chosen stick should have the length equal to their total length, because the only way to make two pairs of equal sticks is to produce the same two sticks as the remaining ones.

Overall complexity: O(1) per testcase.

Solution (Neon)1622B - Berland Music

Idea: adedalic

Tutorial1622B - Berland MusicSince we know that every disliked song should have lower rating than every liked song, we actually know which new ratings should belong to disliked songs and which should belong to the liked ones.

The disliked songs take ratings from 1 to the number of zeros in s. The liked songs take ratings from the number of zeros in s plus 1 to n. Thus, we have two independent tasks to solve.

Let the disliked songs have ratings d1,d2,…,dk. Their new ratings should be 1,2,…,k. We can show that if we sort the array d, then |d′1−1|+|d′2−2|+⋯+|d′k−k| will be the lowest possible. The general way to prove it is to show that if the order has any inversions, we can always fix the leftmost of them (swap two adjacent values), and the cost doesn't increase.

So the solution can be to sort triples (si,pi,i) and restore q from the order of i in these.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1622C - Set or Decrease

Idea: adedalic

Tutorial1622C - Set or DecreaseFirst, we can prove that the optimal way to perform operations is first, decrease the minimum value several (maybe, zero) times, then take several (maybe, zero) maximums and make them equal to the minimum value.

The proof consists of several steps:

Prove that first, we make decreases, only then sets:if some ai=ai−1 is done after some aj=ak then if there were no modification of ai then you can just move ai=ai−1 earlier. Otherwise, there were ai=ak, and you can replace (... ai=ak, ai=ai−1 ...) with (... ak=ak−1, ai=ak ...). We demonstrated how to move decrease operations before set operations.Prove that it's optimal to decrease only one element ai:instead of decreasing ai by x and aj by y (where ai≤aj), we can decrease ai by x+y and replace all ak=aj with ak=ai.It's optimal to decrease the minimum element — it follows from proof of previous step.If we make y set operations, it's optimal to set minimum value to y maximum elements — should be obvious.To use the strategy, we'll firstly sort array a in non-decreasing order. In this case, we'll decrease a1 by x and perform set to y elements an−y+1,…,an. The question is: how to minimize value of x+y?

Note, that 0≤y<n (since setting the same position multiple times has no sense). Let's iterate over all possible values of y and determine the minimum x needed.

The resulting array will consists of (a1−x),a2,a3,…,an−y,(a1−x),(a1−x),…,(a1−x). Let's say that P(i)=a1+a2+⋯+ai (and all P(i) can be precomputed beforehand). Then the sum of array will become (a1−x)(y+1)+P(n−y)−a1, and we need(a1−x)(y+1)+P(n−y)−a1≤k(a1−x)(y+1)≤k−P(n−y)+a1a1−x≤⌊k−P(n−y)+a1y+1⌋or, since we need minimum possible x,x=a1−⌊k−P(n−y)+a1y+1⌋Using the formula above, we can for each y (0≤y<n) calculate minimum x required. But to be accurate, value k−P(n−y)+a1 may be negative, and, usually in programming languages, integer division cd for negative c returns ⌈cd⌉ instead of ⌊cd⌋.

There is an alternative solution: note that if ∑ai≤k, then a1≤kn. Note that if a1≥kn then resulting value of a1−x is in kn−n<a1−x≤nk and there are at most n possible value for x. So, you can iterate over all possible x and for each x calculate minimum required y either with binary search or two pointers.

Solution (adedalic)1622D - Shuffle

Idea: BledDest

Tutorial1622D - ShuffleWe could iterate on the substrings we want to shuffle and try to count the number of ways to reorder their characters, but, unfortunately, there's no easy way to take care of the fact that shuffling different substrings may yield the same result.

Instead, we will iterate on the first and the last character that are changed. Let these characters be i and j. First of all, let's check that they can belong to the same substring we can shuffle — it is the case if the string contains at least k characters 1, and the substring from the i-th character to the j-th character contains at most k characters 1.

Then, after we've fixed the first and the last characters that are changed, we have to calculate the number of ways to shuffle the characters between them (including them) so that both of these characters are changed. Let's calculate c0 and c1 — the number of characters 0 and 1 respectively in the substring. Then, we need to modify these two values: for example, if the i-th character is 0, then since it is the first changed character, it should become 1, so we need to put 1 there and decrease c1 by one. The same for the j-th character. Let c′0 and c′1 be the values of c0 and c1 after we take care of the fact that the i-th and the j-th character are fixed. The remaining characters can be in any order, so the number of ways to arrang them is (c′0+c′1c′0).

We can add up these values for all pairs (i,j) such that we can shuffle a substring containing these two characters. We won't be counting any string twice because we ensure that i is the first changed character, and j is the last changed character. Don't forget to add 1 to the answer — the string we didn't count is the original one.

This solution works in O(n2), but the problem is solvable in O(n).

Solution (BledDest)1622E - Math Test

Idea: BledDest

Tutorial1622E - Math TestNote that there are only two ways to fix the result of the operation of taking an absolute value in the expression |xi−ri|: xi−ri or ri−xi. Since the value of n is small enough that we can iterate over all 2n options, and choose the one for which the sum is maximum.

For each student, let's fix with which sign their total points will contribute to the answer, then xi will contribute with the opposite sign. Now, for the question j we can calculate valj — the coefficient with which pj will contribute to the answer. It remains to choose such a permutation p that the sum ∑j=1mpjvalj is the maximum possible. From here we can see that if valj<vali (for some i and j), then pj<pi must holds, otherwise we can swap pj and pi, and the answer will increase. This means that we can sort all questions in ascending order by the value in the val array, and assign the value x in the array p to the x-th question in ascending order.

For some of 2n options, the permutations we found may be illegal because it can happen that we consider the case that some |xi−ri| evaluates as (xi−ri), but in the best permutation we found for that option, it evaluates as (ri−xi). We can just ignore it because this will never be the case with the option giving the highest possible surprise value — if this thing happened for some option to choose the signs of ri, then, if we flip the signs for the students such that the conditions on them are not met in the optimal permutation, we'll get a combination of signs that yields a higher surprise value.

Solution (Neon)1622F - Quadratic Set

Idea: Neon

Tutorial1622F - Quadratic SetA good start to solve the problem would be to check the answers for small values of n. One can see that the answers (the sizes of the maximum subsets) are not much different from n itself, or rather not less than n−3. Let's try to prove that this is true for all n.

Consider n is even. Let n=2k, let's see what the product is equal to if we take all the numbers from 1 to n.

∏i=12ki!=∏i=1k(2i−1)!(2i)!=∏i=1k(2i−1)!22i=(∏i=1k(2i−1)!)2∏i=1k2i=(∏i=1k(2i−1)!)22kk!From here we can see that for even k the answer is at least n−1, because we can delete k! and the product of the remaining factorials will be the square of an integer, for odd k the answer is at least n−2, because we can delete 2! and k!.

It remains to prove that the answer is at least n−3 for odd n. This is easy to do, because the answer for n is not less than the answer for n−1 minus 1, because we can delete n! and solve the task with a smaller n value. Moreover, it can be seen from the previous arguments that the answer 3 can only be for n≡3(mod4), and we already know that in this case one of the correct answers is to remove the factorials 2,n−12,n.

It remains to learn how to check whether it is possible to remove 1 or 2 numbers so that the remaining product of factorials is the square of an integer.

To do this, we can use XOR hashes. Let's assign each prime number a random 64-bit number. For composite numbers, the hash is equal to the XOR of hashes of all its prime divisors from factorization. Thus, if some prime is included in the number an even number of times, it will not affect the value of the hash, which is what we need. The hash of the product of two numbers is equal to the XOR of the hashes of these numbers. Let's denote the hash function as H.

Using the above, let's calculate H(i) for all i from 1 to n, as well as H(i!) for all i from 1 to n, this is easy to do, because H(i!)=H((i−1)!)⊕H(i). We will also store a map H(i!)→i. Let's calculate the hash H(1!2!⋯n!) and denote it as fp. It remains to consider the following cases:

if fp=0, then the current product is already the square of an integer;for an answer of size n−1, we have to check that there exists such a i that H(i!)⊕fp=0. To find such i, let's check whether the map contains fp;for an answer of size n−2, we have to check that there are such i and j that H(i!)⊕H(j!)⊕fp=0. To do this, iterate over i, and then check whether map contains H(i!)⊕fp;otherwise, the answer is n−3, and there is an answer, where all numbers except 2,n−12,n are taken.

Educational Codeforces Round 119 Editorial

By awoo, history, 14 months ago, translation, In English1620A - Equal or Not Equal

Idea: BledDest

Tutorial1620A - Equal or Not EqualLet's look at a group of E: it's easy to see that each such a group is equal to the same number. Now, let's look at how these groups are distributed on the circle:

If there are no N then all ai are just equal to each other. It's okay.If there is exactly one N then from one side, all of them are still in one group, so they should be equal, but from the other side, one pair should have different values. It's contradiction.If there are more than one N then all numbers are divided in several groups with different values. It's okay.As a result, array a exists as long as the number of N isn't 1.

Solution (Neon)1620B - Triangles on a Rectangle

Idea: BledDest

Tutorial1620B - Triangles on a RectangleThe area of a triangle is equal to its base multiplied by its height divided by 2. Let the two points that have to be on the same side of a rectangle form its base. To maximize it, let's choose such two points that are the most apart from each other — the first and the last in the list.

Then the height will be determined by the distance from that side to the remaining point. Since there are points on all sides, the points on the opposite side are the furthest. Thus, the height is always one of h or w, depending on whether we picked the horizontal or the vertical side.

So we have to check four options to pick the side and choose the best answer among them.

Solution (awoo)1620C - BA-String

Idea: BledDest

Tutorial1620C - BA-StringFind all segments of asterisks in the string. Let there be t of them, and the number of asterisks in them be c1,c2,…,ct. That tells us that the i-th segment of asterisks can be replaced with at most ci⋅k letters 'b'.

Notice that we can compare two strings lexicographically using just the number of letters 'b' that replace each of t segments of asterisks. Let that sequence for some string a be A1,A2,…,At and that sequence for some string b be B1,B2,…,Bt. Then a<b if and only if A<B. That is, there exists such position i that Ai<Bi. The proof is trivial.

So we can actually look at the sequence A1,A2,…,At as some kind of number in a mixed base. The lowest "digit" At can be of one of ct⋅k+1 values (from 0 to ct⋅k). The second lowest — one of ct−1⋅k+1. And so on. Then, comparison of two strings is the same as comparison of these two mixed base numbers.

Thus, the task is to convert number x−1 to this mixed base. Turns out, it's not that hard. In base 10, for example, the lowest digit can be determined as the remainder of the number of dividing by 10. Here it will be the remainder of dividing by ct⋅k+1. After that, divide and floor the number and proceed to the next "digit".

After t steps are done, the "digits" of that mixed base number tell exactly how many letters 'b' should replace each segment of asterisks.

Overall complexity: O(n) per testcase to recover the string, O(nk) to print it.

Solution (awoo)1620D - Exact Change

Idea: adedalic

Tutorial1620D - Exact ChangeLet's define m=max(ai), then it should be obvious that we need at least r=⌈m3⌉ coins to buy a bag of chips of cost m. Now, it's not hard to prove that r+1 coins is always enough to buy a bag of chips of any cost c≤m. Proof:

if m≡0(mod3), we'll take r−1 coins of value 3, coin 1 and coin 2;if m≡2(mod3), we'll take r−1 coins 3 and two coins 1;if m≡1(mod3), we'll take r−2 coins 3, one coin 2 and two coins 1.So the question is how to decide, is r coins enough. The solution is to note that there is no need to take more than 3 coins 1 and more than 3 coins 2, so we can just brute force the number of coins 1 we'll take c1 and the number of coins 2 we'll take c2. Then, the number of coins 3 c3=⌈m−c1−2c23⌉, and we can check: is it possible to pay exactly ai using at most c1, c2 and c3 coins respectively.

There exists casework solution as well, but it's quite tricky, so brute force is preferable. The main problem for case work is the case m≡1(mod3), since there are two different ways to take r coins: either r−1 coins 3 and coin 1 or r−2 coins 3 and two coins 2. In the first way, you can't gather exactly ai≡2(mod3) and in the second one, you can gather neither ai=m−1 nor ai=1.

Solution (adedalic)1620E - Replace the Numbers

Idea: Neon

Tutorial1620E - Replace the NumbersLet's solve the problem from the end. Let's maintain the array px — what number will x become if we apply to it all the already considered queries of type 2. If the current query is of the first type, then we simply add px to the resulting array. If the current query is of the second type, then we have to change the value of px. Since all occurrences of x must be replaced with y, it is enough to assign px=py.

Since we process each query in O(1), the final complexity is O(n).

There is also an alternative solution. Let's process queries in the direct order. Let's store all its positions in an array for each number. Then for the first query, it is enough to put the index in the corresponding array of positions. And for a query of the second type, we have to move all the positions of the number x into an array of positions of the number y. The naive implementation is obviously too slow, but we can use the small to large method, then the complexity of the solution will be O(nlogn).

Solution 1 (Neon)Solution 2 (Neon)1620F - Bipartite Array

Idea: BledDest and Neon

Tutorial1620F - Bipartite ArrayTo begin with, let's understand that an array is bipartite if and only if there is no decreasing subsequence of length 3 in the array.

Now we can write dynamic programming dpi,x,y: is there an array a of length i such that x is the maximum last element of a decreasing subsequence of length 1, and y is the maximum last element of a subsequence of length 2. Note that x>y.

Let's consider all possible transitions from the state (i,x,y) if we are trying to put the number z on the i-th position, where z=±pi:

if z>x, then the new state will be (i+1,z,y);if z>y, then the new state will be (i+1,x,z);if z<y, then such a transition is not valid, because a decreasing subsequence of length 3 is formed in the array.With a naive implementation, such dynamic programming works in O(n3).

We can note that for fixed values of i and x (i and y) it is enough for us to store only the minimum available value of y (x). So, we can write dynamic programming dpi,x, which is defined similarly to the above, but now instead of being Boolean, stores the minimum value of y (or infinity if the state is not valid).

We have speeded up our solution to O(n2), but it is still too slow.

To speed up the solution even more, we have to look at the transitions in dynamics and notice that for a fixed i, either x or y is always equal to ±pi−1. So we can rewrite our dynamic programming in the following form — dpi,pos,sign. Here, the pos flag says which of the numbers x and y is equal to ±pi−1, and the sign flag is responsible for the sign of pi−1, and the minimum value of y or x is stored in the value itself (depending on pos).

Thus, we got a solution with a linear running time.

In fact, this solution can be simplified if we see the following relation: the number we use on position i is not less than dpi,0,sign and not greater than dpi,1,sign. This allows us to get rid of one of the states in our dynamic programming altogether, so we get an easier solution. This optimization wasn't required to get AC, but the code becomes shorter.

Solution 1 (Neon)Solution 2 (Neon)1620G - Subsequences Galore

Idea: BledDest

Tutorial1620G - Subsequences GaloreFor a string t, let's define its characteristic mask as the mask of n bits, where i-th bit is 1 if and only if t is a subsequence of si. Let's suppose we somehow calculate the number of strings for each characteristic mask, and we denote this as G(x) for a mask x. How can we use this information to find f([si1,si2,…,sik])? Suppose this set of strings is represented by a mask x, then the strings which are not included in f are the strings such that their characteristic mask has bitwise AND with x equal to 0, i. e. these characteristic masks are submasks of 2n−1⊕x. We can use SOS DP to calculate these sums of G(x) over submasks in O(2nn).

The only problem is how to calculate G(x) for every mask. Let's analyze when a string is a subsequence of a sorted string si. The subsequence should be sorted as well, and the number of occurrences of every character in a subsequence should not exceed the number of occurrences of that character in si. So, if there are c1 characters a in si, c2 characters b in si, and so on, then the number of its subsequences is ∏j=126(1+cj).

What about subsequences of every string from a set? These conditions on the number of occurrences should apply to every string in the set, so, for each character, we can calculate the minimum number of occurrences of this character in each string of the set, add 1, and multiply these numbers to get the number of strings that are subsequences of each string in a set.

These values can be calculated in O(2n(n+A)) for all 2n subsequences of [s1,s2,…,sn] using recursive approach. Can these numbers be used as G(x)? Not so fast. Unfortunately, these values (let's call them H(x)) are the numbers of subsequences of the chosen sets of strings, but we have no information about the strings that are not included in the chosen set of strings. To handle it, we can use the following equation: H(x)=∑x⊆yG(y), where x⊆y means that x is a submask of y. To transform the values of H(x) into the values of G(x), we can flip all bits in the masks (so H(x) is the sum of G(y) over all submasks of x), apply inverse SOS DP (also known as Mobius transformation), and then flip all bits in the masks again. So, we found a way to calculate all values of G(x) in O(2n(n+A)), and we have already discussed what to do with them in the first paragraph of the editorial.

The overall complexity of the solution is O(2n(n+A)).

Educational Codeforces Round 118 Editorial

By awoo, history, 14 months ago, translation, In English1613A - Long Comparison

Idea: BledDest

Tutorial1613A - Long ComparisonFirst, let's say that appending the number with p zeros is the same as multiplying it by 10p.

The given numbers are so large that they can't fit into any reasonable integer type. Even if you use a language with unlimited length integers (python, for example) or store the numbers in strings, you should still face the time limit issue. So let's learn to shrink the numbers a bit.

Note that the result of the comparison of two numbers doesn't change if you divide both numbers by the same positive number. So we can keep dividing both numbers by 10 until one of them is not divisible anymore. Let's also ignore the trailing zeros in x1 and x2 and leave them as is. If the first number is appended with p1 zeros and the second numbers is appended with p2 zeros, we can subtract min(p1,p2) from both values, effectively dividing both numbers by 10min(p1,p2).

This way, one of the numbers becomes short enough to fit into an integer type (because it has p=0 and x is only up to 106). The other number might still be large enough.

However, if it's really large, we can instantly say that it's larger than another one. Say, if its p is at least 7. This number it at least 107 and the other number is at most 106.

Otherwise, we can calculate this number as well and compare the values normally.

Overall complexity: O(1) per testcase.

Solution (awoo)1613B - Absent Remainder

Idea: BledDest

Tutorial1613B - Absent RemainderThere is one important observation: x mod y<y.

Thus, you can obtain at least n−1 pair by choosing y as the minimum number in the sequence and x as anything else. n−1≥⌊n2⌋ for any positive n.

Overall complexity: O(n) per testcase.

Solution (Neon)1613C - Poisoned Dagger

Idea: BledDest

Tutorial1613C - Poisoned DaggerLet's find out the total damage for a fixed value of k. Since the effect of the poison from the i-th attack deals damage min(k,ai+1−ai) seconds for i<n and k seconds for i=n, then the total damage is k+∑i=1n−1min(k,ai+1−ai). We can see that the higher the value of k, the greater the total sum. So we can do a binary search on k and find the minimum value when the sum is greater than or equal to h.

Solution (Neon)1613D - MEX Sequences

Idea: BledDest

Tutorial1613D - MEX SequencesLet's understand what MEX-correct sequences look like. It turns out there are only two types: [0,…,0––––––––,1,…,1––––––––,…,x−1,…,x−1––––––––––––––––,x,…,x––––––––] and [0,…,0––––––––,1,…,1––––––––,…,x−1,…,x−1––––––––––––––––,x+1,…,x+1––––––––––––––––,x−1,…,x−1––––––––––––––––,x+1,…–––––––––]. For example, the sequences [0,0,1,1,1,2,3,3] and the empty sequence are MEX-correct sequences of the first type, and [1,1,1,1] and [0,0,1,2,4,4,2,4,2,2] of the second one.

Let's calculate the dynamic programming dp1i,j — the number of MEX-correct subsequences of the first type on the prefix of length i with MEX equal to j and similarly dp2i,j — the number of MEX-correct subsequences of the second type on the prefix of length i with MEX equal to j.

Let's look at the transitions in these dps, and show that there are no other MEX-correct sequences at the same time.

Let the current state be dp1i,j, and we are trying to add an element equal to x:

if x<j−1, then such an element cannot be added;if x=j−1, then the value of MEX will not change and the sequence is still of the first type, which means we have a transition to dp1i+1,j;if x=j, then the value of MEX will increase by 1, but it will still be of the first type, which means we have a transition to dp1i+1,j+1if x=j+1, then the value of MEX will not change, but the sequence will become of the second type, which means we have a transition to dp2i+1,j;if x>j+1, then such an element cannot be added.Let the current state be dp2i,j, and we are trying to add an element equal to x:

if x<j−1, then such an element cannot be added;if x=j−1, then the value of MEX will not change and the sequence is still of the second type, which means we have a transition to dp2i+1,j;if x=j, then such an element cannot be added, because MEX will increase by 2, which means the absolute difference between MEX and x is greater than 1;if x=j+1, then the value of MEX will not change and the sequence is still of the second type, which means we have a transition to dp2i+1,j;if x>j+1, then such an element cannot be added.Thus, we considered all possible transitions (adding a new element to the already MEX-correct sequences) and made sure that there are only two types.

While the solution itself works in O(n) time (because each element x has O(1) possible transitions in the dps), it uses O(n2) memory, which does not allow us to write that solution as is. However, note that dp1i and dp1i+1 (similarly for dp2) differ in only a few positions (in those that the element ai allowed us to make), which means we can store only one-dimensional arrays, dp1j and dp2j. Thus, the final complexity of the solution is O(n).

Solution (Neon)1613E - Crazy Robot

Idea: BledDest

Tutorial1613E - Crazy RobotOne way to think about this problem is in game theory terms.

Imagine a following game. Two players alternate moves. The first players chooses a direction. The second player chooses a different direction and moves a robot there. The game ends when the robot reaches the lab, and the first player wins. Otherwise, it's a draw. What's the outcome of the game if both players play optimally (as in the first player tries to win, the second player tries to draw)?

Does it sound easier? Well, it sure does if you ever dealt with solving games on arbitrary graphs. You can skim through this article if that's unfamiliar to you. The state of the game is a pair (cell,direction). If a direction is not chosen (denote it with −1), it's the first player's move. Otherwise, it's the second player's move.

You can even implement it as is. Or you can adjust a part of this algorithm for this particular problem.

Initially, all the states are drawing, only the state (lab,−1) is winning. What we basically need is a way to determine if a state is winning or not. From game theory, we can tell that the state is winning if there's a transition from it to a losing state. The state is losing if all the transitions from it lead to winning states. So (cell,−1) is winning if any of (cell,direction≠−1) are losing.

Promote that one step further. The state is winning if there exists such a direction that all neighbouring free cells except in this direction are winning states. Rephrase it. The state is winning if it has at least one winning state neighbour and no more than one non-winning state neighbour.

Let's store the number of non-winning neighbouring states for each cell. Initially, it's the number of neighbouring free cells. If some state becomes marked as winning, decrease the value for each of its neighbours by 1. If some state's value reaches 0 or 1 after this operation, mark it as winning.

Since what this does is basically a traversal of a grid, this can be done with a DFS/BFS, starting from the lab.

Overall complexity: O(nm) per testcase.

Solution (awoo)1613F - Tree Coloring

Idea: BledDest

Tutorial1613F - Tree ColoringWhen a problem asks us to calculate the number of combinatorial objects that meet some constraints, we can sometimes use inclusion-exclusion formula. Let's try to apply it in this problem.

We could use n−1 constraints that should not be violated. The i-th constraint is formulated as follows: ci≠cpi−1 (there will be a constraint of this type for each i∈[2,n]). Suppose we violated k of these constraints (and have chosen which k constraints to violate), then the number of colorings that meet these violations is (n−k)! (for k vertices, the colors on them depend on some other independent vertices, so we can assign only colors for independent vertices). So, the answer can be calculated as follows:

∑k=0n−1(−1)kf(k)(n−k)!,

where f(k) is the number of ways to choose k constraints to violate.

One initial guess how to calculate f(k) is that f(k)=(n−1k), as it would be calculated in other, more usual inclusion-exclusion problems. Unfortunately, in this problem, the constraints we violate are not independent. For example, if a vertex has several sons, we can violate the constraint only on at most one edge leading from a vertex to its son simultaneously, we cannot violate two or more such constraints.

Let's take care of this issue as follows: we can write a dynamic programming of the form dpi,j is the number of ways to process i first vertices of the tree and choose exactly j edges leading from these nodes to their sons so that no vertex has more than one edge leading to its sons chosen. Then, dpn,k is exactly the number of ways to choose k edges in the tree so that no vertex has more than one chosen edge leading to its sons, and that will be equal to f(k).

We can calculate this dynamic programming in a knapsack fashion in O(n2), but it is too slow. Instead, let's optimize this knapsack DP with FFT: for each vertex i, introduce a polynomial 1+di⋅x, where di is the number of children of the vertex i. Coefficients of this polynomial for the first vertex are the values of dp1,k; coefficients of the product of this polynomial with the polynomial for the second vertex are the values of dp2,k, and so on; to obtain the values of dpn,k, we have to multiply all these polynomials, and using FFT + divide-and-conquer, we can do it in O(nlog2n).

Educational Codeforces Round 117 — Editorial

By BledDest, 15 months ago, translation, In English1612A - Distance

Idea: BledDest, preparation: BledDest

Tutorial1612A - DistanceThere is a solution in O(1), but in fact, a solution that checks all points with x-coordinate from 0 to 50 and y-coordinate from 0 to 50 is fast enough. There's no need to check any other points, since d(A,C)+d(B,C)=d(A,B) implies that point C is on one of the shortest paths between A and B.

Solution (BledDest)1612B - Special Permutation

Idea: MikeMirzayanov, preparation: MikeMirzayanov

Tutorial1612B - Special PermutationThere are many different constructions that give the correct answer, if it exists. In my opinion, one of the most elegant is the following one.

a should always be present in the left half, and b should be present in the right half, but the exact order of elements in each half doesn't matter. So, it will never be wrong to put a in the first position, and b in the second position.

As for the remaining elements, we want elements of the left half to be as big as possible (since they shouldn't be less than a), and elements of the right half — as small as possible (since they shouldn't be greater than b). Let's put the elements n, n−1, n−2, ..., 1 (excluding a and b) on positions 2, 3, 4, ..., n−1, respectively, so the elements in the left half are as big as possible, and the elements in the right half are as small as possible.

After constructing a permutation according to these rules, we should check if it meets the constraints (and print it if it does).

Solution (BledDest)1612C - Chat Ban

Idea: vovuh, preparation: vovuh

Tutorial1612C - Chat BanThis is a pretty obvious binary search problem. If we get banned after y messages, we also get banned after y+1, y+2 and so on messages (and vice versa, if we don't get banned after y messages, we also don't get banned after y−1, y−2 and so on messages).

For simplicity, let's split the problem into two parts: when we check if we're getting banned after y messages, let's handle cases y<k and y≥k separately.

Recall that the sum of the arithmetic progression consisting of integers 1, 2, ..., y is y(y+1)2. Let it be cnt(y).

The first case is pretty simple: the number of emotes we send with y messages when y<k is y(y+1)2 which is cnt(y). So we only need to check if cnt(y)≥x.

The second case is a bit harder but still can be done using arithmetic progression formulas. Firstly, we send all messages for y≤k (the number of such messages is cnt(k)). Then, we need to add (k−1)+(k−2)+…+(y−k) messages. This number equals to cnt(k−1)−cnt(2k−1−y) (i.e. we send all messages from k−1 to 1 and subtract messages from 1 to 2k−1−y from this amount). The final condition is cnt(k)+cnt(k−1)−cnt(2k−1−y)≥x.

Time complexity: O(logk) per test case.

Solution (vovuh)1612D - X-Magic Pair

Idea: vovuh, preparation: vovuh

Tutorial1612D - X-Magic PairThis problem has a GCD-based solution.

Firstly, lets' try to solve it naively. Always suppose that a>b. If this is not true, let's swap a and b. Firstly, if b>a−b, let's do b:=a−b. Okay, now let's subtract b from a until b≥a−b again and repeat this algorithm till a=0 or b=0. If, after some step, we get a=x or b=x, we are done, and the answer is YES. If a=0 or b=0, and we didn't get x then the answer is NO.

Okay, we can see that we always subtract the minimum possible b from a and trying to maintain this condition. It can be proven that this algorithm yields all possible integers that are obtainable by any sequence of the operations from the problem statement (either in a or in b).

Now we have to speed up this solution somehow. Obviously, most operations are redundant for us in this particular problem. The first thing is that we can skip all operations till b becomes greater than a−b. The number of such operations is ⌊a−b2b⌋. And the second thing is that we can skip all operations till we get x in a. The number of such operations is ⌊a−xb⌋. For simplicity, this part can be also written as ⌊a−x2b⌋. This doesn't affect the time complexity much, but the formula for the final number of operations we can skip will be simpler. This number equals cnt=max(1,⌊a−max(b,x)2b⌋) (in fact, we take the minimum between two values written above, because we don't want to skip any of these two cases). So, we can transform the pair (a,b) to the pair (a−b∗cnt,b) and continue this algorithm.

There are also simpler approaches using the same idea but in a cooler way.

Time complexity: O(loga) per test case.

Solution (vovuh)1612E - Messages

Idea: BledDest, preparation: BledDest

Tutorial1612E - MessagesFirst of all, let's rewrite the answer using expectation linearity. The expected number of students who read their respective messages is equal to F1+F2+⋯+Fn, where Fi is a random value which is 1 if the i-th student reads the message mi, and 0 if the i-th student doesn't do it.

Let's analyze the expected value of Fi. Suppose Monocarp pins the messages c1,c2,…,ct. There are three cases:

if mi∉[c1,c2,…,ct], then the i-th student won't read the message mi, so Fi=0;if mi∈[c1,c2,…,ct] and t≤ki, then the i-th student will definitely read the message mi, so Fi=1;if mi∈[c1,c2,…,ct] and t>ki, then Fi=kit.If we iterate on the number of messages we pin t, we can calculate the sum of Fi for each message (considering that we pin it), sort all of the messages and pick t best of them. So, we have a solution working in O(n2logn).

The only thing we need to improve this solution sufficiently is the fact that we don't have to consider the case t>20. Since every ki is not greater than 20, the sum of Fi for a message in the case t=20 is the same as this sum of Fi in the case t=21, but multiplied by the coefficient 2021 — and we pick 21 best values, their sum multiplied by 2021 is not greater than the sum of 20 best values. The same holds for t=22 and greater.

Solution (BledDest)1612F - Armor and Weapons

Idea: BledDest, preparation: BledDest

Tutorial1612F - Armor and WeaponsAmong two armor sets, one with the greater index is always better. The same can be said about two different weapons. So, it is always optimal to use and obtain the best possible weapon or armor.

This observation allows us to model this problem with dynamic programming or shortest paths: let dpx,y be the minimum time in which Monocarp can obtain the armor x and the weapon y; and in each transition, we either get the best weapon we can or the best armor we can. Similarly, we can build a graph where the vertices represent these pairs (x,y), and the edges represent getting the best possible weapon/armor, and find the shortest path from (1,1) to (n,m) using BFS.

Unfortunately, it is O(nm). But we can modify the BFS in the following fashion: let's analyze each layer of BFS (a layer is a set of vertices with the same distance from the origin). In each layer, there might be some redundant vertices: if two vertices (x,y) and (x′,y′) belong to the same layer, x′≤x and y′≤y, then the vertex (x′,y′) is redundant.

If we filter each layer, removing all redundant vertices from it and continuing BFS only from non-redundant ones, the solution will be fast enough. To prove it, let's analyze the constraints on the answer. Suppose n≥m. The answer can be bounded as O(logm+nm), since we can reach the pair (m,m) in O(logm) steps using something similar to Fibonacci sequence building, and then go from (m,m) to (n,m) in nm steps. And the number of non-redundant states on each layer is not greater than m (because, of two states with the same weapon or the same armor set, at least one is redundant). So, if we don't continue BFS from redundant vertices, it will visit at most O(m⋅(logm+nm))=O(mlogm+n) vertices. There might be another logarithm in the asymptotic complexity of the solution, if you use something like a set to store all combinations that synergize well, but this implementation is still fast enough.

Solution (BledDest)1612G - Max Sum Array

Idea: adedalic, preparation: adedalic

Tutorial1612G - Max Sum ArrayFirstly, let's prove that at first and last positions of a the most frequent elements should be placed (but not necessary the same). WLOG, let's prove that ca1≥cai for any i>1.

By contradiction, let's p be the smallest index such that ca1<cap. What happens if we swap them? Since p is the first such index then there are no ai=ap for i<p, so "contribution" of ap will increase by exactly inc=(cap−1)⋅(p−1). From the other side, contribution of a1 consists of two parts: pairs with elements from (p,n] and from (1,p). For all elements from (p,n] decrease will be equal to dec1=ca1[p+1,n]⋅(p−1) and from elements in (1,p) dec2≤ca1[2,p−1]⋅(p−1).

So, the total decrease dec after moving a1 to position p equal to dec=dec1+dec2≤(ca1−1)⋅(p−1). The total difference in such case is equal to inc−dec≥(p−1)⋅(cap−ca1)>0. So, our placement is not optimal — contradiction.

Let's suggest that there is exactly one e with maximum ce. According to what we proved earlier, both a1 and an must be equal to e. Contribution of the first and last elements will be equal to: (n−1) for pair (1,n) and for each element i (1<i<n) with ai=e we add (i−1)+(n−i)=(n−1) for pairs (1,i) and (i,n). So, the total contribution of a1=an=e is equal to (n−1)⋅(ce−1).

Note that this contribution is independent of positions of other e in the array a, so that's why we can cut first and last elements of a and solve the task recursively.

Unfortunately, in the initial task we may have several e1,e2,…,ek with maximum cei. But we in the similar manner can prove that the first (and last) k elements a1,…,ak should be some permutation of e1,…ek. Now, let's prove that any permutation of first and last k elements is optimal.

Suppose, positions of ei are li (1≤li≤k) and ri (n−k+1≤ri≤n). Then the contribution of ei is equal to (cei−1)⋅(ri−li). The total contribution of all ei is ∑i=1k(cei−1)⋅(ri−li) = (ce1−1)(∑i=1kri−∑i=1kli) = (ce1−1)((n−k)k+k(k+1)2−k(k+1)2)=(ce1−1)k(n−k). This contribution doesn't depend on chosen li and ri, so any permutation of first k elements and any permutation of last k elements give optimal answer.

As a result, the algorithm is following:

Find all maximums e1,…,ek in c.If ce1=1 then any permutation of remaining elements has f(a)=0 (there are k! such permutations).Otherwise, add (ce1−1)k(n−k) to the total balance, and multiply the number of variants by (k!)2.Cut prefix and suffix by making cei=cei−2 for each ei (obviously, n=n−2k) and repeat the whole process.We can implement the algorithm fast if we keep the number of ci equal to each val from 0 to C (C≤106). So the total complexity is O(n+C).

Educational Codeforces Round 116 Editorial

By awoo, history, 16 months ago, translation, In English1606A - AB Balance

Idea: BledDest

Tutorial1606A - AB BalanceLet's look at the first and the last characters of s. Note that if s1=sn (where n=|s|), then AB(s) is always equal to BA(s).

It can be proved, for example, by induction: if s consists of equal characters then AB(s)=BA(s)=0; if s has a structure like abb...ba (or baa...ab) then AB(s)=BA(s)=1.

Otherwise, there is at least one character si in the middle that equal to s1 and sn. So we can split string s in s[1..i] and s[i..n]. Both these string has AB=BA (by induction), so our string s also has AB(s)=BA(s).

As a result, if s1=sn then the answer is 0, and we print the string untouched. Otherwise, we replace either s1 or sn and get the desired string.

(It also can be proved that if s1≠sn then AB(s)≠BA(s))

Solution (adedalic)1606B - Update Files

Tutorial1606B - Update FilesLet cur be the current number of computers with the update already installed (initially it is 1). Then, in 1 hour, we can increase cur by min(cur,k). From here we can see that the value of cur will double for the first few hours, and then, when it becomes greater than k, it will begin to increase by exactly k.

The process when the number of computers doubles can be modeled using a loop, because the number of doublings does not exceed logn. And after that, we have to increase the answer by ⌈n−curk⌉ to take the number of additions of k into account.

Note that computing ⌈n−curk⌉ should be done without using fractional data types; to calculate ⌈xy⌉ in integers, you should divide x+y−1 by y using integer division (this will work provided that both x and y are non-negative, and y≠0). If you use real numbers, this may cause precision issues.

Solution (Neon)1606C - Banknotes

Idea: BledDest

Tutorial1606C - BanknotesFirst of all, let's find out how to calculate f(s). This can be done greedily, let's iterate from the higher denominations to the lower ones, the number of banknotes of i-th type is equal to ⌊s10ai⌋ (the value of s here changes to reflect that we have already taken some banknotes; that is, we subtract ⌊s10ai⌋⋅10ai from s each time, which is the same as taking s modulo 10ai). We can see that after we process the i-th type of banknotes, the condition s<10ai holds, which means that the number of banknotes of i-th type does not exceed 10ai+110ai−1 (except in the case of i=n).

Now we can find the minimum number s such that f(s)>k. Let lft be the number of banknotes that still remains to take, initially equal to k+1 (because we want f(s) to be at least k+1). Let's iterate from the lower denominations to the highest ones, the number of banknotes of i-th type we take should be equal to min(lft,10ai+110ai−1) — the minimum of how many we need to take and how many we are allowed to take, so as not to break the minimality of the function f.

Solution (Neon)1606D - Red-Blue Matrix

Idea: BledDest

Tutorial1606D - Red-Blue MatrixImagine you fixed some cut and then colored one row red. Which rows can now be colored red or blue so that the condition on the left matrix is satisfied? If the row has at least one number greater or equal than the numbers in the red row, then the row must be red. Otherwise, it can be either red or blue.

However, imagine a weaker condition. Let's look only at the first cell in each row. Sort the rows by the first cell in them. Similarly, if a row is colored red, all the rows that are further in the sorted order should also be red, because they already have a greater or equal number in them.

It implies that after you sort the rows, the only possible colorings are: color some prefix of the rows in blue and the remaining suffix in red.

So there are n possible colorings and m possible cuts. If we learn to check if they are perfect in O(1), we can get the solution in O(nm).

Turns out, the condition "all numbers in the submatrix should be greater than all numbers in the other submatrix" is the same as "the minimum in the first submatrix should be greater than the maximum in the second submatrix".

Thus, you can first precalculate prefix and suffix minimums and maximums and check a coloring and a cut in O(1).

Overall complexity: O(nlogn+nm) per testcase.

Solution (awoo)1606E - Arena

Idea: BledDest

Tutorial1606E - ArenaLet's calculate the following dynamic programming dpi,j — the number of ways to choose the initial health if there are i heroes still alive, and they already received j damage. Let's iterate over k — the number of heroes that will survive after the next round. Then we have to make a transition to the state dpk,nj, where nj=min(x,j+i−1) (the minimum of the maximum allowed health and j plus the damage done in this round). It remains to understand with what coefficient we should make this transition in dynamic programming. This coefficient is equal to (ii−k)⋅(nj−j)i−k — the number of ways to choose which of the i living heroes will die in this round multiplied by the number of ways to choose health for these i−k heroes (because their health is greater than j so that they are still alive at the moment, but not more than nj so that they are guaranteed to die in this round). Of course, we don't make any transitions from the states dpi,j where i<2, since they represent the fights that have already finished.

The answer is the sum of all dp0,j for every j∈[0,x].

Solution (Neon)1606F - Tree Queries

Idea: BledDest

Tutorial1606F - Tree QueriesA naive solution to this problem would be to implement a recursive function which answers each query: let f(v,k) be the answer to the query "v k", we can calculate it as ∑u∈children(v)max(1,f(u,k)−k), since for each child u of vertex v, we either delete it and change the score by f(u,k)−k, or choose to let it remain, and this increases the score by 1. Unfortunately, it is too slow.

Let's try to optimize it. First of all, f(v,k−1)≥f(v,k) since if we choose the exact same subset of vertices to delete for the query "v k−1" as we've chosen for the query "v k", our score won't decrease. Using this fact, we can show that if it's optimal to remove some vertex in the query "v k", it's also optimal to remove a vertex in the query "v k−1" because it's optimal to remove vertex u if f(u,k)−k>1, and if this condition holds for some value of k, then it holds for each smaller value of k.

Let opt(u) be the maximum value of k when it's optimal to remove the vertex u. We will calculate these values for all vertices of the tree using an event processing method: we'll process the values of k from 200000 to 0 and use a set or a priority queue to store events of the form "at the value i, vertex u becomes optimal to delete". This set/priority queue should sort the events in descending order of the value of k, and in case of ties, in descending order of depths of the vertices (to make sure that vertices with the same value of opt(u) are processed from bottom to up).

Let's analyze the implementation of this process more in detail. For each vertex, we will store two values — the number of vertices we should remove from its subtree, and the number of children this vertex will optimally have. Using these two values, we can easily calculate the value of opt for a vertex. When a vertex is "removed" (that is, the event corresponding to this vertex is processed), these values for this vertex should be added to its current parent (we can use DSU to find the current parent easily, for example; and don't forget that the number of vertices we have to remove for this new parent also increases by 1); then we recalculate the value of opt for the current parent and change the event corresponding to this current parent (note that the value of opt for the current parent shouldn't be greater than the value of opt for the vertex we've deleted).

Okay, this allows us to calculate when it's optimal to delete each vertex. But how do we answer queries? One of the ways to do this is to process queries in the same event processing algorithm (and for every value of k, we first "remove" the vertices u with opt(u)=k, then process the queries). There is an issue that when we remove a vertex, it can affect the answer not only for its current parent, but also for the vertices that could be its parents, but are already deleted; to handle this, instead of adding the values of the deleted vertex only to the values of its current parent, we perform an addition on the whole path from the vertex to the current parent (excluding the vertex itself). This path addition can be performed with a Fenwick or Segment tree over the Eulerian tour of the tree, and this yields a compexity of O(nlogn), though with a high constant factor.

Educational Codeforces Round 115 Editorial

By awoo, history, 16 months ago, translation, In English1598A - Computer Game

Idea: BledDest

Tutorial1598A - Computer GameAt first glance, it seems like a graph problem. And indeed, this problem can be solved by explicitly building a graph considering cells as the vertices and checking that there is a safe path from start to finish via DFS/BFS/DSU/any other graph algorithm or data structure you know. But there's a much simpler solution.

Since there are only two rows in a matrix, it's possible to move from any cell in the column i to any cell in column i+1 (if they are both safe, of course). It means that as long as there is at least one safe cell in each column, it is possible to reach any column of the matrix (and the cell (2,n) as well).

It's easy to see that if this condition is not met, there exists a column with two unsafe cells — and this also means that this column and columns to the right of it are unreachable. So, the problem is reduced to checking if there is a column without any unsafe cells. To implement this, you can read both rows of the matrix as strings (let these strings be s1 and s2) and check that there is a position i such that both s1[i] and s2[i] are equal to 1.

Solution (BledDest)1598B - Groups

Idea: fcspartakm

Tutorial1598B - GroupsSince there are only five days, we can iterate over the two of them that will be the answer.

Now we have fixed a pair of days a and b and want to check if it can be the answer.

All students can be divided into four groups: marked neither of days a and b, marked only day a, marked only day b and marked both days.

Obviously, if the first group is non-empty, days a and b can't be the answer.

Let's call the number of students, who only marked day a, cnta and the number of students, who only marked day b, cntb.

If either of cnta or cntb exceed n2, then days a and b can't be the answer as well. Otherwise, we can always choose n2−cnta students from the ones who marked both days and send them to day a. The rest of the students can go to day b.

Solution (BledDest)1598C - Delete Two Elements

Idea: fcspartakm

Tutorial1598C - Delete Two ElementsFirst of all, instead of the mathematic mean, let's consider the sum of elements. If the mathematic mean is k, then the sum of elements of the array is k⋅n. Let's denote the sum of elements in the original array as s. Note s is always an integer.

If we remove two elements from the array, the resulting sum of elements should become k⋅(n−2)=s⋅(n−2)n. So, the sum of the elements we remove should be exactly 2sn.

If 2sn is not an integer, the answer is 0 (to check that, you can simply compare (2s)modn with 0). Otherwise, we have to find the number of pairs (i,j) such that i<j and ai+aj=2sn. This is a well-known problem.

To solve it, you can calculate the number of occurrences of each element and store it in some associative data structure (for example, map in C++). Let cntx be the number of occurrences of element x. Then, you should iterate on the element ai you want to remove and check how many elements match it, that is, how many elements give exactly 2sn if you add ai to them. The number of these elements is just cnt2sn−ai. Let's sum up all these values for every element in the array.

Unfortunately, this sum is not the answer yet. We need to take care of two things:

if for some index i, 2⋅ai=2sn, then ai matches itself, so you have to subtract the number of such elements from the answer;every pair of elements is counted twice: the first time when we consider the first element of the pair, and the second time — when we consider the second element of the pair. So, don't forget to divide the answer by 2.Solution (Neon)1598D - Training Session

Idea: BledDest

Tutorial1598D - Training SessionThere are many different ways to solve this problem, but, in my opinion, the easiest one is to count all possible triples and subtract the number of bad triples.

The first part is easy — the number of ways to choose 3 elements out of n is just n⋅(n−1)⋅(n−2)6. The second part is a bit tricky.

What does it mean that the conditions in the statements are not fulfilled? There is a pair of problems with equal difficulty, and there is a pair of problems with the same topic. Since all problems in the input are different, it means that every bad triple has the following form: [(xa,ya),(xb,ya),(xa,yb)] — i. e. there exists a problem such that it shares the difficulty with one of the other two problems, and the topic with the remaining problem of the triple.

This observation allows us to calculate the number of bad triples as follows: we will iterate on the "central" problem (the one that shares the topic with the second problem and the difficulty with the third problem). If we pick (xa,ya) as the "central" problem, we need to choose the other two. Counting ways to choose the other problems is easy if we precalculate the number of problems for each topic/difficulty: let cntTx be the number of problems with topic x, and cntDy be the number of problems with difficulty y; then, if we pick the problem (x,y) as the "central one", there are cntTx−1 ways to choose a problem that shares the topic with it, and cntDy−1 ways to choose a problem that has the same difficulty — so, we have to subtract (cntTx−1)(cntDy−1) from the answer for every problem (x,y).

Solution (Neon)1598E - Staircases

Idea: BledDest

Tutorial1598E - StaircasesThe solution consist of two main parts: calculate the initial number of staircases and recalculate the number of staircases on query.

The constraints were pretty loose, so we'll do the first part in O(nm) and the second part in O(n+m) per query.

However, it's worth mentioning that faster is possible. The first part can surely be done in O(n+m) and can probably be done in O(1). The second part can be done in O(logn) per query.

It's important to notice is that the only staircase that satisfy the requirements for both types is the staircase that consists of a single cell. Thus, staircases of both types can be calculated almost separately.

Let's define "base" staircases as the staircases that can't be prolonged further in any direction. There are O(n+m) of them on the grid.

If a staircase consists of at least two cells, it's a part of exactly one base staircase. At the same time, every segment of a base staircase is a valid staircase by itself.

Thus, the main idea of calculating the initial answer is the following. Isolate each base staircase and determine its length len (possibly, in O(n+m)). Add (len−12) (the number of segments of length at least 2) to the answer. Add extra nm one cell staircases afterwards.

If you draw the base staircases on the grid, you can easily determine their starting cell. The base staircases, that start by going one cell to the right, start from the first row. The base staircases, that start by going one cell to the bottom, start from the first column. Notice that both types can start from cell (1,1).

The updates can be handled the following way. The answer always changes by the number of staircases that pass through cell (x,y) (if you ignore its state). If the cell becomes free, then these staircases are added to the answer. Otherwise, they are subtracted from it.

That can be calculated for two cases as well. Go first down, then right, as far as possible. Let it be cnt1 steps. Go first left, then up, as far as possible. Let it be cnt2 steps. Then cnt1⋅cnt2 staircases are added to the answer. Then change the order of steps in both directions to calculate the other type of staircases. Beware of one cell staircases again.

To achieve O(n+m) for precalc, you can calculate the length of each base staircase with a formula. To achieve O(logn) per query, you can first enumerate cells in each base staircase separately, then maintain the set of segments of adjacent free cells in it.

Solution (awoo)1598F - RBS

Idea: BledDest

Tutorial1598F - RBSThe constraint n≤20 is a clear hint that we need some exponential solution. Of course, we cannot try all n! permutations. Let's instead try to design a solution with bitmask dynamic programming.

A string is an RBS if its balance (the difference between the number of opening and closing brackets) is 0, and the balance of its each prefix is non-negative. So, let's introduce the following dynamic programming: dpm,b,f is the greatest number of RBS-prefixes of a string if we considered a mask m of strings si, the current balance of the prefix is b, and f is a flag that denotes whether there already has been a prefix with negative balance. We can already get rid of one of the states: the current balance is uniquely determined by the mask m. So, this dynamic programming will have O(2n+1) states.

To perform transitions, we need to find a way to recalculate the value of f and the answer if we append a new string at the end of the current one. Unfortunately, it's too slow to simply simulate the process. Instead, for every string si, let's precalculate the value go(si,f,x) — how does the flag and the answer change, if the current flag is f, and the current balance is x.

The resulting flag will be true in one of the following two cases:

it is already true;the string we append creates a new prefix with non-positive balance.The second case can be checked as follows: let's precalculate the minimum balance of a prefix of si; let it be c. If x+c<0, the flag will be true.

Calculating how the answer changes is a bit trickier. If the current flag is already true, the answer doesn't change. But if it is false, the answer will increase by the number of new RBS-prefixes. If the balance before adding the string si is b, then we get a new RBS-prefix for every prefix of si such that:

its balance is exactly (−b) (to compensate the balance we already have);there is no prefix with balance (−b−1) in si before this prefix.To quickly get the number of prefixes meeting these constraints, we can create a data structure that stores the following information: for every balance j, store a sorted vector of positions in si with balance equal to j. Then, to calculate the number of prefixes meeting the constraints, we can find the first position in si with balance equal to (−b−1) by looking at the beginning of the vector for (−b−1), and then get the number of elements less than this one from the vector for balance (−b) by binary search.

These optimizations yield a solution in O(2nlogA+AlogA), although it's possible to improve to O(2n+AlogA) if you precalculate each value of go(si,f,x) for every string si.

Solution (BledDest)1598G - The Sum of Good Numbers

Idea: BledDest

Tutorial1598G - The Sum of Good NumbersLet's denote a as the largest of the terms of the sum, and b is the smaller one.

Consider 2 cases: |a|=|x|−1 or |a|=|x|.

If |a|=|x|−1, then |b|=|x|−1. So we need to find two consecutive substrings of length |x|−1 such that if we convert these substrings into integers, their sum is equal to x.

If |a|=|x|, let lcp be the largest common prefix of a and x if we consider them as strings. Then |b|=|x|−lcp or |b|=|x|−lcp−1. So it is necessary to check only these two cases, and whether b goes before or after a (in the string s).

Thus, we have reduced the number of variants where the substrings for a and b are located to O(n). It remains to consider how to quickly check whether the selected substrings are suitable. To do this, you can use hashes (preferably with several random modules).

Educational Codeforces Round 114 Editorial

By awoo, history, 17 months ago, translation, In English1574A - Regular Bracket Sequences

Idea: BledDest

Tutorial1574A - Regular Bracket SequencesThere are many ways to solve this problem. The model solution does the following thing:

start with the sequence ()()()()...;merge the first 4 characters into one sequence to get (())()()...;merge the first 6 characters into one sequence to get ((()))()...;and so on.Solution (BledDest)1574B - Combinatorics Homework

Idea: BledDest

Tutorial1574B - Combinatorics HomeworkLet's start with a simple assumption. For some fixed values a,b,c, the values of m that the answers exist for, make up a range. So there's the smallest possible number of adjacent equal pairs one can construct and the largest one — everything in-between exists as well.

The largest number is simple — put all A's, then all B's, then all C's. So this value is (a−1)+(b−1)+(c−1).

The smallest number is trickier. Let's instead investigate when it's equal to 0. WLOG, assume a≤b≤c. Imagine the following construction. There are c letters C which separate blocks of letters A and B. There are c−1 (c+1 if you consider the ones to the sides of all letters C, but we want the smallest value, so we shouldn't consider them) such blocks, thus it's possible that each block contains no more than one letter A and no more than one letter B. So letters A and B will never produce adjacent pairs.

If there are empty blocks, then there are adjacent letters C. So the condition to still have no empty blocks is to have at least c−1 letters A and B in total. If c−1>a+b, then any extra letter C can only be put adjacent to another letter C, thus producing an extra pair (at least one extra pair, but since we are examining the lower bound, we can always do exactly one). That means that the lower bound is c−1−(a+b).

Now for the proof of the fact that every value in-between is also achievable. Since we have a construction for m=0, let's try modifying it. Let's reduce the test to m=0 the following way. While m>0, decrease the count of the letter that appears the most by 1 and decrease m by 1. Now build the string for m=0 with the reduced values. After that put the letters back, placing them next to the last occurrence of the same letter (there is at least one occurrence of each letter, the proof is trivial). That increases m by 1 and the count of this letter by 1. Thus, we'll return to the initial test.

Overall complexity: O(1) per testcase.

Solution (awoo)1574C - Slay the Dragon

Idea: BledDest

Tutorial1574C - Slay the DragonIt is enough to consider two cases: whether we will increase the strength of the hero who will kill the dragon or not.

If you do not increase the hero's strength, then you have to choose such i that ai≥x. Obviously, among such i, you have to choose with the minimum value ai, because the strength of defending heroes is equal to ∑j=1naj−ai. It remains to increase the total strength of the remaining heroes to y. So the required number of coins is equal to max(0,y−(∑j=1naj−ai)).

If you increase the hero's strength, then you have to choose the maximum value of ai, which is less than x. In this case, the required number of coins is x−ai to increase the strength of the hero who will kill the dragon, plus max(0,y−(∑j=1naj−ai)) to increase the strength of the defending heroes.

To find the heroes with strength as close to x as possible, you can use binary search (don't forget to sort the heroes beforehand).

Solution (Neon)1574D - The Strongest Build

Idea: BledDest

Tutorial1574D - The Strongest BuildConsider the bruteforce solution. You start with a build that contains the most powerful item for each slot. In one move, you swap an item in some slot for the one that is the previous one by power. If a build is not banned, update the answer with its total power (banned builds can be stored in a set, maybe hashset if you hash carefully enough).

Notice that if you reach some unbanned build in this bruteforce, it never makes sense to go further. The answer is already updated with this one, and all the lower ones have smaller power.

If you code that bruteforce in a smart way (or just add memorization), you won't visit any build twice. How many states will you visit, though? Since you can only proceed if you are standing in a banned build, you will check around m+mn builds.

You can code it like that and get accepted. However, there's another way that's easier to code, in my opinion.

The optimal answer can be one of only two types. Either it contains the last item of each slot. Or it's some banned build with one item swapped with the previous one. It's easy to see from the solution above. So you can check the first type, then iterate over the banned build and try swapping each slot in it, checking if the resulting build is banned or not.

Overall complexity: O(mn) or O(mnlogm).

Solution (awoo)1574E - Coloring

Idea: Roms

Tutorial1574E - ColoringFor best understanding we replace the matrix with 0 and 1 with the matrix with black and white cells.

At first let's consider matrix if there are two adjacent horizontal cell with same color (for example cells (5,5) and (5,6) are black). Then the cells (4,5), (4,6), (6,5) and 6,6 must have the opposite color (white); the cells (3,5), (3,6), (7,5) and 7,6 must have the same color (black) and so on. So, two adjacent horizontal cells generate the vertical strip of width two. Reciprocally two adjacent vertical cells generate the horizontal strip of width two. And if simultaneously there are horizontal strip and vertical strip then the answer is 0 (because they contradict each other).

If there are two cells of same color in the same row with even number of cells between them (for example (2,2) and (2,7) with four cells between them) then there is the vertical strip (because there are always two adjacent cells with same color between them). The same is correct for horizontal strips.

Now let's consider how the matrix look if there are the vertical strip. It look like a chess board of size n×m, but colors of some verticals are inverted. The same is correct if there are the horizontal strips.

How we can quickly understand that there are two cells of same color in the same row with even number of cells between them? For this mentally color the matrix in a checkerboard pattern. And then one of this cells has the same color witch cells in chessboard, and the other has the opposite color witch cells in chessboard.

For calculating the answer we have maintain to the following values:

The color of each colored cell;The row and columns containing the cells of same color with even number of cells between them;And the number of row and columns containing at least one colored cell (for calculating the number of beautiful matrix).Solution (Roms)1574F - Occurrences

Idea: BledDest

Tutorial1574F - OccurrencesWhat does the condition "the number of occurrences of Ai in the array a is not less than the number of occurrences of each non-empty subarray of Ai in a" mean? First, if Ai contains two (or more) equal elements, then any occurrence of Ai introduces at least two occurrences of that element; so any element in Ai is forbidden (it should not appear in the resulting array). Now let's consider an array Ai such that every its element is unique. Every element of Ai should be a part of an occurrence of Ai in the array a. Let's rephrase this condition as follows: for each occurrence of Ai,j in a, the next element in a is Ai,j+1, and vice versa: for each occurrence of Ai,j+1 in a, the previous element in a is Ai,j.

Let's build a directed graph on k vertices, where an arc from vertex x to vertex y means that each occurrence of x should be followed by y, and each occurrence of y should be preceded by x (i. e. x is followed by y in some array Ai). Let's consider the weakly connected components in this graph. If we have at least one occurrence of some element from a component in a, it means that all other elements from this component occur in a as well. Some integers from 1 and k are "bad" in a sense that we cannot uniquely determine which element should follow/precede them (in terms of graph theory, it means that the in-degree or out-degree of a vertex is at least 2). Since by picking one element from a component, we will have to use all elements from a component, it means that if a component contains at least one "bad" element, the whole component will be "bad" — we cannot use any element from it.

If a component is a cycle, no vertex has in-degree or out-degree greater than 1, but the component is still "bad" since, if we include at least one element from a, we cannot finish the cycle — the array a is not infinite, but the cycle is.

Okay, the only "good" components are chains. When we use an element from a chain in a, all elements from this chain will be used in exactly the same order that they were in the chain; so, a should consist of some chains linked together (chains may repeat, and some chains may be absent from a). We can write a solution with dynamic programming: let dpi be the number of ways to construct an array of length i using these chains. The transitions are as follows: dpi=∑j=1cdpi−lenj, where c is the number of chains, and lenj is the length of the j-th chain.

The number of chains is up to k, and the number of states in dynamic programming is m+1, so the solution works in O(mk), which is too slow. We can improve it with the following two facts:

all chains of the same length are indistinguishable;there are O(k−−√) different lengths of chains.So, instead of iterating on the chains themselves in dynamic programming, we will iterate on the lengths of the chains (considering only lengths having at least one chain), and process all chains of the same length as one by introducing a multiplier in our dynamic programming: dpi=∑j=1kdpi−j⋅cntj, where cntj is the number of chains of length j. That way, our dynamic programming will work in O(mk−−√) if we skip the values of j with cntj=0.

Educational Codeforces Round 113 Editorial

By awoo, history, 17 months ago, translation, In English1569A - Balanced Substring

Idea: BledDest

Tutorial1569A - Balanced SubstringAny non-empty balanced string contains at least one letter 'a' and at least one letter 'b'. That implies that there's an 'a' adjacent to a 'b' somewhere in that string. Both strings "ab" and "ba" are balanced. Thus, any balanced string contains a balanced substring of length 2. So the solution is to check all n−1 pairs of adjacent letters. If there exists a pair of different ones, print it.

Overall complexity: O(n) per testcase.

Solution (awoo)1569B - Chess Tournament

Idea: BledDest

Tutorial1569B - Chess TournamentSince the chess players of the first type should not lose a single game, each game between two chess players of the first type should end in a draw (so that none of them gets defeated). And a game between a chess player of the first type and the second type should end either with a victory of the first or a draw. Therefore, for convenience, we will say that all games with a chess player of the first type end in a draw.

Now there are only games between chess players of the second type left. If there are only 1 or 2 such players, then there is no answer. Otherwise, we can choose the following method: the i-th chess player of the second type wins against the i+1-th chess player of the second type, and the last one wins against the first; all remaining games are drawn.

Solution (Neon)1569C - Jury Meeting

Idea: BledDest

Tutorial1569C - Jury MeetingNote that if there are at least two members with the maximum value of ai, then any permutation is nice.

Now let's consider the case when there is only one maximum. Let's find out when the permutation is nice. Let x be the index of the jury member with the maximum number of tasks. Then, during the ax-th discussion round, they will be the only one who will tell their task, because the other members of the jury have already told all their tasks. So during the (ax−1)-th discussion round, there should be a jury member who tells a task after the x-th jury member.

Let k be the number of elements in the array a equal to ax−1. Then, if at least one of these k jury members goes after the jury member x in the permutation, then the permutation is nice. Using this, we will count the number of bad permutations. Let's fix the elements in the permutation that are not equal to ax or ax−1, there are n−k−1 of them, then the number of ways is An−k−1n=n!(k+1)!. It remains to place k+1 elements so that the maximum is in the last position among them, there are k! such ways. The total number of bad permutations is n!k!(k+1)!=n!k+1. So the answer is n!−n!k+1.

Solution (Neon)1569D - Inconvenient Pairs

Idea: BledDest

Tutorial1569D - Inconvenient PairsFirstly, let's look at some point (xi,yi). Let's find closest to it vertical and horizontal lines. We will name the closest vertical lines from left and right as lx and rx (and ly and ry as closest horizontal lines). So, lx≤x≤rx and ly≤y≤ry (we can also note that either lx=rx or ly=ry).

Now, let's note that if for some other point j (xj,yj) either lx<xj<rx or ly<yj<ry then to reach j from i we must go reach either lx or rx (or, ly or ry), so the shortest distance will be strictly greater than the Manhattan distance.

If neither lx<xj<rx nor ly<yj<ry, then we can show that it's always possible to find the shortest path equal to the Manhattan distance. As a result, for each point (xi,yi) we should find the number of points (xj,yj) such that j<i and lx<xj<rx or ly<yj<ry. The exception here is when j lies on the same line as i, so we should not count such points.

We can note that since either lx=rx or ly=ry there is no such point j that lx<xj<rx and ly<yj<ry simultaneously, so we can calculate the pairs by x and y coordinates independently.

Let's focus on y coordinates (to calculate for x coordinates, we can just swap all coordinates). Let's sort all points by x coordinate. To get rid of the case when points i and j lies on the same vertical street, we can group them by x coordinate and process by group (since we sorted by x, groups are just segments). There are no problems with the case when points lie on the same horizontal street, since then ly=ry and there are no other yj with ly<yj<ry.

If we store for each horizontal line yj the number of point inside the interval (yi,yi+1) then, when we need for point i calculate the number of points j with j<i and ly<yj<ry, we can just ask for value assigned to ly, because ly and ry are consecutive elements in the array y.

So, we go through each group two times: first collecting answer, then updating values in appropriate ly-s. Note, that we can calculate ly and ry with binary search (using built-in functions).

The resulting complexity is O(n+m+k(logk+logn+logm)).

Solution (adedalic)1569E - Playoff Restoration

Idea: BledDest

Tutorial1569E - Playoff RestorationThere are exactly 2k−1 games in the tournament, each game has only two possible outcomes. So it's possible to bruteforce all 2k−1 possible ways the tournament could go if k is not large. In fact, this solution is fast enough when k<5, so if we somehow can handle the case k=5, we will have a working solution.

To handle k=5, let's divide the teams into two groups: teams from 1 to 16 and teams from 17 to 32. There will be exactly 15 matches in each group, and the winners of these two groups will play in the finals. The number of possible ways the games in a group can go is just 215, so let's try to bruteforce all possible results in each group and somehow "merge" them into the results of the whole tournament.

The main idea is to rewrite h as (h1+h2)mod998244353, where h1=(∑i=116i⋅Api)mod998244353, and h2=(∑i=1732i⋅Api)mod998244353, find all possible values for h1 and h2, and choose a pair of values that yields exactly the given value of h.

We will handle two separate cases: the winner of the first group wins the whole tournament, or the winner of the second group wins the whole tournament. Suppose we are handling the first case (the second is symmetrical). By choosing the results of matches in the first group, we determine the places of the teams from the first group in the whole tournament: the winner of the first group gets place 1, the team eliminated in the last match of the first group gets place 3, and so on. It means that by choosing one of the 215 possible results in the first group, we can calculate h1. Let's bruteforce these 215 combinations of results in the first group and store them in some data structure that allows to check whether some value of h1 is achievable (in the model solution, it's a std::map which maps reachable values of h1 to combinations of results that yield these values). Then, by choosing the results of matches in the second group, we can calculate h2; so the remaining part of the solution is to bruteforce all 215 possible results in the second group, calculate h2 for them, and check that h1 such that (h1+h2)mod998244353=h can be achieved by choosing the results in the first group. Don't forget to also handle the case when the team which wins in the first group loses in the finals (it is almost the same, but the winner in the first group gets place 2 and the winner in the second group gets place 1).

The technique I've described here (instead of bruteforcing all possible variants, split the thing we try to bruteforce into two parts, bruteforce them separatedly, and then try to "merge" the parts) is called meet-in-the-middle and can be used to solve a large variety of problems.

Solution (BledDest)1569F - Palindromic Hamiltonian Path

Idea: BledDest

Tutorial1569F - Palindromic Hamiltonian PathLet's start with making some implications from the low constraints.

What's the upper estimate on the number of answers? 1212. Too high, let's think of a better one. Using some combinatorics, we can normalize the answers in such a way that there are at most 12-th Bell's number of them. The method basically defines the components of equal letters. Given a string, we write down the letters in it in the order they appear for the first time in the string and rename the first of them to 'a', the second one to 'b' and so on. Only 4⋅106 possible answers already. Hmm, but we should also have an even amount of each letter. That is the absolute lowest estimate, and it's equal to about 2⋅106.

What does it exactly mean for a string to be good? There exists a path such that: there's a pair of equal letters that occupy the 1-st and the n-th vertex in the path, a pair on the 2-nd and (n−1)-th and so on.

So for each of 2⋅106 possible answers, we want to determine if there's a way to split the groups of equal letters into pairs of equal letters such that there exists a path through these pairs. Such a path would mean building a palindrome from inside out.

A quick estimation on a number of splittings into pairs. The first letter can be matched against 11 other letters, the first among the unmatched ones — against 9 other letters and so on. Thus, it's equal to 11⋅9⋅...⋅1=10395.

For each splitting into pairs, we can determine if there exists a path. That is a straightforward dynamic programming similar to a usual hamiltonian path search. It stores a mask of visited pairs and the last visited pair. For a transition, you want to either go from the first vertex of one pair to the first vertex of another one and from the second to the second, or the other way around. That would take 26⋅62 for each splitting.

The only thing left is to propagate the results from the splitting into pairs to splitting into even sized components of equal letters.

A splitting into pairs is a splitting into components of size 2. Let that be a base case for the dp. For every splitting into components, find a component of size at least 4 (we still have to split it into pairs) and separate it into a component of size 2 (a pair) and the rest of the component. Moreover, a pair can always be chosen in such a way that one of its elements is the first element of the component. So there are 2⋅106 states and at most 11 transitions from each of them.

Maybe there's a more convenient way to store the states, but the one I found to be fast enough is hashing the state into a base-6 integer (since there are no more than 6 components, numbered 0 through 5) and storing it in a map/hashmap.

Educational Codeforces Round 112 Editorial

By awoo, history, 19 months ago, translation, In English1555A - PizzaForces

Idea: BledDest

Tutorial1555A - PizzaForcesNote that the "speed" of cooking 1 slice of pizza is the same for all sizes — 1 slice of pizza for 2.5 minutes.

If n is odd, then we will increase it by 1 (since the pizza is cooked only with an even number of pieces). Now the value of n is always even. If n<6, then for such n the answer is equal to the answer for n=6, so we can say that n=max(n,6). While n≥12 we can order a small pizza. Eventually the value of n will be equal to 6, 8 or 10. This means that for any n there will be a set of pizzas with exactly n slices. Then the answer is n∗2.5 (in the solution, it is better to use the formula n/2∗5).

Solution (Neon)1555B - Two Tables

Idea: adedalic

Tutorial1555B - Two TablesFirstly, let's notice the next property: if two axis-aligned rectangles don't intersect, then we can draw a vertical or horizontal line between them. In other words, either max(x1,x2)≤min(x3,x4) or max(x3,x4)≤min(x1,x2) if x1 and x2 are coordinates of the one rectangle and x3 and x4 of the other one (analogically, for y coordinates).

Now, suppose you want to move the first table by (dx,dy). Note that if in result they will be divided by vertical line then we can set dy=0 — they still will be divided, but the total distance will decrease. Analogically, if divided by horizontal line, we can set dx=0.

In other words, it's always optimal to move the table either horizontally or vertically.

Let's look at the case of horizontal move: at first, we need to check that both tables can fit in the room, or their total width w+(x2−x1)≤W. If yes, then we calculate the movement distance dx as follows: if we move the table right then there should be at least w to the left of it, or w≤x1+dx ⇔ dx≥w−x1. Since we want to minimize dx then we take dx=max(0,w−x1).

If we want to move the table left, then there should be at least w to the right, or x2−dx≤W−w ⇔ dx≥x2−(W−w), minimizing dx means taking dx=max(0,x2−(W−w)). So, the result is min(max(0,w−x1),max(0,x2−(W−w))).

The vertical case can be handled in the same manner, if h+(y2−y1)≤H then the result is min(max(0,h−y1),max(0,y2−(H−h))).

The answer is the minimum among all possible variants, or −1 if both cases are impossible.

Solution (adedalic)1555C - Coin Rows

Idea: BledDest

Tutorial1555C - Coin RowsFirst, observe that each of the players has only m options for their path — which column to go down in.

Let's consider a Bob's response to a strategy chosen by Alice. The easiest way to approach that is to look at the picture of the Alice's path.

The path clearly separates the field into two independent pieces — suffix of the first row and the prefix of the second row. Bob can't grab the coins from both of them at once. However, he can grab either of them fully. So the optimal path for him will be one of these two options.

You can precalculate some prefix sums and become able to get the Bob's score given the Alice's path. Alice has m possibly paths, so you can iterate over them and choose the minimum answer.

However, prefix sums are not required, since you can quickly recalculate both needed sums while iterating over the Alice's column to go down in.

Overall complexity: O(m) per testcase.

Solution (awoo)1555D - Say No to Palindromes

Idea: BledDest

Tutorial1555D - Say No to PalindromesNote that in the beautiful string si≠si−1 (because it is a palindrome of length 2) and si≠si−2 (because it is a palindrome of length 3). This means si=si−3, i.e. a beautiful string has the form abcabcabc..., up to the permutation of the letters a, b and c.

For each permutation of the letters a, b and c, we will construct a string t, of the form abcabcabc... of length n. Let's define an array a of length n as follows: ai=0 if si=ti (i.e. the character at the i-th position does not need to be changed) and ai=1 otherwise. Let's build an array pr of prefix sums of the array a. Now you can process a query of the number of positions that need to be replaced for the current line t in O(1).

Solution (Neon)1555E - Boring Segments

Idea: vovuh, adedalic

Tutorial1555E - Boring SegmentsTake a look at the condition for a good subset.

The major implication it makes is that every point (even non-integer) of the segment [1;m] should be covered by at least one segment. If some point isn't, then there is no way to jump across the gap it produces.

At the same time, this condition is enough to have a path, since for every half-integer point (0.5, 1.5 and so on) there exists a segment that covers it. So you can take that segment to go from 1 to 2, then from 2 to 3 and so on.

Thus, we are asked to select a subset of segments that covers the entire segment [1;m] in its union.

The main prerequisite to the following solution is knowing the way to maintain the union of segments. For now, I can tell you that there is a data structure that allows you to add a segment, remove a segment and query the length of the current union.

Let's continue with making some observations on the cost function.

If you fix the minimum and the maximum value, you are free to select all segments that have their value in-between.

That allows us to transition from selecting a subset of segment to an interval, if you sort the segments by their weight.

If you fix only minimum, then the required maximum should be as small as possible. However, if some value suffices as a maximum, then any value greater than it also suffices (since it only adds extra segments to the subset). This makes the function on the maximum monotonous.

So the binary search applicable. You could iterate over the minimum and binary search the maximum. However, it's not too clear how to make a check function. You would need to find a union of some interval of segments quickly. I don't really know a way to do that, so let's try something different.

Instead, let's forget about binary search and try to reach a two pointers solution. Let f(x) be the smallest possible maximum, given the fixed minimum is x. We want f(x+1) to be greater than or equal than f(x) for two pointers to be applicable.

That condition indeed holds. Imagine if f(x+1) is smaller than f(x). So there exists some optimal subset for x+1. Add all segments with weight x to that subset. That brings the minimum to x. However, it doesn't change the maximum, so f(x) is at least equal to f(x+1), what contradicts the assumption.

Finally, the solution comes up to the following. Iterate over the minimum value x, while maintaining f(x). When going from x to x+1, keep increasing the value of f until the union of the segments is exactly m.

Going from x to x+1 and increasing the value of f is actually removing some segments and adding some segments to the data structure.

The data structure that helps us with that is a segment tree. The i-th leaf of the tree holds the number of segments that cover the interval (i;i+1). Add/remove segment makes it add/subtract on a range. The union is full if the there are no intervals that are covered by zero segments. Thus, let's store the minimum of the subtree in every intermediate node. If the minimum on the tree is above zero, then the current subset is good.

Instead of applying two pointers on the values of the segments, let's apply them on the sorted segments themselves. That makes moving the pointer exactly one update to the segtree.

Overall complexity: O(nlogn+nlogm).

Solution (awoo)1555F - Good Graph

Idea: adedalic

Tutorial1555F - Good GraphFirstly, let's prove that a good graph has one important property: any two of its simple cycles intersect by at most one vertex, i. e. there is no edge that belongs to more than one simple cycle (cactus definition, yeah).

Let's prove it by showing that if two simple cycles of weight k>0 intersects (by edges) then they will induce a simple cycle of weight ≠k. There are two cases:

if cycles intersect by a single path, then we can create a new cycle by merging parts of cycles excluding the intersecting path — it will be simple and will have weight k⊕k=0≠k;if cycles intersect by more than one path, we can do the next transformation: suppose the common paths are u1−v1, u2−v2, …, and they are ordered in the way how they lie on the first cycle. Let's create a third cycle using two paths from v1 to u2: from the first cycle and from the second cycle. It's easy to see that the third cycle is simple, and more over it has only one common path with the second cycle. So, it's either the third cycle has weight not equal to k or the case 1.Okay, let's analyze the edges we try to add. Let's divide all edges in two types: tree edges and all other edges (we will name them cycle edges). Let's name an edge as a tree edge if it connects two different components at a moment when we are trying to add it in the graph.

It's obvious that we will add all tree edges in the graph, since they can't make it bad (since they don't induce new cycles). But there is a more interesting observation: when we try to add a cycle edge (u,v), it should induce an only one simple cycle where all other edges are tree edges and these tree edges can't be used in any other cycle.

It induces at least one "all-tree-edge" cycle, since u and v are already connected. It can't induce more than one "all-tree-edge" cycle, since it contradicts with tree edge definition, and if it induces a cycle with some other cycle edge, then we can replace that cycle edge with its own tree-edge path: our cycle will become "all-tree-edge" cycle, but it will use already used tree edges. In other words, it's enough to consider only one "all-tree-edge" cycle induced by any cycle edge.

The final trick is to calculate the answer in two steps: at the first step, we will find only tree edges (using DSU) that will form a spanning forest in our graph.

The second step is for each cycle edge (u,v) to calculate the XOR X on a path between u and v in our spanning forest, check that X⊕edge_weight=1 and check that none of edges on the path from u to v are used in other cycle.

Calculating X is easy: if we precalculate for each vertex v the XOR on path from v to root xr[v] then X=xr[u]⊕xr[v].

Checking that none of the edges are used on the path from u to v is a bit tricky: if we mark an edge by adding 1 to it, then we should be able to take a sum on path and add on path. There are structures that are capable of it (like HLD and other), but let's look closely.

Note that we mark each tree edge at most once, so we can manually add 1 to each edge, and only asking sum on path should be fast. In other words, we need a data structure (DS) that can add value at edge and take the sum on path — and such DS is a Fenwick tree (BIT) built on Euler tour of tree: it can add value at edge and ask a sum on path from v to root. So we need to find LCA as well, since sum of path (u,v) is equal to sum(u)+sum(v)−2⋅sum(LCA(u,v)).

As a result, complexity is O((n+m)log(n)) with quite a low constant from LCA and BIT.

Educational Codeforces Round 111 Editorial

By awoo, history, 19 months ago, translation, In English1550A - Find The Array

Idea: BledDest

Tutorial1550A - Find The ArrayThe maximum sum we can construct with n elements is 1+3+5+7+⋯+2n−1=n2, so we need at least ⌈s√⌉ elements to construct the sum equal to s. Let's show how to express s with exactly ⌈s√⌉ elements.

Let ⌈s√⌉=d. By taking 1+3+5+7+⋯+2d−3, we achieve a sum of (d−1)2 using d−1 elements. s−(d−1)2 is not less than 1 and not greater than 2d−1 (since (d−1)2−−−−−−−√=d−1, and (d−1)2+2d−−−−−−−−−−−√>d). Thus, we can just add s−(d−1)2 to our array, and the sum becomes exactly s.

So, the solution is to find the minimum n such that n2≥s.

Solution (BledDest)1550B - Maximum Cost Deletion

Idea: Neon

Tutorial1550B - Maximum Cost DeletionLet l1,l2,…,lk be the length of the substring deleted at the i-th step. Then the number of points will be equal to ∑i=1k(a⋅li+b) or a∑i=1kli+bk. The sum of all li is equal to n (because in the end we deleted the entire string), so the final formula has the form an+bk. Obviously, for b≥0, you should delete the characters one by one so that k=n. Now b<0 and you have to delete the string in the minimum number of operations. Let the string s consist of m blocks of zeros and ones, then ⌊m2⌋+1 is the minimum number of operations for which the entire string can be deleted. As long as the number of blocks is more than 2, we will delete the second block, the number of blocks will decrease by 2 after each such operation (the block that we delete will disappear, and the first and third blocks will merge into one).

Solution (Neon)1550C - Manhattan Subarrays

Idea: adedalic

Tutorial1550C - Manhattan SubarraysLet's figure out criteria for the bad triple p, q, r. It's not hard to prove that the triple is bad, iff point q lies inside the bounding box of points p and r. In other words, if min(xp,xr)≤xq≤max(xp,xr) and min(yp,yr)≤yq≤max(yp,yr).

Now, looking at points p=(ai,i), q=(aj,j) and r=(ak,k) we can see that the bad situation may arise only if i<j<k — so we can check only ordered triples.

Looking closely at inequality min(ai,ak)≤aj≤max(ai,ak) we can note that there are two situations where (i,j,k) forms a bad triple: when either ai≤aj≤ak or ai≥aj≥ak. In other words, subarray is bad if and only if it contains either non-decreasing subsequence of length 3 or non-increasing subsequence of length 3.

The final observation is that any sequence of length at least 5 contains either non-decreasing or non-increasing subsequence of length 3. It's not hard to prove it, either brute-forcing all possible variants (of relative orders) on paper, or searching/remembering the theorem that says it.

As a result you need to check only subarrays of length at most 4 whichever the way you want. The complexity is O(n).

Solution (adedalic)1550D - Excellent Arrays

Idea: adedalic

Tutorial1550D - Excellent ArraysFirstly, let's learn the structure of good array a with maximum F(a). Suppose, ai=i+ki, then ai+aj=i+j ⇔ ki=−kj. In other words, we can group ai by |ki| and pairs will appear only inside each group. It's easy to prove that if the group has size m then it's optimal to split it in half: one with +ki and other with −ki. Then the number of pairs inside the group will be equal to ⌊m2⌋⋅⌈m2⌉.

It's also not hard to prove that in this case it's optimal to place all elements inside one group. In other words, it's optimal to make a half of all elements as ai=i+k and the other half as ai=i−k for some integer k>0. Then F(a)=⌊n2⌋⋅⌈n2⌉.

To achieve maximum F(a) the excellent array should also have this structure. Let half=⌊n2⌋. For a fixed k if n is even then we should choose exactly half positions i to set as ai=i+k, but if n is odd, we can choose either half or half+1 positions.

Let's analyze what happens with different k. Obviously, k≥1. While k≤min(1−l,r−n) both i+k and i−k are in the segment [l,r] for any i. In this case we can choose any ai as i+k, so there are exactly (nhalf) ways for even n and (nhalf)+(nhalf+1) ways for odd n.

When k>min(1−l,r−n) then for i∈[1,lf) (where lf=max(1,l+k)) there is only one choice — to set ai=i+k. Analogically, for i∈(rg,n] (where rg=min(n,r−k)) there is only choice to set ai=i−k.

What remains is rg−lf+1 elements without restrictions, so there are (rg−lf+1half−(lf−1)) ways to choose for even n or (rg−lf+1half−(lf−1))+(rg−lf+1half+1−(lf−1)) ways for odd n.

Note that it's convenient to say that (nk)=0 if k<0 or n<k, so we don't need extra checks.

Lastly, note that we can process all k∈[1,min(1−l,r−n)] with one formula and there are only O(n) of k>min(1−l,r−n) with non-zero number of ways to choose, so we can iterate over all such k straightforwardly.

The total complexity is O(nlogMOD) because of precomputation of factorials and inverse factorials to calculate (nk).

Solution (adedalic)1550E - Stringforces

Idea: BledDest

Tutorial1550E - StringforcesNotice that if there are substrings of length x for each letter, then there are also substrings of length x−1. Thus, the function on the answer is monotonous, so the binary search is applicable.

Let's have some answer x fixed by binary search. We have to place k blocks of letters of length x somewhere in a string. If we fix an order these blocks go into the string, then the greedy algorithm for placing them works. Put each block after the previous one but as far to the left as possible (the correctness can be proven by showing that picking not the furthest to the left position can't be more optimal). If there exists such an order that all blocks fit, then the answer is greater than or equal to x.

The common transition is to move from iterating over permutations to dynamic programming over submasks. Let dp[mask] be the smallest prefix of the string, such that all blocks of letters from the mask fit into this prefix. The transitions are the same: pick a new block and place it as early after that prefix as possible.

So far the solution works pretty slow, since for each of 2k masks we have to find the earliest possible position for a block. Let's use some precalculations to perform the transitions in O(1).

Notice that the transition doesn't depend on a mask, only on a length of the previous prefix. Thus, for every prefix and every letter, we can save the closest position for a block.

Let pos[i][j] be the closest position for a prefix of length i and the j-th letter. pos[i][j] is at least equal to pos[i+1][j]. However, if the block can be placed at the i-th position, then it should be updated. That can happen if the closest occurrence of any letter except j is not smaller than j+x. Thus, we can also maintain the closest occurrence of every letter. With some smart iterations, we can do the precalculations in O(nk).

The dynamic programming works in O(2k⋅k) then.

Overall complexity: O((nk+2k⋅k)logn).

Solution (awoo)1550F - Jumping Around

Idea: BledDest

Tutorial1550F - Jumping AroundNotice that increasing k only increases the range of the jump distances in both directions. So every rock that was reachable with some k, will be reachable with k+1 as well. Thus, let's try to find the smallest possible value of k to reach each rock.

Let's imagine this problem as a graph one and consider the following algorithm. For every pair of rocks, make an edge of weight equal to the smallest k required to jump from one to another. For some rocks v and u that is w=|d−|av−au||. How to check the reachability with these edges? Well, if the jump range value is k, then there should exist of path by edges of weight no more than k. So we can start with an empty graph, first add the edges of the smallest weight, then the second smallest, and so on. The first time a pair of vertices becomes reachable from each other is the minimum such weight.

An experienced reader can notice the resemblance with the Kruskal algorithm for finding the minimum spanning tree. After the spanning tree is constructed, the minimum k is the maximum value on a path between the vertices.

The issue is that Kruskal requires O(n2logn2) to construct an MST for a complete graph. Prim can make it O(n2), which is still too much. Thus, the solution is to resort to Boruvka.

On each iteration of Boruvka we have to find the smallest weight edge from each component to some other one. We can solve it the following way. Maintain a sorted set of rocks coordinates. The smallest weight edges are the ones that are the closest to d distance from each rock. So we could query a lower_bound of ai−d and ai+d on each rock i to find them. Don't forget to look at the both sides of the lower_bound result.

However, the issue is that we can bump into the rocks from the same component. Thus, let's process components one by one. When processing a component, first remove all its vertices from the set. Then query the edges for each vertex. Then add the vertices back. This way, only the edges to other components will be considered.

That makes it an O(nlog2n) construction, with one log from the number of Boruvka iterations and another nlogn from finding the edges. That should pass if coded carefully enough, and that is basically the intended solution.

Still, there exists a O(nlogn) construction. That will require a O(n) algorithm for finding the edges. So there are four possible edges for each rock i: the closest to ai−d from the left, from the right and the same for ai+d. Let's consider only the first case, the rest will be similar.

The coordinates are sorted beforehand, and we are processing the rocks from left to right. We can maintain a pointer to the latest encountered rock to the left of ai−d. The issue with it being from the same component is still there. Let's go around it by also storing the second latest encountered rock such that it's from the different component from the actual latest one. This can be updated in the same manner one calculates the second maximum of the array.

Now you just have to do that for all four cases. This two pointers approach makes it O(n) for each iteration, thus making the construction O(nlogn).

Since the queries ask for a path from some fixed vertex s to a certain vertex i, it's the same as calculating the maximum edge on a path from the root of the tree to each vertex. Can be done with a simple dfs. The only thing left is to check if the minimum possible k is less than or equal to the one provided in the query.

Overall complexity: O(nlog2n+q) or O(nlogn+q).

Educational Round 110 — Editorial

By BledDest, history, 20 months ago, In English1535A - Fair Playoff

Idea: BledDest

Tutorial1535A - Fair PlayoffIt is easier to determine the case when the players with the maximum skills will not meet in the finals. It means that they met in the semifinals, and in the other semifinals, both players are weaker.

It's easy to check this case with the following formula: min(s1,s2)>max(s3,s4) or max(s1,s2)<min(s3,s4).

Solution (Neon)1535B - Array Reodering

Idea: BledDest

Tutorial1535B - Array ReoderingIf the value of ai is even, then gcd(ai,2aj) at least 2, regardless of the value of aj. Therefore, we can put all the even values before the odd ones (it does not matter in what order). Now it remains to arrange the odd values. In fact, their order is not important, because gcd(ai,2aj)=gcd(ai,aj) (for odd ai and aj). This means that each pair will be considered exactly 1 time, regardless of the order of the odd elements.

Solution (Neon)1535C - Unstable String

Idea: BledDest

Tutorial1535C - Unstable StringLet's find a simple condition when the string is not beautiful. A string is not beautiful if there are two characters 0 (or two characters 1) at an odd distance, or 0 and 1 at an even distance (because in this case, the string cannot be made unstable).

Iterate over the right border of the substring r. Let l be the maximum index such that the substring s[l,r] is not beautiful (or 0 if the substring s[1,r] is beautiful). Then we have to add r−l to the answer (since any substring of a beautiful string is also beautiful).

Denote lstc,p as the last occurrence of c (0 or 1) at the position of parity p.

Let sr=0, p is the parity of r, then l=max(lst0,p⊕1,lst1,p), i. e. find the nearest character that breaks a beautiful substring (0 at an odd distance or 1 at an even distance)

The case for sr=1 is similar. If sr=?, then we can choose what this character will be. Obviously, we need to choose the option with the smaller value of l.

Solution (Neon)1535D - Playoff Tournament

Idea: BledDest

Tutorial1535D - Playoff TournamentDenote cnti as the number of teams that can be winners in the i-th game. The answer to the problem is cnt2k−1.

If the i-th game is played between the winners of games x and y (x<y), then:

cnti=cntx if si=0;cnti=cnty if si=1;cnti=cntx+cnty if si=?.So we can calculate all values of cnt for the initial string.

Note that the result of no more than k other games depends on the result of any game. So, if we change sp, it will change no more than k values of cnt, and we can recalculate all of them.

For convenience, you can renumerate the games so that the playoff looks like a segment tree, i. e. the final has the number 0, the semifinals have numbers 1 and 2, etc.

Solution (Neon)1535E - Gold Transfer

Idea: adedalic

Tutorial1535E - Gold TransferNote, that ci>cpi for each vertex i. So if we consider a path from some vertex v to 0, the closer you are to 0, the cheaper the cost. In other words, it's always optimal to choose the highest vertex on the path with ai>0.

Suppose we can find such vertex u for a given v. How many times we will repeat this search operation? If we need to buy w tons and u has au tons, then it's optimal to buy mn=min(w,au) tons in u. After we buy mn tons, either w becomes 0 or au becomes 0.

Since for each vertex u, au can become equal to zero at most once, and since after w is zero we stop buying, then there will be O(q) searches in total. The next question is how to find u efficiently for a given v?

Consider the path from 0 to some vertex v. Since we prefer to buy from higher vertices, all empty vertices on this path will form some prefix of it (possibly, empty prefix). So we can make some sort of binary search to find the first non-empty vertex u. But instead of binary search we will use binary lifting technique.

If we know for each k (0≤k<20) which vertex p[k][v] on the path from v to 0 on distance 2k from v then we can efficiently jump up the path. Let's firstly jump at distance 219: if a[p[19][v]]=0 then we jump too high — let's not jump. But if a[p[19][v]]>0 then we can safely jump (or v=p[19][v]). Now we know that we don't need a second 219 jump, so we try 218 jump and so on.

In other words, using binary lifting we can find the highest vertex u with au>0 in O(log(q)) steps. Also, we can calculate array p[k][v] for vertex v right after we add vertex v to the tree, since p[0][v]=pi and p[k][v]=p[k−1][p[k−1][v]].

The resulting complexity is O(qlog(q)).

Solution (adedalic)1535F - String Distance

Idea: BledDest

Tutorial1535F - String DistanceDisclaimer: the model solution is very complicated compared to most participants' solutions. Feel free to discuss your approaches in the comments!

First of all, it's easy to determine when two strings cannot be made equal using these operations: it's when their multisets of characters differ. So, we divide the strings into different equivalence classes, and for any pair of strings from different classes, the answer is 1337. For any pair of strings from the same class, the answer is either 1 or 2, since 2 operations are always enough to make the strings from the same equivalence class equal (we just sort both of them). Okay, now, for each class, we have to calculate the number of pairs of strings with the distance equal to 1.

Okay, suppose you have two strings s1 and s2, and you want to make them equal using one operation. Suppose that s1<s2 lexicographically. Since applying an operation can't result in getting a lexicographically larger string, we should apply the operation on the string s2, not s1.

Suppose we choose a substring [l,r] of the string s2 and sort it. All characters to the left of position l and to the right of position r are untouched, and all characters in [l,r] are ordered in non-descending order; so, in order to transform s2 into s1, we should choose a subsegment [l,r] such that all characters outside this segment are the same in both strings, and the substring [l,r] of s1 is sorted. So, the best way to choose a subsegment [l,r] is to compute the longest common prefix of s1 and s2, the longest common suffix of s1 and s2, and try sorting everything in the middle in s2.

This gives us a solution in O(n2): for a pair of strings, we can check that one of them can be transformed into the other in O(1). To do so, we need to build some data structure allowing to query longest common prefixes/suffixes in O(1) (a trie with O(1) LCA or precalculating LCP and building a sparse table of them can do the trick); furthermore, we want to be able to check if some subsegment of some string is sorted in O(1) (but precalculating them is quite easy). So, we have a solution that works if the strings are long (in the model solution, this approach is used on classes having not more than 12000 strings).

The second approach can be used on classes having many strings. If the number of strings is big, it means that they are short, so we can do the following thing: for each string, iterate on the subsegment we will sort and check if the resulting string exists. The model solution uses some very complicated data structures to implement this, but I believe that it's quite easy to get this approach working using string hashes.

The only dangerous thing in the second solution you have to consider is that choosing different substrings to sort may result in getting the same resulting string. One good way to deal with this is to ignore some substrings if sorting them doesn't change the leftmost or the rightmost character in the substring; for example, if we sort the substring acb in the string zacb, the character in the beginning of this substring is unchanged, so we can get the same result by sorting cb. So, we consider sorting the substring only if it changes both the first and the last characters of the substring.

Okay, so we have two approaches: one works well with a small number of long strings, and the other works well with a big number of short strings. We can choose which of them to run depending on the size of the equivalence class we are considering, and this idea gives us a working solution.

Educational Codeforces Round 109 — Editorial

By BledDest, 21 month(s) ago, In English1525A - Potion-making

Idea: adedalic

Tutorial1525A - Potion-makingSince you need e liters of essence to be exactly k % of potion then we can write an equality: ee+w=k100 or k=x⋅e and 100=x⋅(e+w) for some integer x. Since we need to minimize e+w and x(e+w)=100, then we should maximize x, but both k and 100 should be divisible by x. In other words, taking x as Greatest Common Divisor of k and 100 is optimal.

As a result e+w=100x=100gcd(k,100).

Solution (adedalic)1525B - Permutation Sort

Idea: BledDest

Tutorial1525B - Permutation SortTo solve the problem, it is enough to consider several cases:

if the array is already sorted, the answer is 0;if a[1]=1 (or a[n]=n), then you can sort the array in one operation by selecting the subarray [1,n−1] (or [2,n]);if a[1]=n and a[n]=1, you can perform the sequence of operations [1,n−1], [2,n] and [1,n−1] and sort the array on each of them (you can't do it faster since you can't move both n to position n and 1 to position 1 in only 2 operations);otherwise, the array can be sorted in 2 operations.Solution (Neon)1525C - Robot Collisions

Idea: BledDest

Tutorial1525C - Robot CollisionsNotice that the robots that start at even coordinates can never collide with the robots that start at odd coordinates. You can see that if a robot starts at an even coordinate, it'll be at an even coordinate on an even second and at an odd coordinate on an odd second.

Thus, we'll solve the even and the odd cases separately.

Sort the robots by their starting coordinate. Apparently, that step was an inconvenience for some of you. There is a common trick that can help you to implement that. Create a separate array of integer indices 1,2,…,n and sort them with a comparator that looks up the value by indices provided to tell the order. This gives you the order of elements and doesn't require you to modify the original data in any way.

Consider the task without reflections of the wall. Take a look at the first robot. If it goes to the left, then nothing ever happens to it. Otherwise, remember that it goes to the right. Look at the next one. If it goes to the left, then it can collide with the first one if that went to the right. Otherwise, remember that it also goes to the right. Now for the third one. If this one goes to the left, who does it collide with? Obviously, the rightmost alive robot that goes to the right.

So the idea is to keep a stack of the alive robots. If a robot goes to the left, then check if the stack is empty. If it isn't, then the top of the stack robot is the one who will collide with it. Pop it from the stack, since it explodes. If a robot goes to the right, simply push it to the stack. The time of the collision is just the distance between the robots divided by 2.

If there are robots left in the stack after every robot is processed, then they all go to the right together, so they never collide.

What changes when the reflections are introduced?

Almost nothing, actually. Well, now if the stack is empty and a robot goes to the left, then it behaves as a one going to the right. You can reflect the part of the way from its start to the wall. Just say that instead of starting at some x going to the left, it starts at −x going to the right. Since there's no one alive to the left of him initially, that will change nothing. That −x should be used for computing the collision time.

However, the final robots in the stack also act differently. First, the top of the stack robots reflects off the wall and collides with the second on the stack one. Then the third and the fourth and so on. So you can pop them in pairs until 0 or 1 are left.

The coordinate reflection trick can be used here as well. Imagine that the top of the stack starts at m+(m−x) and goes to the left instead of starting in x going to the right. For the same reason it changes nothing.

Overall complexity: O(nlogn).

Solution (awoo)1525D - Armchairs

Idea: BledDest

Tutorial1525D - ArmchairsLet's say that the starting position of people are x1,x2,…,xk (in sorted order) and ending positions of people are y1,y2,…,yk (also in sorted order). It's always optimal to match these starting and ending positions in sorted order: the leftmost starting position is matched with the leftmost ending, the second starting position is matched with the second ending, and so on. To prove it, suppose that position X1 is matched with Y2, position X2 is matched with Y1, X1≤X2 and Y1≤Y2. If both persons go to the left or to the right, it means that either X1≤X2≤Y1≤Y2 or Y1≤Y2≤X1≤X2, so nothing changes if we swap the matched positions. If, instead, the person that goes from X1 to Y2 goes to the right, and the person that goes from X2 to Y1 goes to the left, the segment [max(X1,Y1),min(X2,Y2)] belongs to both paths, and swapping the matched pairs removes this segment from both paths (and decreases the total time). So, if the order of starting positions is sorted and the order of ending positions is sorted, these positions should be matched exactly in those order.

Using this fact, we can implement the following dynamic programming: let dpi,j be the minimum time if we considered i first positions and picked j of them as the ending ones. Transitions are the following: we either take the current position as the ending one (if it's not a starting one), match it with the j-th starting position and go to dpi+1,j+1, or we skip the current position and go to dpi+1,j. It works in O(n2) since it has up to O(n2) states and just up to 2 transitions from each state.

Solution (BledDest)1525E - Assimilation IV

Idea: BledDest

Tutorial1525E - Assimilation IVLet I(j) be the indicator function equal to 1 if the j-th point is controlled by any city and 0 otherwise. Then the expected number of controlled points ans can be written as E(∑j=1mI(j))=∑j=1mE(I(j)) (by linearity of expected value).

The expected value of the indicator function is equal to the probability of this function equal to 1 (E(I(j))=P[I(j)=1]). In other words, for each point we need to calculate the probability of this point being controlled by any city.

Let's instead calculate the probability of point j not being controlled by any city. Suppose, the distance between point j and some city i is equal to x. If we build a Monument in city i at step <n+1−x (zero indexed) then the point will be controlled by city i. But building the Monument at any step greater or equal than n+1−x is fine.

Let's for each turn k∈[0,n) calculate the number of cities that you can build Monument in starting this turn as cnt[k]. Our task is to calculate the number of permutations that are consistent with array cnt.

At first turn, we can choose one of cnt[0] cities, at second turn we have cnt[0]+cnt[1]−1 choices, at third step — cnt[0]+cnt[1]+cnt[2]−2 choices, and so on. Using this idea, it's not hard to calculate the number of good permutations and then the initial probablity P[I(j)=1] ≡ 1−good_permutations⋅(n!)−1(mod998244353).

The expected value ans≡∑j=1mP[I(j)=1](mod998244353).

Solution (adedalic)1525F - Goblins And Gnomes

Idea: BledDest

Tutorial1525F - Goblins And GnomesFirst of all, let's try to solve the following problem: given a DAG, cover its vertices with the minimum number of vertex-disjoint paths. Solving this problem allows us to calculate the number of goblins that can pillage all of the halls when the tunnel network is fixed. This problem is a fairly classical one; since the number of vertices in each path is greater than the number of arcs in it exactly by 1, we should take the maximum possible number of arcs into our paths. So we can reduce this problem to bipartite maximum matching — build a bipartite graph where each part consists of n vertices, and for every directed arc (x,y) in the original graph, connect the vertex x of the left part to the vertex y in the right part of the bipartite graph. The maximum matching in this graph allows us to pick the maximum number of arcs into the paths of the original problem (the matching ensures that each vertex has at most one chosen ingoing arc and at most one chosen outgoing arc, so the paths are vertex-disjoint). Okay, now we at least can check if the goblin wave can pillage all of the halls.

Let's say that the minimum number of goblins required to pillage the original city is c. Obviously, in order to pass the c-th wave and waves after it, we have to increase this number. In one minute, Monocarp can block all of the tunnels leading to some hall or out of some hall — and in terms of our reduction to the bipartite matching problem, it means that we remove all edges connected to some vertex of the bipartite graph. Obviously, in one minute, we can increase c by at most 1, since c is equal to the difference between n and the maximum matching size.

It turns out that it's always possible to choose a vertex that belongs to all maximum matchings in the bipartite graph (note that it doesn't work in non-bipartite graphs, but in our problem, it doesn't matter). For the proof of this fact, you can check the last paragraph of the editorial. So, each minute Monocarp prepares for a wave, he increases the maximum number of goblins he can repel by 1.

Now the solution splits into two much easier parts. The first part is finding a sequence in which Monocarp blocks the tunnels, so that each his action reduces the size of the maximum matching by 1. Since the constraints are small, even a naive approach in O(n5) — always iterate on the vertex we try to remove from the graph and check that removing it is possible by running Kuhn's algorithm — is fast enough. The second part is to choose when Monocarp calls waves of goblins and when he prepares for them — this can be easily done with dynamic programming: let dpi,j be the maximum Monocarp's score if he has already passed i waves, and the current size of the maximum matching is j. The most naive implementation of this dynamic programming runs in O(n3), so the whole solution works in O(n5).

We can improve it to O(n3), though it is not needed under these constraints. Instead of finding the vertices to remove from the bipartite graph one-by-one, let's find all of them at once in O(n3). Recall that the size of maximum matching in a bipartite graph is equal to the size of its minimum vertex cover, and the minimum vertex cover can be reconstructed after finding the maximum matching. If we remove a vertex from the minimum vertex cover, the size of the minimum vertex cover of the remaining graph is reduced by 1, so the size of the maximum matching is reduced by 1 as well. It means that we can always choose to remove a vertex from the minimum vertex cover we found. By the way, it also proves that it's always possible to remove a vertex from a bipartite graph so the size of the maximum matching decreases by 1 (obviously, if it's not 0 already).

Educational Codeforces Round 108 Editorial

By awoo, history, 22 months ago, translation, In English1519A - Red and Blue Beans

Idea: adedalic

Tutorial1519A - Red and Blue BeansWithout loss of generality, let's say r≤b (otherwise, we can swap them). Note that you can't use more than r packets (at least one red bean in each packet), so b can't exceed r⋅(d+1) (at most d+1 blue beans in each packet).

So, if b>r⋅(d+1) then asnwer is NO. Otherwise, we can form exactly r packets.

Solution (adedalic)1519B - The Cake Is a Lie

Idea: adedalic

Tutorial1519B - The Cake Is a LieNote that whichever path you choose, the total cost will be the same. If you know that the cost is the same, then it's not hard to calculate it. It's equal to n⋅m−1. So the task is to check: is k equal to n⋅m−1 or not.

The constant cost may be proved by induction on n+m: for n=m=1 cost is 1⋅1−1=0. For a fixed (n,m), there are only two last steps you can make:

either from (n,m−1) with cost n: the total cost is n⋅(m−1)−1+n = n⋅m−1or from (n−1,m) with cost m: the total cost is (n−1)⋅m−1+m = n⋅m−1.So, whichever path you choose, the total cost is the same.

Solution (adedalic)1519C - Berland Regional

Idea: BledDest

Tutorial1519C - Berland RegionalThere are two important observations to make.

The first one is that you can calculate the answers for each university independently of each other and sum them up to obtain the true answer.

The second one is that if there are x students in an university, then that university can only contribute to answers for k from 1 to x.

So if we learn to calculate the contribution of the i-th university for some fixed k in O(1), then we will be able to iterate over all possible k for each university and get the solution in O(∑i=1nxi)=O(n), where xi is the number of students in the i-th university.

To achieve it, you have to gather the sum of the maximum number of students that can form full teams of size k. That must be the highest number less than or equal to xi that is divisible by k, so ⌊xik⌋⋅k. Sort the students of each university, precalculate partial sums, and now you are free to add the prefix sum of that number of students to the answer for k.

Overall complexity: O(nlogn) per testcase.

Solution (awoo)1519D - Maximum Sum of Products

Idea: vovuh

Tutorial1519D - Maximum Sum of ProductsThe naive approach is to iterate over l and r, reverse the subsegment of the array [l,r] and calculate the answer. But this solution is too slow and works in O(n3).

Instead, we can iterate over the center of the reversed segment and its length. If the current segment is [l,r], and we want to go to [l−1,r+1], then the answer for the subsegment will increase by al−1∗br+1+ar+1∗bl−1. It remains to add the answer for [1,l) and (r,n], but without reversion, this is easy to do if you pre-calculate the prefix sums of the values ai∗bi.

Solution (Neon)1519E - Off by One

Idea: BledDest

Tutorial1519E - Off by OneAt first the problem sounds like some sort of matching. However, it seems like you first want to match each point with either of its moves and then some pairs of points to each other. That doesn't sound viable but since the matchings are often connected with graphs, the graph idea might come handy.

Let's first consider a pair of matched points. What does it actually mean that there exists a line through their new coordinates and (0,0)? It's the same as: the angles of a line through the new coordinates of a and (0,0) and a line through the new coordinates of b and (0,0) are the same. Angles are the same means that their tangents are the same (and vice versa since we only consider the first quadrant of the plane). So we can conclude that yx+1 or y+1x of the first point should be equal to any of these of the second point.

Now consider the following graph. Various values of tangents of the lines are the nodes. Each point produces an edge between their yx+1 and y+1x. What are the matched pairs of points in this graph? It's such a pair of edges that they share at least one endpoint.

Building a graph is the slowest part of the solution since you have to use some data structure (or at least a sort and a binary search). O(n) is possible with some sort of hashmap but O(nlogn) should be perfectly fine as well.

So we reduced the problem to a more well-known one: given an arbitrary undirected graph, find the maximum number of pairs of edges such that each pair shares at least one endpoint and each edge is included in no more than one pair.

The upper bound on the answer is the following. Let mi be the number of edges in the i-th connected component. Best case we can make ⌊mi2⌋ pairs from it. Let's come up with an algorithm to achieve this bound.

Consider a dfs tree of a component. It's known that a dfs tree of an undirected graph contains no cross edges. So if we direct all the edges of a dfs tree downwards (convert all back edges to forward edges), each edge will connect some vertex to its descendant.

Imagine we came up with a dfs such that dfs(v) matches all the edges that have their upper node in the subtree of v to each other (except one edge in case there is an odd number of them). dfs(root) will solve the task exactly then.

How should that dfs work exactly?

What if there were no forward edges at all? That case is easy since all edges are tree edges. We'll try to maintain an invariant that the only unmatched edge is an edge that has v as one of its endpoints. If v is a leaf, then there's nothing to match. Otherwise, we go into some child u. If it can't match all its edges, then match its remaining edge to an edge (v,u). If it can then remember that we have an edge (v,u) unmatched so far. Go into another child w. Same, match our edge with its edge if it has one unmatched. However, if (v,w) turned out to get unmatched and (v,u) turned out to be unmatched, then you can match them to each other. This way you will be left with at most one unmatched edge after you process all the children, and that edge has its endpoint at v.

Add the forward edges back. Did anything change? Look at the forward edge that has its upper vertex the lowest. We can see that it points to a vertex u that has its subtree fully matches. So why don't we treat this forward edge the same as an edge to a leaf? Forget that u has some subtree of its own and just believe that you can't match the edge (v,u) so far. Proceed the same as the easy case.

Since we mark exactly which edges you pair up with which, it's trivial to retrieve the answer.

Overall complexity: O(n)/O(nlogn).

Solution (awoo)1519F - Chests and Keys

Idea: BledDest

Tutorial1519F - Chests and KeysFirstly, let's try to find some naive solution for this problem. Let's iterate on the subset of locks Alice puts on the chests. After choosing the subset of locks, how to check whether Bob can gain positive profit? We can iterate on the subset of keys he can buy as well, but in fact, this problem has a polynomial solution.

Construct a flow network as follows: each chest and each key represents a vertex; there are n arcs from the source to the vertices representing the chests (each having capacity ai), m arcs from the vertices representing the keys to the sink (each having capacity bj), and for each chosen lock, an arc from the respective chest-vertex to the respective key-vertex with infinite capacity. If we find the minimum cut from the source to the sink, then Bob's profit is (∑ni=1ai)−mincut. The reasoning behind this solution is the following one: if Bob takes all the chests and all the keys belonging to the first part of the cut, his profit is equal to the total cost of all chests he has taken, minus the total cost of all keys he has taken, minus infinity if he takes a chest he can't open. And the value of the cut is equal to the total cost of chests he doesn't take, plus the total cost of keys he takes, plus infinity if he can't open some chest he takes (since the arc from this chest-vertex to one of the key-vertices belongs to the cut). So, Bob's profit is (∑ni=1ai)−cut, and by minimizing the cut value, we maximize his profit. A minimum cut can be easily found using any maxflow algorithm.

Unfortunately, even iterating through all subsets of locks is too slow. To improve this solution, we should look at the minimum cut and its usage a bit more in detail. Notice that Bob can always take no keys and open no chests to get a profit of zero, so Alice's goal is to ensure that it is the best Bob's option. If Bob takes no chests and no keys, it means that the cut divides the network into two parts: the source and all other vertices. And, in terms of flows, it means that the maximum flow in this network should saturate all arcs going from the source (I highlighted it because it is the key idea of the solution).

Here the constraints on ai, n and m come in handy. We can use a dynamic programming with the flow over all arcs going from the source as one of the states. One of the ways to implement it is to have (f1,f2,…,fn,i,j,r) as the state, where f1 through fn are the values of the flow going from the arcs from the source, i is the current vertex in the left part we consider, j is the current vertex in the right part we consider, and r is the flow we already pushed through the arc connecting vertex j of the right part to the sink (and the value we store for this state is the minimum cost Alice has pay to reach this state). There are two basic types of transitions in this dynamic programming: we either skip the arc from i to j, or pick it and transfer some flow through it; and no matter what we've chosen, we move to the next vertex of the left part (or to 1 and increase j by 1 if we are already considering the n-th vertex of the left part). The constraints were loose enough to implement this dp basically in any form (there was no need to compress the states into single integers, for example, which was what the most participants of the round did).

Educational Codeforces Round 107 Editorial

By awoo, history, 22 months ago, translation, In English1511A - Review Site

Idea: BledDest

Tutorial1511A - Review SiteNotice that the answer depends only on the number of the reviewers of the third type who upvote the movie. Optimally we would want every single reviewer of the third type to upvote. We can achieve it with the following construction: send all reviewers of the first type to the first server, all reviewers of the second type to the second server and all reviewers of the third type to the first server. Since there are no downvotes on the first server, all reviewers of the third type will upvote.

Thus, the answer is the total number of reviewers of the first and the third type.

Overall complexity: O(n) per testcase.

Solution (adedalic)1511B - GCD Length

Idea: BledDest

Tutorial1511B - GCD LengthThe easiest way to force some gcd to be of some fixed length is to use the divisibility rules for 2, 5 or 10: if the number produced by the last y digits x is divisible by 2y, then x is also divisible by 2y (same goes for 5y and 10y).

One of the possible constructions is the following: let x=10..0a−c0..0c−1 and y=11..1b−c0..0c−1. Since 10..0 and 11..1 are pairwise prime, gcd is 10c−1.

Overall complexity: O(1) per testcase.

Solution (awoo)1511C - Yet Another Card Deck

Idea: BledDest

Tutorial1511C - Yet Another Card DeckLet's look at one fixed color. When we search a card of such color, we take the card with minimum index and after we place it on the top of the deck it remains the one with minimum index.

It means that for each color we take and move the same card — one card for each color. In other words, we need to keep track of only k cards, where k is the number of colors (k≤50). As a result, if posc is the position of a card of color c then we can simulate a query in the following way: for each color c such that posc<postj we increase posc by one (since the card will move down) and then set postj=1. Complexity is O(n+qk).

But, if we look closely, we may note that we don't even need array posc. We can almost manually find the first card of color tj and move it to the first position either by series of swaps or, for example, using rotate function (present in C++) and it will work fast.

Why? Let's look at one color c. For the first time it will cost O(n) operations to search the corresponding card and move it to the position 1. But after that, at any moment of time, the position of the card won't exceed k, since all cards before are pairwise different (due to the nature of queries). So, all next moves the color c costs only O(k) time.

As a result, the complexity of such almost naive solution is O(kn+qk).

Solution (Neon)1511D - Min Cost String

Idea: BledDest

Tutorial1511D - Min Cost StringConsider all possible strings of length 2 on the alphabet of size k (there are k2 of them). Let cnti be the number of occurrences of the i-th of them in the string s. The cost of the string s by definition is ∑icnti(cnti−1)2. Now, let's suppose there are two strings i and j such that cnti−cntj≥2. Then, if we somehow reduce the number of occurrences of the string i by 1 and increase the number of occurrences of the string j by 1, the cost will decrease. So, in the optimal answer all the strings of length 2 should appear the same number of times (and if it's impossible, the difference in the number of appearances should not be greater than 1).

Let's suppose that k2=n−1, then our goal is to build a string where each string of length 2 on the alphabet of k characters appears exactly once. The construction of this string can be modeled using Eulerian cycles: build a directed graph with k vertices, where each vertex represents a character, each arc represents a string of length 2, and for every pair of vertices (i,j), there is an arc from i to j (it's possible that i=j!). Then, by finding the Eulerian cycle in this graph (it always exists since the graph is strongly connected and, for each vertex, its in-degree is equal to its out-degree), we find a string of length k2+1 such that all its substrings are different (so each string of length 2 appears there once as a substring).

Okay, what about the cases k2>n−1 and k2<n−1? Since the string we build for the case k2=n−1 represents a cycle, we can make it "cyclical" and repeat the required number of times, then cut last several characters if it's too big. For example, if n=8, k=2, then the string for k=2 is abbaa (it's not the only one, but we can use it). We can expand this string to abbaabbaa (by repeating the last k2 characters), and delete the last character so its length is 8.

By the way, in this problem, you don't have to implement the algorithm that finds Eulerian cycles. The graph where we want to find the Eulerian cycle has a very special structure, and there are many different constructive ways to find the cycle in it. But if you can't use them, you always can rely on the straightforward solution that explicitly searches for the Eulerian cycle.

Solution (Neon)1511E - Colorings and Dominoes

Idea: BledDest

Tutorial1511E - Colorings and DominoesThere are different solutions to this problem involving combinatorics and/or dynamic programming, but, in my opinion, it's a bit easier to look at the problem from the perspective of probability theory.

Let's suppose a coloring is already chosen. Then it can be covered with dominoes greedily: red and blue cells are independent from each other, and, for example, red cells can be analyzed as a set of "strips" of them of different length. Let's say that we cover each "strip" from left to right, so, in each "strip", the first domino covers the cells 1 and 2, the second domino — the cells 3 and 4, and so on.

Let's calculate the average value of the coloring, that is, the expected value of the coloring if it is chosen randomly. Let it be E, then the answer is 2wE.

By linearity of expectation, E can be calculated as ∑d∈Dpd, where D is the set of all places we can use for a domino, and pd is the probability that there is a domino in place d in our domino covering (which we construct greedily). Each domino covers two adjacent cells, so we can iterate on pairs of adjacent cells, and for each pair, find the probability that this pair is covered. Let's suppose that we want to cover the cells (x,y) and (x,y+1) with a domino. Then:

both of these cells should be red;the length of the red "strip" before these cells should be even (otherwise the cell (x,y) will be paired with the cell (x,y−1)).The only thing we need to know in order to calculate the probability of these two conditions being true is the number of white cells before the cell (x,y) — which can be easily maintained. Knowing the number of white cells before (x,y), we can either use dynamic programming to calculate the required probability, or do the math on several easy examples and try to notice the pattern:

if there are 0 white cells before the current one, the probability of that pair being covered with a domino (let's call it P0) is 14 (both these cells should be red);if there is 1 white cell before the current one, the probability of that pair being covered with a domino (let's call it P1) is 14−18 (the cells (x,y) and (x,y+1) should be red, but the cell before them should not be red);P2 is 14−18+116 (either the chosen two cells are red and the cell before them is not red, or all four cells are red);P3 is 14−18+116−132, and so on.So, knowing the number of white cells before (x,y) and (x,y+1), we easily calculate the probability of this pair being covered by a domino. By summing up the probabilities over all pairs of adjacent white cells (don't forget the vertical ones!), we get the average (or expected) value of the coloring. All that's left is to multiply it by 2w.

Solution (BledDest)1511F - Chainword

Idea: BledDest

Tutorial1511F - ChainwordLet's use a trie to store the given words. Now let's imagine a procedure that checks if some string of length m can be represented as a concatenation of some of these words. If the words were prefix-independent — no word was a prefix of another word, that task would be solvable with a greedy algorithm.

We could iterate over a string and maintain the current vertex of the trie we are in. Append a current letter. If there is no such transition in a trie, it can't be represented. If the vertex we go to is a terminal, jump to the root of the trie. Otherwise, just go to that vertex.

However, since the words aren't prefix-independent, we have a terminal on a path to other terminals. Thus, we can't immediately decide if we should jump to the root or just go.

Let's handle this with dynamic programming. dp[i][v] — can we put i letters in such a way that the vertex of a trie we are in is v.

Is building a chainword letter by letter that different from this process? Apparently, it isn't. Consider dp[i][v][u] — how many ways are there to put i letters in a string so that the first hint is in a vertex v and the second hint is in a vertex u. For the transition we can try all 26 letters to put and jump to the corresponding vertices.

That obviously, is too slow. The intuition tells us that this dp should be calculated with some kind of matrix exponentiation (since m≤109). That dp can be rewritten as a matrix pretty easily. However, its size is up to 1681×1681 (the maximum number of vertices in a trie squared).

Some say that there is a way to compute the m-th power of such a huge matrix fast enough with Berlekamp–Massey, but I unfortunately am not familiar with it.

Thus, we'll have to reduce the size of our matrix. First, notice that the only reachable states (v,u) are such that the word that is written on a path from the root to v is a suffix of a word that is written on a path from the root to u or vice versa.

Look at it the other way: if we build a trie on the reversed words, then one of the vertices will be an ancestor of another one. Now it's easy to estimate the number of states as the sum of depths of all vertices. However, since we look at ordered pairs of (v,u), we should more or less double that amount. That should be 241 states at max.

This can probably pass with an optimal enough implementation. We can do better, though. Let's merge the states (v,u) and (u,v) into one state. The intuition is basically that you can swap the hints at will. That makes the pairs unordered: now there are up to 161 pairs. That surely will work fast enough.

The way to generate all the possible states is the following: run a dfs/bfs, starting from (0,0) that makes all valid transition and record all the states that can be visited.

While preparing the tests, I only managed to get up to 102 states and I would really love to hear an approach to either prove a tighter bound or to generate a test closer to the bound of 161.

Solution (awoo)1511G - Chips on a Board

Idea: BledDest

Tutorial1511G - Chips on a BoardThe model solution is O(NNlogN−−−−−−√), where N=max(n,m,q), but it seems that there are faster ones. I'll explain the model solution nevertheless.

It's easy to see (using simple Nim theory) that the answer for a query i is B iff the xor of cj−Li for all chips such that Li≤cj≤Ri is equal to 0. Let's calculate this xor for every query. This number contains at most 18 bits, and we will process these bits differently: we will choose some number K and use one solution to calculate K lowest bits, and another solution to compute 18−K highest bits. One idea is common in both solutions: we split each query into two queries — a query (Li,Ri) can be represented as a combination of two queries Q(Li,Li) and Q(Ri+1,Li), where Q(x,y) is the xor of all numbers cj−y such that cj≥x. After converting the queries, for every x∈[1,m], store each query of the form Q(x,y) in some sort of vector (or any other data structure). We will use an approach similar to sweep line: iterate on x and solve the queries for the current x. These ideas will be used both for the solution calculating K lowest bits and for the solution calculating 18−K highest bits.

How to find K lowest bits in each query? Iterate on x from m to 1 and maintain the number of occurrences of each number ci we met so far. Then, at a moment we want to calculate Q(x,y), simply iterate on all of the values of ci and process each value in O(1) (if the number of occurrences of some value z is odd, update the current answer to the query by xoring the number with z−y, otherwise just skip it). And since we are interested only in K lowest bits, for each ci, we need only the remainder cimod2K, so the number of different values is O(2K). Thus, this part of the solution runs in O(n+m+2Kq).

Okay, what about 18−K highest bits in each query? We can see that, for every number ci, the highest bits of ci−x don't change too often when we iterate on x: there will be about m2K segments where the highest bits of ci−x have different values. We can build a data structure that allows use to process two queries: xor all numbers on a segment with some value and get the value in some position (Fenwick trees and segment trees can do it). Then, we again iterate on x from m to 1. When we want to process a number ci, we find the segments where the highest bits of ci−x have the same value and perform updates on these segments in our data structure. When we process a query of the form Q(x,y), we simply get the value in the position y from our data structure. This part of the solution works in O(nm2Klogm+m+q).

By choosing K optimally, we can combine these two parts into a solution with complexity of O(NNlogN−−−−−−√).

Educational Codeforces Round 106 Editorial

By awoo, history, 23 months ago, translation, In English1499A - Domino on Windowsill

Idea: adedalic

Tutorial1499A - Domino on WindowsillWe can prove that if we have k1+k2 white cells on the board then we can place any w white dominoes as long as 2w≤k1+k2.

The proof is the following: if k1≥k2 let's place one domino at position ((1,k1−1),(1,k1)), otherwise let's place domino at position ((2,k2−1),(2,k2)). Then we can solve the placement of w−1 dominoes in k1−2 cells in the first row and k2 cells of the second row recursively (or, analogically, k1 and k2−2).

At the end, either all dominoes are placed or k1<2 and k2<2. If k1=0 or k2=0 then, since 2w≤k1+k2, then w=0 or we successfully placed all dominoes. If k1=1 and k2=1 then we, possibly, need to place one domino more — and we can place it vertically.

We can prove that we can place any b dominoes as long as 2b≤(n−k1)+(n−k2) in the same manner.

As a result, all we need to check is that 2w≤k1+k2 and 2b≤(n−k1)+(n−k2).

Solution (adedalic)1499B - Binary Removals

Idea: BledDest

Tutorial1499B - Binary RemovalsThere are several different ways to solve this problem. In my opinion, the two easiest solutions are:

notice that, in the sorted string, there is a prefix of zeroes and a suffix of ones. It means that we can iterate on the prefix (from which we remove all ones), and remove all zeroes from the suffix we obtain. If we try to remove two adjacent characters, then we cannot use this prefix;if there is a substring 11 before the substring 00 in our string, then from both of the substrings, at least one character remains, so if the first occurrence of 11 is earlier than the last occurrence of 00, there is no answer. Otherwise, the answer always exists.Solution (Neon)1499C - Minimum Grid Path

Idea: adedalic

Tutorial1499C - Minimum Grid PathSuppose we decided to make exactly k−1 turns or, in other words, our path will consist of exactly k segments. Since we should finish at point (n,n) and vertical and horizontal segments alternates, then it means that length1+length3+length5+⋯=n and legth2+length4+length6+⋯=n.

From the other side we should minimize ∑i=1kci⋅lengthi. But it means that we can minimize c1⋅length1+c3⋅length3+… and c2⋅length2+c4⋅length4+… independently.

How to minimize c1⋅length1+c3⋅length3+… if we know that length1+length3+length5+⋯=n and lengthi≥1? It's easy to prove that it's optimal to assign all lengthi=1 except minimum ci and assign to this minimum ci the remaining part lengthi=n−cntOdds+1.

In other words, to calculate the optimal path consisting of k segments, we need to know the sum of ci on odd and even positions among c1,…,ck and also minimum ci among odd and even positions. Then we can drive out the answer as a quite easy formula sumOdd+minOdd⋅(n−cntOdd) + sumEven+minEven⋅(n−cntEven).

Finally, we should iterate over all k from 2 to n and find the minimum answer among all variants. It's easy to recalculate sums and minimums when we make transition form k to k+1.

Complexity is O(n).

Solution (adedalic)1499D - The Number of Pairs

Idea: Neon

Tutorial1499D - The Number of PairsLet's represent a as Ag and b as Bg, where g=gcd(a,b) and gcd(A,B)=1. By definition lcm(a,b)=abg, so we can represent lcm(a,b) as Ag⋅Bgg=ABg.

Now we can rewrite the equation from the statement as follows: c⋅ABg−d⋅g=x⟹g(c⋅AB−d)=x. Since the left-hand side is divisible by g, the right-hand side should also be divisible. So we can iterate over g as divisors of x. If the right-hand side of c⋅AB=xg+d is not divisible by c then we can skip such g. AB=xg+dc (let's denote as k). If k has some prime divisor p then exactly one of A and B should be divisible by p because gcd(A,B)=1 (A and B have no common divisors). So there are 2the number of prime divisors of k pairs of A and B for current value of g.

We can precalculate the minimum prime divisor for each number up to 2⋅107 (the maximum value of k that you may need) in O(2⋅107loglog(2⋅107)) using Eratosthenes sieve. Now we can solve the problem in O(x−−√logx) for each testcase, but that's not fast enough. To speed up this approach, we can precalculate the number of prime divisors for each number up to 2⋅107. Let's denote mindi as the minimum prime divisor of i and vali as the number of prime divisors of i. Then vali=vali/mindi plus 1 if mindi≠mindi/mindi. Now, to solve the problem, we only need to iterate over the divisors of x, so the time complexity is O(x−−√) per testcase.

Solution (Neon)1499E - Chaotic Merge

Idea: BledDest

Tutorial1499E - Chaotic MergeFirst, let's try to calculate the number of merging sequences just for some fixed pair of strings x and y.

Imagine we build a merge letter by letter. So far i letters are in the merge already. For the (i+1)-th letter we can pick a letter from either string x or string y (put a 0 or a 1 into the merging sequence, respectively). What constraints our choice? Easy to see that it's only the i-th letter of the merge. So we can come up with the following dynamic programming.

dp[i][j][c] is the number of merging sequences such that i characters from x are taken, j characters from y are taken and the last character of the merge is c. c can be either just a letter (a dimension of size 26) or an indicator of a string the last character was taken from (0 for string x and y from string y). Since we know how many characters are taken from each string, we can easily decide the last taken character from that indicator. For each transition we can just take a character from either of the strings.

The sum of dp[|x|][|y|][i] over all i will be the total number of merging sequences.

Now for the substrings. Recall the following definition of a substring: b is a substring of a if you can remove some characters from the beginning of a (possibly, none or all) and some characters from the end of a (possibly, none or all) to get the string b.

What if we incorporated that definition into our dynamic programming? Let dp[i][j][c] be the number of merging sequences that end exactly before the i-th character of x, exactly before the j-th character of y and the last character is still c.

How to remove some characters from the beginning? That actually is the same as attempting to start the merge from every state of dp. So, if we are currently in some state dp[i][j][c], then we can act as if we have just taken the i-th character of x or the j-th character of y as the first character of the merge.

How to remove some characters from the end? Since dp[i][j][c] is the number of merging sequences that end exactly there, why not just sum up all the values of dynamic programming into the answer? We will count the sequences that end in all possible positions of both strings.

That is almost the answer to the task. The only issue we have is that we forgot the condition that asks us to get a non-empty substring from each string. Well, there are multiple ways to resolve the issue.

We can remove bad sequences afterwards: their count is the number of chaotic substrings of x multiplied by all possible empty substrings of y (there are |y|+1 of them) plus the same thing for y and x. These can be counted with two pointers.

Alternatively, we can add an extra dimension or two to the dp to indicate if we have ever taken a character from x and from y. So we get dp[i][j][c][flx][fly] with flx and fly being binary flags that tell if a character from x and from y was ever taken. That way we can only push the states with both flags set to true to the answer.

Overall complexity: O(|x|⋅|y|).

Solution 1 (awoo)Solution 2 (awoo)1499F - Diameter Cuts

Idea: BledDest

Tutorial1499F - Diameter CutsThe task is obviously solved by dynamic programming, so our first reaction should be to start looking for meaningful states for it. Obviously, one of the states is the vertex which subtree we are processing. We can choose the root for the tree arbitrarily, let it be vertex 1. What can be the other helpful state?

Consider the method to find the diameter of the subtree of vertex v. The diameter can be one of the following paths: either the longest path that is completely in some subtree of v or the concatenation of the longest paths that start in vertex v and end in different subtrees.

The diameter is the longest path. Thus, the diameter being less than or equal to k means that all paths should have length less than or equal to k.

If we can guarantee that no path that is completely in some subtree of v have length greater than k, then we will only have to worry about not concatenating long paths from different subtrees. Phrase it the other way around: if we never concatenate the paths from the different subtrees in such a way that their total length is greater than k, then no diameter will be greater than k.

Thus, we can attempt to have dp[v][len] — the number of ways to cut some edges in the subtree of v in such a way that there is no path of length greater than k and the longest path starting at vertex v has length len.

Now for the transitions. For the simplicity, let vertex v have exactly two children. It's not too hard to merge their dp's. Iterate over the length i of the first child, the length j of the second child. If (i+1)+(j+1)≤k, then you can concatenate their longest paths and the longest path for v will be of length max(i,j)+1. You can also cut either of the edges from v to the first child or to the second child.

The approach is good, however, it's not clear how to make it work on a larger number of children. Also, the complexity sounds pretty bad.

Instead of merging children to each other, let's merge each child to the dp of v one by one. dp[v][len] can store the current maximum length over all processed children. When processing a new child, you can choose to cut or not to cut the edge to it. So you can iterate over the current longest path from v and the longest path from that child.

So far, the only way to estimate the complexity is to say that each child has to merge its dp to the parent in O(k2), thus making the algorithm O(nk2). That's obviously too slow.

The trick that makes the solution fast is to iterate not to k but to the height of the subtree of v and the subtree of a child. Surely, that is allowed, since the path just can't grow longer than that value.

Consider the even worse option: not the height but the size of the subtree. It's easy to see that the size is always greater or equal than the height. Interpret the merge the following way: enumerate the vertices inside all the subtrees of the processed children and the vertices inside the subtree of the new child. Iterating up to the size of the subtree is the same number of moves as going over the vertices in it. The merge will go over all the pairs of vertices such that the first vertex of the pair is in the first set and the second vertex is in the second set. Thus, each pair of vertices of the tree will be processed exactly once (in lca of these vertices). There are O(n2) such pairs, thus, such dp's work in O(n2).

Overall complexity: O(n2).

Solution (awoo)1499G - Graph Coloring

Idea: BledDest

Tutorial1499G - Graph ColoringLet's split all edges of the graph into several paths and cycles (each edge will belong to exactly one path or cycle). Each path and each cycle will be colored in an alternating way: the first edge will be red, the second — blue, the third — red, and so on (or vice versa). Since the graph is bipartite, each cycle can be colored in an alternating way. The main idea of the solution is to add the edges one by one, maintain the structure of cycles and paths, and make sure that for each vertex, at most one path starts/ends in it. If we are able to maintain this invariant, then the value of |r(v)−b(v)| for every vertex will be minimum possible — each cycle going through a vertex covers an even number of edges incident to it (half of them will be red, half of them will be blue); so, if the degree of a vertex is odd, one path will have this vertex as an endpoint, and |r(v)−b(v)|=1; otherwise, it won't be an endpoint of any path, so |r(v)−b(v)|=0.

Okay, how do we maintain this structure? Let's add edges one by one (even the original edges of the graph) and rebuild the structure in online mode. For each vertex, we will maintain the indices of the paths that have this vertex as an endpoint. If some vertex has 2 or more paths as its endpoints, we can choose two of them and link them together. Whenever we add an edge from x to y, we just create a new path and check if we can link together some paths that have x or y as their endpoints.

How do we link the paths together? If we try to link a path with itself, it means that we try to close a cycle — and when we do it, we just forget about the resulting cycle, its structure won't change in future queries. When we link a path with some other path, we might need to reverse and/or repaint the paths before merging them into one. There are (at least) two possible data structures we can use to do this:

either an implicit-key treap that supports reversing and repainting;or a deque with small-to-large merging: whenever we try to link two paths together, we repaint and/or reverse the smaller one.Both of those methods give a solution in O((m+q)log(m+q)) or O((m+q)log2(m+q)), depending on your implementation. The model solution uses deques and small-to-large merging.

Educational Codeforces Round 105 Editorial

By awoo, history, 2 years ago, translation, In English1494A - ABC String

Idea: BledDest

Tutorial1494A - ABC StringThere are two key observations.

First, a regular bracket sequence always starts with an opening bracket and ends with a closing one. Thus, the first letter of a corresponds to an opening bracket and the last letter corresponds to a closing bracket. If they are the same, then the answer is "NO".

Second, a regular bracket sequence has exactly n2 opening and n2 closing brackets. Thus, we can check if the counts of the remaining letter and the first letter of the string or the remaining letter and the last letter of the string make it n2 in total. If neither of them do, then the answer is "NO". If both do, then that means that there are 0 occurrences of the remaining letter, so it doesn't matter what bracket it is assigned to.

Finally, after the assignment is complete, check if the resulting string is a regular bracket sequence. For that you have to check if on any prefix the number of opening brackets is greater or equal to the number of closing brackets. And also if the total number of opening and closing brackets is the same.

Overall complexity: O(n) per testcase.

Solution (Neon)1494B - Berland Crossword

Idea: Neon

Tutorial1494B - Berland CrosswordConsider some corner of the picture. If it's colored black, then it contributes to counts to both of the adjacent sides. Otherwise, it contributes to none. All the remaining cells can contribute only to the side they are on. There are n−2 of such cells on each side.

So let's try all 24 options of coloring the corners. After fixing the colors of the corners, we can calculate the number of cells that have to be colored on each side. That is calculated by taking the initial requirement and subtracting the adjacent colored corners from it. If any of the numbers is below 0 or above n−2 then that corner coloring doesn't work. Otherwise, you can always color the cells in some way.

Overall complexity: O(1) per testcase.

Solution (awoo)1494C - 1D Sokoban

Idea: adedalic

Tutorial1494C - 1D SokobanSince you can only push boxes, you can't bring boxes from negative positions to positive ones and vice versa. Thus, negative boxes/special positions and positive boxes/special positions are two separate tasks. You can solve them independently with the same algorithm and add up the answers.

So, we will only consider the positive boxes/special positions case. Notice that it never makes sense to move left. Thus, the only thing that determines the answer is the maximum position to the right you reach.

For a naive algorithm, we could iterate over that position, push all boxes that we have encountered on our way ahead of us and calculate the number of boxes that are on special positions. That works in O((n+m)⋅MAXC), where MAXC is maximum coordinate.

To improve that solution we can notice that the positions that are the most optimal are actually the ones such that the first box is pushed to some special position.

Consider the case the first box isn't on a special position, and there is a special position somewhere to the right of it. There are two types of boxes: the ones that are in the pile you would push if you move right and the remaining suffix.

What happens if you move one step to the right? The number of boxes from the suffix on special positions doesn't change. The number of boxes from the pile on special positions doesn't decrease. This number changes depending on if there is a special position immediately to the right of the pile and underneath the first box. Since we considered the case where there is no special position underneath the first box, the number can't decrease.

So we managed to improve the solution to O((n+m)⋅m). Still slow. Let's now learn to maintain the answer while moving the boxes. Precalculate sui — the number of boxes from the i-th to the last one that are already on special positions. That can be done with two pointers. Now iterate over the special position under the first box in the increasing order. Maintain the size of the pile and the number of special positions under the pile. The first value is just the index of the first box not in a pile. The second value is easier to obtain if you keep the index of the first special position after the pile (or m if there are none). Also achievable with two pointers. The answer is the number of special positions under the pile plus the suffix answer for the boxes after the pile. Take the maximum of all options.

The constraints are pretty free, so you could replace two pointers with binary searches if you wanted to.

Overall complexity: O(n+m) per testcase.

Solution (awoo)1494D - Dogeforces

Idea: Neon

Tutorial1494D - DogeforcesWe can solve the problem recursively from the root to the leaves. Let's maintain a list of leaf indices for the current subtree. If the list size is equal to 1, then we can stop our recursion. Otherwise, we have to find the value of the root of the current subtree and split all leaves between child nodes. The root value is the maximum value of av,u among all pairs v,u belonging to a subtree (since the current root has at least 2 child nodes, there is a pair of leaves for which the current root is the least common ancestor). If the value of the least common ancestor of the leaves v and u (av,u) is less than the value of the current root then v and u belong to the same child of the root. Using this fact, we can split all the leaves between the child nodes and then restore the subtrees for them recursively.

Solution (Neon)1494E - A-Z Graph

Idea: adedalic

Tutorial1494E - A-Z GraphAt first, if there should be both routes v1,v2,…,vk and vk,vk−1,…,v1 then there are both arcs (v1,v2) and (v2,v1), i. e. there should exist at least one pair {u,v} that both arcs (u,v) and (v,u) are present in the graph.

Now, if k is odd, and we have at least one pair {u,v} then we can simply create sequence u,v,u,v,…,v,u. This sequence is a palindrome so, obviously, both routes generate the same string.

If k is even (or k=2x), we can note that in the sequence v1,v2,…,v2x there is a middle arc (vx,vx+1) and it should have the same character as arc (vx+1,vx) (since it's a middle arc in reverse route vk,…,v1), i. e. there should exist at least one pair {u,v} that both arcs (u,v) and (v,u) are present in the graph and have the same label.

Now, if we have at least one such pair {u,v} then routes u,v,…,u,v and v,u,…,v,u generate the same one-letter strings.

Since each arc (u,v) is a part of at most one pair {u,v}, we can just maintain two sets with pairs {u,v}: one for pairs with different labels and the other one for pairs with equal labels. If k is odd, we check that at least one of the sets is not empty. If k is even, we check that the second set is not empty.

Solution (adedalic)1494F - Delete The Edges

Idea: BledDest

Tutorial1494F - Delete The EdgesLet's suppose our graph is split into two graphs G1 and G2, the first graph contains the edges we delete before the mode shift, the second graph contains the edges we delete after the mode shift.

It's quite obvious that the graph G1 has an eulerian path. The structure of G2 is a bit harder to analyze, but we can prove that it is always a star graph (a vertex and some other vertices connected directly to it), and the center of the star coincides with the last vertex in the eulerian path in G1. To prove that G2 is a star graph, we can consider the second part of the path (after the mode shift) backward: the last edge we traversed was deleted, and the previous-to-last move could have been only along that edge. The third-last and the fourth-last moves should have been along another edge connecting some vertex to the center of the star, and so on.

Okay, how do we find a way to split the graph into G1 and G2? Iterate on the center of the star (let it be c). For the graph G1 to contain an eulerian path, it should have at most 2 vertices with an odd degree. Let's construct G2 in such a way that we minimize the number of odd vertices in G1 — for each edge incident to c, we either move it to G1 or G2 in such a way that the resulting degree of the other vertex is even. All other edges belong to G1.

If there is an eulerian path in G1 that ends in c, we are done. Otherwise, we should iterate on some edge adjacent to c and change its status (in order to check if G1 can have an eulerian path after that). We can't "flip" two edges because flipping two edges increases the number of odd vertices in G1 at least by 2 (if it is already 2 or greater, the eulerian path won't exist, and if it's 0, then flipping two edges creates two odd vertices, none of which is c, so eulerian path can't end in c). After flipping each edge, we try to find an eulerian path in G1 once again and flip the edge back. After checking the vertex c as the center of the star, we return all adjacent edges to G1 and move to the next vertex.

The whole algorithm requires checking for the existence of the eulerian path O(n+m) times, so it should work in O((n+m)2) or O((n+m)2logn) depending on the implementation.

Fun fact: initially I wanted to give a harder version of a problem with n,m≤2⋅105 that would require some sort of dynamic connectivity to check for an eulerian path fast, but when I started coding it, I realized that implementation there was a bit painful, so I've decided to drop the constraints to allow quadratic solutions.

Educational Codeforces Round 104 Editorial

By awoo, history, 2 years ago, translation, In English1487A - Arena

Idea: BledDest

Tutorial1487A - ArenaIf for some hero i, no other hero is weaker than i, then the i-th hero cannot win any fights and is not a possible winner. Otherwise, the hero i is a possible winner — he may fight the weakest hero 100500 times and be declared the winner.

So the solution to the problem is calculating the number of minimum elements in the array a, since all other elements denote possible winners of the tournament.

Solution (Neon)1487B - Cat Cycle

Idea: adedalic

Tutorial1487B - Cat CycleIf n is even, then each hour A and B are on the spots with different parity, so they will never meet. Otherwise, let's look closely what happens.

At the start, A in n and B in 1. But since we can form a cycle from spots then it means that n and 1 in reality are neighbors. After that, A and B (starting from neighboring positions) just go in opposite directions and meet each other in the opposite spot after exactly ⌊n2 ⌋ steps.

After meeting B "jumps over" A making 1 extra step and the situation become practically the same: A and B are neighbors and move in the opposite direction.

In other words, each f=⌊n2 ⌋ steps B makes one extra step, so the answer (if both k and spots are 0-indexed) is(k+(nmod2)⋅⌊kf ⌋)modnSolution (adedalic)1487C - Minimum Ties

Idea: BledDest

Tutorial1487C - Minimum TiesIf n is odd, then we can solve the problem without any ties: each team should win exactly ⌊n2 ⌋ matches and lose the same number of matches. Finding which matches each team wins and which matches each team loses can be done with some graph algorithms (like Eulerian cycles or circulations), or with a simple construction: place all teams in a circle in any order, and let the i-th team win against the next ⌊n2 ⌋ teams after it in the circle, and lose to all other teams.

Unfortunately, if n is even, we need to use some ties since the total sum of scores over all teams is exactly 3n(n−1)2 when there are no ties, and this number is not divisible by n when n is even. Each tie reduces the total sum by 1, and the minimum number of ties to make 3n(n−1)2 −t divisible by n is t=n2 (since 3n(n−1)2 modn=n2 ). So, if we find an answer with exactly n2 ties, it is optimal. And it's easy to find one: once again, place all teams in a circle in any order; make the i-th team win against n−22 next teams in the circle, lose against n−22 previous teams in the circle, and tie with the opposite team in the circle.

Solution (BledDest)1487D - Pythagorean Triples

Idea: Neon

Tutorial1487D - Pythagorean TriplesWe have to find the number of triples (a,b,c) such that equations a2+b2=c2 and a2−b=c are satisfied. Let's subtract one equation from another and get that b2+b=c2−c⟹b(b+1)=c(c−1). So we know that c=b+1 and after substituting, we get that a2=2b+1. We can see that there is only one correct value of b (and c) for every odd value of a (greater than 1). So we can iterate over the value of a and check that the corresponding value of c doesn't exceed n. This solution works in O(√n) because a≈√c≤√n, but you can also solve it in O(1).

Solution (Neon)1487E - Cheap Dinner

Idea: BledDest

Tutorial1487E - Cheap DinnerThe main solution is dynamic programming: let dpi for every possible dish i be the minimum cost to assemble a prefix of a dinner ending with the dish i (here, i can be a dish of any type: first course, second course, drink, or dessert). Then, the answer to the problem is the minimum value among all desserts.

The number of transitions in this dynamic programming is too big, since, for example, when transitioning from first courses to second courses, we need to check O(n1n2) options. To speed this up, we need some sort of data structure built over the values of dpi for all first courses i that allows to recalculate dpj for a second course j quickly. There are two main approaches to this:

build any version of RMQ over the values of dynamic programming for the first courses. Then, when we want to calculate the answer for some second course j, sort all types of first courses which don't go well with it, and make several RMQ queries to find the minimum value over all non-forbidden first courses;store all values of dpi in a data structure that supports adding an element, deleting an element, and finding the minimum element (this DS should allow duplicate elements as well). When we want to calculate the answer for some second course j, remove all values of dpi corresponding to the first courses that don't go well with it from the data structure, query the minimum in it, and insert the removed elements back.The same approach can be used to advance from second courses to drinks and from drinks to desserts (you can even use the same code in a for-loop with 3 iterations, so the resulting solution is actually short and simple).

Solution (BledDest)1487F - Ones

Idea: Neon

Tutorial1487F - OnesLet's build the number from the lowest digit to the highest digit with the following dynamic programming: dpi,carry,cp,cn — the minimum number of ones, if i least significant digits are already fixed, the carry to the next digit is carry (can be negative), there are cp positive numbers (of the form 111⋯111) of length greater than or equal to i and cn negative numbers of length greater than or equal to i.

First, consider the transitions when we reduce the values of cp and/or cn. Such transitions correspond to the fact that in the optimal answer there were several numbers of length exactly i, and they should not be considered further.

If the value of (cp−cn+carry)mod10 matches the i-th least significant digit in n, then we can use transition to (i+1)-th state with the new value of carry and the number of ones in the answer increased by cp+cn.

It remains to estimate what the maximum value of cp (cn) and carry we need. The value of cp doesn't exceed the total number of numbers that we use in the answer. Using at most 5 numbers we can decrease the length of n by at least 1. Thus, the maximum value of cp and cn is at most 5|n| (where |n| is the length of the number n). For the value of carry, the condition carry≥carry+cp10 should be met (similarly for a negative value). Thus, we can assume that the absolute value of carry doesn't exceed 5|n|9 .

The total complexity of this solution is O(|n|4), yet with a high constant factor.

Solution (Neon)1487G - String Counting

Idea: BledDest

Tutorial1487G - String CountingSuppose there is no constraint on the number of letters used. Then this problem can be solved with the following dynamic programming: let dpi,j,k be the number of strings of length i ending with characters j and k that don't contain palindromes of odd length greater than 1 (obviously, each forbidden palindrome contains a subpalindrome of length 3, so we only need to ensure that there are no palindromes of length 3).

The thing we are going to use in order to ensure that all the constraints on the number of characters are met is inclusion-exclusion. Since each ci>n3 , at most two characters can violate their constraints in a single string, so we will iterate on some character of the alphabet and subtract the number of strings violating the constraint on this character from the answer, then iterate on a pair of characters and add the number of strings violating the constraints on these two characters to the answer.

Okay, how to calculate the number of strings violating the constraint on some fixed character? Let's use dynamic programming fi,j,k,l — the number of strings such that they contain i characters, j of them have the same type that we fixed, the previous-to-last character is k and the last character is l. The number of states here seems to be something about n2⋅262, but in fact, k and l can be optimized to have only two different values since we are interested in two types of characters: the ones that coincide with the character we fixed, and the ones that don't.

Okay, what about violating the constraints on two characters? The same method can be used here: let gi,j,k,l,m be the number of strings consisting of i characters such that the number of occurrences of the first fixed character is j, the number of occurrences of the second fixed character is k, the previous-to-last character is l and the last character is m. Again, at first it seems that there are up to n3⋅262 states, but l and m can be optimized to have only 3 different values, so the number of states is actually n3⋅32.

It seems that we have to run this dynamic programming for each pair of characters, right? In fact, no, it is the same for every pair of characters, the only difference is which states violate the constraints and which don't. We can run this dp only once, and when we need an answer for the pair of characters (x,y), we can use two-dimensional prefix sums to query the sum over gi,j,k,l,m with i=n, j>cx and k>cy in O(1). In fact, this dynamic programming can also be used for the first and the second part of the solution (calculating the strings that don't violate any constraints and the strings that violate the constraints on one character), so the hardest part of the solution runs in O(n3), though with a pretty big constant factor.

Educational Codeforces Round 103 Editorial

By awoo, history, 2 years ago, translation, In English1476A - K-divisible Sum

Idea: vovuh

Tutorial1476A - K-divisible SumLet's denote s as the sum of array a. From one side, since s should be divisible by k then we can say s=cf⋅k. From other side, since all ai are positive, then s≥n.

It's quite obvious that the smaller s — the smaller maximum ai so we need to find the smallest cf that cf⋅k≥n. Then cf=⌈nk⌉=⌊n+k−1k⌋.

Now we now that s=cf⋅k and we need to represent it as a1+⋯+an with maximum ai minimized. It's easy to prove by contradiction that maximum ai≥⌈sn⌉.

Moreover we can always construct such array a that its sum is equal to s and the maximum element is equal to ⌈sn⌉.

As a result, the answer is ⌈sn⌉=⌊cf⋅k+n−1n⌋, where cf=⌊n+k−1k⌋.

Solution (adedalic)1476B - Inflation

Idea: adedalic

Tutorial1476B - InflationSuppose we decided to increase some pi by x>0. How does it affect all inflation coefficients? Let's the j-th inflation coefficient be cfj. We now that cfj=pjp0+⋯+pj−1.

If j<i, then cfj doesn't change. If j>i then it's denominator increases by x and cfj decreases. If j=i then it's numerator increases and cfj increases as well.

But, if we increase pi−1 instead of pi then all decreased cfj will decrease as well and also cfi will decrease. Finally, if we increase p0 then all cfj decrease and there is no cfj that increases — so it's always optimal to increase only p0.

Now we need to calculate what is minimum x we should add to p0. There are two ways: we can either binary search this value x knowing that x=100⋅109 is always enough. Then we just need to check that all cfj≤k100 (that is equivalent to checking that pj⋅100≤k⋅(x+p0+⋯+pj−1)).

Or we can note that each cfj=pj(p0+⋯+pj−1)+x and we need to make cfj≤k100 or that 100⋅pj−k⋅(p0+⋯+pj−1)≤k⋅x or x≥⌈100⋅pj−k⋅(p0+⋯+pj−1)k⌉. Since we should fulfill all conditions then we should take x as maximum over all fractions.

Since (p0+⋯+pj−1) is just a prefix sum, we can check condition for each cfj in O(1). It total, the time complexity is either O(n) or O(nlogn) per test case.

Solution (adedalic)1476C - Longest Simple Cycle

Idea: adedalic

Tutorial1476C - Longest Simple CycleSuppose, we've built the graph and chosen any simple cycle. Due to the nature of the graph, any simple cycle right part is part of one of the chains. So, let's for each chain calculate the longest simple path with its right part on this chain and denote it as leni.

Obviously, len1=0. Now, let's look at chain i. If we go along the cycle in both ways, we will step to vertices ai and bi of the previous chain. If ai=bi then we closed cycle and it's the only possible cycle, so leni=ci+1.

Otherwise, we can either go from ai and bi and meet each other closing the cycle with part of the (i−1)-th chain between ai-th and bi-th vertices — this part has |ai−bi| edges and our cycle will have length ci+1+|ai−bi|.

But if we decide to go in different ways, then we will meet the first and the last vertices of the (i−1)-th chain. After that, we'll go to the ai−1-th and the bi−1-th vertices of (i−2)-th chain and will make almost the same choice.

But, instead of recurrently solving the same problem, we can note that, in fact, we took a cycle that ends at the (i−1)-th chain, erased the part between vertices ai and bi, and merged it with our i-th chain part, so the length of this merged cycle will be equal to ci+1+leni−1−|ai−bi|. Since we maximize leni we just choose, what part: |ai−bi| or leni−1−|ai−bi| is longer and take it.

As a result, we can iterate from left to right, calculate all leni and print the maximum among them.

Solution (adedalic)1476D - Journey

Idea: BledDest

Tutorial1476D - JourneyThere are two key observations to this problem:

after each pair of moves, the directions go back to the original ones;after each move, we can immediately go back (and combining these observations, we can derive that if we go from city i to some other city j, we can always go back).One of the solutions we can write using these observations is to build an undirected graph on 2n+2 vertices. Each vertex represents a pair (v,k), where v is the city we are currently staying in, and k is the number of moves we made, modulo 2. Since each move is to a neighboring city, each vertex (v,0) is unreachable from (v,1), and vice versa. And since we can always go back, and each pair of steps doesn't change the directions, this graph is actually an undirected one. So, we can find the connected components of this graph using DFS/BFS/DSU, and for each city v, print the size of the component the vertex (v,0) belongs to.

Another solution is to find the leftmost and the rightmost city reachable from each city. For example, finding the leftmost reachable city can be done with the following dynamic programming: let dpi be the leftmost city reachable from i. Then, if we can't go left from i, dpi=i; if we can make only one step to the left from i, dpi=i−1; and if we can make two steps, we can take the answer from the city i−2: dpi=dpi−2. The same approach can be used to calculate the rightmost reachable city.

Solution 1 (BledDest)Solution 2 (BledDest)1476E - Pattern Matching

Idea: BledDest

Tutorial1476E - Pattern MatchingLet's write down the indices of the pattern that the j-th string matches. If mtj is not among these, then the answer is NO. Otherwise, all the patterns except mtj should go in the resulting ordering after mtj.

Consider that as a graph. Let's add an edge from mtj to each of the matches. If you add the edges for all the strings, then the topological ordering of the graph will give you the valid result. If the graph has any cycles in it (you can't topsort it), then there is no answer.

To find all the patterns we can use the fact that k is rather small. Consider all the 2k binary masks of length k. Each mask can correspond to a set of positions in the string that are replaced with wildcards. Now, if there is a pattern that is exactly equal to the string with the fixed set of positions replaced by wildcards, then that pattern is a match.

To search for an exact match, you can either store all patterns in a map beforehand (or in a sorted array) or build a trie of them. The second version is faster by a factor of logn but both solutions should pass easily.

Overall complexity: O(nklogn+mk⋅2klogn) or O(nk+m⋅2k).

Solution 1 (awoo)Solution 2 (awoo)1476F - Lanterns

Idea: BledDest

Tutorial1476F - LanternsThe main idea of the solution is to calculate the following dynamic programming: dpi is the maximum prefix we can fully cover with i first lanterns.

Let's look at how can we solve it in O(n2) with this kind of dynamic programming. First of all, let's write it forward. Which transitions from dpi do we have?

iterate on the lantern facing left that will cover the lantern dpi+1. Let this lantern be j. It should cover all lanterns in [dpi+1,j−1], so all lanterns from [i,j) can be turned to the right (and we need a max query to determine the new covered prefix);if dpi>i (lantern i is already covered), we can just extend the prefix by turning the i-th lantern to the right. Note that turning it to the right when it is not covered yet will be modeled by the first transition.It is obviously O(n2), how can we optimize it? Let's write this dynamic programming backward. The second transition is changed to backward dp easily, what about the first one? Suppose we want to turn some lantern i to the left. Let's iterate on the prefix j that we will "connect" to it; for this prefix, dpj should be at least i−pi−1, and we update dpi with the maximum of i−1 (since it is covered by lantern i) and the result of max query on [j+1,i−1].

In fact, we need only one such prefix — the one with the minimum j among those which have dpj≥i−pi−1. So, we build a minimum segment tree where each pair (i,dpi) is interpreted as the value of i in position dpi, and with min query on the suffix from i−pi−1 we find this optimal prefix, from which we should update (and to update, we can use any DS that allows max queries on segment — in my solution, it's another segment tree).

Solution (BledDest)1476G - Minimum Difference

Idea: Neon

Tutorial1476G - Minimum DifferenceLet's consider a problem without queries of the second type.

Now we can try to solve the problem using Mo's algorithm. Let's maintain array cnti — the number of occurrences of i on the current segment and array ord — array cnt sorted in descending order.

Let's take a look at how we should handle adding an element equal to x. Surely, we should increase cntx by 1, but now we should erase an element equal to cntx−1 from the array ord and insert an element cntx is such a way that the array is still sorted. Instead, we can increase the leftmost element equal to cntx−1 by 1. Similarly, we can handle deleting an element (decrease the rightmost element equal to cntx by 1). In order to quickly find the leftmost (rightmost) element equal to x, we can store the left and the right bounds of the array ord where all the numbers are equal to x.

To answer the query of type 1, we should find two elements in the ord array at distance k whose absolute difference is minimal. Since the size of the array ord (without zero elements) is O(n), we can't look at the whole array. But using the fact that there are no more than O(n−−√) different values in the ord array, we can create an auxiliary array of pairs (value,cnt) (the value from the array ord and the number of occurrences of that value). In such an array, we need to find a subarray where the sum of the second elements in the pairs is at least k, and the absolute difference between the first elements in the pairs is minimal. That can be solved using standard two pointers method in O(n−−√).

The total complexity of the solution is O(nn−−√).

In fact, we can use Mo's algorithm even with updates. But its complexity is O(n53+mn−−√). You can read the editorial of the problem 940F on Codeforces or the following blog to learn about processing updates in Mo: https://codeforces.com/blog/entry/72690

Educational Codeforces Round 102 Editorial

By awoo, history, 2 years ago, translation, In English1473A - Replacing Elements

Idea: BledDest, adedalic

Tutorial1473A - Replacing ElementsLet's note that since all ai are positive, any ai+aj>max(ai,aj). It means that we can't make the first and second minimums lower than they already are: suppose the first and second minimums are mn1 and mn2, if we choose any other element to replace, we can't make it less than mn1+mn2 and if we choose to replace mn1 or mn2 we will only make them bigger.

As a result, it means that we can choose for each element either not to change it or make it equal to mn1+mn2. So, to be able to make all elements ≤d we need just check that either mn1+mn2≤d or maximum ai≤d.

We can do it, for example, by sorting our array a in increasing order and checking that either a1+a2≤d or an≤d.

Solution (Neon)1473B - String LCM

Idea: BledDest

Tutorial1473B - String LCMWe should notice that if some string x is a multiple of string y, then |x| is a multiple of |y|. This fact leads us to the conclusion that |LCM(s,t)| should be a common multiple of |s| and |t|. Since we want to minimize the length of the string LCM(s,t), then its length is LCM(|s|,|t|).

So we have to check that LCM(|s|,|t|)|s| copies of the string s equal to LCM(|s|,|t|)|t| copies of the string t. If such strings are equal, print them, otherwise, there is no solution.

Solution (Neon)1473C - No More Inversions

Idea: adedalic

Tutorial1473C - No More InversionsAt first, let's look at sequence s: s1,s2,…,sp−1,sp,sp−1,…s2,s1. Let's prove that the number of inversions in s is the same regardless of what si are (the only condition is that si should be distinct).

Let's group all elements si by their value — there will be 1 or 2 elements in each group. Then we can take any two groups with values x and y and calculate the number of inversions between elements in these groups. It's easy to note that construction will always be like …x,…,y,…,y,…,x… (or …x,…,y,…,x…) and regardless of x>y or x<y in both cases there will be exactly two inversions between groups equal to x and to y (or one inversion in the second case). So the total number of inversion will be equal to 2(p−1)(p−2)2+(p−1)=(p−1)2.

Now we can split sequences a and b into two parts. Let m=n−k, then the first part is elements from segment [1,k−m−1] and the second is from [k−m,k+m]. Note that the second parts both in a and b are exactly the sequence s described above.

The total number of inversions is equal to the sum of inversions in the first part, in the second part, and the inversions with elements from both parts. Note that in a the first and the third components are equal to 0 and the second component is constant, so in b we must also have 0 inversions in the first part and 0 inversion between parts.

It means that b must start from 1,2,…,(k−m−1). But since the number of inversions in the second part is constant we can set the remaining elements the way we want. And since we want to build lexicographically maximum b, we should make the second part as k,k−1,…,(k−m+1),(k−m),(k−m+1),…,k.

In the end, optimal b is 1,2,…,(k−m−1),k,(k−1),…,(k−m+1),(k−m),(k−m+1),…,k. The permutation p to make such b is equal to 1,2,…,(k−m−1),k,(k−1),…,(k−m).

Solution (adedalic)1473D - Program

Idea: BledDest

Tutorial1473D - ProgramThe value of x always changes by 1, thus, the set of values of x is always some contiguous segment. The length of such segment can be determined by just its minimum and maximum values.

So we have to solve two separate tasks for each query: find the minimum and the maximum value x gets assigned to. I'll describe only the minimum one.

This task, however, can as well be split into two parts: minimum value on a prefix before l and on a suffix after r. The prefix is easy — it doesn't get changed by a query, so it can be precalculated beforehand. Minimum value on a prefix of length i is minimum of a minimum value on a prefix of length i−1 and the current value.

The suffix minimum is not that trivial. First, in order to precalculate the minimum value on a suffix of length i, we have to learn to prepend an instruction to the suffix of length i+1. Consider the graph of values of x over time. What happens to it if the initial value of x is not 0 but 1, for example? It just gets shifted by 1 upwards. That move is actually the same as prepending a '+' instruction. So the minimum value for a suffix of length i is a minimum of a minimum value for a suffix of length i−1, increased by the current instruction, and 0 (the start of the graph).

So now we have a minimum value on a suffix after r. However, it can't be taken into the answer as it is, because it considers the graph for the suffix to be starting from 0. And that's not the case. The graph for the suffix starts from the value the prefix ends on. So we can shift the answer for the suffix by the value of x after the prefix. The overall minimum value is just the minimum on a prefix and on a suffix, then.

Overall complexity: O(n+m) per testcase.

Solution (awoo)1473E - Minimum Path

Idea: Neon

Tutorial1473E - Minimum PathLet's consider a problem where you can subtract the weight of any edge (not only the maximum one) that belong to the current path and similarly add the weight of any edge (not only the minimum one) that belong to the current path.

To solve that problem we can build a new graph where the node can be represented as the following triple (node from the initial graph, flag that some edge has been subtracted, flag that some edge has been added). Now we can run Dijkstra's algorithm to find the length of the shortest paths in such a graph.

We can notice that on the shortest path, the maximum weight edge was subtracted and the minimum weight edge was added. Let's assume that this is not the case, and an edge of non-maximum weight was subtracted from the path, then we can reduce the length of the path by choosing an edge of maximum weight. But this is not possible, because we considered the shortest path. Similarly, it is proved that the added edge was of minimal weight.

Using this fact, it is not difficult to notice that by solving the modified problem, we have solved the original one.

Solution (Neon)1473F - Strange Set

Idea: BledDest

Tutorial1473F - Strange SetWe will model the problem as the minimum cut in a flow network.

Build a network as follows: create a source node s, a sink node t, and a vertex for every number from 1 to n. Let's say that we are going to find the minimum s-t cut in this network, and the vertices belonging to the same cut part with s represent the numbers that are taken into the answer.

Using the edges of the network, we should model these constraints:

taking an element i that depends on another element j should force us to take j as well;taking an element i with bi>0 should add bi to our score;taking an element i with bi<0 should subtract |bi| from our score.Constraint 1 can be modeled in the following way: for every pair (i,j) such that element i depends on element j (i>j and aimodaj=0), add a directed edge with infinite capacity from i to j. That way, if i is taken and j is not, the value of the cut will be infinite because of this edge, and this cut cannot be minimum.

Constraint 2 is modeled in the following way: for every i such that bi>0, add a directed edge with capacity bi from s to i. That way, if we don't take some element i with bi>0 into the answer, bi is added to the value of the cut.

And for constraint 3, for every i such that bi<0, add a directed edge with capacity |bi| from i to t. That way, if we take some element i with bi<0, |bi| is added to the value of the cut.

It's now easy to see that the answer is ∑i=1nmax(bi,0)−mincut, since it is exactly the sum of elements that were taken (for positive elements, we add them all up and then subtract the ones that don't belong to the answer; for negative ones, we just subtract those which belong to the answer). To find a minimum cut, just run maximum flow in this network.

There's one caveat though. If, for example, all ai are equal (or many ai are divisible by many other values in a), this network can contain O(n2) edges. To reduce the number of edges, let's see that if for some index i there exist two equal divisors of ai to the left of it (let's say that these divisors are j and k: j<k<i; aimodaj=0; aj=ak), then we only need to add an edge from i to k, because taking k also should force taking j into the answer. So, for every divisor of ai, we are interested in only one closest occurrence of this divisor to the left, and we need to add a directed edge only to this occurrence, and ignore all other occurrences. That way, for every vertex i, we add at most D(ai) edges to other vertices (where D(ai) is the number of divisors of ai).

It can be proven that any maximum flow algorithm that relies on augmenting paths will finish after O(V) iterations in this network, so it won't work longer than O(VE), and both V and E are proportional to n, so any maximum flow solution will run in O(n2).

Solution (BledDest)1473G - Tiles

Idea: Neon, adedalic

Tutorial1473G - TilesThe group of the rows where the number of rectangular tiles increases a times and then decreases b times can be represented as a rectangular table, with a+b+1 diagonals, where the size of the first diagonal is equal to the number of rectangular tiles before the operations are applied (let their number be m), and the size of the last diagonal is m+a−b.

In such a rectangular table, one can move from the cell (i,j) to the cells (i+1,j) and (i,j+1) (if they exist), which lie on the next diagonal (next row in terms of the original problem). It's a well-known fact that the number of different paths from one cell to another is some binomial coefficient.

Let's define ansi,j as the number of paths from the 1-st row to the j-th tile in the (∑k=1i(ai+bi))-th row (i.e. row after the i-th group of operations).

Now we want to find the values of ansi using the values of ansi−1 (let its size be m). Using the fact described in the first paragraphs we know that ansi,j depends on ansi−1,k with some binomial coefficient. In fact ansi,j=∑k=1m(ai+bibi−k+j)ansi−1,k for 1≤j≤m+ai−bi. But this solution is too slow. To speed up this solution we have to notice that the given formula is a convolution of ansi−1 and some binomial coefficients. So we can use NTT to multiply them in O(nlogn) instead of O(n2) time.

Educational Codeforces Round 101 Editorial

By awoo, history, 2 years ago, translation, In English1469A - Regular Bracket Sequence

Idea: BledDest

Tutorial1469A - Regular Bracket SequenceThere are two solutions to this problem: casework and greedy.

The greedy solution goes as follows: the number of opening brackets in an RBS should be exactly |s|2, and if there is a closing bracket before an opening bracket, it's optimal to swap them, if possible. So, we should replace the first |s|2−1 question marks with opening brackets, other question marks with closing brackets, and if the answer exists, this method will find it. All that's left is to check that the resulting sequence is an RBS.

The casework solution goes as follows: first of all, each RBS should have an even length, so if |s| is odd, there is no answer. Furthermore, an RBS always begins with an opening bracket and always ends with a closing bracket, so if the first character is a closing bracket or the last character is an opening bracket, there is no answer. Since there is at most one opening bracket and at most one closing bracket in the original sequence, these three constraints are enough: if the opening bracket is before the closing bracket, then they balance out, and all other characters can be replaced in such a way that they form an RBS of length |s|−2. If the opening bracket is after the closing bracket, then the first and the last characters are question marks (since the first character is not a closing bracket, and the last one is not an opening bracket). We should replace the first character with an opening bracket, the last character with a closing bracket, so we get four characters (two opening and two closing brackets) which balance themselves out. All other question marks can be replaced in such a way that they form an RBS of length |s|−4. So, all we have to check is that |s| is even, the first character is not a closing bracket, and the last character is not an opening bracket.

Solution 1 (BledDest)Solution 2 (BledDest)1469B - Red and Blue

Idea: BledDest and adedalic

Tutorial1469B - Red and BlueDenote pi as the sum of first i elements of r, and qj as the sum of first j elements of b. These values can be calculated in O(n+m) with prefix sums.

The first solution is to use dynamic programming. Let dpi,j be the maximum value of f(a) if we placed the first i elements of r and the first j elements of b. Transitions can be performed in O(1): we either place an element from r (then we go to dpi+1,j and update it with max(dpi,j,pi+1+qj)), or place an element from b (then we go to dpi,j+1 and update it with max(dpi,j,pi+qj+1)). The answer is stored in dpn,m, and this solution works in O(nm).

The second solution: observe that the sum of several first elements of a is the sum of several first elements of r and several first elements of b. So each prefix sum of a (and the answer itself) is not greater than maxni=0pi+maxmj=0pj. It's easy to show how to obtain exactly this answer: let k be the value of i such that pi is maximized, and l be the value of j such that qj is maximized. Let's place the first k elements of r, then the first l elements of b (so the current sum is exactly maxni=0pi+maxmj=0pj), and place all of the remaining elements in any possible order. So, the answer is maxni=0pi+maxmj=0pj. This solution works in O(n+m).

Solution 1 (Ne0n25)Solution 2 (pikmike)1469C - Building a Fence

Idea: adedalic

Tutorial1469C - Building a FenceLet's set sections from left to right. Note that for the i-th section all valid heights x (heights for which it's possible to choose heights for all sections 1…i meeting all rules and finishing with the height of i equal to x) form a segment.

It's not hard to prove by induction. For the first section, the valid segment is [h1,h1]. The step of induction: if the valid segment for i−1 is [li−1,ri−1] then valid xi-s for i is the segment [max(li−1−(k−1),hi),min(ri−1+(k−1),hi+(k−1))], since for each xi you can find at least one xi−1 in [li−1,ri−1] which don't break the first rule.

If for any i the correct segment is empty or if we can't fulfill the third rule (hn∉[ln−1−(k−1),rn−1+(k−1)]) then there is no answer, otherwise at least one answer is always exist.

As a result, to solve the problem, you should just maintain the segment of valid xi (using the formula above) while iterating i. Complexity is O(n).

Solution (adedalic)1469D - Ceil Divisions

Idea: adedalic

Tutorial1469D - Ceil DivisionsThere are many different approaches. We will describe a pretty optimal one.

Let's solve the problem recursively. Let's say we need to process segment [1,x]. If x=2, we don't need to do anything. Otherwise, x>2. Let's find the minimum y such that y≥⌈xy⌉.

The chosen y is convenient because it allows making x equal to 1 in two divisions (and it's the minimum number of divisions to get rid of x). Now we can, firstly, make all z∈[y+1,x) equal to 1 in one step (by division on x) and then make x equal to 1 with two divisions on y.

As a result, we've spent x−y+1 operations and can solve our task recursively for [1,y]. In total, we will spend n−2+number_of_segments and since the segments are like (x−−√,x], (x−−√−−−√,x−−√],... There will be at most log(logn) segments and n+3 operations are enough for n≤232.

Solution (adedalic)1469E - A Bit Similar

Idea: BledDest

Tutorial1469E - A Bit SimilarLet's denote z as the number of substrings of s having length exactly k (so, z=n−k+1).

The first and crucial observation is that if 2k>z, then the answer always exists. Each of z substrings forbids one of the strings from being the answer (a string is forbidden if every each character differs from the corresponding character in one of the substrings); we can forbid at most z strings from being the answer, and the number of possible candidates for the answer is 2k.

This observation leads us to a more strong fact that actually allows us to find a solution: we can set the first k−⌈log2(z+1)⌉ characters in the answer to 0; all the remaining characters are enough to find the answer.

There are at most 2⌈log2(z+1)⌉ possible combinations of the last characters, and this number is not greater than 2n. Let's iterate on each substring of s of length k and check which combination it forbids by inverting the last ⌈log2(z+1)⌉ characters of the substring. After that, find the minimum unforbidden combination. Note that there may be a case when a substring doesn't actually forbid any combination — if there are zeroes in the first k−⌈log2(z+1)⌉ characters of the substring, it is a bit similar to the answer no matter which combination we choose. This can be checked by precalculating the closest position of zero to the left/right of each index.

The whole solution works in O(nlogn) per test case — the hardest part is inverting the suffix of each substring we are interested in.

Solution (BledDest)1469F - Power Sockets

Idea: adedalic

Tutorial1469F - Power SocketsAt first, let's realize that the tree structure doesn't matter that much. What we actually need is the array cnti such that it stores the number of white vertices on depth i.

Initially, cnt0=1 and all other are zero. If you take a chain and attach it to some vertex on depth d, then the number of vertices on depth d decreases by 1. Also, the added vertices update some other counts.

So far, it's extremely unclear what to begin with. Let's start by introducing some greedy ideas.

For each t let's find the most optimal tree using exactly t chains and update the answer with each of them.

First, it's always optimal to attach a chain with its middle vertex. Just consider the changes in the white vertices counts.

Second, for each t it's always optimal to take the longest t chains to use. If not the longest t are used, then you can replace any of them and there will be more white vertices.

It would be nice if we were able to just add another chain to the tree for t−1 to get the tree for t. However, that's not always the case. But we can still attempt it and show that the optimal answer was achieved somewhere in the process.

Let's show that it's always optimal to attach a new chain to the closest white vertex.

So there are basically two cases: there is not enough white vertices yet and there is enough. What happens if there is not enough vertices, and we pick the closest one to attach a chain to? If there are still not enough vertices, then we'll just continue. Otherwise, we'll have to show that the answer can't get any smaller by rearranging something.

Consider what the answer actually is. Build a prefix sum array over cnt, then the answer is the shortest prefix such that its prefix sum is greater or equal to k.

So we put the t-th chain to the closest white vertex at depth d. It decreases cntd by 1 and increases cntd+2 and further by 1 or 2. Every chain we have put to this point was attached to a vertex at depth less or equal to the answer (otherwise, we could've rearranged it and obtain the answer before). The optimal answer can be neither d, nor d+1 (also because we could've rearranged). Thus, the answer is at least d+2 and every single chain we have put was packed as tightly as possible below that depth.

The second case works similarly. We could've obtained the optimal answer before t. So the answer is below d+2 and we can do nothing about that. Or the optimal answer is ahead of us, so putting the chain at d can decrease it as much or stronger as any other choice.

Thus, updating the answer on every iteration will give us the optimal result.

Now we are done with the greedy, time to implement it. I chose the most straightforward way. We basically have to maintain a data structure that can add on range, get the value of a cell and find the shortest prefix with sum at least k. That can be easily done with segtree.

Overall complexity: O(nlogn).