[Ctrl f] for tasks. Remember to remove Submissions and reformat when updating, clearing out all triple line breaks to double line breaks.

----------

Qualification Round 2008 - Code Jam 2008

Fly Swatter (5pts, 20pts)

This was the hardest problem of the set, as we can see from the submission statistics, but 652 people succeeded in solving it correctly.

The first step in solving this problem involves a standard trick. Instead of looking for the region that will hit the fly's circular body, we look for the region where the center of the fly must avoid. The problem is thus transformed into an equivalent one where we thicken the size of the racquet to t + f and the radius of the strings from r to r + f. And we consider the fly to be simply a single point, only focusing on its center.

Now the main problem is to find the area of the holes (the areas between the strings, or between the strings and the racquet), and divide it by the area of the disc corresponding to the original racquet. It is easy to see this gives the probability that the racquet does not hit the fly.

A simplifying observation is that we can just look at the first quadrant of the circle, because the racquet is symmetrical.

Now we can go through each square gap in the racquet and check a few cases. Each case consists in computing the area of intersection between the square and the circle representing the inside of the racquet. The case where the square is totally inside or totally outside the circle is easy. The cases left when one corner, two or three corners of the square are inside the circle and the other corners are outside. There are no other cases because we just look at the intersection of a quarter of the circle with small squares.

Each of these cases can be solved by splitting the intersection area into triangles and a circular segment. Here's some code that does this:

double circle_segment(double rad, double th) {
  return rad*rad*(th - sin(th))/2;
}

double rad = R-t-f;
double ar = 0.0;
for (double x1 = r+f; x1 < R-t-f; x1 += g+2*r)
for (double y1 = r+f; y1 < R-t-f; y1 += g+2*r) {
  double x2 = x1 + g - 2*f;
  double y2 = y1 + g - 2*f;
  if (x2 <= x1 || y2 <= y1) continue;
  if (x1*x1 + y1*y1 >= rad*rad) continue;
  if (x2*x2 + y2*y2 <= rad*rad) {
    // All points are inside circle.
    ar += (x2-x1)*(y2-y1);
  } else if (x1*x1 + y2*y2 >= rad*rad &&
             x2*x2 + y1*y1 >= rad*rad) {
    // Only (x1,y1) inside circle.
    ar += circle_segment(rad, acos(x1/rad) - asin(y1/rad)) +
          (sqrt(rad*rad - x1*x1)-y1) *
          (sqrt(rad*rad - y1*y1)-x1) / 2;
  } else if (x1*x1 + y2*y2 >= rad*rad) {
    // (x1,y1) and (x2,y1) inside circle.
    ar += circle_segment(rad, acos(x1/rad) - acos(x2/rad)) +
          (x2-x1) * (sqrt(rad*rad - x1*x1)-y1 +
                     sqrt(rad*rad - x2*x2)-y1) / 2;
  } else if (x2*x2 + y1*y1 >= rad*rad) {
    // (x1,y1) and (x1,y2) inside circle.
    ar += circle_segment(rad, asin(y2/rad) - asin(y1/rad)) +
          (y2-y1) * (sqrt(rad*rad - y1*y1)-x1 +
                     sqrt(rad*rad - y2*y2)-x1) / 2;
  } else {
    // All except (x2,y2) inside circle.
    ar += circle_segment(rad, asin(y2/rad) - acos(x2/rad)) +
          (x2-x1)*(y2-y1) -
          (y2-sqrt(rad*rad - x2*x2)) *
          (x2-sqrt(rad*rad - y2*y2)) / 2;
  }
}
printf("Case #%d: %.6lf\n", prob++, 1.0 - ar / (PI*R*R/4));

This solution takes O(S2) time, where S is the number of vertical strings of the racquet. It's not hard to come up with an O(S) solution because there are at most 4S border squares which can be found efficiently, but the previous solution was fast enough.

Instead of solving the problem exactly, an iterative solution which approximates the area to the needed precision would have also worked. One such solution uses divide and conquer by splitting the square into four smaller squares and then checking the simple cases where the squares are totally inside or totally outside the square. In the cases where the circle and square intersect just recurse if the current square is larger than some chosen precision. An observation is that we can divide every length by the radius of the racquet because it gets canceled in the division between the area of the gaps in the racquet and the disc area. This observation helps the iterative solution since we can make the number of iterations smaller. Here's some sample code:

double intersection(double x1, double y1,
                    double x2, double y2) {
  // the normalized radius is 1
  if (x1*x1 + y1*y1 > 1) {
    return 0;
  }
  if (x2*x2 + y2*y2 < 1) {
    return (x2-x1) * (y2-y1);
  }
  // EPS2 = 1e-6 * 1e-6
  if ((x2-x1)*(y2-y1) < EPS2) {
    // this trick helps in doing 10 or 100 times less
    // iterations than we would need to get the same
    // precision if we just return 0;
    return (x2-x1) * (y2-y1) / 2;
  }
 
  double mx = (x1 + x2) / 2;
  double my = (y1 + y2) / 2;
 
  return intersection(x1, y1, mx, my) +
    intersection(mx, y1, x2, my) +
    intersection(x1, my, mx, y2) +
    intersection(mx, my, x2, y2);
}

Qualification Round 2008 - Code Jam 2008

Saving the Universe (5pts, 20pts)

The first task in the new Google Code Jam is about, no surprise, search engines, and also, the grandiose feat of saving the universe in a parsimonious way. However, putting all the fancies aside, the problem itself is easy.

List all queries one by one, and break them up into segments. Each segment will be an interval where we use one search engine, and when we cross from one segment to another we will switch the search engine we use. What can you say about each segment? Well, one thing for sure is:

Never ever ever have all S different search engines appear as queries in one segment. (*)
Why is this? Because if all S names appeared in one segment, then any search engine used for that segment will encounter at least one query that is the same as its name, thus exploding the universe!
Working in the opposite direction, (*) is all we need to achieve; as long as you can partition the list of queries into such segments, it corresponds to a plan of saving the universe. You don't even care about which engine is used for one segment; any engine not appearing as a query on that segment will do. However, you might sometimes pick the same engine for two consecutive segments, laughing at yourself when you realize it; why don't I just join the two segments into one? Because your task is to use as few segments as possible, it is obvious that you want to make each segment as long as possible.

This leads to the greedy solution: Starting from the first query, add one query at a time to the current segment until the names of all S search engines have been encountered. Then we continue this process in a new segment until all queries are processed.

Sample code in C++, where st is the set of queries in the current segment, q is the next query, and count is the number of switches.

st.clear();
count = 0;
for (int i=0; i<Q; i++) {
  getline(cin, q);
  if (st.find(q) == st.end()) {
    if (st.size() == S-1) {
      st.clear();
      count++;
    }
    st.insert(q);
  }
}
If st is a hashset, you expect the solution run in O(n) time. Note that this solution uses the fact that each query will be a search engine name and so we can ignore the list of names provided in the input.

Let us justify that the greedy approach always gives the optimal answer. Think of the procedure as Q steps, and we want to show that, for each i, there is (at least) one optimal choice which agrees with us on the first i steps. We do this inductively for i = 0, then i = 1, and so on. The proposition for i = Q when proven true will imply that our algorithm is correct.

So, the key points in the inductive step i:
If adding the next query will explode the universe, we must start a new segment. Any optimal choice agrees with us for the first (i-1) steps must also do that.
If adding the next query will not explode the universe, we do not start a new segment. We know there is an optimal solution R agreed with us for (i-1) steps. Even if in R a new segment is started at step i, we can modify it a little bit. Let R' be the plan that agrees with R, but instead of starting a new segment on the i-th step, we delay this to the (i+1)st. It is clear that R' will also make the universe safe, and has no more switches than R does. So, R' is also an optimal solution, and agrees with our choice for the first i steps.
The similar lines of justifications work for many greedy algorithms, including the well beloved minimum spanning trees.

Qualification Round 2008 - Code Jam 2008

Train Timetable (5pts, 20pts)

This problem can be solved with a greedy strategy. The simplest way to do this is by scanning through a list of all the trips, sorted by departure time, and keeping track of the set of trains that will be available at each station, and when they will be ready to take a trip.

When we examine a trip, we see if there will be a train ready at the departure station by the departure time. If there is, then we remove that train from the list of available trains. If there is not, then our solution will need one new train added for the departure station. Then we compute when the train taking this trip will be ready at the other station for another trip, and add this train to the set of available trains at the other station. If a train leaves station A at 12:00 and arrives at station B at 13:00, with a 5-minute turnaround time, it will be available for a return journey from B to A at 13:05.

We need to be able to efficiently identify the earliest a train can leave from a station; and update this set of available trains by adding new trains or removing the earliest train. This can be done using a heap data structure for each station.

Sample Python code provided below that solves a test case for this problem:

def SolveCase(case_index, case):
  T, (tripsa, tripsb) = case
  trips = []
  for trip in tripsa:
    trips.append([trip[0], trip[1], 0])
  for trip in tripsb:
    trips.append([trip[0], trip[1], 1])

  trips.sort()

  start = [0, 0]
  trains = [[], []]

  for trip in trips:
    d = trip[2]
    if trains[d] and trains[d][0] <= trip[0]:
      # We're using the earliest train available, and
      # we have to delete it from this station's trains.
      heappop(trains[d])
    else:
      # No train was available for the current trip,
      # so we're adding one.
      start[d] += 1
    # We add an available train in the arriving station at the
    # time of arrival plus the turnaround time.
    heappush(trains[1 - d], trip[1] + T)

  print "Case #%d: %d %d" % (case_index, start[0], start[1])
Luckily Python has methods that implement the heap data structure operations. This solution takes O(n log n) time, where n is the total number of trips, because at each trip we do at most one insert or one delete operation from the heaps, and heap operations take O(log n) time.

Round 1A 2008 - Code Jam 2008

Minimum Scalar Product (5pts, 10pts)

In spite of the geometric flavor in the name of this problem, it is truly a story of two arrays.

There are two permutations involved. However, after you fix the permutation for V1, you have complete freedom in choosing the permutation for V2. It is clear that the first permutation really does not matter. The important thing is which xi gets to be matched to which yj.

To make thinking easier, we may assume that the first permutation has

x1 ≤ x2 ≤ ... ≤ xn. (1)
And our task is to match the y's to the x's so that the scalar product is as small as possible.

At this point, if you need a little exercise in the middle: Think about the case n = 2, and just use a concrete example. What you will surely discover is that, in order to achieve the minimum scalar product, you always want to match the smaller xi with the bigger yj.

What we would like to prove is: under condition (1), one of the optimal solutions is of the form

  y1 ≥ y2 ≥ ... ≥ yn. (2)
The rigorous proof of (2) is given at the end of this analysis. For an easy reading, we point out that the key step is really the n = 2 case. If x < x' and y < y', then

(xy + x'y') - (xy' + x'y) = (x - x')(y - y') > 0. (*)
So we prefer to match bigger y's with smaller x's.

Therefore, this problem is solved by the following simple algorithm.

sort(v1.begin(), v1.end());
sort(v2.begin(), v2.end(), greater<int>());
long long ret = 0;
for (int i = 0; i < n; i++)
  ret += (long long)(v1[i]) * v2[i];
Proof of (2). We prove that any permutation that does not satisfy (2) can be transformed into one satisfying (2), and in each step we do not increase the scalar product.
Indeed, any unsorted array can be transformed to a sorted one by only interchanging adjacent elements that are out of order. (For more rigorous readers: prove this, maybe by counting the number of flipped pairs in the array.)
At each step, some x = xi is matched to some y, and x' = xi+1 is matched to some y' so that y ≤ y'. We interchange y and y' in this step. The inequality similar to (*), with > replaced by ≥, tells us that the scalar product is not increased in this step. ◊

More information:
The Scalar product

Round 1A 2008 - Code Jam 2008

Milkshakes (10pts, 25pts)

On the surface, this problem appears to require solving the classic problem "Satisfiability," the canonical example of an NP-complete problem. The customers represent clauses, the milkshake flavors represent variables, and malted and unmalted flavors represent whether the variable is negated.

We are not evil enough to have chosen a problem that hard! The restriction that makes this problem easier is that the customers can only like at most one malted flavor (or equivalently, the clauses can only have at most one negated variable.)

Using the following steps, we can quickly find whether a solution exists, and if so, what the solution is.

Start with every flavor unmalted and consider the customers one by one.
If there is an unsatisfied customer who only likes unmalted flavors, and all those flavors have been made malted, then no solution is possible.
If there is an unsatisfied customer who has one favorite malted flavor, then we must make that flavor malted. We do this, then go back to step 2.
If there are no unsatisfied customers, then we already have a valid solution and can leave the remaining flavors unmalted.
Notice that whenever we made a flavor malted, we were forced to do so. Therefore, the solution we got must have the minimum possible number of malted flavors.

With clever data structures, the above algorithm can be implemented to run in linear time.

More information:
The Satisfiability problem - Horn clauses

Round 1A 2008 - Code Jam 2008

Numbers (15pts, 35pts)

This problem with a simple statement was in fact one of the hardest in Round 1. The input restrictions for the small input were chosen so that straightforward solutions in Java or Python, which have arbitrary-precision numbers, would fail. The trick was calculating √5 to a large enough precision and not using the default double value. It turned out that using the Windows calculator or the UNIX bc tool was enough to solve the small tests as well.

Solving the large tests was a very different problem. The difficulty comes from the fact that √5 is irrational and for n close to 2000000000 you would need a lot of precision and a lot of time if you wanted to use the naive solution.

The key in solving the problem is a mathematical concept called conjugation. In our problem, we simply note that (3 - √5) is a nice conjugate for (3 + √5). Let us define

(1)     α := 3 + √5,   β := 3 - √5,   and Xn := αn + βn.
We first note that Xn is an integer. This can be proved by using the binomial expansion. If you write everything down you'll notice that the irrational terms of the sums cancel each other out.

(2)     
Another observation is that βn < 1, so Xn is actually the first integer greater than αn. Thus we may just focus on computing the last three digits of X.

A side note. In fact, βn tends to 0 so quickly that that our problem would be trivial if we asked for the three digits after the decimal point. For all large values of n they are always 999.

Based on (1) and (2), there are many different solutions for finding the last three digits of Xn.

Solution A. [the interleave of rational and irrational]
One solution goes like this: αn can be written as (an + bn√5), where an and bn are integers. At the same time, βn is exactly (an - bn√5) and Xn = 2an. Observe that

(3)     α(n + 1) = (3 + √5)(an + bn√5) = (3an + 5bn) + (3bn + an)√5.
So an + 1 = 3an + 5bn and bn + 1 = 3bn + an. This can be written in matrix form as

(4)     
Since α0 = 1, we have (a0, b0) = (1, 0).

Now we use the standard fast exponentiation to get An in O(log n) time. Note that we do all operations modulo 1000 because we just need to return the last three digits of an.

Here's some Python code that implements this solution:

def matrix_mult(A, B):
  C = [[0, 0], [0, 0]]
  for i in range(2):
    for j in range(2):
      for k in range(2):
        C[i][k] = (C[i][k] + A[i][j] * B[j][k]) % 1000
  return C

def fast_exponentiation(A, n):
  if n == 1:
    return A
  else:
    if n % 2 == 0:
      A1 = fast_exponentiation(A, n/2)
      return matrix_mult(A1, A1)
    else:
      return matrix_mult(A, fast_exponentiation(A, n - 1))

def solve(n):
  A = [[3, 5], [1, 3]]
  A_n = fast_exponentiation(A, n)
  return (2 * M_n[0][0] + 999) % 1000
Solution B. [the quadratic equation and linear recurrence]
Experienced contestants may notice there is a linear recurrence on the Xi's. Indeed, this is not hard to find -- the conjugation enters the picture again.

Notice that

(5)     α + β = 6, and α β = 4.
So α and β are the two roots of the quadratic equation x2 - 6x + 4 = 0. i.e.,

(6)     α2 = 6α - 4, and β2 = 6β - 4.
Looking at (1) and (6) together, we happily get

(7)     Xn+2 = 6Xn+1 - 4Xn.
Such recurrence can always be written in matrix form. It is somewhat redundant, but it is useful:

From here it is another fast matrix exponentiation. Let us see radeye's perl code that implements this approach here:

sub mul {
    my $a = shift ;
    my $b = shift ;
    my @a = @{$a} ;
    my @b = @{$b} ;
    my @c = ($a[0]*$b[0] + $a[1]*$b[2],
             $a[0]*$b[1] + $a[1]*$b[3],
             $a[2]*$b[0] + $a[3]*$b[2],
             $a[2]*$b[1] + $a[3]*$b[3]) ;
    @c = map { $_ % 1000 } @c ;
    return @c ;
}
sub f {
    my $n = shift ;
    return 2 if $n == 0 ;
    return 6 if $n == 1 ;
    return 28 if $n == 2 ;
    $n -= 2 ;
    my @mat = (0, 1, 996, 6) ;
    my @smat = @mat ;
    while ($n > 0) {
        if ($n & 1) {
            @mat = mul([@mat], [@smat]) ;
        }
        @smat = mul([@smat], [@smat]) ;
        $n >>= 1 ;
    }
    return ($mat[0] * 6 + $mat[1] * 28) % 1000 ;
}
sub ff {
   my $r = shift ;
   $r = ($r + 999) % 1000 ;
   $r = "0" . $r while length($r) < 3 ;
   return $r ;
}
for $c (1..<>) {
    $n = <> ;
    print "Case #$c: ", ff(f($n)), "\n" ;
}
Solution C. [the periodicity of 3 digits]
For this problem, we have another approach based on the recurrence (7). Notice that we only need to focus on the last 3 digits of Xn, which only depends on the last 3 digits of the previous two terms. The numbers eventually become periodic as soon as we have (Xi, Xi+1) and (Xj, Xj+1) with the same last 3 digits, where i < j. It is clear that we will enter a cycle no later than 106 steps. In fact, for this problem, you can write some code and find out that the cycle has the size 100 and starts at the 3rd element in the sequence. So to solve the problem we can just brute force the results for the first 103 numbers and if n is bigger than 103 return the result computed for the number (n - 3) % 100 + 3.

Solution D. [the pure quest of numbers and combinatorics]
Let us see one more solution of different flavor. Here is a solution not as general as the others, but tailored to this problem, and makes one feel we are almost solving this problem by hand.

Let us look again at (2). We want to know Xn mod 1000. We know from the chinese remander theorem that if we can find Xn mod 8 and Xn mod 125, then Xn mod 1000 is uniquely determined.
(a) For n > 2, Xn mod 8 is always 0. Since 5i ≡ 1 (mod 4), 3n-2i ≡ 1 or -1 (mod 4) depending on n, so, for n > 2,

(b) To compute Xn mod 125, we only need to worry about i=0,1,2. All the rest are 0 mod 125. In other words, all we need to compute is

There are various ways to compute the elements in the above quantity. The exponents can be computed by fast exponentiation, or using the fact that 3n mod 125 is periodic with cycle length at most 124. The binomial numbers can be computed using arbitrary precision integers in languages like Java and Python, or with a bit careful programming in languages like C++.

More information:
Binomial numbers - Linear recurrence - Exponentiation - Chinese remainder theorem

Round 1B 2008 - Code Jam 2008

Crop Triangles (5pts, 10pts)

The small case was easy to solve using brute force. For each combination of three points you needed to test if the center had integer coordinates. One tricky part was generating the sequence of points, since using 32-bit integers would have resulted in overflow when doing multiplications. One way to get around that problem was to use 64-bit integers.

The observation that helps in solving the large case is that we don't care about the range of the coordinates for the points in the input. We only care about the coordinates modulo 3. We have 9 different classes of points. In bucket[i] we will count the number of points for which x % 3 = i / 3 and y % 3 = i % 3. There are three different ways of choosing 3 points out of the 9 classes of points: three points from the same class, two points from the same class and one from a different class and three points from three different classes.

If the three points are in the same class then it's obvious that the center will have integer coordinates. It is easy to see that there is no triangle with two points in the same class and one point in a different class.

Here is some code that implements this idea:

    for (int i = 0; i < n; i++) {
      bucket[((int)X0 % 3) * 3 + (int)Y0 % 3]++;
      X0 = (A * X0 + B) % M;
      Y0 = (C * Y0 + D) % M;
    }

    // The first case.
    for (int i = 0; i < 9; i++)
      // We use the formula for n choose 3 so that,
      // we don't use the same point twice or count
      // the same triangle more than once.
      ret += bucket[i] * (bucket[i]-1) * (bucket[i]-2) / 6;

    // The third case.
    for (int i = 0; i < 9; i++)
      for (int j = i + 1; j < 9; j++)
        for (int k = j + 1; k < 9; k++)
          if (((i / 3) + (j / 3) + (k / 3)) % 3 == 0) &&
              ((i % 3) + (j % 3) + (k % 3)) % 3 == 0)
            ret += bucket[i] * bucket[j] * bucket[k];
    cout << "Case #" << prob++ << ": " << ret << endl;

Round 1B 2008 - Code Jam 2008

Number Sets (10pts, 25pts)

This problem describes a process where integers are grouped into sets, and then we are asked how many sets there are in the final result.

The process described is fairly slow:

Create singleton sets of integers.
Take each pair of integers;
factor the two integers and see if they have a prime factor in common that is greater than or equal to P;
if so, merge the sets that have these two integers.
We need to find the result of this process using a faster method. A few observations help here.

Firstly, we only need to find prime factors less than the size of the interval. If a prime is larger than or equal to the size of the interval, then at most one integer in the interval can have that prime as a factor, so it will never be used to merge sets.

Secondly, we can take a faster approach than finding the prime factors of each integer separately. Instead we consider each prime in turn and find all the integers in the interval which have this prime as a factor, a technique generally called a sieve.

Thirdly, joining sets can be implemented efficiently with a data structure called union-find (or the disjoint sets data structure). As we consider each prime and find all of the integers with that prime as a factor, we join all of the sets containing those integers. We could also build an undirected graph with nodes for the integers in the interval and the primes, and edges representing which integers are divisible by which primes; then the final sets are the connected components of this graph.

More information:
Prime sieving - Union-Find - Connected components

Round 1B 2008 - Code Jam 2008

Mousetrap (15pts, 35pts)

In this nice problem, there are two viewpoints one can start with.

The current position is fixed and the array of cards is rotating each time.
Think of the cards as a fixed ring (represented byan array) and the current position in focus as a pointer moving on the ring.
For most of us (and our programs), it is more convenient to adopt the second viewpoint.

After reading the story, it is not hard to see that the task is clear: put card 1 in the first position, then for each card i (in the order 2, 3, ..., K), we start from the current position, and find the i-th empty spot to the right, wrap around as many times as necessary, then put card i there.

The problem is to simulate the process described above. What makes it interesting is that K can be as big as 1000000, which makes the naive Θ(K2) simulation too slow for our 8-minute time limit. For each step, we need to compute the next position much faster than Θ(K) as in the naive approach. We describe three solutions below.

Solution A.
Let S = √K, we partition the K positions into S intervals of roughly equal size (also S). In addition to bookkeeping which positions are occupied (an array of size K, we call the first level counter), we also count for each interval how many positions are occupied (an array of size S that we call the second level counter). With this information, we may skip intervals of length S as many as possible, until we arrive at an interval where we know the card must belong to. Then in that interval we only need to deal with at most S first level counters.

Once we put down a card, it is a simple matter to update the counters. We only need to update one on the first level and one on the second level.

This solution runs in time O(K1.5).

Solution B.
Push the idea in the previous solution further. Why not have more levels of counters? In fact, one nice plan is to organize the levels into a binary tree. On the bottom (first) level of the tree we have each position as a separate interval. Every time we go up one level, we combine every other interval with the next one. Thus we will have logK levels; the top level being a single interval with all the positions. We omit the details, since we will see this again in the analysis of a Round 1C problem. We mention that the total number of counters is O(K), and for each card we will need O(log K) time to find the position and another O(log K) time to update the counters. The running time of this method is O(K log K).

For similar ideas in computer science, we refer to the Wiki page for interval trees.

Solution C.
Now let us do something different. At each step, after one position is occupied by card number i, we delete the position from the deck.

Notice that n, the number of queries is at most 100. We do not need to relabel all the positions, it is enough to do this for those n that we are interested in.

The solution can be implemented in two flavors, based on which viewpoint in the beginning of the analysis you pick. The short C++ program is, again, based on the second one, where the position (pos) changes as a pointer, and the deck does not move, except we delete one position in each step.

for (int j = 0; j < n; j++) answers[j] = -1;
for (int i = 1, pos = 0; i <= K; i++) {
  // Compute the next position, after wrap-around.
  pos = (pos + i - 1) % (K - i + 1);
  for (int j = 0; j < n; j++)
  if (answers[j] < 0) {
    if (queries[j] == pos+1) {
      queries[j] = -1; answers[j] = i;
    } else if (queries[j] > pos+1) {
      // The effect of deleting the next position.
      queries[j]--; 
    }
  }
}
You can use a trick to combine the two arrays queries[] and answers[] into one. The programs runs in Θ(n K) time.

More information:
Interval trees

Round 1C 2008 - Code Jam 2008

Text Messaging Outrage (5pts, 10pts)

This was the one of the easiest problems of Round 1. We simply need to fill the cell phone keyboard greedily. We put the K most frequent letters in the first positions of the K keys, the next K most frequent letters in the second positions, and so on. Any optimal solution will have this structure because if it does not, then it can be improved by swapping a more frequent character from a higher indexed position of some key with a less frequent character from a lower indexed position, thus decreasing the number of key presses.

Here is code that implements this solution:

long long A[1000];

int main() {
  int N;
  cin >> N;
  for(int t = 1; t <= N; t++) {
    long long result = 0;
    int P, K, L;
    cin >> P >> K >> L;
    for(int i = 0; i < L; i++) cin >> A[i];
    sort(A, A+L);
    reverse(A, A+L);

    for(int i = 0; i < L; i++)
      result += (1 + i / K) * A[i];

    cout << "Case #" << t << ": " << result << endl;
  }
}

Round 1C 2008 - Code Jam 2008

Ugly Numbers (10pts, 25pts)

As the problem clearly states, there are 3D-1 ways you need to consider. When D is no larger than 13, as it is in the small dataset, one may simply generate all the possible combinations and calculate each of them. However, with D as large as 40, brute force is clearly too slow.

As you might expect from a programming contest, the magic term is dynamic programming. This is the first problem in Google Code Jam 2008 that falls into the standard dynamic programming (DP) category; and one can be assured that there are more to come.

As in any DP problem, the first task is to structure the problem in a good way so there are not too many numbers to compute, and each number can be computed easily from previous ones. If you observe the solutions of the top scorers in this sub-round, you will see, although their algorithms vary to some degre, they all contain the following magic number

2 · 3 · 5 · 7 = 210.
Let us say we have two numbers, x and y, knowing the ugliness of x and y is not enough to decide whether x + y and x - y are ugly. On the other hand, we do not need the exact value of x and y. Knowing x % 210 and y % 210 is enough for us to decide if, say, x + y is ugly or not.

For those who prefer mathematical terms, we are using the Chinese remainder theorem. Our problem can be viewed as arithmetics on the cyclic group (Z210, +).

So, let us outline the most central step of our dynamic programming solution. We want to compute

dyn[i][x] := number of ways we get an expression evaluating 
          to x (mod 210) if we only consider the first i
          characters of the string. (*)
So, we have only 40·210 numbers to consider. For each dyn[i][x], we try all possible positions for inserting the last '+' or '-' sign. If the last sign was a '+' before position j (j<i), and the number formed by digits from position j to position i is d, then we want to know dyn[j-1][(x-d)%210]. On the other hand, if the sign inserted was a '-', then we want to look at dyn[j-1][(x+d)%210].

Here is a masterful implementation from our own Derek Kisman. His complete C++ solution follows the idea above, with a little twist and some nice programming tricks.

#define MOD (2*3*5*7)
string s;
long long dyn[41][MOD];

main() {
  int N, prob=1;
  for (cin >> N; N--;) {
    cin >> s;
    memset(dyn, 0, sizeof(dyn));
    dyn[0][0] = 1;
    for (int i = 0; i < s.size(); i++)
    for (int sgn = (i==0) ? 1 : -1; sgn <= 1; sgn += 2) {
      int cur = 0;
      for (int j = i; j < s.size(); j++) {
        cur = (cur*10 + s[j]-'0')%MOD;
        for (int x = 0; x < MOD; x++)
          dyn[j+1][(x+sgn*cur+MOD)%MOD] += dyn[i][x];
      }
    }
    long long ret = 0;
    for (int x = 0; x < MOD; x++)
      if (x%2 == 0 || x%3 == 0 || x%5 == 0 || x%7 == 0)
        ret += dyn[s.size()][x];
    cout << "Case #" << prob++ << ": " << ret << endl;
  }
}
More information:
Dynamic programming - Chinese remainder theorem

Round 1C 2008 - Code Jam 2008

Increasing Speed Limits (15pts, 35pts)

This was the hard problem for round 1C, with 398 people solving the small i/o set and 49 people solving the large i/o set during the contest.

The problem asks us to count the number of strictly increasing subsequences of the original sequence. This lends itself nicely to a dynamic programming solution for the small input.

Let f(x) be the number of strictly increasing subsequences that start with node x and S[x] be the value of the sequence at position x. At any node x we can either end the subsequence at x or connect it to any of the following nodes that are strictly greater than node x. Therefore, you could do something like this to find out how many strictly increasing subsequences start at each index.

f(x) = 1
for i = n to 1
  f(i) = 1
  for j = i + 1 to n
    if S[i] < S[j]
      f(i) = f(i) + f(j)
After doing that, solving the original problem is just a matter of summing the number of ways you make a strictly increasing subsequence starting at each position.

The key to solving the large i/o set is to realize we can make the inner loop run much faster. What we really want to do is sum all of the previous f(j)s for which S[j] > S[i]. This is a classic tree problem. There are a number of tree data structures you could use including a binary indexed tree, a segment tree, or a binary search tree, to name a few, that will allow you to solve the problem in O(n log n) time per case.

The first two types of trees are fairly easy to implement although they add a complication since they use an amount of memory proportional to the maximum value in the sequence. Fortunately this can be worked around by normalizing S to contain only values between 0 and n-1 without changing the the S[i] < S[j] property for any i and j. This can be done by transforming S[i] to be the first occurrence of S[i] in a sorted list of S.

Here is an implementation of these ideas in C++.

#define MAXN (1<<20)
int sum_bit[MAXN];

int sum_bit_get(int x)
{
  int ret = 0;
  for(int i = x | MAXN; i < 2 * MAXN; i += i & -i)
    ret = (ret + sum_bit[i ^ MAXN]) % 1000000007;
  return ret;
}

void sum_bit_add(int x, int v)
{
  for(int i = x | MAXN; i; i &= i - 1)
    sum_bit[i ^ MAXN] = (sum_bit[i ^ MAXN] + v) % 1000000007;
}

int S[1000000];
int S2[1000000];

int main()
{
  int T; cin >> T;
  long long X, Y, Z, A[100];
  for(int t = 1; t <= T; t++) {
    int n, m; cin >> n >> m >> X >> Y >> Z;
    for(int i = 0; i < m; i++) cin >> A[i];

    // Generate S
    for(int i = 0; i < n; i++) {
      S[i] = A[i % m];
      A[i % m] = (X * A[i % m] + Y * (i + 1)) % Z;
    }

    // Normalize S
    memcpy(S2, S, sizeof(S));
    sort(S2, S2+n);
    for(int i = 0; i < n; i++)
      S[i] = lower_bound(S2, S2+n, S[i]) - S2;

    // Calculate f(i) and sum them in to result.
    int result = 0;
    memset(sum_bit, 0, sizeof(sum_bit));
    for(int i = n - 1; i >= 0; i--) {
      int add = 1 + sum_bit_get(S[i] + 1);
      sum_bit_add(S[i], add);
      result = (result + add) % 1000000007;
    }
    
    cout << "Case #" << t << ": " << result << endl;
  }
  return 0;
}
In addition to trees, there was a "hacker's" O(n √n) solution. The idea behind it is similar to the idea mentioned in solution 1 of the Mousetrap editorial (from Round 1B) where, in addition to maintaining the value at each position, we maintain sums for ranges of length √N. These tables can be maintained in constant time and take √n time to query.

More Information:
Binary Indexed Tree

Round 2 2008 - Code Jam 2008

Cheating a Boolean Tree (5pts, 10pts)

This is an easy exercise in dynamic programming.

Let us define for each node v in the tree, F(v, x) to be the smallest number of gates we need to flip in order to make the output of v be x (0 or 1). The value F(v, x) can be computed using dynamic programming as follows.
If v is a leaf with input value 0, then F(v, 0) = 0 -- no gate needs to be changed; and F(v, 1) can be assigned to -1 or some really big value to indicate "mission impossible".
If v has two children, u and w, and the gate at v is OR, then F(v, 0) can be computed by taking the better of the following options.

Do not change the gate and use the plan for F(u, 0) and F(w, 0);
Change the gate to AND and use the plan for F(u, 0);
Change the gate to AND and use the plan for F(w, 0).
So

F(v, 0) = min{ F(u, 0) + F(w, 0), 1 + F(u, 0), 1 + F(w, 0) }.
The other cases are similar. One can compute the F values iteratively bottom-up or recursively top-down.

In addition to the solution above, we introduce some interesting observations. Look closely at the formula above. Suppose we want the output 0 at the top, when you start a top-down computation, you will only use values F(_, 0) and never use any F(_, 1). Similarly, if the desired output at the top is 1, you will never need to care about the values for F(_, 0). Furthermore, in computing F(v, 1), you never want to change an OR gate to an AND gate. The deep reason for these is that, when there is no negation gate involved, the circuit computes a monotone function, and you will never want to change an output from 1 to 0 in the middle.
By de Morgan's law, if we interchange all the input values, change all the gates to the opposite type, and change the desired output from 0 to 1 (or 1 to 0), we obtain a dual problem, and the minimum number of gates one needs to change remains the same. So we can assume that the desired value is 1 (or 0, depends on your taste), and forget about implementing half of the cases.

Round 2 2008 - Code Jam 2008

Triangle Areas (5pts, 15pts)

This problem was originally in the form of a little puzzle: For how many numbers is the area possible? The answer is that there are MN such numbers, for any integer 0 < A ≤ MN, we can find a triangle with area A/2 formed by integer points on the graph paper.

The following picture gives the key step in the proof, as well as a solution to our problem.

Assume the integer part of A/M is k, we have kM ≤ A ≤ (k+1)M. Denote S(XYZ) the area of triangle ΔXYZ. Clearly 2S(ABC) = kM and 2S(ABC') = (k+1)M. Now take a point C* from your pocket, put it on C, and move it up towards C', one unit at a time. What can we say about the quantity 2S(ABC*)?

It starts with kM and ends with (k+1)M.
It is monotone increasing because the distance from C* to AB is monotone.
It is always an integer.
The journey takes exactly M steps.
Putting these together, the simple conclusion is that 2S(ABC*) will hit every integer between kM and (k+1)M, including A.

Exercises
(1) For the careful readers, there is one more thing we did not prove yet. There is no way to form a triangle on the graph paper with an area bigger than MN/2. Find a simple reason for this.

(2) We argued that 2S(ABC*) must hit A because it will hit every integer between kM and (k+1)M. Reason directly that, while C* is moving upward, S(ABC*) will increase by 0.5 every time C* moves one unit higher.

Round 2 2008 - Code Jam 2008

Star Wars (10pts, 20pts)

This problem offers a nice excursion into some basic pictures in algebra and geometry, and certainly some programming basics.

The problem is to find the smallest power Y0, such that there is a position where power Y0 is good enough to reach all of the ships. Clearly, any power bigger than Y0 is big enough, while any power smaller than Y0 is not. So, the first step towards the solution is to use the binary search. Reduce the problem of finding the smallest Y to a sequence of easier problems of deciding whether a given Y is big enough. Below we can focus on the decision problem for a given Y instead of the original optimization problem.

For a given Y, we have a requirement that each ship i satisfies

(1)     (|xi - x| + |yi - y| + |zi - z|) ≤ piY
Geometrically, this means that the point (x, y, z) for the cruiser must be in the octahedron centered at (xi, yi, zi). Each of the N ships gives one octahedron, and a good position for the cruiser exists if and only if all these N octahedra intersect.

Algebraically, (1) is equivalent to the following set of inequalities (prove it!)

   x + y + z ≤ xi + yi + zi + piY
   x + y + z ≥ xi + yi + zi - piY
   x + y - z ≤ xi + yi - zi + piY
   x + y - z ≥ xi + yi - zi - piY
   x - y + z ≤ xi - yi + zi + piY
   x - y + z ≥ xi - yi + zi - piY
   -x + y + z ≤ -xi + yi + zi + piY
   -x + y + z ≥ -xi + yi + zi - piY
For the geometrically inclined, each octahedron is associated with one of the four directions given by the vectors (1, 1, 1), (1, 1, -1), (1, -1, 1) and (-1, 1, 1). Each pair of inequalities states that the projection (the inner product) of (x, y, z) on a given direction vector must be in a certain range.

Now we have the problem of solving a set of inequalities of the form

   A ≤ x + y + z ≤ B
   C ≤ x + y - z ≤ D
   E ≤ x - y + z ≤ F
   G ≤ -x + y + z ≤ H
where A, B, C, D, E, F, G and H are given. In general, this is a linear program. But it is such a trivial one that we do not need to pull out any serious linear programming algorithms.

Certainly, for the solution to exist, we must have A ≤ B, C ≤ D, E ≤ F, and G ≤ H. But these conditions are not enough. The inequalities can be rewritten as

   A - x ≤ y + z ≤ B - x
   G + x ≤ y + z ≤ H + x
   C - x ≤ y - z ≤ D - x
   -F + x ≤ y - z ≤ -E + x
As long as y + z and y - z have solutions, we can get y and z. We want to see whether there is an x such that the range [A - x, B - x] intersects [G + x, H + x], and the range [C - x, D - x] intersects [-F + x, -E + x].

It is easy to see that in order for the first two ranges to intersect, we must have

(2)     x in [(A - H) / 2, (B - G) / 2].
And for the other two ranges, we must have

(3)     x in [(C + E) / 2, (D + F) / 2].
The last step of our solution is simply to decide whether the two intervals in (2) and (3) have a non-empty intersection.

Round 2 2008 - Code Jam 2008

PermRLE (5pts, 30pts)

Section A. The Hamiltonian cycle in a small world
A Hamilton cycle in a graph is a cycle that visits each node exactly once. Given a weighted, directed, complete graph on n nodes, there are (n-1)! distinct Hamiltonian cycles. It is well known that the problem of finding the shortest (or longest) Hamilton cycle is NP-hard. It is also known to many contestants that, for n as small as 20, dynamic programming makes a difference of n*2n vs. n!, which is the difference between a second and an eternity.

Let's have a look at the n*2n DP trick, in case you have not seen it before.

Without loss of generality, we may view node 0 as the start point of the cycle, as well as its end point. For any subset A of the node set V and any node x in A, we define

dp[A][x] := The shortest path that starts from x, visits each point in A exactly once and ends up at node 0. (*)
To clarify, 0 does not necessary belong to A, but we do count the length of the edge from the last point to node 0. Thus the problem of finding the shortest Hamilton cycle is just dp[V][0]. (Convince yourself, maybe looking at (*).)

We need to compute dp[A][x]. For the easy cases where A = {x}, the answer is just the length of edge x→0. Otherwise, we focus on the first step of the path. If the first step is x→y, with edge length q, then we pay dp[A - x][y] + q. In general, dp[A][x] is

length(x→0), if A = {x}.
min { dp[A - x][y] + length(x→y) | y in A - x }, if |A|>1.
Section B. Wrap everything into a small world
For any string, define the number of switches to be the number of times adjacent characters are different in the string. We want to find a permutation that transforms S to one S' where the number of switches is minimal. Assume the length of S is mk. Then S can be viewed as a string with m blocks of length k.

Now we introduce a visual aid to simplify our writing. Let us draw the string S as m rows, each block on a single row. The key image is to count the number of switches one column at a time.

Let us take a semi-concrete example. Suppose that at one point we have decided that 5 is permuted to the 7th position, and that 2 goes to the 8th position. Then without knowing the rest of the permutation, we can inspect the 5th and 2nd characters in each block. Suppose that in Z of the blocks the 5th and the 2nd characters are different, then we know that in any such permutation, we will have to pay the price of Z.

The one exception is the last element of the permutation. In all cases but one, we simply wrap around to the beginning because the end of each k-block touches the beginning of the next k-block in the string, except for the last character in the string. We can handle both cases if we fix the last element of the permutation by trying all possibilities.

Next, we reduce our problem to the one in Section A. Suppose we fix T as the last element in the permutation. Define a weighted, directed, complete graph G on k vertices {1, 2, ..., k}. The weight on the edge x→y is

the number of blocks where the x-th character is different from the y-th character in the same block. (if x ≠ T)
the number of blocks (excluding the last one) where the x-th character is different from the y-th character in the next block. (if x = T)
It is easy to check that for any permutation, the number of switches is the same as the length of the corresponding Hamiltonian cycle in G.

We have k different choices for T. For each T, finding the shortest Hamilton cycle takes O(2k k) time. The construction of the graph takes O(k2m) = O(k |S|) time for each T; it is also easy to construct in O(k2m) time the graphs for all the T's. The running time of the solution is O(2k k2 + k |S|).

Round 3 2008 - Code Jam 2008

How Big Are the Pockets? (5pts, 15pts)

(i). Computing the area of the polygon
As Polygonovich walks, the interior of the polygon either always stays on his right side, or always on his left side. This is by no means a trivial fact. The polygon can be so complicated that given a poriton of the walk in the middle, one has no way to decide whether the left side is the interior, or the right side is. Let us accept this fact and assume the former case -- his right hand always touches the interior of the polygon.

Fix a vertical bar of unit width, observe the interaction between P, Polygonovich's walk, and B, the vertical bar.

P crosses B an even number of times. Because the walk is closed, and any time Polygonovich crosses from the left to the right, the next time he must cross in the other direction. From top down, we label them as the 1st crossing, the 2nd one, and so on.
Furthermore, look down at B from high above. In the beginning, it is outside the polygon. Every time it encounters an edge of P, it changes from being outside the polygon to being inside and vice versa. So, a unit square on B is inside the polygon if and only if it is between some (2k-1)-st crossing and (2k)-th crossing.
Note that if the polygon is always on the right-hand side, then we know that the (2k-1)-st crossing is always from left to right, and the (2k)-th one is from right to left.
Given any unit square U, we can decide whether it is in the polygon by counting the number of left-to-right crossings vertically above U, minus the number of right-to-left crossings above U. We can have a little counter inside U, and each time there is a left-to-right crossing above U, we increase the counter by 1; and for the right-to-left crossings, decrease the counter by 1.

After these mental exercises, we make the final jump back to the interaction between P and B. We may imagine a bounding box outside the polygon so that there are finitely many counters, and we now consider the area, it is nothing other than the summation of all the counters. To compute the area, we do the following.

Set A = 0 in the beginning. Walk along P, for each horizontal edge from left to right at height h, we increment A by h. This is the effect of this edge on all the counters for the unit squares below. And for each edge from right to left at height h, we decrement A by h.
All the time we assume the interior is always on the right hand side. In the opposite case, the highest edge on each bar will be a right-to-left cross, and all of the reasoning above is similar with only a difference in signs. So if we ever find that A is negative at the end, we can negate it and get the right area.

We note that this is just a special case of the simple algorithm for computing polygon areas in general. However, isn't the pictures nice, in the special form of integer grids and axis-parallel edges?

(ii). Computing the area of the polygon plus the pockets
The polygon plus the pockets gives staircases in four (NE, NW, SE, SW) directions. A formal proof would be tedious. A picture with a good example should suffice.

As in the picture, let A be any one of the topmost edges, C be the bottommost edge, B the right-most, and D the left-most one. We have 4 staircases: one from A to B, one from B to C, one from C to D, and one from D to A. Theoretically speaking, these are formed by the maximal points with respect to four directions, and each of these staircases can be computed in mlogm time, where m is the number of points on the polygon. In this problem, we use the guarantee that there are at most 6000 vertical strips. For each vertical strip x, define

t(x) := the topmost polygon edge to cross the strip.
b(x) := the bottommost polygon edge to cross the strip.
H(x) := the topmost polygon edge or pocket on that strip.
L(x) := the bottommost polygon edge or pocket on that strip.
For x between A and B, H(x) is the maximum t(x') for all x' between x and B. We may compute H(x) as follows
H(x) = t(x) for the last strip
for x = (the second last strip) down to A
  H(x) = max(H(x+1), t(x))
The values on the other staircases can be computed in the same manner.

On strip x, the polygon plus pockets are the unit squares between L(x) and H(x). So we can sum H(x) - L(x) over all the 6000 possible strips and get the area of the polygon with pockets.

(iii). Solve our problem
(ii) minus (i).

Round 3 2008 - Code Jam 2008

Endless Knight (5pts, 20pts)

The small dataset can be solved by simple, two-dimensional dynamic programming. In fact, this is obviously the easiest dataset most of the contestants found in this round, with 884 correct submissions. Quite on the contrary, the large dataset turns out to be the most difficulty for this round, solved by only 32 contestants.

Below we outline three tricks one may use in this problem.

Section A. A nice simplification
Most people are familiar with lattice walks from (0, 0) to (m, n), where each step one can either increase row by 1 or increase column by 1. The total number of such walks is the binomial number (m+n) choose n. One simple reason is that the set of paths is bijective to the ways you choose m steps for vertical moves among the (m+n) steps.

If there are no rocks, one can see that the situation in this problem is quite similar. In fact, they are essentially the same. After a nice transformation of the input, we can forget about the knight and focus on the normal lattice walks.

In our problem, the two kinds of moves the knight can make correspond to the vectors u = (1, 2) and v = (2, 1). The set of reachable positions forms a lattice with u, v as the basis. To find the coordinate of (r, c) in the new system is a matter of linear transformation between two bases. In our problem, it is as simple as

r' (2, 1) + c' (1, 2) = (r, c) - (1, 1).
Solving, we get r+c = 2 mod 3, and

r' = r - 1 - (r+c-2)/3, and c' = c - 1 - (r+c-2)/3.
We illustrate using the following picture.

When reading the input, we can transform the points to the new system, and throw away any points that are not reachable. Assume the destination is reachable, otherwise we can simply output 0. We can also disregard any rock with row number or column number that exceeds the destination. In short, we are now in a rectangular lattice in the new coordinate system.

Section B. A key idea
Notice that there is an important restriction in our problem: there are at most 10 rocks. The key idea to our solution -- although other solutions are possible as well -- is the inclusion-exclusion principle.

Let S be any subset of rocks (including the empty set). Define f(S) to be the number of ways we can walk from the origin to destination and hitting every rock in S, and possibly hitting some other rocks.

Number of paths that do not hit any rock = ΣS f(S) (-1)|S|.
Also note: we refer to the Round 1C problem Ugly Numbers. There we needed to consider the multiples of 2, 3, 5, and 7. Some solutions can be viewed as an application of the inclusion-exclusion principle, although this is not necessary for solving that problem.

Section C. N choose K mod 10007?
Now we need to calculate f(S) for any S. Let's sort rocks in S from left to right, and for rocks on the same column, pick the higher one first. It should be clear that if some later rock is higher than some earlier ones, then f(S) = 0 -- there is no way to hit all the rocks in S.
Otherwise, the sorted set S forms a chain from the top-left corner to the lower-right corner

(0, 0) =: (r0, c0) → (r1, c1) → ... → (rk+1, ck+1) := destination
We can view any path hitting all the k rocks as (k+1) stages. f(S) is the product of the number of ways we can do each stage. (This is another important counting principle. It is so important and obvious that usually people don't call it by name. But it does have a name -- the multiplication rule.)

Now, we are back to the classical problem in the beginning of this analysis. Let m = (ri - ri-1) and n = (ci - ci-1), the number of ways we can do the i-th stage is exactly (m+n) choose n.

So, is this the end of the story? Not yet. Many contestants failed this problem because it is tricky to compute A choose B mod 10007 quickly and correctly. In this problem, both A and B can be on the order of 108. To compute A choose B mod P for a prime number P, one needs some tricks. There are many clever ways you can find, like pre-computing N! for all N, pre-computing the inverse of each number mod P, utilizing the periodicity of the numbers in factorials.
What we want to introduce below is a nice theorem that is not as well known as it should be. It removes all the worries about the multiples of P. It is not a difficult theorem, but looks very cute and especially useful for this problem.

Lucas' Theorem: Suppose (ntnt-1...n0) and (ktkt-1...k0) are the representation of n and k in base P, where P is a prime number. Then (n choose k) is the product of (ni choose ki) in ZP.

The following is the choose function from Reid's beautiful Haskel code.

choose :: Int -> Int -> Int10007
choose n k | k > n = 0
choose n k | n < 10007 = 
  product [ (fromIntegral i) :: Int10007 |  i <- [n-k+1..n] ]
  / product [ (fromIntegral i) :: Int10007 | i <- [1..k] ]
choose n k = choose qn qk * choose rn rk
  where (qn, rn) = n `divMod` 10007
        (qk, rk) = k `divMod` 10007
More information
The inclusion-exclusion principle - Lucas' Theorem - Staircase walk
For the study of lattice points, we refer to any standard text in Discrete Geometry.

Round 3 2008 - Code Jam 2008

Portal (10pts, 15pts)

The challenge in this problem was mainly coding a bug-free solution. It was pretty obvious that solving it involved using a shortest path algorithm. And as you can see, even some people in the top 20 skipped this problem to go for other ones, which were more difficult to figure out, but easier to code.

Let's look at bmerry's solution, because it is very readable, and he was the winner of this round. Then I will continue with some other, more efficient solutions.

A state of the game corresponds to the position of the player in the maze and the positions of the two portals, if they exist. This solution considers the map of the maze indexed from 1 so the values (0, 0) for the coordinates of the portals mean that the portals do not exist.

At each step, the player can create a new portal, move one step North, South, East or West or, if he is currently near a portal, and another portal exists, travel from one portal to the other one. The first type of move can be made instantaneously while the second and third types take one turn.

One optimization step is to find, for each cell, the positions of the portals that can be created from that cell, since you don't want to take O(R + C) every time you need to find the possible moves from a state.

If each move took exactly one turn, then the classic breadth-first search algorithm could provide us with the answer, but in this graph with two types of weights for the edges it seems like we need Dijkstra's shortest path algorithm. In fact, we can still use an algorithm very similar to breadth first search. The tweak is that instead of adding a new state of the same cost with the current state at the end of the state queue, we add it at the beginning. This way the states will be expanded in the order of their costs, which is exactly what Dijkstra's algorithm does. The complexity of this algorithm is O((R*C)3) instead of O((R*C)3 log (R*C)), which is the cost of Dijkstra's shortest path algorithm.

Code
Here is bmerry's code, modified slightly and with some additional comments.

struct state {
    int r;
    int c;
    // portal rows
    int pr[2];
    // portal columns
    int pc[2];
};

#define ADDR(state) state.r][state.c] \
                   [state.pr[0]][state.pc[0]] \
                   [state.pr[1]][state.pc[1]

static unsigned char prio[16][16][16][16][16][16];

static const int dr[4] = {-1, 0, 1, 0};
static const int dc[4] = {0, -1, 0, 1};

int main() {
    int cases;
    cin >> cases;
    for (int cas = 0; cas < cases; cas++) {
        cin >> R >> C;
        grid.clear();
        grid.resize(R + 2);

        state start;
        memset(&start, 0, sizeof(start));
        grid[0] = string(C + 2, '#');
        grid[R + 1] = grid[0];
        for (int i = 1; i <= R; i++) {
            string line;
            cin >> line;
            grid[i] = "#" + line + "#";
            if (grid[i].find("O") != string::npos) {
                start.r = i;
                start.c = grid[i].find("O");
                grid[i][start.c] = '.';
            }
        }

        memset(prio, 255, sizeof(prio));
        prio[ADDR(start)] = 0;
        deque<state> q;
        q.push_back(start);
        int ans = -1;
        while (!q.empty()) {
            state cur = q.front();
            unsigned char pri = prio[ADDR(cur)];
            if (grid[cur.r][cur.c] == 'X') {
                ans = pri;
                break;
            }
            q.pop_front();

            for (int d = 0; d < 4; d++) {
                int hr = cur.r;
                int hc = cur.c;
                do {
                    hr += dr[d];
                    hc += dc[d];
                } while (grid[hr][hc] != '#');
                hr -= dr[d];
                hc -= dc[d];

                // adding a new portal
                for (int p = 0; p < 2; p++) {
                    state nxt = cur;
                    nxt.pr[p] = hr;
                    nxt.pc[p] = hc;
                    if (prio[ADDR(nxt)] > pri) {
                        prio[ADDR(nxt)] = pri;
                        // We push the state at the
                        // front since adding a portal
                        // is instantaneous.
                        q.push_front(nxt);
                    }
                }
            }

            for (int d = 0; d < 4; d++) {
                state nxt = cur;
                nxt.r += dr[d];
                nxt.c += dc[d];
                if (grid[nxt.r][nxt.c] != '#') {
                    if (prio[ADDR(nxt)] > pri + 1) {
                        prio[ADDR(nxt)] = pri + 1;
                        q.push_back(nxt);
                    }
                }
            }
            if (cur.pr[0] > 0 && cur.pr[1] > 0)
                for (int p = 0; p < 2; p++)
                    if (cur.pr[p] == cur.r &&
                       cur.pc[p] == cur.c) {
                        state nxt = cur;
                        nxt.r = cur.pr[1 - p];
                        nxt.c = cur.pc[1 - p];
                        if (prio[ADDR(nxt)] > pri + 1) {
                            prio[ADDR(nxt)] = pri + 1;
                            q.push_back(nxt);
                        }
                    }
        }

        printf("Case #%d: ", cas + 1);
        if (ans == -1)
            printf("THE CAKE IS A LIE\n");
        else
            printf("%d\n", ans);
    }
    return 0;
}
The restrictions on the inputs in the problem are small enough so that this solution passes all the tests. There are other, more efficient solutions which we thought of.

Other solutions
It does not make sense to create the starting portal before actually being able to jump through it. This way we can improve the previous solution and get the complexity down to O((R*C)2).

As we have just seen, premature portal creation is the root of complexity, so let's think now of the destination portal. Once we have created a destination portal, we need to move as quickly as possible to the nearest wall, create a starting portal, walk through it and arrive at the destination. So what we can do is just keep R*C states and move from one to another by either going North, West, South or East or do a teleport move which takes a few turns. We can compute in O(R*C) how much time each teleport move takes by doing a breadth first search starting from all the cells in the maze where a portal can be created. Now we can use Dijkstra's algorithm to find the shortest path. The final algorithm can have either O(R*C log (R*C)) complexity, or if we use the fact that the teleport edges have the cost at most R * C, by using a array of lists in Dijkstra's algorithm instead of a priority queue, we can get the complexity of the solution down to O(R * C).

More Information
Dijkstra's algorithm - Breadth First Search

Round 3 2008 - Code Jam 2008

No Cheating (10pts, 20pts)

Programming contests have certainly changed over the years. Nowadays, dynamic programming is a trivial matter to many, and bipartite matching is no longer a secret idea. More than half of the contestants solved the small datasets of this problem, and 77 passed the large tests.

Small dataset (Test set 1 - Visible)
The small tests can be solved using dynamic programming. The main idea is to do this row by row.

The naive dynamic programming has states (R1, r), where r is the row number and R1 is the possible configurations of the current row. In order to compute the value of (R1, r), (the maximum number of students who can be put in the first r rows, with the r-th row identical to R1), one can loop through all of the possible values of R2 and use the value of (R2, r-1).

A more sophisticated approach also goes row by row, but in each row, we do it by squares. The states are (S, r, c), where (r, c) is the current position, and S is the bitmap for up to n positions before (r, c). These are all the squares to the left on the same row, and all the squares from the (c-1)-st to the last on the last row. In this way, each value of (S, r, c) can be computed in constant time.

One may estimate the number of states to be O(M N 2N). But actually the number of states is much smaller. It is not hard to see, with the restriction that no two horizontally adjacent seats can be both occupied, that the number of states S is O(FN), the N-th Fibonacci number. This means that the dynamic programming solution can even handle a classroom of size about 35 by 35.

Large dataset (Test set 2 - Hidden)
Well, but universities do have very large classrooms. The large dataset has tests with 80 rows and 80 columns.

The key idea is to see through the puzzle and notice that this is a standard graph problem. Let each square be a node. What the problem requires is to pick a set of nodes, such that some pairs of nodes cannot be picked together. Let us draw an edge between each such pair of nodes. This is the well-known problem of finding a maximum independent set of the graph. While the general independent set problem is NP-hard, we do know that the problem is tractable on bipartite graphs. A-ha! The graph is indeed bipartite, because all of the edges are between a square in odd-numbered columns and squares in even-numbered columns.

We can easily find the maximum matching on bipartite graphs. Let's see how a maximum matching can help us solve the problem. Let's assume that the size of the matching is X, and the number of nodes in the graph is N. Notice that the the maximum independent set cannot contain more than N - X nodes, because that would mean you will have two nodes that share an edge of the maximum matching in this set.

In general, we have the following relations between the important graph parameters. For a graph G = (V, E), let α(G) be the size of the maximum independent set, m(G) be the size of a maximum matching, μ(G) be the size of a minimum vertex cover,

The complement of an independent set is a vertex cover, and vice versa. So α(G) + μ(G) = |V|.
To cover all the edges, we need one vertex from any edge of a matching. So μ(G) ≥ m(G).
If G is bipartite, then μ(G) = m(G). Equivalently, α(G) + m(G) = |V|.
The last fact can be proved directly, or as a consequence of Hall's theorem, or as an application of the powerful max-flow-min-cut theorem. Below we provide a constructive proof that suits to our problem.

Proof
We will prove that N - X is smaller than or equal to the size of the maximum independent set. Let's have a maximum matching M and a set S which contains all the nodes that are not in M. Now let's add to S all the nodes from M which are on the left side of the bipartite graph. S is of size N - X. If S is not an independent set, then it has a node u from M and another one v that doesn't belong to M, such that (u, v) is an edge. To fix this problem we remove u from S and add node u' to S where (u, u') is an edge from the matching. If this operation caused a problem with another node in the matching, then we can remove that node and add it's match, and so on. This repeated procedure is guaranteed to finish since we only delete nodes from the left side of the graph and add nodes from the right side. There will be no problem caused by the nodes on the right side of the graph and nodes outside the matching because that would mean that we can find an alternating path, and the maximum matching is not the largest possible.

We have proved that in any bipartite graph, the size of the maximum independent set equals the size of the graph minus the size of the maximum matching.

Code
We believe that many contestants have the standard bipartite matching algorithm prepared. You may download many nice samples from the scoreboard. Below we show a sample solution which keeps the bipartite graph only in mind, and actually runs the matching algorithm on the board itself. From the judges:

int C, N, M;
string bd[100];

int nbx[100][100], nby[100][100], v[100][100];
int T=0;

bool dfs(int a, int b) {
  if (a<0) return true;
  if(v[a][b]==T) return false;
  v[a][b]=T;
  for (int i=a-1;i<=a+1;i++)
    for (int j=b-1;j<=b+1;j+=2)
      if (i>=0 && i<M && j>=0 && j<N && bd[i][j]=='.') {
        if (dfs(nbx[i][j], nby[i][j])) {
          nbx[i][j]=a; nby[i][j]=b;
          nbx[a][b]=i; nby[a][b]=j;
          return true;
        }
      }
  return false;
}

int play() {
  memset(nbx,-1,sizeof(nbx));
  memset(nby,-1,sizeof(nby));
  memset(v, -1, sizeof(v)); T=-1;
  int rst=0;
  for(int i=0;i<M;i++) for(int j=0;j<N;j++) {
    if (bd[i][j]=='.') {
      rst++;
      if (j%2) {
        T++;
        if (dfs(i,j)) rst--;
      }
    }
  }
  return rst;
}

int main() {
  cin>>C;
  for (int i=1; i<=C; i++) {
    cin>>M>>N;
    for (int r=0;r<M;r++) cin>>bd[r];
    cout<<"Case #"<<i<<": "<<play()<<endl;
  }
  return 0;
}
More Information
Maximum independent set - Bipartite Matching

APAC Semifinal 2008 - Code Jam 2008

What are Birds? (5pts, 12pts)

The Simple Solution
Let us visualize this problem. Each animal is characterized as a pair (H, W), which can be viewed as a point in the two dimensional Cartesian coordinate system. For each animal that is known to be a bird, we color it red. For each animal known to be a non-bird, we color it blue. The problem states that there exists a rectangle such that a point is red if and only if it is in the rectangle. The problem is trivial if there are no red points. From now on we assume there are red points.

For any two distinct points U and V, there is naturally a rectangle determined by U and V. It is the smallest rectangle containing both U and V, with the four corners U, V, (HU, WV), and (HV, WU). When U and V are on the same horizontal or vertical line, the rectangle degenerates to a segment. Formally, the rectangle contains all the points (H, W) such that

(|H - HU| + |W - WU|) + (|H - HV| + |W - WV|) = |HU - HV| + |WU - WV|.
We have the following proposition.

If U and V are two red points, and R be the rectangle determined by U and V; then any point in R must also be red, since any rectangle containing both U and V must contain R.
For the set of all red points in the input, there is also naturally a smallest rectangle, R0, that contains all of them. There are different ways to define this rectangle:

(a) The intersection of all the rectangles that contain all the red points.
(b) The union of all the rectangles determined by U and V, where (U, V) range over all the red point pairs.
(c) The rectangle with four corners A=(Hmin, Wmin), B=(Hmin, Wmax), C=(Hmax, Wmin), and D=(Hmax, Wmax), where the min and max are taken over all the red points.
The interested reader may check the equivalence of the above definitions.
Now, given a point X, we want to know if it is a bird or not. Clearly, if X is inside R0, then it must be bird. Otherwise, we may pretend it is red, and compute the new rectangle R' based on (c). This can be done in a constant number of comparisons. If there is a blue point from the input that lies in R', then X must not be a bird. Otherwise, X might (taking R' as the red rectangle) or might not (taking R0 as the red rectangle) be a bird.

By the limits of this problem, a Θ(NM) algorithm is fast enough. That is, we simply take any blue input point, and check if that is inside R'.

Further Discussion
Another way to view the last step of the solution, when the query point X is given, is to see whether there is a known red point Y such that the rectangle determined by X and Y contains any blue point. It is enough to check Y = A, B, C, or D as in (c). To check this, we do not need to go over all the blue input points. There are standard pre-processing techniques that can reduce the query time from N to log N.

For example, for A, we can define two sets based on the blue inputs: those points that are higher (along the W-axis) than A (call them A1) and the rest of the A points (A2 = A - A1). The task of determining whether there is a blue point between X and A becomes the query of lowest or highest point in the range between HX and HA.

APAC Semifinal 2008 - Code Jam 2008

Apocalypse Soon (8pts, 14pts)

This problem is a sheep in wolf's clothing. However, it must have been very convincing wolf's clothing! Fewer competitors solved it than any other problem, despite the simplicity of the actual solution. Since this was an onsite round, they were all experienced competitors; in some sense, that worked against them.

Why does the problem seem so hard? To a practiced eye, it has all the trappings of a horrible exponential search. You have 5 actions to choose from on each day, and the results of those actions don't tend to overlap, leading to an exponential number of possible states. Furthermore, even the subtlest change in a nation's strength can drastically affect the final outcome, so at each step the entire state of the world matters. This makes a Dynamic Programming-based solution look unlikely. And due to the chaotic nature of the rules, there's unlikely to be a greedy strategy that works in all cases.

So a horrible exponential search seems essential. On a 50x50 grid of numbers between 0 and 1000, this is clearly intractable! Give up and go home. No, wait... it's supposed to be solvable somehow, right? How could that be?

Well, if you try a few cases by hand, you'll soon realize why: everyone ends up killing their neighbors really, really quickly. A random map tends to stabilize (no living neighbors) in only 4-6 days. It's actually very difficult to come up with a case that takes longer. The more you try, the more you'll realize just how difficult it is; the army sizes required grow exponentially, and the tighter you pack them the sooner they all die. The map size doesn't turn out to matter much - it's the army size bound of 1000 that really sets the cap on how long you'll need to search.

What exactly is this cap? That's a really tough question. For instance, if army sizes are unbounded, there is no small limit. Here's a simple (2n-2)x2 case that takes n days to stabilize:
1 2 4 8 ... 22n-3
1 2 4 8 ... 22n-3

However, the army sizes must grow exponentially relative to the number of days. Here's a simple proof: on day n, let Sn be the strength of the strongest nation that still has living neighbors. Let N be any nation with strength ≥ Sn/2. Suppose that after 8 days N still has strength ≥ Sn/2. Then it must have killed all its neighbors, since none had strength ≥ Sn (that is, any neighbor would die after at most 2 attacks). So every such N either ends up isolated or weaker than Sn/2 after 8 days. Thus, Sn+8 ≤ Sn/2. This puts a strict bound of 8(log2Sn+1) more days before all nations are dead or isolated.

Unfortunately, for army sizes of 1000 this bound of 80 days still isn't very helpful. The bound can be improved with tighter reasoning, but due to the compact, constrained nature of the grid, the "true" bound is probably much smaller - around 9-10 days! However, we don't know how to prove a bound anywhere near this. If you can think of a way, let us know! :)

The worst test case in our data had an answer of "9 day(s)". So as it turns out, even a straight brute force solution (on the worst-case order of 50x50x59) would suffice to solve it. Adding simple pruning heuristics and memoization can potentially also help, but isn't necessary.

Incidentally, one potential coding error you could make is to assume that effects can only propagate at one grid square per day. The "speed of light" (as it's known in cellular automata) is actually two grid squares per day, because if an army's neighbor changes, it may choose to attack its opposite neighbor instead. In the "9 day(s)" test case, there are nations 13 squares away from yours that end up affecting the answer. So if you tried to speed up your solution by considering only the local region around your nation, you'd have to be careful not to make it too local.

In the end, all it took to get this problem was bravery (or foolishness). If you code it properly and try it, it works! It's just a question of how well you can convince yourself that the simple solution has a hope of working. You certainly wouldn't want to download the Large dataset and then realize your solution was too slow...

APAC Semifinal 2008 - Code Jam 2008

Millionaire (13pts, 16pts)

The problem explicitly states that the contestant has an infinite number of degrees of freedom. She can bet ANY fraction of her current amount. This makes it impossible to brute force over all of the possible bets. The trick is to discretize the problem.

Following one of the principles in problem solving, we look at the easier cases. The problem is easy if there is one round. We suggest you do it for the case when there are two rounds. It is not hard, but it reveals the interesting nature of the problem. The figure below illustrates the situation in the second-to-last round, the last round and after the last round.

The colors represent different probability zones. All sums in a probability zone share the same probability of winning. The important sums are marked and labeled.

If we know the probability of winning for all sums in the next round Pnext(sum), then the probability of winning in this round is:

p * Pnext(sum + stake) + (1 - p) Pnext(sum - stake),
where stake is the amount we are betting.

Now, given the existence and location of the important sums in the next round, we can find the locations of the important sums in this round. The figure below illustrates how we find these sums. The midpoints between the important sums of the next round become important in this round. I.e in the case of the green point in figure 2, we can move up by lowering the stake, and we can move down by increasing the stake, without changing the end probability. The probability of the blue, green and red points is the same. This also illustrates that the probability in a 'probability zone' which is limited by two important sums is equal to the probability at the lower important sum.

Conclusion: The important sums of round i are the union of the important sums of round (i+1) and their midpoints. We need to consider $0 an important sum in the last round because we can't bet more than we have.

Now, we only need to compute probabilities at the important sums, bootstrapping at 1.0 for $1000000, and 0.0 for $0 in the last round. Then, going backwards, we fill the probabilities at the important sums in the previous rounds.

By the limits of the input, we may try all the stakes that lead to important points in the next round. It is an interesting question whether there are mathematical properties that could reduce the complexity of this computation, but that is beyond the scope of this analysis.

APAC Semifinal 2008 - Code Jam 2008

Modern Art Plagiarism (7pts, 25pts)

This problem is a classical graph problem called subtree isomorphism. As an interesting note, the modern era in art history actually starts far earlier than the so called classical period in computer science.

In this problem, we are given two trees T1 and T2. We are going to decide if T2 is isomorphic to any subtree of T1. The general sub-graph isomorphism problem is notoriously hard. But as you perhaps have seen many times, when the things come to trees, it is very solvable. The problem is actually well studied, and is a standard exercise in algorithm design.

First, a bit of terminology. Just for convenience, in our discussion, for two rooted trees, we say one fits into the other if there us an isomorphism that maps the former to a subtree of the latter such that the root is mapped to the other's root.

We may fix T2 and consider it as a tree rooted at vertex 0. We do not know which vertex of T1 corresponds to 0 of T2 in the isomorphism. But we may try each vertex in T1, and see if T2 fits into T1.
For a concrete example, let's say we root T1 at vertex x. Assume that there are 3 children of 0 in T2 -- y1, y2, and y3. Assume that there are 5 children of x in T1 -- x1. x2, x3. T2 fits into T1 if and only if we can find that the subtree at y1 fits into the subtree at xi for some i, the subtree at y2 fits into the subtree at xj for some other j ≠ i, and the subtree at y3 fits into the subtree at xk for some k ≠ i, k ≠ j.

The solution for this problem as follows. Once we have fixed the root x, each vertex has a level in its tree. For each vertex u in T1 and v in T2, if they have the same level (just a bit of reasonable optimization, not necessary for this problem), we want to decide if the subtree at v fits into the subtree at u. We do this from bottom up, the deeper levels first.
For any such pair (u, v) with children { ui | i = 1,2,... } and { vj | j = 1,2,... }, we know which vj fits into which ui since we are doing the computations bottom up. We find a fit if and only if we can find, for each vj, a distinct ui such that vj fits into ui. This is clearly a bipartite graph matching problem.

We leave it an exercise to prove that the algorithm, runs in O(N2M2) time. There are algorithms with better complexities. For interested readers, we refer to the following paper
R. Shamir, D. Tsur, "Faster Subtree Isomorphism", Journal of Algorithm, 33, 267-280 (1999).
which contains a recent result as well as references to earlier works.

More information
Subtree Isomorphism - Graph Isomorphism - Bipartite Matching

AMER Semifinal 2008 - Code Jam 2008

Mixing Bowls (5pts, 9pts)

Looking at the restrictions on the recipe, it should be obvious that the recipes form a tree. It is also clear that basic ingredients can be ignored: You just throw them into whatever bowl you're currently working on.

Lets consider a recipe with k mixtures m1, ..., mk. How many bowls do we need to prepare this recipe? It does not make sense to work on more than one mixture at a time, so let's try preparing the mixtures in the given order.

We'll need b1 bowls to prepare the first mixture. For the second mixture, we can reuse these bowls, except for one bowl that we need to hold the first mixture. While preparing the third mixture (which requires b3 bowls in itself) we will need two additional bowls to hold the first two mixtures, and so on. Finally, once we have prepared all the mixtures, they sit in k different bowls, and we'll need one additional bowl to put everything together, and finish the recipe.

How many bowls did we use in total? Keeping in mind that we can reuse bowls from the previous step (except for the bowls to hold finished mixtures), we will need b = max(bi + i - 1) bowls to prepare all mixtures, for a total of max(b, k + 1) to put everything together.

Looking at this formula, it is obvious that the number of bowls will be minimized if we prepare the mixtures requiring the most bowls first. Thus, our algorithm will first invoke itself recursively for every ingredient, sort the results in descending order, and then use the formula given above to return a result.

AMER Semifinal 2008 - Code Jam 2008

Test Passing Probability (5pts, 14pts)

The Problem
There are 4Q points in the space, each of which contains an answer combination -- one answer for each of the Q questions. Because the answers to the questions are independent of each other, the probability of success at each point can be computed simply as the product of the probabilities from each problem.

So the simple mathematical model involves 4Q points, each with a probability value. There is a single, hidden good point among the 4Q points; you may pick up to M points; and you win the game if one of them is the good point. After you select a point, you learn if it is good or not, but that does not change the relative probabilities of the remaining points. The best strategy is simply looking at the points in the order of their probability. Your task in this problem is to compute the summation of the highest (up to) M among the 4Q values.

Solutions
While Q looks moderate, 4Q steps of computation can be a huge task that is probably not within your computer's capacity (or any computer's). The important restriction in this problem is the fact that M is also moderate, and we can compute the highest M values one by one.

One solution uses a priority queue L that keeps all the answer combination candidates for the next highest value -- after we computed the first k values. A bit of thought should convince your that the next-highest (the k+1th) combination must be gotten by taking one of the first k combinations, and moving the answer to one of the questions one step "worse", to the next-most-probable answer. So in each step we can get the first element S (the one with the highest probability) from L, then add (up to) Q new candidates back to L. Each of the new candidates is created by moving one answer in S one step worse. The size of L will never exceed MQ, and the time complexity is O(MQ log MQ).

Another solution has an even simpler implementation: the idea is to add the questions one by one. For the first k questions, there are 4k possible solution combinations, each with its own probability of answering all of the first k questions correctly. When we add the k+1th question, the new set of possible solution combinations, along with its probabilities, can be calculated by taking each of the 4k previous values and multiplying it by the four probabilities for each answer to question k+1. We can easily compute all of these values and truncate to the top M, then repeat the process for the remaining questions. The truncation is O(M log M), and this process is repeated Q times, for a total time complexity of O(QM log M).

More information
Discrete Probability - Joint Distribution

AMER Semifinal 2008 - Code Jam 2008

Code Sequence (7pts, 15pts)

The Problem
Normally when you count in base 2, each bit is worth a fixed amount: {..., 8, 4, 2, 1}. So when you count 0000, 0001, 0010, 0011, and so on, you get the sequence [0, 1, 2, 3, ...]. In this problem, we ask you to consider what would happen if the bits were worth hidden amounts, "bit values," specified by us. For example, if the bits were worth {..., 200, 10, 1}, then you could get the sequence [0, 1, 10, 11, 200, 201, ...].

What this problem asks you to do is find the next number in the generated sequence. To make things more difficult, we don't necessarily start at 0000 or 0001, so what may seem obvious:

[1, 10, 11, 200, 201, ?, ...]
...may not be. This could have come from the bit values {..., 200, 10, 1}, which would give 210; but it could also have come from bit values {..., -1180, 1190, 990, 190, 1}. If we start counting from 10000, that gives:

[-1180, -1179, -990, -989, -190, -189, 0, 1, 10, 11, 200, 201, 1000]
The Solution
So how do we solve the problem? Let's have a look at the generated sequence above, and look for interesting features. One thing that may strike you is that every second number is 1 higher than the last, so let's look at the sequence of differences between adjacent numbers:

(1, 189, 1, 799, 1, 189, 1, 9, 1, 189, 1, 799)
The differences of 1 make a lot of sense: every second increase in a binary number just changes the lowest bit from 0 to 1. So if your sequence of differences looks like (x, a, x, b), or (a, x, b), then the next difference must be x. Figuring that out should let you solve the small input.

But how far can we take this? Let's eliminate all the times only the lowest bit was changed. This leaves us with the following sequence of differences:

(189, 799, 189, 9, 189, 799)
Every second number there is a 189. That makes sense too: every fourth increase in a binary number is the same, changing the lowest two bits from 01 to 10. Likewise, every eighth increase changes the lowest three bits from 011 to 100.

So where does this leave us? Looking for repetition; seeing if we can use it; and if not, eliminating it. For example, if our generated sequence is:

[0, 1, 3, 4, 7, 8, 17, 18, 21, 22, 24, 25]
Our sequence of differences is:

(1, 2, 1, 3, 1, 9, 1, 3, 1, 2, 1)
Since the sequence ends with the repeated number, 1, we don't know yet what comes next. Eliminating the 1s, we arrive at:

(2, 3, 9, 3, 2)
Aha! The next difference must be 3, and the answer is 28. If that hadn't worked, we would have kept going recursively until we found the answer.

But what if the sequence of differences is:

(1, 2, 1, 2, 1)
Here, we can't tell if the lowest bit was changing for a difference of 1 or a difference of 2, so we can't say for sure what's next. We (eventually) arrive at the same problem in all of the following sequences of differences:

(1, 2)
(3)
(4, 3, 4)
(5, 4, 5, 3, 5, 4, 5)
(6, 5, 6, 4, 6, 5, 6, 3, 6, 5, 6, 4, 6, 5, 6)
Here it's IMPOSSIBLE to find the answer: that (1, 2) could be (1, 2, 1) or (1, 2, 10, 2). (4, 3, 4) could have come from:

10001 10010 10011 10100 10101
(4, 3, 4, 3)
or from:

00100 00101 00110 00111 01000
(4, 3, 4, ?)
There's no way to tell where you are in the range [0, 1000000000], so you could be about to hit a whole new bit and you wouldn't know it.

Comments
When we add up the values of our bits, we work modulo 10007. I used negative numbers earlier because the concept is well-defined in mod space: "-1180" mod 10007 is 8827, which is to say that adding 8827 to anything higher than 1180 is the same as subtracting 1180 from it.

One of the nice things about Code Jam is that you can test your assumptions. When writing the input generator for this problem, I asserted that the difference must look like [x, a, x, b, x, c, ...] or [a, x, b, x, c, x, ...]. This let me catch a bug in my code: I wasn't taking the differences mod 10007! In a live contest, I would have had time to fix that and submit.

Finally, there's one limit that's a red herring: that the sequence can't go past 109. It isn't hard to convince yourself that the restriction is meaningless: any sequence of length 1000 can't affect more than 11 separate bits, so it doesn't matter if the highest bit is #13 or #32.

AMER Semifinal 2008 - Code Jam 2008

King (7pts, 38pts)

The problem corresponds to a relatively unknown game named Slither. This game was popularized by Martin Gardner in one of his columns written for Scientific American. It wasn't solved then, but eventually William Anderson developed a solution. Of course Code Jam participants are much smarter, so we thought the problem would easily get solved in two hours.

I'll explain first the solution for a simpler variant of the problem. Suppose we have a 3x3 board with the king being in one of the corner cells. Now let's cover the rest of the board with dominoes of size 2x1. This tiling of the board shows us a winning strategy for the second player. Each time the first player moves to one cell of a domino, the second player will move to the other cell of that domino. Since the second player can always make a move after the first player moves, the second player has a winning strategy.

The general solution goes along the same lines: the domino tiling corresponds to a maximum matching. We think of the board as a graph where the cells are the nodes, and neighboring cells have edges between them. The dominoes, then, correspond to edges in the maximum matching.

The solution is given by the following theorem: "The first player has a winning strategy if and only if all maximum matchings contain the king's node".

This means that no alternating path starting from the king's node ends in a vertex that is not in the matching, because if it ended in a unmatched vertex we could construct another matching with the same number of edges that doesn't use the king's node. Thus the first player could always move along a matched edge.

If there is a maximum matching that doesn't contain the king's node, then in this graph every alternating path that starts from the kings node ends in a matched node, otherwise we could increase the size of the matching which contradicts the assumption that the matching is maximum. Thus now the second player can always move in the second node of the matching edges. So the second player has a winning strategy.

The graph in our problem is not bipartite so we can't use the standard algorithm for bipartite matching. Instead we use a solution that combines brute force and dynamic programming. Our solution will be exponential rather than polynomial, but the size of the problem is small so we can afford it. We go row by row from left to right, and for each cell (i, j) and each subset S of {0, 1, .. n - 1} we compute the best matching that has the following nodes matched already: nodes on the active line ([i][0..j], [i-1][j+1 .. n - 1]) that correspond to the numbers in S, and matched nodes from before the active line.

Here's Derek Kisman's code that implements this solution:

#include <algorithm>
#include <iostream>
using namespace std;

int gx, gy;
char g[15][15];

char memo[15][15][1<<16];
char doit(int x, int y, int b) {
  if (x == gx) {
    x = 0;
    if (++y == gy) return 0;
  }
  char& ret = memo[x][y][b];
  if (ret != -1) return ret;
  int b2 = (b<<1) & ((1<<(gx+1))-1);
  if (g[y][x] != '.') {
    ret = doit(x+1, y, b2);
  } else {
    ret = doit(x+1, y, b2+1);
    if (x && (b&1)) ret >?= 1 + doit(x+1, y, b2-2);
    if (x && (b&(1<<gx)))
      ret >?= 1 + doit(x+1, y, b2);
    if (b&(1<<(gx-1)))
      ret >?= 1 + doit(x+1, y, b2-(1<<gx));
    if (x < gx-1 && (b&(1<<(gx-2))))
      ret >?= 1 + doit(x+1, y, b2-(1<<(gx-1)));
  }
  return ret;
}

main() {
  int N, prob=1;
  for (cin >> N; N--;) {
    cin >> gy >> gx;
    int kx, ky;
    for (int y = 0; y < gy; y++)
    for (int x = 0; x < gx; x++) {
      cin >> g[y][x];
      if (g[y][x] == 'K') {kx = x; ky = y;}
    }
    memset(memo, -1, sizeof(memo));
    int m1 = doit(0, 0, 0);
    g[ky][kx] = '.';
    memset(memo, -1, sizeof(memo));
    int m2 = doit(0, 0, 0);
    cout << "Case #" << prob++
         << ": " << ((m2 > m1) ? 'A' : 'B')
         << endl;
  }
}

EMEA Semifinal 2008 - Code Jam 2008

Painting a Fence (7pts, 13pts)

This was meant to be the easiest problem in the match. A straightforward brute force solution suffices.

Step 1. Pick a set C of (up to) three colors to be used. There are N different colors, so O(N3) such choices.

Step 2. From the offers, filter out the ones where the color is not in the set C. We want to figure out whether the remaining offers can cover the whole fence, and if so, what the minimum required number of offers is.

Step 2 is a classical problem with a greedy scanline solution. We sort the offers (intervals) by their left endpoint, and scan from left to right, considering the offers one by one. At any moment, if we have covered the fence from section 1 to k, we always pick the next offer so that it starts before k and ends as far to the right as possible.

If we sort all the offers by their left endpoing in the beginning, then step 2 takes time O(N), and the whole algorithm runs in time O(N4).

EMEA Semifinal 2008 - Code Jam 2008

Scaled Triangle (9pts, 13pts)

In this problem we are given a triangle and another triangle obtained by applying an affine transformation on the first one -- translation, rotation and scaling. We are asked to find a fixed point for this transformation.

This is actually a particular case of "Bannach's fixed point theorem" which guarantees the existence and uniqueness of fixed points of certain self maps of metric spaces, though knowing the theorem was not a requirement.

To solve this problem, we need to find the parameters of the transformation.

Such transformations are familiar to anyone who has ever played with computer graphics. To view the transformation as a linear operator, it is convenient to use homogeneous coordinates, where our plane is embedded as the plane z=1 in 3D space. i.e., consider each point (x, y) as (x, y, 1). (Note: As usual, all the vectors corresponding to points are considered column vectors, in spite of the horizontal way we write them here.) In this setting, rotating a point by an angle alpha around the point (0, 0) corresponds to multiplying the vector v = (x, y, 1) by the matrix R = [[cos alpha sin alpha 0] [-sin alpha cos alpha 0] [0 0 1]]. Translating a point x, y by (dx, dy) corresponds to multiplying v by T = [[1 0 dx] [0 1 dy] [0 0 1]], and a scaling transform centered at 0 corresponds to multiplying a point v = (x, y, 1) by the matrix S = [[S 0 0] [0 S 0] [0 0 1]]. The total transform looks like this

v' = T R S v = M v.
Note that above we focused on the effect of the transformation on the plane z=1. The interested reader can verify that it maps every horizontal plane z=z0 to itself.

To get the matrix M, we may solve separately for the matrices T, R, and S. There is an easier way. From the input constraints, we know that point A is mapped to A', B to B' and C to C'. View each point as a vector in the plane z=1, so that A, B, C are linearly independent. So the 3 by 3 matrix [A B C] is invertible. Therefore,

M [A B C] = [A' B' C']
has a unique solution for M.

From here, there are still two solutions to our problem. One observation is that if we apply the transformation and we have a point that doesn't change, we can apply it again on the resulting triangle and the point will remain the same. So we apply this transformation until the triangle becomes very small. The fixed point will still be inside the triangle so we can stop when the side lengths of the current triangle get smaller than the needed precision.

The more algebraic solution looks at the equation again. We know there is a unique point v = (x, y, 1) such that M v = v. From here we know that (i) 1 must be an eigen-value for the matrix M; (ii) the space of the eigen-vectors corresponding to 1 must be one-dimensional.
So we can just solve the equation [M - I] v = 0, and find the intersection of the solution space (a line) and the plane z=1.

More information
Banach fixed point theorem - Homogeneous coordinate - Eigen value, eigenvectors, and more

EMEA Semifinal 2008 - Code Jam 2008

Rainbow Trees (9pts, 15pts)

Pick any vertex r. We view the tree as rooted at r, and -- as usual -- draw the tree with the root at the top, and draw the nodes of depth d at the same level, d units below the root.

By a partial coloring on a subset of edges we mean the assignment of colors to the edges in the subset so that the "rainbow coloring" constraint is satisfied on that subset. We do not care about the coloring of the edges that are not in the subset.
For each node x, we define the value

f(x) := number of ways to color the subtree rooted at x, given any partial coloring for the set of edges that are incident to the parent of x.
It is not at all obvious that f(x) is well defined. (Why would the number always be the same for any given partial coloring?). To see that f(x) is indeed well defined, let's look at an algorithm for computing it.

Suppose z is the parent of x, and the degree of z is D. Note that the rainbow constraint is a very local condition -- the color of any edge other than those incident to z does not affect the coloring for the subtree rooted at x.

Assume that x has t children -- y1, y2, ..., yt. To color all the edges in the subtree rooted at x, we do the following.

Color the edge x y1. There are k - D choices, since this edge cannot have the same color as any of the D edges incident to z, and no other edge in the partial coloring puts any constraint on it.
Color the edge x y2. There are k - D - 1 choices, since this edge cannot have the same color as any of the D edges as above, nor the same color as x y1.
...
Color the edge x yt. There are k - D - t + 1 choices.
Now we have a partial coloring where all the edges incident to x are colored. We color the subtree rooted at y1. There are f(y1) ways.
...
There are f(yt) ways to color the subtree rooted at yt.
Indeed, the computation only depends on D, the degree of the parent of x. f(x) is the product of the numbers above.

There is nothing very special about the root r, except that we need to agree D = 0 in that case. And the solution to our problem is just f(r).

EMEA Semifinal 2008 - Code Jam 2008

Bus Stops (8pts, 26pts)

The problem can be solved by using dynamic programming and matrix multiplication for solving linear recursive sequences.

Consider a configuration of buses that are within a window of width P and advance the leftmost bus in such a way that all of the buses are still within a (shifted) window of width P.

Now we can define a state as the position of the buses within P units. For instance if we have P = 10 and K = 5 buses, then we have 252 possible states (10 choose 5). Let NS be the number of states.
To be sure we don't count the same state in different windows we can required that there always be a bus at the leftmost position in the window.

To advance the window of size P we move the leftmost bus. There is always a bus there. Now remember that the new state has to have a bus at the leftmost position, so the window might move to the right by more than one spot.

We compute all the possible state transitions. Let C be the matrix of state transitions having Ca,b be the number of ways we can go from state a to state b.
For each state j, with the window in the position i we will have something like:

A[Sj][i+1] = C1,jA[S1][i] + C2,jA[S2][i] + ... + CNS,jA[SNS][i]
A[s][p] is the number of ways we can get into state s while having the window in position p.

This would be enough to solve the small input. For the large input we need to speed things up, so we are going to compute that linear recurrence of order NS by using matrix multiplication. Let's look instead at a much simpler example of a linear recurrence -- Fibonacci numbers.

FN = FN-1 + FN-2
This can be rewritten as:

(1 0) * (FN-2) = (FN-1)
(1 1)   (FN-1)   (FN  )
In a shorter form we have:

M * VN-1 = VN

This says that to compute VN = (FN, FN-1) from VN-1 = (FN-2, FN-1) we need to multiply with M. Now we can apply this recursively:

MX * V0 = VX
MX can be computed using an algorithm called successive squaring in time that is logarithmic in X.

World Finals 2008 - Code Jam 2008

Juice (3pts, 10pts)

In this problem, we need to find the best (A*, B*, C*) such that there is a maximum number of (A, B, C) triplets in the input satisfying:

A ≤ A*, B ≤ B*, C ≤ C*, and A* + B* + C* ≤ 10000.
It is easy to see that we can just consider integer A*, B*, C*'s. In fact C* can be one of the C's from the input, so are A* and B* -- otherwise we can just decrease it until it hits some C for a satisfied customer.

So we have at most 5000 possible candidate values for C* (or 10000, if you don't want to use the above observation). We try each of them. And the problem is nicely visualized in 2-dimensional grid.

For a fixed C*, we know A* + B* ≤ 10000 - C*. We filter out all the inputs that have

C ≤ C* or A + B ≥ 10000 - C*,
and view the remaining as points in the 2-d plane, with their A, B as the coordinates. For the best solution (A*, B*), we can just try all the integer points (10001 - C* of them) on the line A* + B* = 10000 - C*. For each point, we need to know quickly how many input points are dominated by that point, i.e., lying in the axis-parallel rectangle between the origin and that point.

The last step must be computed fast enough to meet the time limit of the competition. Assume we move the point (A*, B*) from the top-left to bottom-right. In a certain step, we are at (A', B'), with Q points dominated by it. In the next step, we are at (A' + 1, B' - 1), the number of points dominated by the new point can be computed as

Q' = Q - HB' + VA' + 1,
where VA is the counter of points on the A-th vertical line, and HB is the number of points on the B-th horizontal line. Q' can be computed in constant time if we pre-compute the counters.

Solution from the judges:

int T, n, ans;
int A[5000], B[5000], C[5000], H[10001], V[10001];

int main() {
  cin>>T;
  for (int t=1; t<=T; t++) {
    cin>>n;
    for(int i=0; i<n; ++i) cin>>A[i]>>B[i]>>C[i];

    int ans = 0;
    for (int CC=0; CC<=10000; ++CC) {
      memset(H, 0, sizeof(H));
      memset(V, 0, sizeof(V));
      for (int i=0; i<n; ++i)
        if (C[i]<=CC && A[i]+B[i]+CC<=10000)
        { V[A[i]]++; H[B[i]]++; }
      int Q = 0;
      for (int AA=-1; AA<10000-CC; ++AA) {
        Q = Q + V[AA+1] - H[10000-CC-AA];
        ans >?= Q;
      }
    }

    cout<<"Case #"<<t<<": "<<ans<<endl;
  }
  return 0;
}

World Finals 2008 - Code Jam 2008

Ping Pong Balls (4pts, 11pts)

Special Cases
The small dataset can be solved by a brute force. With algorithms like breadth first search (BFS), one can find all the points that are triggered.

Another case we can use BFS is when the two displacement vectors are collinear. In this case all the points we need to consider are in a line, and there will be no more than 1000000 points on any line.

General Case
Interestingly, for the general case, where the two vectors are not collinear, one can use a shorter program with a simpler data structure. Here we introduce a solution that only involves vector additions. This is not the most efficient solution, but it is good enough for our purposes.

Note: This is not the first time we've encountered a 2-dimensional integer lattice where it is better to change the coordinate system. See the similar picture we have in the analysis for Problem D in online round 3.

Let us use [x, y] to denote the points in the ordinary coordinate system, and (a, b) to denote the points in the new system. Suppose the two displacement vectors are V1 = [δx1, δy1], V2 = [δx2, δy2], Suppose that the first ball hits position P = [x0, y0]. The points in the new system is given by

(a, b) := P + aV1 + bV2 = [x0 + aδx1 + bδx2, y0 + aδy1 + bδy2]. (*)
The problem is to figure out, from (0, 0), how many points will be hit by repeatedly adding (1, 0) or (0, 1).

It is not hard to prove, using (*) and the fact that the two vectors are not collinear, that for any points (a, b) inside the room, the numbers a and b are bounded by some quantity Q, where Q is the size of the room times the maximum value of the δs. With the limits in our problem, Q ≤ 2 × 107.

For any fixed a, it is easy to see that there is a contiguous sequence of numbers b such that (a, b) is hit. In other words, there are numbers ba and b'a such that (a, b) is hit if and only if ba ≤ b ≤ b'a. This is because, if we let ba and b'a be the minimum and maximum b that get hit, respectively, then, by convexity, (a, b) is inside the room for all b between ba and b'a. Once we hit (a, ba), we keep adding (0, 1) and get all the other points. It is clear that (a, b'a) is the last point that stays in the room, and (a, b'a + 1) is outside the room.

It is good enough if we can iterate over a = 0, 1, 2, ..., (at most Q), and quickly compute ba and b'a for each a. Notice that, by definition, (a, ba - 1) is not hit. In order to hit (a, ba), the last step must be from (a - 1, ba). So

ba-1 ≤ ba ≤ b'a-1.
To find ba, we can simply start from ba-1 and keep increasing until we hit a point inside the room. Note that for a single a this might take a long time. However, the ba's are monotone, so the cost never exceeds Q.

Once we find ba, we can use a binary search to find b'a. Although a binary search is simple and quick enough, another approach seems dumber but actually works faster. Similarly to the way we get ba from ba-1, we may also get b'a from b'a-1. Here the b'a are not monotone, so we need to try both increasing and decreasing. Nevertheless, based on the rectangular shape of the room, the direction will not change more than once. The total cost is still O(Q).

Sample code from the judges for the non-collinear case:

int T, W, H;
int x, y, dx1, dx2, dy1, dy2;

bool inside(int a, int b) {
  int xx = x + a*dx1 + b*dx2;
  int yy = y + a*dy1 + b*dy2;
  if(xx<0 || xx>=W) return false;
  if(yy<0 || yy>=H) return false;
  return true;
}

long long play() {
  long long ans=0;
  int b1=0; int b2=1000001;
  for(int a=0; ; a++) {
    while(!inside(a, b1)) {
      b1++;
      if(b1>b2) return ans;
    }
    if(inside(a, b2)) {
      while(inside(a, b2)) b2++;
      b2--;
    } else {
      while(!inside(a, b2)) b2--;
    }
    ans+=(b2-b1+1);
  }
  return 0;
}

World Finals 2008 - Code Jam 2008

Mine Layer (4pts, 13pts)

For the convenience of discussion, we denote ci,j as the input number at the position (i,j), i.e., the number of mines in the 3 by 3 square centered at (i,j). In the solution below we will see that, any solvable square must have a unique answer for the number of mines in the middle row.

(1) The number of mines in any 3 rows and 3K columns
In the picture below, sum up the ci,j's of the starred positions.

(2) The number of mines in any 3 rows and C columns
If C is not a multiple of 3, based on C mod 3, use one or two squares on the boundary.

In summary, to get the number of mines in any three consecutive rows, we look at the middle row, start from either the first or second square, and mark every 3rd square. Let the middle row be the a-th row. We denote this sum by Fa.

Also note that with exactly the same method, F0 gives the number of mines in the first two rows, and FR-1 does the same for the last two rows.

(3) The number of mines in the whole board
Similarly, based on R mod 3, start from a = 0 or 1 and sum up the Fa for every 3rd number.

(4) The number of mines in the first 3K or 3K+2 rows
We omit the picture here. It is similar to the pictures above. We either group the first 2 rows together, or the first 3. By symmetry, we can also get the number of mines in the last 3K or 3K+2 rows.

(5) The number of mines in the middle row
Let h = (R - 1) / 2. There are h rows above the middle row, as well as h rows below.

If h mod 3 is 0 or 2, we can get the sum of those 2h rows from step (4) and subtract it from the total number of mines from step (3).

If h mod 3 is 1, we can get the sum of the first h+1 rows from step (4), as well as the sum of the last h+1 rows. Only the center row is counted twice, so we can subtract the sum by the total number of mines from step (3).

Solution from the judges
int T, R, C;
int c[100][100];

int SumRowsCentered(int a) { // F(a) as in the discussion.
  int r=0;
  for(int i=(C%3)?0:1; i<C; i+=3) r+=c[a][i];
  return r;
}

int play() {
  int i;
  // Get the total.
  int total=0;
  for(i=(R%3)?0:1; i<R; i+=3) total+=SumRowsCentered(i);
  // Get the answer.
  int h=(R-1)/2; int S=0;
  if (h%3==1) {
    for(i=h-1;i>=0;i-=3) S+=SumRowsCentered(i);
    for(i=h+1;i<R;i+=3) S+=SumRowsCentered(i);
    return S-total;
  } else {
    for(i=h-2;i>=0;i-=3) S+=SumRowsCentered(i);
    for(i=h+2;i<R;i+=3) S+=SumRowsCentered(i);
    return total-S;
  }
  return 0;
}

int main() {
  int i,j,k;
  cin>>T;
  for (i=1; i<=T; i++) {
    cin>>R>>C;
    for(j=0;j<R;j++) for (k=0;k<C;k++) cin>>c[j][k];
    cout<<"Case #"<<i<<": "<<play()<<endl;
  }
  return 0;
}

World Finals 2008 - Code Jam 2008

Bridge Builders (8pts, 17pts)

This problem seems intimidating at first glance. There are many different routes between the islands and the costs are somewhat unintuitive. However, the problem is actually a disguised Minimum Spanning Tree between the forests; and the nice thing about MSTs is that they are very friendly to greedy approaches. Almost any reasonable greedy approach that focuses on connecting up the forests will get the right answer.

It is fairly easy to convince yourself that the correct approach to the Small dataset is to head directly for the other forest; it can't possibly hurt, since all the other islands pay their minimum possible cost (ie, the distance to the nearest forest). In fact, assuming that islands pay their minimum cost is key: you can ignore this "base" cost, and then the only time you actually have to pay "extra" is when connecting up the two forests.

So for the Large dataset, once again you ignore the "base" cost for each island and simply focus on reaching all the forests, at which point you're done. Based on this intuition, you might immediately think of building a MST across the forests. The "extra" cost of connecting forests increases with distance, so use Prim's Algorithm: always head towards the forest nearest to the forests you've already visited. After connecting all the forests, you can build minimal-cost bridges to the remaining islands.

This greedy approach always works, and is quite an intuitive answer when you think of the problem in these terms. However, the proof that it works turns out to be quite technical. The problem is that you have to be sure that the forest connections really do form a graph with well-defined, order-independent costs. If your choice of path between one pair of forests could somehow help you save cost later, this would be incorrect (and the specter of NP-Completeness would loom menacingly). It seems reasonable that this can't happen, especially if you try a few examples yourself. But writing a proof in contest time is probably impossible. In fact, it took us several days of collaboration to come up with a proper, correct proof. :) This is what you have to deal with as a competitor in the Finals of the Google Code Jam!

Here's an outline of this proof:

First, suppose that we already have a tree (no cycles) of bridges which connects up every island. We show that the cheapest way we could have built these bridges is given by our MST algorithm (where distances are measured along the given tree). Assume we have some cheapest ordering of the bridges. First, we can reorder at no cost to ensure that it consists of direct sequential paths from a previously-visited island to a forest. Now suppose there is ever an island A from which we build two bridges, A-B and A-C. Further, suppose A-B leads to a closer island than A-C, but we build A-C first. Finally, suppose this is the last such occasion; so we build directly to the closest islands in B's subtree and in C's subtree.

Let w be the distance, when A-C is built, from A to its nearest forest along built bridges. Let x be this distance when A-B is built (so x <= w). Let y be the distance from A to its closest island in B's subtree, and z to the closest island in C's subtree (so y <= z). Then the cost of building these A-C and A-B paths is (w+1 + w+2 + ... + w+z) + (x+1 + x+2 + ... + x+y). Instead, if we do the A-B path first and leave A-C and its subtree until later, we pay at most (w+1 + w+2 + ... + w+y) + (x+1 + x+2 + ... + x+z), and possibly less since intermediate steps may also cost less. The second subtracted from the first is (w+y+1)-(x+y+1) + (w+y+2)-(x+y+2) + ... + (w+z)-(x+z) = (z-y)*(w-x), which is non-negative.

Thus, without increasing the cost, we can rearrange the order of bridges to build A-B first. Repeating this, we determine that one cheapest way to build the tree of bridges is always to head for the nearest forest first, which is what our algorithm does.

Now, for any graph (a set of islands with a set of possible bridges), we prove inductively that our algorithm always gives the cheapest cost. It's true for 1 island; assume it's true for k. Suppose we have a minimal plan to connect up k+1 islands. Let A-B be the final bridge built. B could not have been visited already; so consider this plan restricted to the first k islands. Inductively, we know this plan is no cheaper than our algorithm, when run on those k islands; furthermore -- a nice fact about our algorithm, if you have not noticed yet -- after running our algorithm A is as close as possible to a forest. So doing this then building A-B is also a cheapest plan, and, importantly, this forms a tree of bridges. Let's call it T.

Now, from above, we know the cost of building this tree of bridges: it's just the total cost based on running our algorithm on the forest-to-forest distances in T. But these distances are at least as large as the corresponding forest-to-forest distances along all the potential bridges between our k+1 islands. We've paid at least as much as a normal spanning tree on our forests - so at least as much as a MST. But our algorithm achieves this MST cost, which we now know to be minimal.

QED.

Interestingly, this result also holds for any graph of islands and potential bridges, not just a grid or a planar graph. And the distance-to-cost metric can be any nondecreasing function. These facets of the problem are artificial and not essential to the algorithm.

Finally, here is pseudocode for the solution, assuming you know any standard MST algorithm.

for each unordered pair of forests (a, b)
  x = distance(a, b)
  y = x / 2  // Assume integer division
  c = x * (x + 1) / 2  // Add cost of connecting forests
  c -= y * (y + 1)  // Subtract "base" cost of islands
  if x is even
    c += y // Avoid double-counting middle island
  add edge (a, b) at cost c to forest_graph

result = min_cost_spanning_tree(forest_graph)

for each island x
  d = INFINITY
  for each forest y
    d = min(d, distance(x, y))
  result += d  // Add "base" cost of island x

World Finals 2008 - Code Jam 2008

The Year of Code Jam (7pts, 23pts)

The different appearances of perimeter
In spite of the nice story about the calendar and the fancy definition of happiness, we hope that you have discovered the following abstract description of this problem.

On a board of N by M unit squares, some squares are blue, some are white, and some are undecided. For each undecided square, color it either blue or white, so that the total perimeter of blue region is maximized.
We see that the perimeter can be defined in two ways.

(a) The sum of the contributions from each blue square, as indicated in our problem statement.
(b) The sum of the contributions from each unit segment. Count 1 for each unit segment that separates two squares with different colors. (Assume all the squares outside the board are white.)
The second interpretation is useful for the following analysis.

There are some simple observations that seem useful. For example, for any undecided unit square, if we have already decided that two of its neighbors are blue, then in one optimal solution we can assign it to be white. Yet, as far as we know, no such heuristics give a complete and fast solution to our problem.

A similar problem
Let us discuss a problem that at least looks similar to ours. What if we wanted to minimize the perimeter instead of maximizing it?

We rephrase the problem in terms graph theory. Let the set of NM unit squares be our vertices, and let there be an edge between two adjacent squares. Our task is to color each undecided vertex either blue or white, so that the number of edges (in the graph) between the blue vertices and the white vertices is minimized.

This is rather nice, isn't it? If you add a source s and a destination t, add one edge from s to every blue vertex with infinite capacity (4 is enough) and one edge from every white vertex to t with infinite capacity, then the problem is asking for a minimum cut from s to t, which can be solved by your favorite max flow algorithm.

Solving the original problem
While minimum cut is polynomial-time solvable by max flow, the max-cut problem is too hard in general graphs. Therefore, our problem still seems much harder than the minimization version of the problem.

However, we have not used an important fact yet. Our graph is far from arbitrary. We play this game on the N by M board; the resulting graph is bipartite (imagine it as a chessboard). Maximizing the number of edges between different colored vertices is the same as minimizing the number of edges between vertices of the same color. In other words, if we flip the colors of one half of the chessboard, the problem reduces to the minimization version we discussed above.

We mark the board in a chessboard fashion, label the unit squares as odd or even. Then we flip the color of all the even squares. Now look at property (b). A contribution becomes a non-contribution and vice versa. The total number of edges is fixed, so the maximization problem reduces to its minimization version.

Qualification Round 2009 - Code Jam 2009

Alien Language (10pts, 23pts)

First we store all the words in a 2 dimensional array.
After that, we read each pattern, parse it, and count how many words match.

One possible way of storing a pattern is a 2 dimensional array P[15][26]. P[i][j] is True only if the i-th token contains the j-th letter of the alphabet, otherwise False. In other words, P[i] is a bitmap of the letters contained by the i-th token.

Parsing can be done like this:
- read one character c
- if c is '(', read characters until we hit ')'. The characters read are the token.
else the token is the character c
- populate P[i] for the characters in the token

To count how many words match, we make sure that each letter i from the word is contained in the bitmap P[i].

Total complexity is O(N * L * D).

In some programming languages this can solved by transforming the pattern into a regular expression. For instance in python replace '(' and ')' with '[' and ']'.

Qualification Round 2009 - Code Jam 2009

Watersheds (10pts, 23pts)

For each cell, we need to determine its eventual sink. Then, to each group of cells that share the same sink, we need to assign a unique label.

The inputs to this problem are small enough for a simple brute-force simulation algorithm. Start with a cell and trace the path that water would take by applying the water flow rules repeatedly. Here is one possible solution in Python.

import sys

def ReadInts():
  return list(map(int, sys.stdin.readline().strip().split(" ")))

def Cross(a, b):
  for i in a:
    for j in b:
      yield (i, j)

def Neighbours(ui, uj, m, n):
  if ui - 1 >= 0: yield (ui - 1, uj)
  if uj - 1 >= 0: yield (ui, uj - 1)
  if uj + 1 < n: yield (ui, uj + 1)
  if ui + 1 < m: yield (ui + 1, uj)

N = ReadInts()[0]
for prob in xrange(1, N + 1):
  # Read the map
  (m, n) = ReadInts()
  maze = [ReadInts() for _ in xrange(m)]
  answer = [["" for _ in xrange(n)] for _ in xrange(m)]

  # The map from sinks to labels.
  label = {}
  next_label = 'a'

  # Brute force each cell.
  for (ui, uj) in Cross(xrange(m), xrange(n)):
    (i, j) = (-1, -1)
    (nexti, nextj) = (ui, uj)
    while (i, j) != (nexti, nextj):
      (i, j) = (nexti, nextj)
      for (vi, vj) in Neighbours(i, j, m, n):
        if maze[vi][vj] < maze[nexti][nextj]:
          (nexti, nextj) = (vi, vj)

    # Cell (ui, uj) drains to (i, j).
    if (i, j) not in label:
      label[(i, j)] = next_label
      next_label = chr(ord(next_label) + 1)
    answer[ui][uj] = label[(i, j)]

  # Output the labels.
  print "Case #%d:" % prob
  for i in xrange(m):
    print " ".join(answer[i])

Qualification Round 2009 - Code Jam 2009

Welcome to Code Jam (10pts, 23pts)

In this problem, every one is welcomed by a (somehow) standard dynamic programming problem.

The word we want to find is S = "welcome to code jam", in a long string T. In fact the solution is not very different when we want to find any S. It is actually illustrative to picture the cases for short words.

In case S is just a single character, you just need to count how many times this character appears in T. If S = "xy" is a string of length 2, instead of brute force all the possible positions, one can do it in linear time, start from left to the right. For each occurrence of 'y', one needs to know how many 'x's appeared before that 'y'.

The general solution follows this pattern. Let us again use S = "welcome to code jam" as an example. The formal solution will be clear from the example; and you can always download the good solutions (with nice programming techniques) from the scoreboard.

So, let us define, for each position i in T, T(i) to be the string consists of the first i characters of T. And write

Dp[i,1]: How many times we can find "w" in T(i)?
Dp[i,2]: How many times we can find "we" in T(i)?
Dp[i,3]: How many times we can find "wel" in T(i)?
Dp[i,4]: How many times we can find "welc" in T(i)?
...
Dp[i,18]: How many times we can find "welcome to code ja" in T(i)?
Dp[i,19]: How many times we can find "welcome to code jam" in T(i)?
Assume Dp[i,j] is computed for each j, let us see how easy we can compute, say, Dp[i+1,4]:
If the (i+1)-th character of T is not 'c', then Dp[i+1,4] = Dp[i,4].
If the (i+1)-th character of T is 'c', then we can include all the "welc"s found in T(i), as well as those "welc"s ends exactly on the (i+1)-th character, so Dp[i+1,4] = Dp[i,4] + Dp[i,3].
Finally, let n be the length of the text T, Dp[n,19] will be our answer.
That's it. Welcome to Code Jam; and we hoped you enjoyed this round.

Round 1A 2009 - Code Jam 2009

Multi-base happiness (9pts, 18pts)

How can you tell if a number is unhappy? You can tell if it enters a cycle when you apply the process described in the problem. Here is one example, in base 10:

42 → 20 → 4 → 16 → 37 → 58 → 89 → 145 → 42 repeated
Naturally, x → y denotes that y is the next number if we apply the process on x. Starting from any number, if you apply the process, eventually you will reach 1 or enter one of those cycles.
But is that really true? A careful reader might ask: Couldn't the process keep hitting new numbers, and never enter a cycle? Look at the cycle for 42: Before you hit 42 again, the numbers just jump around without a clear pattern.

It turns out that this can never happen: that there are only finitely many such cycles, and they're all finite in length. In fact, in base 10, this is the only one! All the numbers involved in such a cycle must be reasonably small. Indeed, when you start a number that is big (think about 99999..9999), applying the process will lead to numbers that become smaller and smaller very rapidly. One can easily prove that, in any base B, there is a threshold H of O(B3) such that any number larger than H will have a smaller successor.

Another question is: Given a set of bases, does there exist a number that is happy in all of them? We don't know the answer to that question, but based on our computation we know such numbers exist for all bases up to 10. On the other hand, if you intuit that the property of a number being happy is somehow random, and somehow independent across different bases, then you can believe that multi-base happiness is rare, and that the density of such numbers decreases exponentially with the number of bases. In our problem. The smallest happy number for the input (2, 3, 4, 5, 6, 7, 8, 9, 10) is 11814485; just barely affordable with a brute-force search.

In the computation, one obvious trick is to cache, for a pair (x, B), whether x is happy in base B, so we can avoid following the jumping sequence or the cycles every time. We only need to do this for small values of x -- for example all x ≤ 1000 would be more than enough -- since any 32-bit integer bigger than 10000 becomes smaller than 1000 in one step.
Since the maximum base is 10, one may realize that there are only 29 - 10 = 502 possible distinct inputs. So why don't we just calculate them all? This can actually be faster than solving the input cases one by one, if we solve the smaller sets first. For a set S of bases, we do not need to start the search from 2; we can start from the answer for any S' ⊂ S, since a number happy in all bases from S is at least happy in all bases from S'.

If you think your implementation is still not fast enough. Then run your program while you are solving other problems. There are only 502 possible input cases. Solve all of them, produce the list of answers, and then start the submission process; just don't forget you also need to submit the slow program that produced the list. This is why we had a special note at the bottom of the problem statement.

More Information
Wikipedia article: Happy Numbers

Round 1A 2009 - Code Jam 2009

Crossing the Road (13pts, 20pts)

A grid of roads; a person moving from place to place; this problem has all the hallmarks of a graph problem, and a shortest path problem at that. Our pedestrian is trying to get from one corner of the map to the opposite corner. Some steps that she takes can be made at any time, and take 2 minutes; for others, she has to wait until the light turns green, and then take another minute. How can we minimize the time she takes to reach her goal?

To solve this problem, we need to describe the state of the world. The pedestrian can be in any of 2m positions in the x direction, in any of 2n positions in the y direction, and at some time t. Since the pedestrian can always just wait if she needs to, she would always prefer to be at (x, y, t) over (x, y, t+1); so an algorithm for computing the earliest time at which the pedestrian can arrive at (x, y) will solve this problem.

This problem differs from a standard graph problem in that, for two neighboring locations across a road from one another, the weight of the edge between them is not fixed. As it turns out this is only a minor complication, and we can use slightly modified versions of some standard shortest-path algorithms.

From each location, the pedestrian can go north, south, east or west, unless the direction in question is off the edge of the map. The amount of time this takes will be 2 minutes if it's along a block, or (the amount of time until the traffic light is green) + 1 minute if it's to cross a road.

Bellman-Ford is probably the easiest algorithm to implement for this. The longest path will take at most O(m+n) steps, so your algorithm will terminate in at most that many stages. At each stage, for each point, you'll try to update its (up to) four neighbors. The running time is O(nm(n+m)).

Dijkstra's algorithm is usually more efficient. With 4nm states and 4 directions in which the pedestrian can move from each state, this algorithm will complete in O(nm log(nm)) time.

The correctness of either algorithm can be proved in almost the same way as its standard counterpart. We leave these proofs as worthy exercises for the reader.

More information:
Dijkstra's algorithm - Bellman-Ford algorithm

Round 1A 2009 - Code Jam 2009

Collecting Cards (10pts, 30pts)

This problems requires some basic knowledge of probability and combinatorics. We want to calculate the expected number of packs we need to buy to obtain all C different cards.

Let's denote by E(x) the expected number of packs we would need to buy if we started with x different cards (it doesn't matter what those cards are). The answer to the problem is the value for E(0). We also know that E(C) = 0, because if we already have C different cards we don't need to buy any additional packs.

We can derive useful equations for other values of E(x) by thinking about all the possible outcomes after buying one additional pack. Let's call T(x,y) the probability of ending up with y different cards after opening a new pack. Then we have the following equation for E(x):

We need to buy at least one new pack, so that's where the 1 comes from. The expected number of packs we need to buy after that depends on how many new cards we get. If we end up with y different cards we need to add the expected number of packs to reach C starting from y, which we called E(y), multiplied by the probability of this particular alternative given by T(x, y).

All these equations put together form a system of linear equations with an upper triangular matrix which can be solved using the standard back substitution method.

But we still haven't said how to calculate the entries of the matrix T (that is, the values of T(x, y) for all different x and y). We'll calculate this with the help of binomial coefficients: the number of different possible packs is . To end up with y different cards, we need to choose y-x out of C-x possible new cards, and the remaining N-(y-x) have to be chosen from the x cards we already have. The answer then is:

(For those with some knowledge of probability, this is called the hypergeometric distribution).

The special case, where N = 1, is the well known coupon collector's problem.

Round 1B 2009 - Code Jam 2009

Decision Tree (10pts, 11pts)

The hard part here was parsing the tree. An easy way to do this is by using a technique called recursive descent. The tree grammar is defined recursively, so it makes sense to parse it recursively, too. Here is a solution in Python.

import re
import sys
inp = sys.stdin

tokens = None
ti = -1

def ReadInts():
  """Reads several space-separated integers on a line.
  """
  return tuple(map(int, inp.readline().strip().split(" ")))

def NextToken():
  """Consumes the next token from 'tokens'.
  """
  global ti
  assert 0 <= ti < len(tokens)
  ti += 1
  return tokens[ti - 1]

def ParseNode():
  """Parses from 'tokens' and returns a tree node.
  """
  global ti
  assert NextToken() == "("
  node = {"weight": float(NextToken())}
  tok = NextToken()
  if tok == ")":
    return node
  node["feature"] = tok
  node[True] = ParseNode()
  node[False] = ParseNode()
  assert NextToken() == ")"
  return node

def ParseTree(s):
  """Initializes 'tokens' and 'ti' and parses a tree.
  """
  global tokens
  global ti
  s = re.compile(r"\(").sub(" ( ", s)
  s = re.compile(r"\)").sub(" ) ", s)
  s = re.compile(r"[ \n]+").sub(" ", " %s " % s)
  tokens = s[1:-1].split(" ")
  ti = 0
  return ParseNode()

def Evaluate(tree, features):
  ans = tree["weight"]
  if "feature" in tree:
    ans *= Evaluate(tree[tree["feature"] in features], features)
  return ans

if __name__ == "__main__":
  N = ReadInts()[0]
  for prob in xrange(1, N + 1):
    n_lines = ReadInts()[0]
    lines = [inp.readline() for _ in xrange(n_lines)]
    tree = ParseTree(" ".join(lines))
    n_queries = ReadInts()[0]
    print "Case #%d:" % prob
    for _ in xrange(n_queries):
      features = set(inp.readline().strip().split(" ")[2:])
      print "%.7f" % Evaluate(tree, features)
For each test case, we read the 'n_lines' lines containing the tree definition, we glue them together using spaces and pass the resulting string to ParseTree().

In ParseTree(), we do some "massaging" to make the string easier to parse. First, we put spaces around each parenthesis by using two simple regular expressions. Next, we replace each sequence of whitespace characters by a single space and make sure there is always exactly one space character at the beginning and the end of the input. Finally, we strip off the leading and trailing spaces and split the rest into tokens.

The ParseNode() function does the rest. It uses the NextToken() function to read one token at a time from the 'tokens' list and returns a simple dictionary representation of a tree node.

Once we have the tree as a dictionary, we then use Evaluate() to do a tree traversal from the root to a leaf and compute the answer for each input animal.

Using the parsers built into dynamic languages
A number of contestants have noticed that there is an even easier way to parse the tree. Most dynamic, interpreted languages give you access to their built-in parser, and by manipulating the input a little bit, it is possible to use make the language's interpreter do the parsing for you! This means using "eval" in JavaScript or Python, or "read" in Lisp. Check out some of the shortest solutions at 1b-a.pastebin.com/d4631e678.

Round 1B 2009 - Code Jam 2009

The Next Number (9pts, 26pts)

Let x be our input. We want to find the next number y. We denote L(s) to be the length of s, i.e., the number of digits in s.

Case 1. If all the digits in x are non-increasing, for example x = 776432100, then x is already the biggest one among the numbers in the list with L(x) digits. The next number, y, must have one more digit, that is, one more 0. Actually y must be the smallest one with L(x)+1 digits. To get y, we put the smallest non-zero digit in front, and all the other digits are put in non-decreasing order.

Case 2. Otherwise, x can be written as the concatenation x = ab, where b is the longest non-increasing suffix of x, so d, the last digit of a is smaller than the first digit of b. Let d' be the smallest among all the digits in b that are bigger than d.
Because b is non-increasing, x is the biggest number among those who has L(x) digits and starts with prefix a. The first L(a) digits of y must be bigger than a. The smallest we can do is to replace d with d', and then for the rest digits, we arrange them in non-decreasing order.
Let us do another example. x = 134266530. Then a = 1342, b = 66530, d = 2, and d' = 3. The next number is y = 134302566.

In fact, we can unify the two cases above. Since the number of 0's is not restricted, we can just imagine there is one more 0 in the beginning of x, thus Case 1 is reduced to Case 2.

The above described is actually exactly the procedure to get the next permutation of a finite sequence in certain languages. Below is a solution that is essentially one line in C++. From the author:

  deque<char> f;
  ...
  f.push_front('0');
  next_permutation(f.begin(), f.end());
  if (f.front() == '0') f.pop_front();

Round 1B 2009 - Code Jam 2009

Square Math (12pts, 32pts)

The problem would be a lot easier if there were only plus signs used. With the presence of the negative signs, a valid expression might get to a large value in the middle, then decrease back to the value we are looking for. Will that be the shortest answer for a certain query at all?

The best path cannot be long
First of all, let us denote a non-zero digit in the square pos if it has at least one neighbor that is a plus sign; call it neg if it has one minus sign as its neighbor. A digit can be both pos and neg. We assume there are both pos digits and neg digits in the square. Let q be the value we are looking for.
Let g be the gcd (greatest common divisor) of all the digits in the square. If g is not 1, in order to find a solution for q, it must be a multiple of g. Dividing everything by g, we may assume from now on that the gcd of all the numbers in the square is 1. The situation is further simplified when all our numbers are between 0 and 9 -- in this case, the above implies that there must be two digits, a and b, in the square, such that gcd(a, b) = 1.

Case 1. a is pos and b is neg. Take any shortest path P from a to b in the square. Let q' be the value of this path. q' is between [-200, 200]. Let t = q - q'. We prove the case t ≥ 0; the other case is similar and left to the readers.
Since gcd(a, b)=1, one of the numbers t, t+b, t+2b, ..., t+(a-1)b is a multiple of a. This means we can find non-negative numbers x and y such that ax - by = t, where y < a (therefore x < t/a + b). We start from a, since it is a positive digit, we use the plus sign to repeat at a x times, then follow P. After reaching b, we use the minus sign to repeat y times. The path just described evaluates to the query q.

Case 2. Both a and b are pos, there is a neg digit c. If c is co-prime with either a or b, we handle it as Case 1. Otherwise c has to be 6.
Pick any shortest path P that connects a, b, and c. Suppose it evaluates to q'. Pick a non-negative z such that q - q' + 6z ≥ ab - a - b + 1. We use a basic fact in number theory here

If positive integers a and b are co-prime, then for any t ≥ ab - a - b + 1, there exist non-negative integers x and y such that ax + by = t.
To get an answer for query q, we use the path P, repeat a x times with plus sign, b y times with plus sign, and c z times with minus sign.
Case 3. Both a and b are neg. This is similar to Case 2, and we leave it to the readers.

Thus we proved that for any solvable query, there is a path that is not long, as well as any partial sum on the path cannot be too big. The rough estimate shows that any of the paths cannot take more than 1000 steps. One can get a better bound by doing more careful analysis.

The algorithm
Our solution is a BFS (breadth first search). The search space consists all the tuples (r, c, v), where (r, c) is the position of a digit, and denote A(r, c, v) to be the best path that evaluates to v, and ends at the position (r, c). We know there are at most 200 such (r, c)'s (202/2), and at most (much less than) 20000 such v's from the bound we get.

The only difference with a standard BFS is that, because of the lexicographical requirement, we may need to update the answer on a node. But we never need to re-push it into the queue, since it is not yet expanded.

Round 1C 2009 - Code Jam 2009

All Your Base (8pts, 15pts)

IN A.D. 2101
WAR WAS BEGINNING
...
CATS: HOW ARE YOU GENTLEMEN !!
CATS: ALL YOUR BASE ARE BELONG TO US

This problem, of course, takes place in the halcyon days of A.D. 2100, before all our base became belong to Cats. It also firmly demonstrates that its author still lives in A.D. 2001.

In this problem our job is to read a series of symbols and interpret them as digits of a number in some base. In order to minimize the number, we'll want to use the smallest base possible. We can check how many different characters show up in the number; if that number of characters is k, then we can work in base max(k,2) (base 1 is not allowed). From there, it's simply a matter of assigning values from 0 to k to all of the characters. We can't start with a 0, so we should make the first digit a 1; and after that, simply assign the lowest number available to characters from left to right. In that way, "ab2ac999" becomes "10213444" in base 5, or 85499 seconds.

The following is a brief solution in Python:

import sys

N = int(sys.stdin.readline().strip())
for qw in range(1, N+1):
  print 'Case #%d:' % qw,

  num = sys.stdin.readline().strip()
  values = {num[0]: 1}
  for c in num:
    if c not in values:
      sz = len(values)
      if sz == 1:
        values[c] = 0
      else:
        values[c] = sz
  result = 0
  base = max(len(values), 2)
  for c in num:
    result *= base
    result += values[c]
  print result
One final note: on our official Google Group, user Damien pointed out the following:

"The first example case in All Your Base implies the alien language uses a left-right notation. If that assumption is wrong and they actually used right-left you'd be 54 seconds late for the war: 11001001 binary = 201 decimal, but the reverse 10010011 is 147 decimal. Dangerous assumption to make don't you think? :-)"

I hate to say it, but he's right; even though 2219 people solved this problem, we may still be taken by surprise!

Round 1C 2009 - Code Jam 2009

Center of Mass (10pts, 17pts)

Let Pi and Vi be the initial position and the velocity of the i-th firefly; its position at time t is given by Pi + tVi. Therefore, the center of mass at time t is

M(t) = ∑ (Pi + t Vi) / n = ∑ Pi / n + t ∑ Vi / n = Pave + t Vave,
where Pave (= M(0)) and Vave are the average initial position and average speed. Thus we show that the center of mass is also moving on a straight ray (a half line) with constant speed. (One special case is that the average speed could be 0, when the center of mass does not move at all.)
So, the problem is actually finding the closest distance from a point (the origin) to a ray, an elementary geometry problem. This is an easy exercise with many possible short solutions. One can use basic manipulations or calculus to derive the exact formula for the answers. One may also notice that the distance function d(t) is convex in t, thus use trinary search to find the best t. As always, we encourage the readers to download correct solutions from the scoreboard.

Note
We did the calculations using vectors. It can also be carried out with 3-dimensional coordinates.

Reference
The exact formula of the distance from a point to a line in 3d space.

Round 1C 2009 - Code Jam 2009

Bribe the Prisoners (15pts, 35pts)

This problem is solved by dynamic programming. For each pair of cells a ≤ b, we want to compute dp[a][b], the best answer if we only have prisoners in cells from a to b, inclusive. Once we decide the location of c, the first prisoner between a and b to be released, we face the smaller sub-problems dp[a][c-1] and dp[c+1][b]. The final answer we want is dp[1][P].

It is clear we only need to solve those dp[a][b]'s where both a and b are either 1, P, or adjacent to a prisoner to be released. Thus the number of sub-problems we need to solve is just O(Q2).

Here is the annotated judge's solution.

int p[200]; // prisoners to be released.
map<pair<int, int>, int> dp;

// Finds the minimum amount of gold needed, 
// if we only consider the cells from a to b, inclusive.
int Solve(int a, int b) {
  // First, look up the cache to see if the 
  // result is computed before.
  pair<int, int> pr(a, b);
  if(mp.find(pr) != mp.end()) return mp[pr];

  // Start the computation.  
  int r = 0;
  for(int i=0; i<Q; i++) {
    if(p[i] >= a && p[i] <= b) {
      int tmp = (b-a) + Solve(a, p[i]-1) + Solve(p[i]+1, b);
      if (!r || tmp<r) r=tmp;
    }
  }
  mp[pr]=r;
  return r;
}

Round 2 2009 - Code Jam 2009

Crazy Rows (6pts, 10pts)

Reformulation of the problem
It is easy to see, for each row, only the position of the last '1' matters. We can re-formulate the problem

CR: Given a list of numbers (a0, ... aN-1). You are allowed to swap adjacent numbers. Find the minimum number of swaps needed so that the i-th number is at most i in the final configuration.

The well known special case
Perhaps many of you know the following special case.
CR*: Given a permutation (x0, ... xN-1) of the numbers 0 to N-1. You are allowed to swap adjacent numbers. Find the minimum number of swaps needed in order to sort the list in increasing order.
Perhaps you also know the complete solution to CR*. It is very simple and elegant, and important to our problem. So we produce here.
Solution (to CR*). Define the disorder of the list to be the number of pairs i < j, where xi > xj. In one swap (of adjacent numbers), the total number of disorder is changed by exactly one. (Check it!) Therefore, let the disorder of initial configuration be D, you need at least D swaps. D swaps is also sufficient -- as long as the list is not sorted, there exist adjacent pairs in wrong order. You can swap any of such pairs and decrease the disorder by 1.    ◊
In particular,

(1) One type of optimal solutions of CR* involves first to swap the number 0 all the way to the left, then let it stay there forever.

Solution to our problem
Imagine we know which of the ai's will finally go to position 0, and which one will go to position 1, etc., then we can simply use the algorithm for CR*. But there might be multiple candidates for a single position. For example, there might be several i's such that ai = 0, and even some ai = -1.

Below is the judge's C++ solution. b[i] is the "decoded" position where a[i] will be in the final configuration. The algorithm says: For those candidates for position 0, pick the leftmost one. Then in the rest, for those candidates for position 1, pick the leftmost one. And so on.

  // -1 means no position is assigned for a[j].
  for(i=0;i<N;i++) b[j]=-1;
  for(i=0;i<N;i++) {
    for(j=0;j<N;j++) if(b[j]<0 && a[j]<=i) {
        b[j]=i; break;
      }
  }
  int r=0;
  for(i=0;i<N;i++) for(j=i+1;j<N;j++) 
    if(b[i]>b[j]) r++;
  // output r as the answer
Note that, once the b[i]'s are fixed, you only need to count the disorders as in CR*; no real swapping needs to be simulated.

Proof of our solution
The key observation is, for multiple candidates for position 0, you will never need to swap any two of them. Suppose you do swap, say u and v, both are at most 0. I can simply ignore it, and pretend that they are swapped (i.e., exchange the roles of u and v thereafter). The final configuration is still a good one. Thus we proved that, for all the candidates for position 0, the leftmost one, call it u*, will finally go to position 0.

Now, imagine we have decoded the final positions for every one. Then (1) tells us that there is one solution where we first move u* all the way to the left, and never worry about it again. Therefore we now face the next question: Which of the rest of the numbers should go to position 1?
This is exactly the same problem, but with one number fewer.   ◊

Round 2 2009 - Code Jam 2009

A Digging Problem (9pts, 17pts)

The setup of this problem, where you can go left, right and down but never up hints that some dynamic programming solution is the way to go, but the details are a bit involved.

When we're on a row we need to know which are the rock cells that were dug previously so that we know how much we can move left or right. This means that we could have a state in our algorithm be (i, j, air_holes) where i is the current row, j is the current column and air_holes is the set of cells on row i that were dug previously or were empty. Filling the values for these states in the whole matrix would take exponential time as air_holes can take on as many as 2^C values. This would be enough to solve the small input, but for the large we need to improve our algorithm a bit.

First let's observe that it only makes sense to dig out cells that will form a connected empty zone. After we fall one row, we'll be able to use just the current zone of empty boxes. Now our state is (i, j, start, end) where start is the starting column index of the current empty zone and end is the index of the zone's end column. This idea yields a polynomial solution, as we have O(R * C^3) possible states and there are at most C^2 different states that we can create on the next row.

But we can improve on this solution. It doesn't make sense to change directions after we started to dig; if we're moving right, it doesn't matter how many empty cells we have on the left. This changes the state to (i, j, dir, count) where dir is the current direction (left or right), and count is the number of empty cells in that direction. This reduces the state space to O(R * C^2).

The implementation details are somehow tricky, and you have to make sure you don't fall more than F steps -- a detail we've skipped here. There are many possible ways to implement this problem, some of which result in much simpler code than others. We encourage you to download and study various correct implementations from the scoreboard.

Before the contest began, we evaluated this problem as the second easiest in the contest; but the many details needed to solve it resulted in this problem having the second-smallest number of successful solutions.

Something related
If you are among our older contestants, this problem may bring back sweet memories of the classic game Lode Runner, and perhaps memories of many happy days associated with it.

Round 2 2009 - Code Jam 2009

Stock Charts (7pts, 21pts)

It will come as no great surprise, but the author of this problem came up with it while reading a local newspaper's end-of-year economics summary, seeing a number of overlaid charts in it and then wondering how to minimize the number of charts needed.

Charts and DAGs
Consider two simple charts: A and B. They can be related to each other in one of three ways: all of A's values can be strictly less than all of B's values (A < B), in which case they can appear on the same overlaid chart; their lines can cross (A \ B), in which case they can't appear on the same overlaid chart; or all of A's values can be strictly greater than all of B's values (A > B), in which case they can appear on the same overlaid chart.

Given this sort of relationship we can construct a graph, where the nodes are simple charts and there is an edge from A to B iff A > B. This gives us a directed, acyclic graph that is its own (non-reflexive) transitive closure. Any directed path in the DAG represents a valid composite chart. To solve the problem, then, we want to find the minimum number of paths that we need so that all nodes are part of exactly one path.

How would we find the paths? We may start from a chart that is relatively high, then find one below it, and keep adding lower charts, until we cannot find more. This completes our first overlaid charts. We start the same process for the second path, and so on. In any step, there might be several choices for the next chart we can use. In order to minimize the number of paths, we need to make a good choice in each step.

Now, behold, the Aha moment of this problem.

Solution from maximum matching

For the DAG with n points, we make a bipartite graph with n points on each side. Draw an edge from XA to YB if the relation A > B holds, i.e., B can be the next chart below A. Observe (yes, you really need to see it, instead of hear it!) how any path in the DAG corresponds to a series of edges in the bipartite graph; and how any matching of the bipartite graph corresponds to a way to partition the DAG into paths. Any unmatched point XA on the left side corresponds to the lowest point on a path (the lowest chart on an overlaid graph). Each path has exactly one such point. We want to minimize the number of paths, the same as minimizing the number of unmatched points on the left side. That is, we want to find the maximum matching in a bipartite graph.

Here is the judge's solution in C++.

namespace Solver {
int N,K;
bool cbn[111][111]; // can be next
int prev[111];
bool visited[111];

bool FindNextDfs(int a) {
  if(a<0) return true;
  if(visited[a]) return false;
  visited[a]=true;
  for (int i=0;i<N;i++) if(cbn[a][i]) {
      if(FindNextDfs(prev[i])) {
        prev[i]=a;
        return true;
      }
    }
  return false;
}

int Solve(const vector<vector<int> >& stock) {
  N=stock.size(); K=stock[0].size();
  int i,j,k;
  for(i=0;i<N;i++) {
    for(j=0;j<N;j++) {
      if(i==j) cbn[i][j]=false;
      else {
        cbn[i][j]=true;
        for(k=0;k<K;k++)
          if(stock[i][k]<=stock[j][k]) cbn[i][j]=false;
      }
    }
  }
  memset(prev, -1, sizeof(prev));
  int ret=0;
  for(i=0;i<N;i++) {
    memset(visited, 0, sizeof(visited));
    if(!FindNextDfs(i)) ret++;
  }
  return ret;
}
Note that this is indeed the bipartite matching program. We named the variables as if we are really constructing the set of paths and unaware of the bipartite graph. In fact, it's a worthy exercise to go over this without bipartite matching in your mind.
This completes the solution of our problem. But we may continue with more stories.

Theoretical background
In combinatorics, DAGs are called partially ordered sets, or posets. A directed path is called a chain in the poset. An independent set in the DAG, corresponding to a set of points where no '>' relation holds between any two of them, is called an anti-chain. Our problem is then, given a poset, find the minimum number of chains needed to cover all the points.
If we see an anti-chain of size α, we need at least α chains to cover the set, because each chain can contain at most one of these points. Suppose we find the maximum anti-chain to be of size α*, we know the answer must be at least α*. Is this enough though?
We are ready to introduce one of the classical theorems in combinatorics.

Theorem (Dilworth 1950) In a poset, the minimum number of chains needed to cover the whole set equals the size of the biggest anti-chain.

Dilworth's theorem is closely related to other classical theorems in combinatorics. In fact it is equivalent to Hall's marriage theorem on bipartite graphs, and the max-flow-min-cut theorem.

The number in Dilworth's theorem is (naturally) called the width of the poset. Our algorithm above thus finds the width of a poset. Interested readers might find, in our input file, perturbed copies of the following posets:

The complete Boolean lattice: All the 2k subsets of a k-element set, where A > B if B is a subset of A. All the ⌈k/2⌉-subsets form a maximum anti-chain of size (k choose ⌈k/2⌉), and indeed we can partition the Boolean lattice into this many chains. (This is called the Sperner's theorem.)
All the integers from 1 to n, A > B if A is a multiple of B. The set of all the prime numbers seems to be a big anti-chain. But it is not big enough. We leave it as an exercise to prove that the width of this poset is ⌈n/2⌉.
More information
Partially ordered set - Dilworth's theorem

Round 2 2009 - Code Jam 2009

Watering Plants (5pts, 25pts)

This problem involves finding circles that enclose other circles. The problem of finding a circle surrounding a set of points is the fairly well-known minimal enclosing circle problem, but changing the points to circles makes the problem slightly trickier.

One solution is to do a binary search to find the minimum sprinkler radius. This reduces the problem to the problem of determining whether two sprinklers of a given radius can cover all the plants. To solve this, we can make the assumption that any sprinkler used in the solution either:

covers exactly one plant, or
the boundary of the sprinkler touches the boundary of at least two of the plants it covers.
This assumption is safe because if a sprinkler covers more than one plant but does not have two plants on its boundary, the sprinkler can be shifted and rotated, while still covering the same plants, until it does. Given this assumption, we can create a set of candidate sprinkler locations including:
a sprinkler centered on each plant, and
for each pair of plants, the set of sprinklers covering those plants and touching their boundary (there are 0, 1, or 2 of these per pair.)
Then we check every pair of candidate sprinklers, and see if any of them together cover every plant.
A second solution is to directly find the minimum sprinkler radius. To do this, we can use a slightly different simplifying assumption -- that every sprinkler either:

covers exactly one plant (using the same radius as the plant),
covers two plants which touch the edge of the sprinkler and whose centers are collinear with the center of the sprinkler, or
covers three plants which touch the edge of the sprinkler.
This assumption is safe because any other sprinkler can be shrunk to a sprinkler of smaller radius that covers the same set of plants. We try each set of plants of size 1,2 or 3, create the corresponding sprinkler from the three cases above, and check its radius and which set of plants it covers. Then we find the pair of sprinklers that covers every plant using the minimum maximum radius.
Finding the circle which touches 3 given circles is harder than the equivalent problem for 3 points. Here are three possible approaches:

The set of points where a sprinkler can be centered in order to touch two plants is a hyperbola, so we could algebraically compute the intersection of two of those hyperbolae.
We can use a gradient-descent approach to find numerically the point minimizing the function from potential centers of sprinklers to the radius required for a sprinkler centered at that location to cover all three plants.
We can subtract from the radius of each of the three plants the radius of the smallest plant, then compute an inversion about that plant's center. Then we find appropriate tangents to the two inverted plants, re-invert to find the corresponding circle, and add back the radius of the smallest plant.

Round 3 2009 - Code Jam 2009

EZ-Sokoban (7pts, 10pts)

This is a state space search problem: given a set of states (positions on the board), an initial state and a final state, and rules for state transformations, find a sequence of moves that transforms the initial state to the final state. In our case, the problem asks for the length of the shortest such sequence of moves.

Conceptually, we can represent such a state space as a graph. The nodes of the graph are the possible positions, and the edges are the allowed moves. The problem then becomes: find the shortest path in the graph. The standard algorithm to solve this graph problem is breadth-first search.

Let's estimate the number of nodes in the graph. Assuming the maximum number of boxes (5), first estimate the number of positions where all the boxes are connected. 5 connected boxes form a pentomino. There are 63 different pentominoes, counting all rotations and reflections. Each of these can be positioned at no more than 12*12 different positions. Hence we get an upper bound of 63*12*12 = 9072 connected positions. It is a little more difficult to estimate the "dangerous" positions accurately, but we can see that from each connected position there are not too many moves, so we can guess that the total is not going to be too large for our computer to handle.

One approach would be to first generate the graph with all the edges explicitly, and then run the breadth-first-search on it. Another is to not store the graph at all, but compute the possible moves (edges) from a given position as we go, and only store the set of visited positions in a data structure.

Some details to work out are:

How to represent positions. A simple list of box coordinates can work. One can also use a whole-board bit-mask.
How to look up positions. We need this to see if a position has already been visited, or to avoid constructing the same node in the graph multiple times. We can use some kind of a dictionary data structure - a hash table or a binary search tree.
How to generate moves. Just try moving all the boxes in every direction, if the space in front and in the back of the box is empty. We also have to make sure that we don't move from a dangerous position to another dangerous position.
How to check whether a position is dangerous. To do this, we need to check if our 1 to 5 boxes are all connected. This can be represented as another, small graph problem (graph connectivity). We run another breadth-first-search on the little graph, where the nodes are the boxes and the edges indicate whether two boxes touch. Another approach would be to pre-generate all polyominoes of sizes up to 5, store them in a hash table, and then look up the shape appearing in a given position.

Round 3 2009 - Code Jam 2009

Alphabetomials (4pts, 20pts)

1. Product of two sets
The problem was created during a short walk in Manhattan in a summer evening. At first there came the picture of an N by M matrix. Let A be a set of N words {Ai | 1 ≤ i ≤ N}, and B be a set of M words {Bj | 1 ≤ j ≤ M}. For any word X, let us denote X(c) the number of occurrences of character c in X. In the (i,j)-entry of the matrix, we write, say, the product Ai('a')Bj('a')Bj('b'). What is the sum of all the entries in the matrix?
One can compute the NM terms one by one. But it is easy to see, as you can sum them row by row, as well as column by column, it is nothing but the full expansion of the following expression

(1)     ∑X ∈ A, Y ∈ BX('a')Y('a')Y('b') = ∑X ∈ AX('a') ∑Y ∈ BY('a')Y('b').
i.e., you can compute the sum on A and B separately, then compute their product.
What if, as in our problem, instead of writing Ai('a')Bj('a')Bj('b') for the (i,j)-entry, we write Z('a')2Z('b'), where Z is the concatenation of Ai and Bj? i.e., we do not care which 'a's are from the first set, and which are from the second. In this case we do not have something as simple as (1). But, we can reduce it if we do care which of the 'a's are from the first set. We use a somehow simplified notation (which is closer to our problem statement) -- the symbol a now also represent the number of 'a's in a string. For a fixed string Z that is a concatenation of words from A and B, let a1 denote the number of 'a's contributed by the first set, and a2 the number of 'a's contributed by the second set. For each entry in the matrix, we write a2b, this is equal to (a1+a2)2(b1+b2). Expand it, we have

(2)     ∑X ∈ A, Y ∈ B a2b = ∑X ∈ A, Y ∈ B (a1+a2)2(b1+b2)
= ∑X ∈ A, Y ∈ B (a12b1 + a12b2 + 2a1a2b1 + 2a1a2b2 + a22b1 + a22b2)
= ∑A,B (a12b1) + ∑A,B (a12b2) + 2∑A,B (a1a2b1) +
    2∑A,B (a1a2b2) + ∑A,B (a22b1) + ∑A,B (a22b2)
= ∑A a2b ∑B 1 + ∑A a2 ∑B b + 2∑A ab ∑B a +
    2∑A a ∑B ab + ∑A b ∑B a2 + ∑A 1 ∑B a2b
In the last step, for each summand, since the characters from the first set and those from the second set are nicely separated, we can apply the same reason for (1).

2. Product of ten sets
Our problem asks to compute the sum of expressions such like a2b over the product of a dictionary set by itself up to 10 times. (Or more generally, we can think of the product of 10 sets.) The size of the product set is the huge number n10. But if we use the same trick as in the previous section, write a2b as
(a1 + a2 + ... + a10)2 (b1 + b2 + ... + b10)
and expand it. Then we reduce the problem to at most KD easier problems, where D is the maximal degree (K ≤ 10 and D ≤ 4 in our problem). Each of the easier problems can be solved similar as in (1).
This gives one solution to our problem. Focus on one monomial a time. We are basically choosing the origin (one of the ten sets) for each letter in the monomial, so each set gets a sub-monomial, which can be expressed as a subset of the indices of the letters in the original monomial.
We pre-compute ∑X ∈ Aq(X) for each sub-monomial q, there are at most 16 of these. Then for each of the KD "easier problems", the computation involves only multiplying K of the pre-computed numbers.

3. Speedups
The solution above can be easily sped up to solve for much bigger K and slightly bigger D, if one compute the summations incrementally, do the product of two sets in each step (you call it dynamic programming or not).
First, we compute the sum for A × A, not only for the monomial, but for all its sub-monomials. Then we compute the sums over A3. Now we can view A3 as A2 × A. Each sub-monomial can be expanded to at most 2D terms, and every one of them submit to the simple trick in (1). Because we already have the summation of A2 on any sub-monomial, each of the 2D problems can be solved in constant time. The running time of this solution is mostly dominated by O(3DK) for each monomial form the input.

Observe the last line in (2), notice that we only have one dictionary set in our problems, so we can ignore the subscripts -- A, B, etc are the same set. There are other speedups available along this line by exploring the symmetry in indices. We omit the details.

4. The theory behind it
In case you are familiar with discrete probability, you may well know that probability theory provides some powerful abstract machinery for counting. What we went through so far are just some easy exercises in probability. And solving this problem with the abstract theory in mind certainly speeds up your thinking and boosts your confidence in the solution.
Especially, we went through the important fact that the expectation of the product of K random variables equals to the product of the expectation of each of the variables, given that the K random variables are mutually independent. The interested readers may carry the formal correspondences out.

Round 3 2009 - Code Jam 2009

Football Team (8pts, 19pts)

0. Drawing the graph
It is clear from the statement that this is a graph coloring problem -- on 1000 vertices. But certainly this is not the end of the story. There must be something special that makes this famous NP-complete problem solvable in this contest.

Represent each player with a vertex, and draw an edge between two players if they cannot use the same color. Obviously we can just draw the graph with each player in his position (x, y).
So there are at most 30 rows of players. Can this help? Observing this fact leads to solutions that are good enough for the small dataset, but not for the large tests.
On each row, the edges form a horizontal line segment from the leftmost point to the rightmost. All the other edges are between two adjacent rows. And more importantly, it is not hard to observe and prove the following:

If there are at least two edges between two adjacent rows, then all these edges together with the horizontal row edges form triangles.
If you draw all the edges using straight line segments, none of the edges cross each other in the middle.
So, surprising or not, the graph is planar! There are special cases where we can handle easily:
One color is enough if and only if there are no edges.
In a general graph, it is well known that the graph is 2-colorable if and only if there are no odd cycles. In our problem, it is even simpler than that. One can prove that whenever there is a cycle, there must also exist a triangle. So, the graph is 2-colorable if and only if it is a tree, as well as if and only if there is no triangles.
Otherwise, the answer is at least 3. But you may also know very well about one of the famous theorems of the past century, the Four Color Theorem: 4 colors is enough to color any planar graph.
So this is a good news: besides the simple special cases, all we need to decide is whether the answer is 3 or 4. Unfortunately it is also known that even this problem is NP-complete in general.
As we mentioned earlier, we are dealing with a special case of even planar graphs. It is mostly triangulated, and nicely positioned in rows with all edges only from adjacent rows. It turns out that there are many solutions to this problem. Let us describe a few in the rest of this writing. In all the solutions, we try to color the graph with just 3 colors A, B,and C, and if this fails, the answer is 4. Also, to simplify our writing, we assume every point has degree at least 3 -- otherwise we can simply remove it without changing the 3-colorability.

1. The colors are pretty much forced
Pick any triangle. Its vertices must be colored with different colors. We color them with A, B,and C. Now, there might be another triangle next to it. i.e., sharing an edge e with it. In that triangle, two vertices of e are already colored; if the third one is not, its color is decided now!

From another point of view, this is simply the following: Whenever there is a vertex with two adjacent neighbors colored, we color it with the third color.

Because every color is forced, the answer is 4 as soon as we see a conflict.

We can do this forced coloring almost all the way. It is easy to see we only need to pause a little when there is a cut point. But this is not hard either: the point separates the graph into an upper part and a lower part, as long as we can color both parts with 3 colors, we can paste them together.

2. The dual graph
There is a deeper concept revealed in the solution above. We were actually considering the dual graph of a planar graph, which itself is also planar. Each face (in our problem, each triangle) in the original graph becomes a point, and two points are connected if the two faces share an edge.

If you are familiar with the algorithm that 2-colors a graph and detects odd cycles, you may find it very similar to our solution above. We are going to argue that our original graph is 3-colorable if and only if the dual graph is 2-colorable (with one color for all the clockwise triangles).

Observe that if the original graph is 3-colorable, and one triangle gets its three vertices colored A, B, C in clockwise order, then any triangle adjacent to it must have its vertices colored A, B, C in counter-clockwise order. Thus, the original graph being 3-colorable implies its dual is 2-colorable.

The proof of the reversed direction is also not hard to envisage, but it involves more details and requires some additional conditions on our graph. One easy way to prove it is with induction on rows.

3. The local property
The solution above does not allow a much simpler implementation. But it leads to an even nicer observation. We call a vertex in the original graph an inner vertex if it is surrounded by triangles in all the directions. The following fact gives the simplest solution to our problem:

The graph is 3-colorable if and only if there is no inner vertex with odd degree.
The rest of this section is devoted to the proof of this fact. If you feel that some of the claims about the planar graph and its dual are not obvious, try to draw some and convince yourself.
Any inner vertex v with degree d is surrounded by d triangles sharing the same vertex. In the dual, they give a cycle of length d that enclose v in the middle.

One direction of the proof is easier. If v has odd degree, then we see an odd cycle in the dual, therefore the original graph is not 3-colorable.

Now assume there is any odd cycle, let us pick C -- the one with the smallest enclosing area. We claim that C must enclose only one inner vertex, and therefore we find an inner point with odd degree. This can be proved in several ways with some details. Basically, if there are two inner vertices enclosed, one can find a path that separates them and hit C in two places. This divides the enclosing area of C into two smaller areas, and it is not hard to see one of the areas must be enclosed by an odd cycle. This contradicts the assumption that C is the odd cycle with smallest area, and completes the sketched proof of our short solution.

In the picture below, there is a big cycle with length 9, and v is an inner vertex surrounded by a cycle with length 5.

4. Other solutions
We mention two brute-force solutions that are good enough for the small dataset. In order to use them for the large dataset, one needs to optimize a great deal, and possibly just discover one of keys for the polynomial solutions as mentioned above.
Again, we look at the original graph. We assume the graph is connected, and every vertex has degree at least 3.

Notice that there are at most 30 rows. One can fix the color of the first vertex in each row. There are 2rows ways of doing this (as opposed to 3rows). Once this is fixed, all the rest of the coloring is determined, for a reason similar to the one explained in Section 1.

Another solution is dynamic programming: do a sweep from the right to the left, and remember the color of the first vertex to the right on each row. Notice that you cannot do this from left to right, since knowing the color of only one vertex on a row is not enough in that direction.

More information
Graph coloring - Four color theorem - Dual graph

The solution with the local property resembles very much a theorem of Heawood from 1898: that a maximal planar graph is 3-colorable if and only if every degree is even. A more general theorem, which implies our solution, can be found in the paper A new 3-color criterion for planar graphs by Krzysztof Diks, Lukasz Kowalik, and Maciej Kurowski.

Round 3 2009 - Code Jam 2009

Interesting Ranges (9pts, 23pts)

1. How big can the answer be?
First, let's get a crude upper bound on the answer. More specifically, if L=1 and R=N, there are N*(N+1)/2 subsegments of [L,R]. Not all of them will contain an even number of palindromes, but a significant part (roughly half) will.

So even for the small input we might get more than 10^25 segments in the answer; it's obvious we can't enumerate them one by one. We have to figure out a way to process them in bulks.

2. First optimization
The first idea required to solve the small dataset helps us reduce the number of segments to consider from O(N^2) to O(N). To achieve that, we rely on the following observation: [L,R] can be represented as [0,R] minus [0,L-1]; thus it contains an even number of palindromes if and only if [0,L-1] and [0,R] both contain even or both contain odd number of palindromes.

But how exactly does that help us? Suppose we know which of [0,X] segments contain even number of palindromes (we'll call them just "even 0-segments" further on) and which contain odd number of palindromes ("odd 0-segments"). We know that each interesting segment corresponds to exactly one pair of even 0-segments or to exactly one pair of odd 0-segments. But this is true the other way around as well: each pair of distinct even 0-segments corresponds to exactly one interesting segment, and so does each pair of distinct odd 0-segments! (when X is between L-1 and R, inclusive).

That means that if there are A even 0-segments and B odd 0-segments, the answer is A*(A-1)/2+B*(B-1)/2.

2. Second optimization
But we can't even afford O(N) running time since N is 10^13 even in the small dataset! So we have to do another optimization.

Let's write out an infinite string of zeroes and ones, with X-th symbol (0-based) being equal to 0 if [0,X] contains an even number of palindromes, and equal to 1 if [0,X] contains an odd number of palindromes. What we need to find in this problem is how many zeroes (number A above) and ones (number B above) are there in the substring starting with (L-1)-th character and ending with R-th character of this string.

This string looks like this: 1010101010011111111111000000000001111111111100...

Now we can spot the second optimization: zeroes and ones tend to go in big blocks in this string. More specifically, one changes to zero or vice versa only when we pass a palindrome. And there are only O(sqrt(N)) palindromes up to N - so the number of groups of consecutive zeros or ones in the first N characters is O(sqrt(N)).

All groups except maybe two boundary ones fit into [L-1,R] segment entirely. So we just need to sum them all and handle the boundary ones carefully, and we get an O(sqrt(N)) algorithm that is sufficient to solve the small dataset.

We can also reduce the number of boundary groups to consider from two to one relying on the fact that the number of zeroes/ones in [L-1,R] is the number of zeroes/ones in [0,R] minus the number of zeroes/ones in [0,L-2].

3. Third optimization
So what about the large one? Sqrt(10^100)=10^50, so we're definitely not there yet.

The final optimization idea still relies on the above infinite string of zeroes and ones. You might have noticed already that many blocks of ones and zeroes have the same length. For example, block of ones from 11 to 21 has length 11, and so is the block of zeroes from 22 to 32, ones from 33 to 43, zeroes from 44 to 54, and so on.

This is because of the fact that a palindrome number is uniquely determined by its first half. For example, consider 6-digit palindrome number 127721. What is the next 6-digit palindrome number? 128821. It's followed by 129921, 130031, and so on. As you can see, in most cases the difference between two consecutive 6-digit palindrome numbers is 1100 (change of +1 in two middle digits). And the difference between two consecutive palindrome numbers is exactly the length of the block of ones/zeroes!

But there are also some blocks which have length different from usual. For example, the block of zeroes from 88 to 98 still has 11 numbers, but the block of ones from 99 to 100 has just two numbers; then follow several blocks of 10 (zeroes from 101 to 110; ones from 111 to 120; ...; zeroes from 181 to 190), then we have a block of 11 ones from 191 to 201.

The above explanation about consecutive palindromes allows us to understand why there are such unusual-length blocks. It happens in two cases: when the amount of digits in the palindrome changes, and when the middle digit of the starting palindrome of the block is 9. The first several unusual-length blocks are: zeroes from 9 to 10, ones from 99 to 100, ones from 191 to 201, ones from 292 to 302, ..., ones from 898 to 908, ones from 999 to 1000, ones from 1991 to 2001, ones from 2992 to 3002, and so on.

And here comes the final step. All such unusual-length blocks except the one from 9 to 10 consist of ones! Or, in other words, all blocks of zeroes have the same length within palindromes of the same amount of digits, except the block from 9 to 10.

Why? Because there's ten blocks between two consecutive palindromes with a 9 in the middle (with the only exception being 9 and 99), and ten is an even number. That's why every time we have a 9 in the middle, we start a block of ones.

And that allows us to calculate in one step the total amount of zeroes in the part of our infinite string that corresponds to one amount of digits in the palindrome. That means we can calculate the overall amount of zeroes in any segment in O(number of digits in the maximal palindrome) steps. Of course, we have to be careful near L and R.

And what about the amount of ones? It's equal to the total length minus the amount of zeroes :)

4. Modulo calculations
With the numbers in the input being quite big, the above calculations have to be performed carefully.

Some programming languages allow easy calculations with arbitrarily long numbers. But what if we use a language without this feature? Luckily, we're asked to find the answer modulo a (relatively) small number M.

That allows us to perform most calculations modulo M, and thus use only small numbers in those calculations. The calculations we need to find the answer are: addition, multiplication, subtraction and division by 2. The first three are performed in a standard way (first calculate the answer in integers, then take it modulo M). The fourth one is made possible by the fact that the modulo used in the problem is odd. When we divide a number X by 2 modulo M, we get just X/2 if X is even, and (X+M)/2 when x is odd. One can easily check that multiplying by 2 gets X back in both those cases.

World Finals 2009 - Code Jam 2009

Year of More Code Jam (5pts, 12pts)

The setting of this problem is no doubt discrete probability. From the definition, the space consists of NT equally likely possible outcomes. That can be, under our limits, as huge as 10450. Clearly, a naive approach is not feasible.

But let us do a little exercise in probability. Define the random variable Xi to be the number of contests on the i-th day, the quantity we want to compute is the average of Xi2, i.e., the expectation Ε(∑ Xi2). By the linearity of the expectation, we have

Ε(∑1≤i≤N Xi2) = ∑1≤i≤N Ε(Xi2).
So let us focus on the computation of the variable for a fixed day for the moment. Pick any i, and let X := Xi. Let us define more random variables. Define Yj to be the indicator of whether the j-th tournament will have a contest on the i-th day. Clearly, X = ∑j Yj. So,
(*)     Ε(X2) = Ε((∑1≤j≤T Yj)2) = ∑1≤j≤T Ε(Yj2) + 2 ∑1≤j<k≤T Ε(YjYk).
We observe that each terms in the last expression is easy to compute. Being the indicator random variables, the Y's take value 0 or 1. So
Yj2 always has the same value as Yj, and its expectation is just the probability that Yj is 1, i.e., tournament j has a contest on day i.
YjYk is 1 if and only if both Yj and Yk are 1. The expectation is the probability that both the j-th and the k-th tournament has a contest on day i.
Let the input for the tournament j, i.e., the contest pattern be d1=1, d2, ..., dm. Denote D(i,j) be the number of the d's for tournament j that are less than or equal to i. There are N choices for the starting date of a particular tournament. It is easy to see that the first probability above is D(i,j) / N; while the second probability is D(i,j)D(i,k) / N2.
So far we addressed the problem just for a single day i. We need to do this for every i. There are 109 of them. But notice that, as long as there is no input dt = i, D(i-1, j) = D(i, j) for all j. This means that the expectation for the i-th day is the same as the expectation for the (i-1)-th. There are at most T max(M) ≤ 2500 such d's in the input, so we need to compute (*) for at most 2500 days. For our problem, it is good enough to realize that there is no d > 10000. So all the expectations after the 10000-th day are the same. We can just do the computation for the first 10000 days, and for the rest, a simple multiplication.

The last problem is the need for big integers. At the first glance we might have both numerators and denominators as big as 10450. But that is not the truth. Simply observe the above answer, which is a sum of various D(i,j) / N and D(i,j)D(i,k) / N2. We actually proved that the denominator is never bigger than N2. A careful implementation with 64-bit integers will be good enough.

For a further speed-up. The formula in (*) involves computing O(T2) terms. But if we do it from day 1, keep D(i,j) for each j and two more variables -- S1 for the sum of all the D(i,j)'s, and S2 for the sum of D(i,j)2's, then we just need constant update time when we see an input d, and also constant computation time for each day we want to compute (*).

World Finals 2009 - Code Jam 2009

Min Perimeter (5pts, 15pts)

This problem is similar to the classical problem of finding the closest pair in a set of points. Algorithms that solve the closest-pair problem can be adapted to solve this one.

The number of points can be pretty large so we need an efficient algorithm. We can solve the problem in O(n log n) time using divide and conquer. We will split the set of points by a vertical line into two sets of equal size. There are now three cases for the minimum-perimeter triangle: its three points can either be entirely in the left set, entirely in the right set, or it can use points from each half.

We find the minimum perimeters for the left and right sets using recursion. Let the smallest of those perimeters be p. We can use p to make finding the minimum perimeter of the third case efficient, by only considering triangles that could have an area less than p.

To find the minimum perimeter for the third case (if it is less than p) we select the points that are within a distance of p/2 from the vertical separation line. Then we iterate through those points from top to bottom, and maintain a list of all the points in a box of size p x p/2, with the bottom edge of the box at the most recently considered point. As we add each point to the box, we compute the perimeter of all triangles using that point and each pair of points in the box. (We could exclude triangles entirely to the left or right of the dividing line, since those have already been considered.)

We can prove that the number of points in the box is at most 16, so we only need to consider at most a small constant number of triangles for each point.

Splitting the current set of points by a vertical line requires the points to be sorted by x and going through the points vertically requires having the points sorted by y. If we do the y sort at each step that gives us an O(n log2 n) algorithm, but we can keep the set of points twice, one array would have the points sorted by x and one would have the points sorted by y, and this way we have an O(n log n) algorithm.

The time limits were a bit tight and input limits were large because some O(n2) algorithms work really well on random cases. This is why during the contest some solutions that had the right idea but used a p x p box size or sorted by y at each step didn't manage to solve the large test cases fast enough.

You can read Tomek Czajka's source to get the details of a good implementation:
#include <algorithm>
#include <cassert>
#include <cmath>
#include <cstdio>
#include <cstdlib>
#include <vector>
using namespace std;
#define REP(i,n) for(int i=0;i<(n);++i)
template<class T> inline int size(const T&c) { return c.size();}

const int BILLION = 1000000000;
const double INF = 1e20;
typedef long long LL;

struct Point {
  int x,y;
  Point() {}
  Point(int x,int y):x(x),y(y) {}
};

inline Point middle(const Point &a, const Point &b) {
  return Point((a.x+b.x)/2, (a.y+b.y)/2);
}

struct CmpX {
  inline bool operator()(const Point &a, const Point &b) {
    if(a.x != b.x) return a.x < b.x;
    return a.y < b.y;
  }
} cmpx;

struct CmpY {
  inline bool operator()(const Point &a, const Point &b) {
    if(a.y != b.y) return a.y < b.y;
    return a.x < b.x;
  }
} cmpy;

inline LL sqr(int x) { return LL(x) * LL(x); }

inline double dist(const Point &a, const Point &b) {
  return sqrt(double(sqr(a.x-b.x) + sqr(a.y-b.y)));
}

inline double perimeter(const Point &a,
                        const Point &b,
                        const Point &c) {
  return dist(a,b) + dist(b,c) + dist(c,a);
}

double calc(int n, const Point points[],
            const vector<Point> &pointsByY) {
  if(n<3) return INF;
  int left = n/2;
  int right = n-left;
  Point split = middle(points[left-1], points[left]);
  vector<Point> pointsByYLeft, pointsByYRight;
  pointsByYLeft.reserve(left);
  pointsByYRight.reserve(right);
  REP(i,n) {
    if(cmpx(pointsByY[i], split))
      pointsByYLeft.push_back(pointsByY[i]);
    else
      pointsByYRight.push_back(pointsByY[i]);
  }
  double res = INF;
  res = min(res, calc(left, points, pointsByYLeft));
  res = min(res, calc(right, points+left, pointsByYRight));
  static vector<Point> closeToTheLine;
  int margin = (res > INF/2) ? 2*BILLION : int(res/2);
  closeToTheLine.clear();
  closeToTheLine.reserve(n);
  int start = 0;
  for(int i=0;i<n;++i) {
    Point p = pointsByY[i];
    if(abs(p.x - split.x) > margin) continue;
    while(start < size(closeToTheLine) &&
          p.y - closeToTheLine[start].y > margin) ++start;
    for(int i=start;i<size(closeToTheLine);++i) {
      for(int j=i+1;j<size(closeToTheLine);++j) {
        res = min(res, perimeter(p, closeToTheLine[i],
                                 closeToTheLine[j]));
      }
    }
    closeToTheLine.push_back(p);
  }
  return res;
}

double calc(vector<Point> &points) {
  sort(points.begin(), points.end(), cmpx);
  vector<Point> pointsByY = points;
  sort(pointsByY.begin(), pointsByY.end(), cmpy);
  return calc(size(points), &points[0], pointsByY);
}

int main() {
  assert(0==system("cat > Input.java"));
  fprintf(stderr, "Compiling generator\n");
  assert(0==system("javac Input.java"));
  fprintf(stderr, "Running generator\n");
  assert(0==system("java -Xmx512M Input > input.tmp"));
  fprintf(stderr, "Solving\n");
  FILE *f = fopen("input.tmp", "r");
  int ntc; fscanf(f, "%d", &ntc);
  REP(tc,ntc) {
    int n; fscanf(f, "%d", &n);
    vector<Point> points;
    points.reserve(n);
    REP(i,n) {
      int x,y; fscanf(f, "%d%d", &x, &y);
      points.push_back(Point(2*x-BILLION,2*y-BILLION));
    }
    double res = calc(points);
    printf("Case #%d: %.15e\n", tc+1, res/2);
  }
  fclose(f);
}

World Finals 2009 - Code Jam 2009

Wi-fi Towers (3pts, 25pts)

Connection graph
We start by representing the problem as a graph problem. Each tower is a vertex in the graph and has a weight equal to its score. If a tower A has another tower B in its range, we represent this fact as a directed edge from A to B. The problem is to choose a set of vertices with maximum weight such that for every edge from A to B, if A is chosen then B is also chosen. In the following, we assume the number of towers (vertices) is V and the number of edges is E.

Reduce to an instance of MIN-CUT
To reduce the problem to an instance of MIN-CUT, we create a flow network as follows. Create a source vertex, a sink vertex, and one vertex for each tower. Suppose a tower has score s. If s > 0, create an edge from the vertex to the sink with capacity s. If s < 0, create an edge from the source to this vertex with capacity |s|. Finally, for every edge in the connection graph, create a similar edge in the flow network with infinite capacity. The network has V + 2 = O(V) vertices and O(V + E) edges.

Now every finite cut in the graph represents a choice of towers - we choose every tower on the same side of the cut as the source. The infinite capacity edges enforce that the choice follows the given constraints (otherwise we get a cut of infinite weight). The edges from the source and to the sink penalize the solution appropriately for choosing towers with negative scores and for not choosing towers with positive scores. If the value of the best cut is C, the answer is S - C, where S is the sum of positive tower scores.

Solving MIN-CUT
By the max-flow min-cut theorem, we can solve the MIN-CUT instance generated above by computing the maximum flow in the same graph. To compute the maximum flow, we can use the Edmonds-Karp algorithm (a variant of Ford-Fulkerson which selects augmenting paths using BFS), which results in complexity bounded by O(V(V+E)2) = O(V5). In practice, this was fast enough to solve all possible test cases.

One could also use a more complicated push-relabel max-flow algorithm which, with a FIFO vertex selection rule, results in complexity O(V3).

Yet another algorithm, thanks to integral capacities, is the capacity scaling variant of Ford-Fulkerson (start by searching for augmenting paths with weights being large powers of 2 first, and then decrease). This results in O((V+E)2 log F) = O(V4 log F) where F is the maximum value of the flow.

Reducing the number of edges
Finally, there is a geometric trick using which we can reduce the number of edges E to O(V), thus reducing the complexity of the algorithms above. The complexity of Edmonds-Karp becomes O(V3) and of capacity scaling: O(V2 log F).

First, notice that we can remove edges without changing the final answer as long as the transitive closure of the graph stays the same.

The crucial trick is to see that if there are two directed edges A-C and B-C, and the angle ACB is smaller than 60 degrees, then we can remove the longer edge. Suppose A-C is longer than B-C. Then if we remove the edge A-C, there is still going to be an indirect connection A-B-C (using shorter or equal length edges), thanks to the fact that the range of A is a circle.

If we keep doing this, every vertex will end up with at most 6 incoming edges, thus reducing the total number of edges to at most 6V.

More information
This problem is also equivalent to the Minimum Closure Problem, which was studied in the 1970s and has applications in the mining industry.

Max Flow Min Cut Theorem

World Finals 2009 - Code Jam 2009

Doubly-sorted Grid (10pts, 20pts)

A counting problem, with a board size not too big. The problem gives a quick impression of dynamic programming on a space of exponentially many states. And it is indeed so.

By the limits of this problem, let us say the size is the larger of m and n. A solution with 22⋅size states is fine, while a solution with 24⋅size states is probably only good for the small dataset. However, for regular programming contest goers, there are many conventional ways to define the states for similar grid problems that fall into the latter category.

So, the key part of the problem is to find the right state space. Once it is found, the finalists can no doubt carry out the dynamic programming solution easily.

The basic picture is the lattice paths. Specifically, let us consider, in a doubly sorted grid, all the letters less than or equal to a particular character. They form an upwards closed region towards the top-left corner. In other words, if the letter in (r, c) is no greater than the prescribed character, so is the letter in (r', c'), if r' ≤ r and c' ≤ c. As a result, the boundary separating this region and the rest of the grid forms a lattice path from the bottom-left to the top-right, and can only go north or east. This is a well known subject -- there are (m+n choose m) such paths in total. Let us call them monotone paths. For two monotone paths, we say one dominates the other if one never goes above the other. Any doubly sorted grid corresponds in a one to one fashion to 26 monotone paths (some of which may be identical), one for each letter, and the path for a bigger letter dominates the paths for the smaller letters. The left picture below depicts the situation when there are three letters; and the monotone boundaries for 'a' and 'b' are highlighted.

Just one step further. Let us focus not only the exact boundary for a letter but any monotone path. For any monotone path P and any letter c, define

dp[P][c] := the number of ways one can fill all the squares above the path P, using only the letters no greater than c, so that the upper part is doubly sorted, and any pre-filled letter in the upper part is respected.
For any monotone path except the most dominated one, we have one or more maximal points, those are the points where the path goes east then followed by a step upwards. In the second picture above, we highlight a monotone path with its maximal points colored. To compute dp[P][c], we can divide the situation into two cases. (1) The letter c does not appear at all. There are dp[P][c-1] ways to do so. (2) Otherwise, c must appear in at least one of the maximal points of P. For each non-empty subset of the maximal points, we can assign the letter c to them, reducing our task to dp[P'][c], where P' is a path that only differs from P in that subset of maximal points. We use inclusion-exclusion formula on all the non-empty subsets to compute the contribution to dp[P][c] in this case.

Such a solution is relatively intuitive, and is fast enough under our constraints. By adding one more helper, one can find a faster solution. Now let us refine

dp[P][c][k] := the number of ways one can fill all the squares above the path P, using only the letters no greater than c, and the letter c does not occur anywhere after column k,so that the upper part is doubly sorted, and any pre-filled letter in the upper part is respected.
We leave the implementation details as an easy exercise for interested readers. We mention that, when m=n, the number of states is 26⋅(2n choose n)⋅n = Θ(4nn0.5). Although the computation of a single dp[P][c][k] may involve up to n steps, the running time can be shown as Θ(4nn0.5) by a simple amortized analysis -- for fixed P and c, we need O(n) steps in total to compute the table for all k.

World Finals 2009 - Code Jam 2009

Marbles (7pts, 32pts)

Last year we were trying to solve different instances of this problem. It took a long time to converge to this particular shape, and even after we settled on the current requirements, we were still tweaking the input limits at 2am the night before the contest to make the problem a bit more interesting. The problem nicely combines together dynamic programming, greedy and graph related notions like biconnected components and trees.

The first step is deciding if a particular configuration is or isn't solvable. If for two colors their corresponding marbles alternate it means that the two pairs of marbles need to be joined by curves on opposite sides of the horizontal line Y=0. We can build a graph where the nodes represent pairs of same-color marbles and form a graph with edges between pairs of marbles that alternate. We can draw the paths with no intersection if and only if this graph is bipartite.

Next, for solvable configurations we compute the minimum height. The pairs graph can have many connected components, and for each such component we can choose two ways of drawing the lines (with the first pair of marbles above the Y=0 line or below it), so in total we would have O(2components) configurations. This idea can solve the small case but is too time consuming for the large case.

Solving the large case requires us to use dynamic programming. Our state will be defined by left, right, height_up and height_down. For each state we compute a boolean value which tells us if the subproblem which uses the set of marbles with indexes from left to right can be solved in the vertical range [-height_down .. height_up]. Computing this value is a bit tricky, what we need to notice is that we can try each of the two ways of drawing the component that starts at index left. Then an important observation is that we can draw each path with the maximum height possible if the line is above the X axis or maximum depth possible if the line is below the X axis as long as our drawing is within the [-height_down .. height_up] vertical range. Using these ideas we can come up with an O(n5) algorithm.

We can improve on this solution by using the state (left, right, height_up) and for each state finding the smallest height_down for which the subproblem [left .. right] is solvable. Now we notice that we should use dynamic programming on pairs of left and right where connected components of marbles start and finish. This will make right uniquely defined by left. Thus we have reduced the state space to O(n2) states. We also notice that the connected components form a tree-like structure where we need to solve the innermost components first and then solve outer components, much like visiting the leaves of a tree first and getting closer and closer to the root. Now each connected component will be analyzed just once at an upper component level so the overall algorithm will take O(n2) time.

Here's Tomek Czajka's solution:
#include <algorithm>
#include <cassert>
#include <cstdio>
#include <map>
#include <string>
#include <vector>
using namespace std;
#define REP(i,n) for(int i=0;i<(n);++i)
template<class T> inline int size(const T&c) { return c.size();}
const int INF = 1000000000;

int n; // number of types of marbles
vector<vector<int> > where; // [n][2]
vector<int> marbles; // [2*n]

void readInput() {
  char buf[30];
  map<string,int> dict;
  scanf("%d", &n);
  marbles.clear(); marbles.reserve(2*n);
  where.clear(); where.resize(n);
  for(int i=0;i<2*n;++i) {
    scanf("%s",buf);
    string s = buf;
    map<string,int>::iterator it = dict.find(s);
    int m;
    if(it==dict.end()) {
      m = size(dict);
      dict[s] = m;
    } else {
      m = it->second;
    }
    marbles.push_back(m);
    where[m].push_back(i);
  }
}

struct Event {
  int x,t;
  // t=0 start top, 1 end top
  // t=2 start bot, 3 end bot
};

vector<int> vis;

bool cross(int m1,int m2) {
  return
      where[m1][0] < where[m2][0] &&
      where[m2][0] < where[m1][1] &&
      where[m1][1] < where[m2][1] ||
      where[m2][0] < where[m1][0] &&
      where[m1][0] < where[m2][1] &&
      where[m2][1] < where[m1][1];
}

void dfs(int m,int sign) {
  if(vis[m]==sign) return;
  if(vis[m]==-sign) throw 0;
  vis[m]=sign;
  REP(i,n) if(i!=m && cross(m,i)) dfs(i,-sign);
}

vector<vector<Event> > cacheCalcEvents;

const vector<Event> &calcEvents(int startx) {
  vector<Event> &res = cacheCalcEvents[startx];
  if(!res.empty()) return res;
  vis.assign(n,0);
  dfs(marbles[startx],1);
  REP(x,2*n) {
    int m = marbles[x];
    if(vis[m]==0) continue;
    int nr=0;
    if(where[m][nr] != x) ++nr;
    assert(where[m][nr]==x);
    Event e; e.x=x;
    e.t = (1-vis[m]) + nr;
    res.push_back(e);
  }
  return res;
}

vector<vector<int> > cacheCalcH2;

int calcH2(int a,int b,int h1) {
  if(h1<0) return INF;
  if(a==b) return 0;
  int &res = cacheCalcH2[a][h1];
  if(res!=-1) return res;
  const vector<Event> &events = calcEvents(a);
  res = INF;
  for(int mask = 0; mask<=2; mask+=2) {
    int top=0, bot=0;
    int h2 = 0;
    REP(i,size(events)+1) {
      int alpha = i==0 ? a : events[i-1].x + 1;
      int beta = i==size(events) ? b : events[i].x;
      h2 = max(h2, calcH2(alpha, beta, h1 - top) + bot);
      if(i!=size(events)) {
        switch(events[i].t ^ mask) {
          case 0: ++top; break;
          case 1: --top; break;
          case 2: ++bot; break;
          case 3: --bot; break;
        }
      }
    }
    res = min(res, h2);
  }
  return res;
}

int solve() {
  int res = INF;
  cacheCalcH2.assign(2*n, vector<int>(n+1,-1));
  cacheCalcEvents.clear(); cacheCalcEvents.resize(2*n);
  try {
    REP(h1,n+1) {
      res = min(res, h1 + calcH2(0,2*n,h1));
    }
    return res;
  } catch(int) { return INF; }
}

int main() {
  int ntc; scanf("%d", &ntc);
  REP(tc,ntc) {
    readInput();
    int res = solve();
    if(res==INF) res = -1;
    printf("Case #%d: %d\n", tc+1, res);
  }
}

World Finals 2009 - Code Jam 2009

Lights (21pts, 45pts)

There are various ways to solve this problem. The solutions can be split into two kinds of approaches. One way is to solve the problem exactly (or, to arbitrary precision), the other is to approximate the answer with high enough precision. The exact solutions generally try to divide the square into subareas of the same color, compute the area of each subarea separately, and add up the totals for each color. We discuss these first.

Intersection of (mostly) triangles
First, consider just one light. We want to compute the total area illuminated by the light. To do that, compute the tangent lines from the light to each pillar, and lines from the light to each corner of the room, and consider each "cone" between two adjacent lines separately. Each cone will either end up hitting a wall, or hitting a pillar. In the first case we get a triangle, whose area we can easily compute. In the second case we get a "quasi-triangle", that is, a triangle minus a disk segment. Here, we need to subtract the area of the disk segment. We can compute the area of a disk segment by subtracting a triangle from the area of a disk sector (a "pie slice").

Once we have the total area covered by each light, we need one more thing: the area covered by both lights. We can take each pair of triangles or quasi-triangles generated in the previous step, and compute the common area between them. Now we need to compute the area of the intersection of two triangles or quasi-triangles.

A simple way to approach this is to first treat quasi-triangles as triangles (include the disk segment). Now we compute the intersection between the two triangles, which gives us a polygon (up to six sides). If one or both of the triangles were actual triangles, or when the pillars subtracted from the two quasi-triangles were different, the polygon is the correct answer - there is no need to account for the subtracted disk segments, because each segment is outside the other triangle anyway.

The only nasty case comes up when we have two quasi-triangles ending at the same pillar. In that case, we first compute the intersection polygon, and then we subtract the pillar from the polygon. To do that, remove those edges and parts of edges of the polygon that fall inside the circle and replace them with one edge. The answer will be the area of the reduced polygon, again minus a disk sector cut off by a line, which we already know how to compute.

Line sweeping
Line sweeping is a common technique in computational geometry. We sweep a vertical line from the left edge to the right. As in the solution above, the interesting rays are the tangent rays from lights to circles. The interesting moments are when the x-coordinate of the vertical line reaches one of the following. (1) A light. (2) A pillar starts or ends. (3) An interesting ray touches or intersects a circle, or hits the wall. (4) Two interesting rays intersect.

Now, let x1 < x2 be two adjacent interesting moments. The vertical strip between x1 and x2 is divided into pieces. Each piece is bounded above and below by a general segment -- a line segment or an arc. By the definition of the interesting moments, nothing interesting will happen in the middle, and each piece is of one color. So we can sample an arbitrary point from each piece to decide the color. The pieces are not convex, but this is not a problem -- they are convex on any vertical line so we can easily find a point that is inside each piece. The area of each piece is also easy to compute -- it is basically a trapezoid, possibly degenerating into a triangle if the upper and lower boundaries meet at one end, and one needs to subtract a disk segment for each arc-boundary.

Line sweeping is often used with nice data structures to achieve good complexity. But that is not our primary concern here. We used it for the simplicity of the implementation -- the only geometric operations needed here are intersections between lines and circles.

Approximations
The problem requires a relative or absolute error of at most 10-5, while the total room area is 10000. Cases requiring the most care are those when one of the four colors has an area less than 1, in which case the error we can make relative to the area of the whole room is 10-9.

The simplest approach would be to sample a lot of points either randomly, or in a regular grid, compute the color of each sample and assume that the sample is representative of the correct answer. The above error estimation suggests though that to get enough precision, we would need to sample on the order of 109 points (or more, due to random deviations). This is too much for a solution to run within the time limit. A smarter approach is needed.

Computing the area can be seen as a problem of computing a two-dimensional integral. A hybrid approach is also possible: we can see it as a one-dimensional integral along the x coordinate, and for each x coordinate we sample we can compute the exact answer by looking at which segment of the vertical line is in what color. This one-dimensional sub-problem is somewhat simpler to do than solving the full two-dimenstional problem exactly.

In either case, whether we compute a two-dimensional integral or just a one-dimensional one for a more complex function, we need a smart way to approximate the integral. Uniform or random sampling is not enough.

You can search the web for methods of numerical integration. In this problem, an adaptive algorithm is needed, which means that we sample more "interesting" areas with more samples than the less "interesting" ones. "Interesting" can be defined as "large changes in values" (large first derivative) or "wild changes in values" (large second derivative).

One simple algorithm is to write the integration procedure as a recursive function. We recursively try splitting the interval into smaller ones, and see how much the answer changes through such increases of precision. We stop the recursion when the answer changes very little, which means the interval is small enough or the function is smooth enough in the interval. This will result in sampling the more "interesting" areas more accurately.

Qualification Round 2010 - Code Jam 2010

Fair Warning (10pts, 23pts)

This turned out to be the hardest problem in the qualification round. One of the reasons may have been that the solution involves big arithmetic precision. Using big numbers in timed programming contests is sometimes not considered fair because some languages like python or java have internal libraries that deal with this while for other languages you may have needed to write your own or search for a external one like the GNU Multi-Precision Library. Considering that the qualification round was 24 hours long we took this chance to give you a warning and one which is fair ... that big numbers are fair game for now on, so have a library on hand!

This problem talks about divisors and multiples so it hints at using the greatest common divisor concept in some way. To solve the problem we need to find T and afterwards we can easily find y.

Let's simplify the problem and just look at two numbers a and b. In this case we need to find the largest T so that some positive y exists where a + y and b + y are multiples of T. So (a + y) modulo T = (b + y) modulo T which means that (a - b) modulo T = 0. Thus T must be a divisor of |a - b|.

Coming back to the problem with N numbers, we have proved that T must divide every |ti - tj|. This means that the solution is the greatest common divisor of all |ti - tj| numbers. A simple observation can improve our algorithm from O(N2) to O(N) iterations of the Euclidean algorithm. Let's say a ≤ b ≤ c, then gcd(b - a, c - a) = gcd(b - a, c - a, c - b). To prove this we look at the first iteration of the Euclidean algorithm. Since a ≤ b ≤ c it means that b - a ≤ c - a so in the first step we subtract b - a from c - a: gcd(b - a, c - a) = gcd(c - b, b - a), this proves our observation. Now we can just compute the greatest common divisor of all |ti - t1| to find T.

Here's some python code from Xiaomin Chen that solves one test case:

def Gcd(a, b):
  if b == 0:
    return a
  return Gcd(b, a % b)

def Solve(L):
  y = L[0]
  L1 = [abs(x - y) for x in L]
  g = reduce(Gcd, L1)
  if y % g == 0:
    return 0
  else:
    return g - (y % g)
Useful concepts: Greatest common divisor, Euclidean algorithm, Arbitrary precision arithmetic.

Qualification Round 2010 - Code Jam 2010

Snapper Chain (10pts, 23pts)

The difficult part was understanding how a single Snapper works. Each Snapper can be in one of two states -- On or Off. Also, each snapper can either be powered or unpowered. The first Snapper is always powered because it is plugged into the power socket in the wall. The i'th Snapper is powered if and only if the (i-1)'th Snapper is powered and On. Snapping your fingers changes the state of each powered snapper (from On to Off, or from Off to On).

With these rules in mind, let's represent the On/Off state of the Snappers by a sequence of bits, 1 meaning On. If we list the bits right-to-left, we get a binary integer. Initially, the integer has value 0. Similarly, we can write down the binary integer for the powered/unpowered state of each Snapper. Initially, this integer is 1 because only the rightmost Snapper is powered.

Snapping your fingers is equivalent to doing an XOR of the On/Off integer and the powered/unpowered integer and putting the result into the On/Off integer. After that, we update the powered/unpowered bits according to the rule above,

For example, let's say that the On/Off number is now 10100011111. This means that the powered/unpowered number is 00000111111. When we XOR these two numbers, we get the new value of the On/Off number: 10100100000.

The interesting thing is that these two updates are equivalent to a simple increment of the On/Off integer! Snapping your fingers adds 1 to the On/Off integer, and we do not even need to care about the powered/unpowered integer.

The solution to the problem is then very simple. The answer is "ON" if and only if the rightmost N bits of K are 1.

Qualification Round 2010 - Code Jam 2010

Theme Park (10pts, 23pts)

At first glance, this problem appears to be a straightforward simulation: keep adding groups until you run out space on the roller coaster (or run out of groups), then re-form the queue and start again. Repeat this R times and you're done.

For the Small, that was enough; and we got more than a few questions during the contest from contestants thinking that they were unable to solve the Large because their computers were so slow. The fact is that, with limits so large -- up to 103 groups queuing for 109 rides -- you need to come up with a smarter algorithm to solve the problem, since that one is O(NR).

Optimization One
When you're sending the roller coaster out for 109 rides, you've got to expect that a few of them are going to be the same. If you store the queue of groups as an unchanging array, with a pointer to the front, then every time you send out a ride s, you could make a note of where it ended, given where it started. Then the next time you see a ride starting with the same group in the queue, you can do a quick lookup rather than iterating through all the groups.

That speeds up the algorithm by a factor of 103 in the worst case, leaving us with O(R) operations. There are some other ways of speeding up the calculation for any given roller coaster run: for example, you could make an array that makes it O(1) to calculate how many people are in the range [group_a, group_b] and then binary search to figure out how many groups get to go in O(log(N)) time. That gives a total of O(R log N) operations.

Optimization Two
As we observed in Optimization One, you're going to see a lot of repetition between rides. You're also going to see a lot of repetition between groups of rides. In the example in the problem statement, the queue was made up of groups of size [1, 4, 2, 1]. 6 people get to go at once. Let's look at how the queue changes between rides:

1, 4, 2, 1  [5]
2, 1, 1, 4  [4]
4, 2, 1, 1  [6]
1, 1, 4, 2  [6]
2, 1, 1, 4  [4]
4, 2, 1, 1  [6]
1, 1, 4, 2  [6]
As you may have noticed, there's a cycle of length three: starting from the second run, every third queue looks the same. We make 16 Euros when that happens, which means we'll be making 16 Euros every 3 runs until the roller coaster stops rolling.
So if the roller coaster is set to go 109 times: the first time it makes 5 Euros; then there are 999999999 runs left; and every three of those makes 16 Euros. 3 divides 999999999 evenly -- if it didn't, we'd have to do some extra work at the end -- so we make 5 + (999999999 / 3 * 16) = 5333333333 Euros in total.

It turns out that a cycle must show up within the first N+1 rides, because there are only N different states the queue can be in (after N, you have to start repeating). So you only have to simulate N rides, each of which takes O(N) time in the worst case, before finding the cycle: that's an O(N2) solution.

Optimization Three
Either of the optimizations above should be enough. But if you're a real speed demon, you can squeeze out a little more efficiency by combining the binary search that we mentioned briefly in Optimization One with the cycle detection from Optimization Two, bringing our running time down to O(N log N). An alternate optimization can bring us down to O(N); we'll leave that as an exercise for the reader. Visit our Google Group to discuss it with the other contestants!

Round 1A 2010 - Code Jam 2010

Rotate (11pts, 12pts)

This is a relatively straightforward simulation problem -- the problem statement tells you what to do, and you just need to do it.

Well, except for one fun point: The name of the problem is Rotate, and in the statement we talk about the rotation a lot. However, that is the one thing you do not need to implement! Rotating 90 degrees clockwise and pushing everything downwards has the same effect as pushing everything towards the right without rotating. As long as you push the pieces in the correct direction, it doesn't matter whether you actually do the rotation. Any K pieces in a row will be the same in these two pictures, and your output will be the same too.

So, a simple solution to this problem looks like this:
(1) In each row, push everything to the right. This can be done with some code like the following:

    for (int row = 1; row < n; ++row) {
      int x = n-1;
      for (int col = n-1; col >= 0; col--) 
        if (piece[row][col] != '.') {
          piece[row][x] = piece[row][col]; x--;
        }
      while(x>=0) {piece[row][x]='.'; x--;}
    }
(2) Test if there are K pieces in a row of the same color. There are tricks that can be done to speed this up, but in our problem, N is at most 50, and no special optimizations are needed. For each piece, we can just start from that piece and look in all 8 directions (or we can do just 4 directions because of symmetry). For each direction, we go K steps from the starting piece, and see if all the pieces encountered are of the same color. The code -- how to go step by step in a direction, and how to check if we are off the board -- will look similar in many different programming languages. We encourage you to check out some of the correct solutions by downloading them from the scoreboard.

Round 1A 2010 - Code Jam 2010

Make it Smooth (12pts, 24pts)

The Basic Solution
Just about any solution to this problem is going to ultimately rely on building up a smoothed array by first solving smaller sub-problems. The challenge is two-fold: (1) What are the sub-problems you want to solve? And (2) How can you efficiently process them all?

It is natural to start off by making the first N-1 pixels smooth, and then figuring out afterwards what to do with the last pixel. The catch is what we do with the last pixel q depends on p, the final pixel we set before-hand. If p and q differ by at most M, then we are already done! Otherwise, we have two choices:

Delete q with cost D, leaving the final pixel of our smoothed picture as p.
Move q to some value q' with cost |q - q'|. If |q' - p| > M, we will then have to add some pixels before-hand to make the transition smooth. In fact, we will need to add exactly (|q' - p| - 1)/ M of these pixels.
Fortunately, both of these cases are easy to analyze, as long as we are willing to loop through every possible value of q'.
Perhaps the trickiest part of this setup is understanding insertions. After all, when deciding what steps to take to smooth out the transition from one pixel in the starting image to the next pixel, there are a lot of options: we could change either pixel and we could have any number of insertions between them. The insight is that once we have decided where to move both pixels, it is obvious how many insertions we need to do.

The pseudo-code shown below recursively finds the minimal cost to make pixels[] smooth, subject to the constraint that the final pixel in the smoothed version must equal final_value:

  int Solve(pixels[], final_value) {
    if (pixels is empty) return 0

    // Try deleting
    best = Solve(pixels[1 to N-1], final_value) + D

    // Try all values for the previous pixel value
    for (all prev_value) {
      prev_cost = Solve(pixels[1 to N-1], prev_value)
      move_cost = |final_value - pixels[N]|
      num_inserts = (|final_value - prev_value| - 1) / M
      insert_cost = num_inserts * I
      best = min(best, prev_cost + move_cost + insert_cost)
    }
    return best
  }
To answer the original problem, we just take the minimum value from Solve over all possible choices of final_value.
Unfortunately, this algorithm will be too slow if implemented exactly like this. Within each call to Solve, we are making 257 recursive calls, and we might have to go 100 levels deep. That won't finish in any of our life times, let alone within the time limit! Fortunately, the only reason it is this slow is because we are duplicating work. There are only 256 * N different sets of parameters that we will ever see for the Solve function, so as long as we store the result in a cache, and re-use it when we see the same set of parameters, everything will be much faster. This trick is called Dynamic Programming or more specifically, Memoization.

The Fancy Solution
The run-time of the previous solution is O(256 * 256 * N), which is plenty fast in a competitive language. (Some interpreted languages are orders of magnitude slower at this kind of work than compiled languages - beware!) The extra 256 factor comes from the fact that we need to try 256 possibilities within each function call. It is actually possible to solve this problem in just O(256 * N) time. Here are some hints in case you are interested:

As before, you want to calculate Cost[n][p], the cost of making the first n pixels smooth while setting the final pixel to value p. Unlike before, you want to do a batch of these updates at the same time. In particular, you want to simultaneously calculate all values for n+1 given the values for n.
So how do we do this batch update? First, let's do an intermediate step to calculate Cost'[n][p], the minimum cost for each value after doing all insertions between pixel n and pixel n+1. To make this more tractable, it helps to notice that there is never any need to insert a pixel with distance less than M from the previous pixel. (Do you see why?)
The real challenge is that when calculating Cost[n][] from Cost'[n][], you are going to want to take minimums over several elements. For example, Cost[n][q] = min(Cost[n-1][q], {Cost'[n][q-M], Cost'[n][q-M+1], Cost'[n][q-M+2], ..., Cost'[n][q+M]}). In other words, given the array Cost'[n][], you are going to need to be able to calculate in linear time the minimum element in each range of length 2M+1. This is an interesting and challenging problem in its own right, and we encourage you to think it through! For a more thorough discussion of this sub-problem, see here.

Round 1A 2010 - Code Jam 2010

Number Game (16pts, 25pts)

Let us begin by focusing on a single game. Given A and B, we can first assume without loss of generality that A ≥ B. Now, how can we decide if it is a winning position or not? (It is impossible to have a tie game since A+B is always decreasing.) Well, a position is winning if and only if, in one step, you can reach a losing position for your opponent. This is an important fact of combinatorial games, so make sure you understand why it's true!

Observations, easy and not so easy
One trivial observation is that (A, A) is a losing position.

Another observation is much trickier unless you have already been exposed to combinatorial games before:

If A ≥ 2B, then (A, B) is a winning position.
To justify this: in such a position, suppose k is the largest number of B's we can subtract from A, i.e., A - kB ≥ 0 and A - (k+1)B < 0. We do not know yet whether (A-kB, B) is a winning position or not. But there are just two possibilities. If it is losing, great, we can subtract kB from A, and hand the bad position to our opponent. On the other hand, if it is winning, we can subtract (k-1)B from A, and our opponent has no choice but to subtract another B from the result, giving us the winning position (A-kB, B). Therefore, (A, B) is a winning position either way!
Expand further
The observation above gives us a fairly quick algorithm to figure out who wins a single game (A, B). Instead of using dynamic programming to solve the subproblem for all (A', B') with A' ≤ A and B' ≤ B, which is the most common way of analyzing this kind of game, we can do the following:
In round 1: If A ≥ 2B, it's a winning position and we're done. Otherwise, we have only one choice: subtract B from A, and give our opponent (B, A-B).
In round 2: If B ≥ 2(A-B), it is a winning position for our opponent. Otherwise, the only choice he has is to subtract A-B from B, and hand us (A-B, 2B-A).
In round 3: If A-B ≥ 2(2B-A), it is a winning position for us again. Otherwise, we are going to make it (2B-A, 2A-3B).
And so on. This leads to the following algorithm for efficiently solving a single game (A, B), assuming A ≥ B:
  bool winning(int A, int B) {
    if (B == 0) return true;
    if (A >= 2*B) return true;
    return !winning(B, A-B);
  }
Does this sound familiar? One connection you might see is that the Number Game closely resembles Euclid's algorithm for greatest common divisor. It is not hard to see that this algorithm, like Euclid's, will need to recurse at most O(log A) times.

Unfortunately, we still cannot afford to run the algorithm for every possible (A, B)! To solve the problem, we need to work with many positions at once. Let us go through the same rounds, but imagine having a fixed B and consider all possible A's at the same time:

Round 1: (A, B). If A ≥ 2B, i.e., A/B ≥ 2, then (A, B) is winning.
Round 2: (B, A-B). If B ≥ 2(A-B), i.e., A/B ≤ 3/2, then (A, B) is losing.
Round 3: (A-B, 2B-A). If A-B ≥ 2(2B-A), i.e., A/B ≥ 5/3, then (A, B) is wining.
Round 4: (2B-A, 2A-3B). If 2B-A ≥ 2(2A-3B), i.e., A/B ≤ 8/5, then (A, B) is losing.
And so on.
This gives a fast enough solution to our problem. For each B, we consider all A's in the above manner, and in O(log 106) rounds, we can classify all the A's.
Simplify
The above method is perfectly correct, but it can be made simpler. First of all, does anything in the above list look familiar? There are Fibonacci numbers all over the place! Let F(i) be the i-th Fibonacci number. One can show by induction that the previous analysis is actually saying the following:

Round 2t-1: If A/B ≥ F(2t+1)/F(2t), then (A,B) is a winning position.
Round 2t: If A/B ≤ F(2t+2)/F(2t+1), then (A,B) is a losing position.
It turns out that both F(2t+1)/F(2t) and F(2t+2)/F(2t+1) approach the golden ratio φ = (1 + √ 5) / 2 as t gets large. This means there is a very simple characterization of all winning positions!
Theorem: (A, B) is a winning position if and only if A ≥ φ B.
Using this theorem, it is easy to solve the problem. Loop through each value for B, and count how many A values satisfy A ≥ φ B.
Why it is Golden?
Once we have stumbled upon the statement of this theorem, it is actually pretty easy to prove. Here is one method: Using mathematical induction, assume we proved the theorem for all smaller A's and B's. If A ≥ 2B, then (A, B) is a winning position as we discussed earlier. Otherwise we will leave our opponent with (B, A-B). Then (A, B) is winning if and only if (B, A-B) is losing. By our inductive hypothesis, this is equivalent to B ≤ φ (A - B), or A ≥ ((1 + φ) / φ) * B. Since φ = (1 + φ) / φ, this proves (A, B) is winning if and only if A ≥ φ B, as required.

Here is another, more geometric viewpoint. You start with a piece of paper which is an A by B rectangle (A ≥ B), and cut out a B by B square from it. If the rectangle is golden, where A = φB, then the resulting rectangle will also be golden. In our game, A and B are integers, so the rectangle is never golden. We call it thin if A > φB, otherwise we call it fat. From a thin rectangle you can always cut it to a fat rectangle, and from a fat one you can only cut it to a thin one. They correspond to the winning positions and losing positions, respectively.

   
   A picture of golden rectangles from Wikipedia.

Other Approaches
There are many ways of arriving at the solution to this problem -- our analysis focuses on only one of these ways. Another approach would be to start with a slower method and compute which (A, B) are winning positions for small A, B. From looking at these results, you could easily guess that (A, B) is winning if and only if A ≥ x B for some x, and all that remains would be to figure out what x is!

More Information
Euclid's Algorithm - Fibonacci Numbers - Golden Ratio

Round 1B 2010 - Code Jam 2010

File Fix-it (12pts, 14pts)

This is an easy problem. Especially when efficiency is not a big issue here.

For each directory you want to create, we get all the directories you need. Those are all the ancestors of that directory. For example, if one items in the input of wanted directories is

/home/gcj/round1b/problema/input
We need all the following
/home
/home/gcj
/home/gcj/round1b
/home/gcj/round1b/problema
/home/gcj/round1b/problema/input
Let A be the collection of all the directories we need, and B be the collection of all the directories that are already exist. Simply count how many elements are from A but not B. You need to use one mkdir for each of them.

Round 1B 2010 - Code Jam 2010

Picking Up Chicks (13pts, 17pts)

The solution for this problem consists of several steps, each making the problem a bit easier until it becomes simple.

Step 1. Do swaps as fast as possible (or don't do them at all)
Suppose a chick catches another one in front of it. We have the following options:

lift the slow one immediately to let the fast one pass;
let them run together for some time, and then lift the small one;
let them run together all remaining time and not let the fast one pass at all.
The first observation that we need to solve this problem is that option 2 is never necessary. Intuitively: what's the point of holding the fast one if we will still do the same work on swapping them later? Formally: suppose we have a sequence of swaps that forms the solution for the problem, and that swaps chicks P and Q at time T1, while they first meet at time T0, T0<T1. Let's move this swap to time T0. Note that for each particular chick and each particular moment, the position of this chick at this moment will either stay unchanged, or move closer to the barn. Because of this, the changed solution is also valid.

Step 2. Never swap two chicks that will make it to the barn in time
Suppose that we swap two chicks that would both make it to the barn in time in the end. Then we can avoid this swap and still have both chicks arrive at the barn in time. We will need to do the same number of swaps for the fast chick during the rest of the way than we had to previously (or even fewer, since some chicks might get out of our way), so we'll save at least one swap by keeping the two chicks together until the end.

Step 3. If a chick can't make it to the barn in time, all chicks behind it will have to swap with it
Suppose a chick can't theoretically make it to the barn in time: Xi+T*Vi<B. Then all chicks that start behind it will need to be swapped with this check or else they won't make it to the barn in time either.

Step 4. Split the chicks into two classes to get a lower limit on the number of swaps
Let's paint the chicks that can theoretically make it to the barn in time with red color, and the ones that can't do that with blue color.

For every red chick, the amount of swaps needed to get to the barn on time is at least the number of blue chicks that start closer to the barn. This gives us a lower bound on our answer: if we count this number for each red chick, then the answer is at least the sum of K smallest such numbers.

Step 5. The lower bound that we've found is actually the correct answer
At the previous step, we've found some swaps that are necessary in order to solve this problem. Now we note that we don't need any more swaps. More precisely, let's take K red chicks that are closest to the barn initially, and swap them with the blue chicks as soon as they reach any. Since we took K red chicks that are closest to the barn, the number of blue chicks that will get in their way is minimum possible (remember that we never need to swap two red chicks!).

Step 6. Code!
After making all of the above observations, the actual solution becomes very simple:

  num_red = 0
  num_blue = 0
  answer = 0
  for i = N - 1 .. 0:
    if num_red == K:
      break
    if X[i] + T * V[i] < B:
      num_blue += 1
    else:
      num_red += 1
      answer += num_blue
  if num_red >= K:
    output answer
  else
    output "IMPOSSIBLE"

Round 1B 2010 - Code Jam 2010

Your Rank is Pure (14pts, 30pts)

Foreword
This is a very "mathematical" problem, and solving it required thinking in very formal terms. So please bear with a lot of formulas and little text in the below explanation.

Initial approach
Let's study the process described in the problem statement. Suppose the rank of number N with respect to set S is K. Since N is the largest number in S, that just means the number of elements in S is K.

Then let's consider the set S' = S ∩ {1, 2, ..., K}. From the definition of a pure number, K is now pure with respect to S'.

Have we got a Dynamic Programming solution yet?
Does that mean that we've managed to reduce the problem for N to a smaller problem for K? Not yet: suppose we know the number of possible sets S' for which K is pure. How do we find the number of sets S that contain this set (and for which N is pure and that have K elements)?

In order to do that, we need to know how many numbers are there in S'. Suppose there are K' numbers in S'. Then the number of ways to extend this set S' back to S is the number of ways to choose K-K'-1 numbers from the set {K+1, K+2, ..., N-1}.

Now we have a Dynamic Programming solution!
Let's define Count[N, K] to be the number of sets S that are subsets of {2, 3, ..., N}, have K elements, contain number N and for which number N is pure.

The above discussion proves that Count[N, K] is equal to the sum over all K' of Count[K, K'] times C[N-K-1, K-K'-1], where C[A, B] is the number of ways to choose B items out of A (so-called combination number).

We can calculate Count values in increasing order of N. That will give us O(N2) values to calculate, each requiring O(N) operations, for the total running time of O(N3). That seems to be too slow for N=500 and 100 testcases.

Final observation
However, one can notice that the above algorithm calculates the answer for smaller values of N as well. That means we can run it just once for N=500, and get the answers for all testcases at once, so the total runtime will be just O(N3), which is okay.

Round 1C 2010 - Code Jam 2010

Rope Intranet (9pts, 13pts)

This problem was easier than it could have been, since the constraints don't require you to write an efficient solution. To solve it you can simply iterate through each pair of ropes and test if they intersect. Checking for intersection can be done in various ways. One way is to write the two line equations and then solve a system of linear equations with two variables to find the intersection point. An easier solution is to simply check if the order of the ends of the pair of ropes on the first building is the opposite of the order of the ends of the ropes on the second building. This translates to the code:

(A[i] - A[j]) * (B[i] - B[j]) < 0
This algorithm takes O(n2) and it's fast enough to solve our problem.
This problem is very similar to the classic one which asks for the number of inversions within a given permutation. An inversion for a permutation p is a pair of two indexes i < j such that pi > pj. Let's see why these problems are equivalent. If ra is the rank of A[i] when we sort A and rb is the rank of B[i] when we sort B, then the ropes problem becomes the inversion count problem on the permutation p where pra = rb for each i.

This new problem is a good application for divide and conquer algorithms, and can be solved in O(n log n) time. Merge sort can be adapted nicely to not only sort an array, but count the number of inversions as well. Other solutions use data structures like segment trees, augmented balanced binary search trees or augmented skip lists.

Number of inversions in a permutation

Round 1C 2010 - Code Jam 2010

Load Testing (14pts, 22pts)

Understanding the sample cases
In order to solve this problem, one first needs to get a feeling of what's asked, and looking at the sample cases is a good way to achieve that.

Take a look at the first sample case. The answer given is 2. How do we achieve the result with just 2 loadtests? Let's make a couple guesses. Suppose we do a loadtest that checks if the site supports 100 users. If we learn that the site can't support 100 users, then we're done: we know we can support 50 users but can't support 100 which is 50*2. However, if we learn that the site can actually support 100 users, we have a very difficult task at hand now: we have only one loadtest left, and we know that the site can support 100 users but can't support 700. Is it possible to solve it?

Suppose we now loadtest to check if the site supports 300 users. If we learn that the site can't support 300 users, then we've failed to solve the problem: we know we can support 100, but can't support 300 - but 300 is more than 100*2, so we don't have enough knowledge. Moreover, this actually helps us to prove that our loadtest must test for 200 users or less, otherwise we will hit the same issue.

Now we know that our second loadtest must use at most 200 users. But even when it's exactly 200, suppose we learn that our site can actually support all of them. Then we've failed to solve the problem again: we know we can support 200, but can't support 700 - but 700 is more than 200*2.

So there's no good choice for our second loadtest. It means that the choice of the first loadtest was wrong - 100 users is too small for it.

What have we learned?
However, we've learned an important lesson in our failed attempt to understand the example case: when we have only one loadtest left, and we know that the site can support L people but can't support P people, we must loadtest with such number X that L*C>=X, and at the same time X*C>=P. The first inequality will help us solve the problem when the loadtest fails, and the second one is helpful if the loadtest succeeds.

Since there's no such number X for L=100, P=700, C=2, our first attempt above has failed.

The question now is: how to check if such X exists? From the first equation, we get X<=L*C. From the second one, we get X>=P/C. Such X exists if and only if L*C>=P/C, which means L*C2>=P. Forgetting the formulas, the upper bound of our range should be at most C2 times more than the lower bound. In that case, we can just take X=L*C for our only loadtest.

The second attempt at understanding the first sample case
Equipped with this knowledge, we get back to the first sample case. 100 was wrong since 100*22=100*4<700. Maybe we should loadtest for 300 people first? If the loadtest succeeds, then we will have one loadtest left, 300 people OK, 700 people not OK, and since 300*4>=700, we can solve the problem. However, what if the loadtest doesn't succeed? We know that our system can support 50 people but can't support 300 people and have only one loadtest left. Since 50*4<300, we can't do that. So the choice of 300 was also wrong.

What if we try 200 as the first loadtest? In case it succeeds, we get one loadtest, 200 OK, 700 not OK, 200*4>=700 - we can do that. In case it fails, we get 50 OK, 200 not OK, 50*4>=200 - we can do that as well. So we've finally figured out the algorithm to solve the first sample case using just 2 loadtests:

Loadtest for 200 people. If the site can support 200 people:
  Loadtest for 400 people.
If the site can't support 200 people:
  Loadtest for 100 people.
What have we learned?
So how do we figure out if two loadtests are enough? This is actually surprisingly similar to the study of the one loadtest case.

When we have two loadtests left, and we know that the site can support L people but can't support P people, we must loadtest with such number X that L*C2>=X, and at the same time X*C2>=P. The first inequality will help us solve the problem using one remaining loadtest when the loadtest fails, and the second one is helpful if the loadtest succeeds.

Using the same argument as above, one can see that such number X exists if and only if L*C4>=P (we got C4 as C2*C2).

More loadtests?
Now it's not so hard to figure out what happens with more than two loadtests. It's possible to solve the problem using three loadtests if and only if L*C8>=P. For four loadtests, we get L*C16>=P. And so on. That pretty much describes the solution for this problem.

Understanding the sample cases, attempt 3
Now we can finally figure out the algorithm to solve the third sample case: L=1, P=1000, C=2. In order to do this in four loadtests, our first loadtest can be for L*C8=256:

Loadtest for 256 people. If the site can support them:
  Loadtest for 512 people.
If we can't support 256 people:
  Loadtest for 16 people. If the site can support them:
    Loadtest for 64 people. If the site can support them:
      Loadtest for 128 people.
    If we can't support 64 people:
      Loadtest for 32 people.
  If we can't support 16 people:
    Loadtest for 4 people. If the site can support them:
      Loadtest for 8 people.
    If we can't support 4 people:
      Loadtest for 2 people.
This looks quite similar to the binary search algorithm, but performed on exponential scale.

Conclusion
We started solving this problem by trying to understand the answers for the sample cases, and by the time we actually understood them, we already have a complete solution. The only remaining thing is to implement the solution carefully avoiding the integer overflow issues.

Round 1C 2010 - Code Jam 2010

Making Chess Boards (18pts, 24pts)

Finding the largest chess board
First, we need a quick way to find the largest chess board. There is classic dynamic programming trick that goes like this. Let's compute, for each cell (i, j), the size of the largest square whose bottom-right corner is (i, j). Let's call this value larg[i][j]. It is easy to compute larg[i][0] and larg[0][j] -- they are always 1. For any other cell, the value of larg[i][j] is always at least 1, and it is larger only if the following condition holds:

  if (board[i - 1][j] != board[i][j] &&
      board[i][j - 1] != board[i][j] &&
      board[i - 1][j - 1] == board[i][j]) {
    larg[i][j] = 1 + min(larg[i - 1][j],
                         larg[i][j - 1],
                         larg[i - 1][j - 1]);
  }
In a single, linear-time, row-by-row scan, we can compute the values of larg[][] for all cells.
Finding the chess board to remove
Now that we have larg[][], it is very easy to find the first chess board that we should cut out. Its bottom-right corner is in the cell that has the largest possible value in larg[][]. If there are several such cells, we use the tie-breaking rules described in the problem and choose the one that comes first in lexicographic order of (i, j).

We can do this in linear time by scanning larg[][], but since we will have to do this many times, it is better to make a heap of triples of the form

  (-larg[i][j], i, j)
and take the smallest element from that heap. This way, we are sorting all cells by decreasing size, then by increasing row, then by increasing column. As long as we can update this heap efficiently after cutting out a chess board, we can always retrieve the smallest element in O(log(m*n)) time. We could also use a balanced binary search tree instead of a heap.
Removing the chess board and updating larg[][]
Consider removing the first 6x6 chess board from the example input described in the problem statement. How should we update larg[][]? First of all, we can fill the 6x6 square of cells with zeros because there are no more chess boards to be removed from those locations. But that is not all. There are other cells that might need to be updated. Where are they, and how many of them are there?

Naively, we can simply recompute the values of all non-zero cells in larg[][] and continue. If we do that, we will have a O(m2*n2) algorithm, which is too slow.

First of all, notice that we do not need to update rows above or to the left of the 6x6 square. Any square chess boards whose bottom-right corners are in those areas still exist and can be cut out later. The only chess boards that we need to worry about are those that overlap the 6x6 board that we have just removed. Also, notice that we have just removed the largest possible chess board, so we only need to care about remaining boards of size 6 or smaller. Where can their bottom-right corners lie in order for those boards to overlap our board? They must be in the 12x12 square whose center is at (i, j) -- the bottom-right corner of our board.

That's an area of size 4*62. In fact, whenever we remove a board of size k-by-k, we only need to update an area of larg[][] of size at most 2k-by-2k. Since we can only remove each cell at most once, all of the updating work requires linear time in total; 4*m*n updates, to be precise.

Updating the heap
Each time we update larg[][], we must also update the heap that lets us find the next board to remove. This means finding and removing an old entry, as well as inserting a new entry. With pointers from cells to heap elements, or by using a balanced binary search tree, both steps can be done in O(m*n) time.

In total, this algorithm runs in O(n*m*log(n*m)) time, which is plenty fast for the problem's constraints.

Round 2 2010 - Code Jam 2010

Elegant Diamond (4pts, 8pts)

On this very difficult round, even the first problem was pretty challenging. The constraints were fairly small, but it might not be clear how to even get started. After all, there are a lot of ways you can enhance a diamond!

Let's start off with a slightly simpler question: given a position (cx, cy), is it possible to enhance (i.e, extend) the given diamond into an elegant diamond centered at (cx, cy)? (Note that this center might be either at a number, or at a space between numbers, depending on whether the elegant diamond has even or odd side length.) The resulting diamond would have to be symmetrical about the lines x = cx and y = cy. Specifically, this means that for each (x, y), the values in positions (x, y), (2cx - x, y), (x, 2cy - y), and (2cx - x, 2cy - y) all have to be equal. If the starting diamond already has two different values in one of these quartets, there's nothing we can do to change that.

Otherwise, it turns out that the diamond always can be extended to an elegant diamond with center at (cx, cy). For each of the quartets above where we have one value, we have to fill out the remaining values to be equal. After that, we can just enclose everything in a diamond shape and fill all remaining squares with 0. Here's an example:

  1
 2 3
5 6*6
 2 3
  1
Let's try to extend this to an elegant diamond centered at the *. First, we fill in all quartets, and then we extend to a proper diamond by adding 0's:

                             0
  1           1 1           1 1
 2 3         2 3 2         2 3 2
5 6*6  -->  5 6*6 5  -->  5 6*6 5
 2 3         2 3 2         2 3 2
  1           1 1           1 1
                             0
Done!

This is pretty clearly the smallest extension with the given center, so all we need to do is try each possible center, and choose the one that gives the smallest possible elegant diamond.

Of course, there are a lot of possible centers, but there is no need to consider one completely outside of the bounding box of the original diamond. We can always move such a center onto the edge of the bounding box without creating any inconsistencies, and that will lead to a smaller diamond in the end.

So in summary: we need to iterate over all possible centers contained within the original diamond, check if there is an elegant extension with that center, and then take the smallest of these. As always, please take a look at some of the correct submissions to see some full implementations!

By the way, the solution presented here is O(n^4), which is fast enough for this problem. Faster solutions do exist though, and you might enjoy looking for them.

Round 2 2010 - Code Jam 2010

World Cup 2010 (10pts, 15pts)

Reformulation
In the analysis, we will turn the picture upside-down. This conforms to the usual convention that the root of a binary tree is drawn on top. This is actually easy to do implementation-wise. We just read the input numbers in reverse order; the element at position 0 is the root, i.e., the final match, and for any node i, its two children are indexed at 2i+1 and 2i+2. In our particular solution, we include nodes for the teams as well as for the matches, so our graph is a rooted complete binary tree, where all the internal nodes are matches, and all the leaves are the teams.

Perhaps it is clear that the solution will be dynamic programming on the tree. There are many ways to do it, but some can be much more complicated than others. Let us introduce one solution that is very simple. It is based on the following reformulation of our problem.

Let us color a node yellow if we are going to buy the ticket for the corresponding match. For each leaf (a team), the path to it from the root has P internal nodes. We call a plan good if no matter what the outcomes of the matches are, we will never miss more than M[i] matches for team i. We have

(*) A plan is good if and only if for every team i, the path from the root to the leaf corresponding to team i has at least P-M[i] yellow nodes.
This is indeed a conceptual leap in solving the problem. For in the new form, we no longer need to worry about the outcome of any particular match. And once it is stated, it is very easy to justify. We leave the justification to the readers.
Simple solutions for small inputs and large inputs both follow easily from this reformulation.

Small dataset
For the small dataset the best plan must be upwards closed, i.e., if a node is yellow, all the nodes on the path from the root to it must all be yellow. One can prove this by using (*) and noting that all the prices are the same, and you can always get a cheaper plan if you push some yellow node upwards. So, one solution for the small dataset looks like this: For each team i, we find the path from root to it, and color the topmost P-M[i] nodes yellow. It can be done in linear time by a simple modification of depth first search.

The justification and the implementation details are a nice piece of dessert. We leave them to the readers.

Large dataset
We use dynamic programming on the tree. Let us just define the subproblems clearly. For any node a and any number b between 0 and P, let P(a, b) be the problem

If there are b yellow nodes on the path from the root to a (not including a), what is the cheapest way to color the nodes in the sub-tree rooted at a, such that all the leaves in this sub-tree satisfy the condition in (*)?
And we can use a impossibly big answer to indicate that the condition can not be satisfied.
Once the DP is set up, the code follows easily. For each internal node there are just two choices, color it yellow or not. We provide an excerpt from one of the judges' solutions:

i64 A[1<<11][11];  // answers for the dynamic programming.
i64 inp[1<<11];    // input, in reversed order.
int P;

// rooted at a, already buy b tickets from above.
i64 dp(int a, int b) {
  i64& r=A[a][b];
  if(r>=0) return r;
  if(a>=((1<<P)-1)) {r=(b>=(P-inp[a]))?0:1LL<<40; return r;}
  r=std::min(
      inp[a]+dp(2*a+1,b+1)+dp(2*a+2,b+1), 
      dp(2*a+1,b)+dp(2*a+2,b));
  return r;
}

int main() {
  int T;
  cin>>T;
  for (int cs=1; cs<=T; ++cs) {
    cin>>P; int M=(2<<P)-1;
    for (int i=0; i<M; i++) cin>>inp[M-1-i];
    memset(A, -1, sizeof(A));
    cout << "Case #" << cs << ": " << dp(0, 0) <<endl;
  }
  return 0;
}

Round 2 2010 - Code Jam 2010

Bacteria (6pts, 25pts)

The fact that the input is a set of rectangles is not essential to this problem. We just wanted to limit the amount of data you had to download to test your program. So let us forget about the rectangles, and instead consider any initial configuration of n bacteria on the grid. The challenge here is to compute the answer fast. A turn by turn simulation will not be good enough. We are going to aim for an O(n) solution instead.

Examples
One way to start attacking this problem is to look at examples. The Bacteria game is pretty fun to play around with after all! Here are some examples you can try:

A rectangle of H by W bacteria.
A H by W rectangle where the bacteria only lie on the four boundary edges.
Bacteria along a "type 1" diagonal (a southwest/northeast diagonal).
Bacteria along a "type 2" diagonal (a northwest/southeast diagonal).
A random path where each pair of bacteria are connected either horizontally, vertically, or along a type 1 diagonal.
If you work out these examples, you should get some inspiration for the general problem. We call a type 1 diagonal higher than another if it is to the north (and hence west) of the other. If the initial configuration is a "connected piece" (we will define this more precisely later), we can find the highest type 1 diagonal containing at least one bacterium X + Y = C, the right-most point containing a bacterium X = Xmax, and the bottom-most point containing a bacterium Y = Ymax. We claim that after one turn, the configuration will still be a single connected piece, the highest diagonal will become X + Y = C+1, and the max X and Y coordinates will not change. This will continue until the final second when we come down to a single point (Xmax, Ymax). So the number of turns before everything disappears is Xmax + Ymax - C + 1.
One thing we note is that the 3rd and 4th examples behave very differently. In the 3rd example, the number of bacteria decreases by 1 in each turn, while in the 4th, all the bacteria disappear immediately.

Solution
Define a graph on the bacteria. Two bacteria are neighbors if they are adjacent on the grid via a horizontal line, a vertical line, or a type 1 diagonal. We begin by finding the connected components of this graph.

This may seem like a simple definition, but it is really the key to solving the whole problem. It is quite similar to the more common graph where each grid node has 8 neighbors, one in each compass direction, except we do not consider the 2 directions along a type 2 diagonal. Remember the 3rd and 4th examples above? If we start with n nodes, there is just one connected component in the former, but n connected components in the latter.

We observe that after each turn, a connected component will give birth to a new connected component (unless it was a single point and disappears), and different components will not become connected together. See the details in the next section.

So, the answer is the maximum elimination time over all the components. For a single piece (i.e., connected component), we find the highest type 1 diagonal X + Y = C, as well as the maximum coordinates Xmax and Ymax. The number of turns for that piece to disappear is then Xmax + Ymax - C + 1.

Justification
Our solution depends very heavily on several key observations. If you try some examples, you should be able to convince yourself empirically that these observations just have to be true. But in the interest of completeness, we will also sketch out a more formal proof here.

In the following discussion, we will fix a configuration called the old state, and we will consider the new state obtained after one turn. When we talk about a connected piece, we will always assume the non-trivial case where there are at least 2 points in the piece.

For each bacterium in the new state, let us consider the reason why it is there. If it was in the old state, we credit the reason for its existence to the set of itself and its north and/or west neighbor. If it was not in the old state, we credit the reason for its existence only to the set of its north and west neighbors. In both cases, we call such a set the credit set for that single bacterium.

Proposition 1. If the old state is a single piece, and the highest type 1 diagonal is X + Y = C, then in the new state, the highest type 1 diagonal will become X + Y = C+1.

Proof. Consider a bacterium at position (X, Y) on the top diagonal of the old state. Since this is the top diagonal, that bacterium does not have a north neighbor or a west neighbor, and it will die. In particular, the diagonal X + Y = C will be completely empty in the new state.
Now pick the south-most bacterium in the old state on the diagonal X + Y = C. Let its position be (X, C-X). If there is also a bacterium at position (X+1, C-X-1) in the old state, there will be a bacterium at (X+1, C-X) in the new state (no matter whether it was in the old state or not). Otherwise, since (X, C-X) is part of a connected piece and it's on the highest diagonal, there will be a bacterium at either (X, C-X+1) or (X+1, C-X) in the old state, and by our rules, this bacterium will survive to the new state. Hence, in any case, we find at least one bacterium will be on the diagonal X + Y = C+1 in the new state, which proves the proposition.

The following two Propositions can be justified in the same way:

Proposition 2. If the old state is a single piece, the maximum X coordinate and the maximum Y coordinate will be unchanged in the new state.

Proposition 3. If the old state is a single connected piece, the new state will be a single connected piece as well.
To see Proposition 3, note that if the old state is connected, then any two credit sets can be joined by some path consisting of horizontal, vertical, and type 1 diagonal segments. As the following picture shows, any possible segment from such a path will give birth to connected segments in the new state:

    

Finally, we are going to prove that different pieces will not be merged together in a turn.

Proposition 4. If two bacteria are neighbors in the new state, then their credit set are all from the same piece in the old state.

This is obviously true if both bacteria are also in the old state. Otherwise, it is one of the cases in the following picture. The two bacteria are the circled points. A blue point indicates that the bacterium was also in the old state, a red point indicates that the bacterium was only in the new state. Then the yellow stars represents the points in the credit sets. In any situation we either reach an impossible configuration, or get to a point where it is clear the credit sets are connected.

    

Round 2 2010 - Code Jam 2010

Grazing Google Goats (7pts, 25pts)

This problem is tough from both the implementation side and the algorithm side. In the analysis, we will begin by explaining some general concepts that show how the task is actually very similar to convex hull. We will then go into the particulars needed to build a solution and to prove that it is right. The resulting code isn't as scary as the length of the analysis makes you think.

Let us just focus on one bucket Q at a time. The problem asks us to choose rope lengths: i.e., for each pole position, we need to choose the radius of the circle centered at that pole. The common area will be the intersection of all these circles. It is clear that decreasing the radius of any circle will not increase the intersection. Therefore, we want the lengths to be as small as possible. Obviously, for each pole Pi, PiQ is the smallest length we can pick so that the goat can still reach the bucket. Our main task is to efficiently compute the common intersection area of these circles.

From the limits of this problem, it is also clear that computing the intersection in Ω(N2) will be too slow. We are aiming for an algorithm that will, for each bucket, compute the intersection in time O(N log N).

The intersection is a convex shape where the boundary consists of arcs of the circles. One can prove in various ways (some of the reason will be obvious below) that each circle will only contribute at most one such arc. We will mostly focus on how to compute these arcs -- to which circle does it belong, where does it stop and where does it start. Once all this is computed, it is relatively easy to get the area.

Some theoretical background
While this section is not absolutely necessary for solving the problem, it will provide some nice insights. Once we have seen the problem from a few different angles, the solution will be conceptually clear and very natural.

So, first of all, note that all the circles we consider pass through a common point Q. If you have not seen it before, it is now our honor to introduce to you the beautiful geometric transformation called inversion. It takes Q as the center. And each point X ≠ Q is mapped to a point X' where QX and QX' are in the same direction, and the lengths satisfy |QX| |QX'| = 1.

Here is one very nice property of inversion: Every circle passing through the center point Q is mapped to a line that does not pass Q, and vice versa. And the interior of the circle is mapped to a half-plane that does not include Q. For our problem, the intersection of the N circles is then mapped to the intersection of N half-planes. For more details on inversion, we refer you to the Wikipedia page on inversive geometry.

We leave it to the reader to verify that, if the intersection is not empty, then we can rotate the plane such that Q is above all the half-planes. And the intersection of the half-planes is bounded by something called the lower envelope of line arrangements. The segments of the lower envelope are exactly the images of the arcs in the circle intersection from our original problem.

Then there is a concept of duality in computational geometry that maps each line y = ax + b to the point (a, -b). The lower envelope is mapped by this transformation to the upper convex hull of the set of corresponding points.

So, our problem is indeed equivalent to the convex hull problem and the lower envelope of line arrangements problem. Both subjects are well studied and there are simple O(N log N) algorithms for both.

Out of these three settings, perhaps the algorithm for line arrangement is the simplest to visualize: Sort the lines by their slopes, and add them one by one. In each step, the existing lower envelope will be cut by the new line in two parts. The time for sorting is O(N log N), and the amortized complexity for the rest is O(N).

Solution to our problem
The previous section suggests a couple approaches that begin with explicitly doing an inversion about Q. In this section, we will discuss a direct solution to the problem, where we do not need to view the problem using the transformations from the previous section. Note that it will be in some ways equivalent to the above algorithm we introduced for line arrangements.

Let's look from Q along a straight ray. The intersection of this ray with each circle will be either just Q, or a segment between Q and the second intersection point of this ray with the boundary of the circle. That means that in order to build the area required in the problem statement, we need to find the circle "closest" to Q in each direction from it. Let's denote each direction by its polar angle, which is defined up to a multiple of 2π.

We start by considering just one circle. Suppose the polar angle of a ray that goes from Q towards the center of that circle is φ. When the polar angle of a ray goes from φ-π/2 towards φ, the distance from Q to the second intersection point with that ray increases from 0 to the diameter of the circle. When the polar angle continues from φ towards φ+π/2, the distance decreases back to 0. When the polar angle changes from φ+π/2 to φ+3π/2 (the latter is the same as φ-π/2, where we started), the intersection of the ray with the circle is just Q.

Now consider two circles, one with center at polar angle φ (remember, all polar angles discussed here are relative to Q) , another one with center at polar angle ξ, and suppose 0 < φ - ξ < π. Then we'll see the following as we turn the ray originating from Q: starting from polar angle ξ-π/2 until φ-π/2 the ray will intersect only the second circle; from φ-π/2 the ray will intersect both circles, but the intersection point with the first circle will be closer to Q until the circles intersect; after the intersection and until ξ+π/2 the ray will still intersect both circles but the second is now closer; and from ξ+π/2 until φ+π/2 the ray will only intersect the first circle.

The important thing about the two circles case discussed above is that in the range of polar angles where the ray intersects both circles (and that's exactly the range we care about in finding the intersection area). The situation is very simple: before the intersection point one circle is closer to Q, after the intersection point another circle is closer to Q.

Now suppose we have many circles. First, we find the polar angle of the center of each circle, and thus also find the range of polar angles where a ray going from Q intersects each of the circles. We can then intersect all those ranges of polar angles to obtain a small range [α, β] of polar angles where the ray would intersect all circles. If this range is empty, then we already know there's no intersection.

Now let's start adding circles one by one, starting from the one with the smallest polar angle. You might ask, what does "smallest" mean when we're on a circle? Luckily, here we have less than full circle: since all circles intersect all rays in [α, β] range, the polar angles of all centers are between β-π/2 and α+π/2. That leaves us a segment of length less than π where we have a definite order.

As we're adding the circles, we'll maintain which circle is closest for each angle from [α, β]. For example, after adding the first circle it will be the closest for the entire [α, β] range. Now we add the second circle. Let's say the polar angle of the intersection point between two circles is γ. If you look carefully at the above analysis, you'll find that:

When γ is less than α, the first circle is still the closest for the entire [α, β] .
When γ is between α and β, the second circle is the closest for [α, γ], and the first circle is the closest for [γ, β].
When γ is more than β, the second circle is now closest for the entire [α, β] .
Here we rely on the fact that we process circles in increasing order of the polar angle of their center.

Now look at the general case. Suppose before adding circle number i we have the following picture: on [α, γ1] circle j1 is the closest; on [γ1, γ2] circle j2 is the closest; ...; on [γk-1, β] circle jk is the closest.

Consider the polar angle δ of the intersection point between circle i and circle j1. There are 3 ways it could compare with the range where j1 is the closest:

When δ is less than α, it means that circle j1 is closer than circle i on the entire [α β] segment, and we can just stop processing circle i since it doesn't affect the answer.
When δ is between α and γ1, we now have that circle i is the closest on [α, δ], and circle j1 is the closest on [δ, γ1]. After doing this change, we can also stop processing circle i since circle j1 will be closer to the center all the remaining way.
Finally, when δ is more than γ1, we can forget about circle j1 since circle i is closer than it on the entire [α, γ1] segment. In that case we continue processing circle i by comparing it with circle j2, and so on.
That's it! After we do this processing for all circles, we know the contour of the intersection figure, and we continue by computing its area.

The above algorithm requires a data structure that maintains a list of arcs with integer tags, allowing us to change the first arc, remove the first arc, and add a new first arc. The simplest way to get this data structure is to just use a stack where the first ac would be on the top, and the last one would be on the bottom.

The running time for the above algorithm can be estimated using amortized analysis as follows. When processing each circle, we do at most two "push to stack" operations, and one or more "pop from stack" operations. That means the total number of push operations is O(N), and since the number of pop operations can't be greater than the number of push operations (there'd be nothing to pop!), it's also O(N), giving us the total runtime of O(N). However, we must also remember the step where we sort all circles by the polar angle of their center, so overall runtime is O(N log N).

We've so far avoided discussing the low-level computational geometry needed for this problem. In the above solution we required two geometric routines: find the polar angle of the other intersection point of two circles which have the first intersection point at origin; and find the area of a figure that is bounded by arcs.

The first routine is straightforward if you can already intersect circles (a useful thing to know how to do!); alternatively, you could derive the formulas for the polar angle directly. The second one is slightly more tricky: first, we split the figure into "rounded triangles" with rays going from Q and having polar angles γ1, γ2, ..., γk-1. Each rounded triangle has two straight sides (one of those might have zero length) and one circular side. We can then split the rounded triangle into the corresponding triangle plus the round part (a sliced off part of a circle). You have probably seen how to compute the area of a triangle before - one good approach is the "shoelace formula". To calculate the rounded area, it helps to start with a "pie slice" from the center, whose area is a simple fraction of the total circle area, and then add or subtract triangle areas.

Of course there is a lot to do here, but that's why we made it Problem #4!

Round 3 2010 - Code Jam 2010

De-RNG-ed (4pts, 10pts)

Finding the next integer in the sequence means figuring out the values of A, B and P that fit the given numbers and using them to generate the next number.
This problem has a few special cases. In all of them, it is important to note that the value of P must always be larger than every element of the "randomly" generated sequence. (8 can never be a remainder after dividing by 7.) Also, the problem statement requires that P be no larger than 10D. Let's call all primes that satisfy both of these bounds "valid primes". Now, let's look at the special cases.

K = 1
First of all, when K is 1, the answer is always "I don't know." This is because we can pick any valid prime P, set A to 0 and B to 0 or 1. This will give us two different answers.

K = 2 and the two sequence elements are the same
In this case, the answer is unique because the next element of the sequence depends only on the current element. If two consecutive elements are the same, then the entire sequence consists of a single repeated number.

K > 2 and all the sequence elements are the same
Similarly, the next element must be the same as all other elements.

K = 2 and the two sequence elements are different
Here, the answer is always "I don't know." To see that, pick any valid prime and consider the cases A=0 and A=1. If we call the first element of the sequence x and the second element y, we can express y as a function of x, A, B and P: y = (A*x + B) % P. In both cases, we can solve this equation for B. The next element, z is then z = (A*y + B) % P, and it must be different in the two cases (A=0 and A=1) as long as x is different from y.

K = 3
We are going to brute force all valid primes and solve for A and B. We will then use these values to generate the next element of the sequence. If all the values we get this way are the same, then the answer is unique. If we get different valid answers, then the answer is "I don't know."

Let's call the 3 elements x, y and z. By writing y as a function of x and z as a function of y and subtracting y from z, we get
z - y = (A*y + B) - (A*x + B) = A*(y - x) (mod P).

We have already dealt with the case when x equals y, so we can assume that x and y are different, so we can divide by their difference. This lets us solve for A.
A = (z - y)*(y - x)-1 (mod P).
Computing the inverse of (y - x) can be done using the Euclidean algorithm, which runs in O(log(P)) time.

Once we have A, solving for B is easy:
B = y - A*x.

The answer is then A*z + B.

K > 3
In this case, we brute force P, use the first 3 elements of the sequence to solve for A and B, and check whether the remaining elements fit the sequence generated with these parameters.

Round 3 2010 - Code Jam 2010

Hot Dog Proliferation (6pts, 22pts)

Background
This analysis will have nothing to do with hot dogs. Instead of a long street with billions of corners, let's think of the line of integers; instead of vendors, let's think of chips -- after all, chips are much easier to maneuver than real people with hot dog stands!

We denote the number of total chips by n. Also, let's call a configuration stable if no two chips occupy the same integer point.

If you play around with the game for a while or if you have good (and brave!) intuition, you might realize that the problem statement is a little misleading. It turns out that no matter which move you do at each step, the final configuration, as well as the total number of moves you need to perform, will always be the same.

Indeed, this is a famous theorem for "chip-firing games", and our scenario is a special kind of chip-firing game. Intuitively, the reason why your choices don't matter is that (a) if you ignore a move now, you will still have to do it later, and (b) one move will not change the effect of another move down the line. This means that while you can control the order of moves, you will always do the same set of moves in the end, and they will always have the same effect.

This observation is enough to solve the small input. Just keep doing moves until the configuration stabilizes, and count how long it took. For the large input though, more insight is required. A configuration might require over 10^13 moves to stabilize, so simulating them one at a time is out of the question. The obvious optimization is to do several moves at once for very large piles. Surprisingly however, this does not help very much.

There are a few different ways to proceed, and we will discuss two of them.

Preliminary Observations
One very useful way of understanding this game is in terms of invariants. The first of these is pretty obvious, but the other requires either some special insight or some experience to see. In each move, we take two chips at some position x and send them to positions x-1 and x+1. Notice that:

(x-1) + (x+1) = x + x
(x-1)2 + (x+1)2 = x2 + x2 + 2.
This immediately leads to the following two observations:
Observation 1. The sum of the positions of all the chips never changes.

Observation 2. The sum of the squared positions of all the chips increases by 2 during each move.
So how do we use these observations? They aren't necessary, but they will have their uses as you will see. The former one will help us quickly construct a configuration with certain known properties from the initial configuration (more on this later). And with the latter observation, computing the number of steps becomes the same task as constructing the final configuration. For example, using Observation 2, we can easily estimate that the number of steps could be on the order of n3, thereby verifying that straightforward simulation really is hopeless.

Adding one chip
One good approach is to add chips one at a time, at each step doing enough moves to completely stabilize the configuration. The question is: how do we do this last part efficiently? So let's consider adding a chip to a stable configuration.

If the new chip arrives at a position where there was no chip before, we are done. Otherwise, it lands on a segment, and the picture looks something like this:

             *
?????????.***************.????????
The two "."s represents empty positions. If you play around with a couple examples, you should be able to see that the ending result will always be two segments, one starting from the position of the left "." in the picture, and the other ending at the position of the right ".". We might also view the result as a single segment with a hole. Furthermore, you might also realize that if there were A points to the left of our new chip in the original configuration, and B points to the right, then the two new segments will have lengths B+1 and A+1 respectively, and the total number of moves required will be (A+1)*(B+1).
We could also have computed the position of the hole using Observation 1. The sum of the positions in the initial configuration is (1+2+...+15)+4, and we know in the new configuration that the sum is (0+1+...+16)-H, where H is the position of the hole. Therefore, H must be 12. The final picture is

?????????************.****????????
We could then use Observation 2 to easily determine how many moves were required to get here.
So here is one possible solution to the problem. Add the chips one by one. At each stage, we have up to n disjoint segments. If the new chip lands on an unoccupied position, it forms a segment unto itself; otherwise, it transforms one segment into two as described above. In either case, the new segments might touch the ones to their left and/or right, and we merge them if that happens.

All that's left is to figure out how to store these segments in your program. If you are clever, you might realize that if we add the chips from left to right, then each new chip will always be on or next to one of the last two segments. You could then use a stack to store all the segments -- all the operations will be on the top two elements of the stack. This approach gives an O(n) solution. If you missed this last insight, you could also use a binary search tree (e.g. an STL set) to get an O(n log n) solution.

Adding one pile
In our problem, we have C piles of chips, and usually C is much smaller than n. We now sketch a lightning-fast solution that runs in O(C) time. This level of insight is not necessary to solve the problem, but it's still pretty interesting. As you will see, it is essential to understand the details of the above O(n) solution.

Instead of adding one chip at a time, we will try to process all the chips from a single position at the same time.

First let's resolve the case when there is only one pile of n chips at position x. By symmetry and the discussions in the previous section, it is easy to see that the stable configuration is a segment centered at x if n is odd; and a segment centered at x with a hole in the center if n is even.

Let's define an H-segment to be a segment with a hole. It is a tuple (x, y, z), where x < y ≤ z, representing a segment of chips from position x to position z, inclusive, but with position y empty. Note that, when y = z, the hole is at the very end, and it is actually a normal segment.

Our solution adds the piles one by one. And we keep a stack of existing H-segments from the left to the right. When a new pile comes, it is transformed into a new H-segment. If the H-segment does not overlap with any existing H-segments, we are done. Otherwise, it overlaps with the topmost H-segment in the stack; that is, it creates some positions with two chips. But using the observations from the last section, we know that if we resolve the conflicts one at a time, we will always have at most one hole. That means the result will be another H-segment. If the new one overlaps with the current top H-segment in the stack, we continue with the same resolving process. We do this until the stack is empty, or the H-segment is disjoint from the top of the stack. Then we push the new one and proceed to the next pile.

It remains only to explain how to compute a new H-segment quickly. And the answer is: just use Observation 1 again! When resolving two H-segments, we know S -- the sum of the positions in them; we also know the total number of chips K, so (remember the hole), z = x+K. We need to decide the start position x. Depending on y, the sum S satisfies

K(2x + K - 1) / 2 ≤ S < K(2x + K + 1) / 2
There is a unique x satisfying this, and it can be solved in constant time. We can then find y exactly like we did in the O(n) solution.

More Information
If you liked this problem, you might also enjoy reading the following classical paper on chip-firing games:
- Anders Björner, László Lovász, and Peter Shor Chip-firing games on graphs. European Journal of Combinatorics, Volume 12 , Issue 4 (July 1991).

Round 3 2010 - Code Jam 2010

Different Sum (7pts, 22pts)

The small input
The solution for the small input of this problem was quite straightforward. One could iterate over all possible ways to partition N into a sum of positive integers, and verify that each column has distinct digits.

Since there are 190569292 partitions of 100 into a sum of positive integers, this algorithm might not run fast enough. However, we can optimize it with an easy observation: the summands must be distinct. That brings the total number of partitions of 100 down to just 444793, which is small enough for our needs.

But if you want to cut the search space down even further, you can use backtracking. This is a general technique that works as follows in this problem: as you're generating the partition, you can check if there's a column that has two equal digits after adding each number, not just in the end. That way, many bad partitions get filtered out early and you have even less possibilities to check.

The large input
In order to approach the large input, we need to rotate ourselves 90 degrees. In the above solution, we've generated our cryptarithm from top to bottom. Now, we will generate it from right to left.

First, we check all possibilities for the digits in the rightmost (least significant) column such that the last digit of their sum matches the required one. Then, we continue with the digits for the next-to-rightmost column, and so on.

Suppose we have already filled a few rightmost columns. We can note that the things that are relevant for us now is the value V of carry from the already filled columns to the next one, the amount K of summands in the column that was just filled, and the boolean flag F indicating whether there has been a zero in the column that was just filled (this flag is important since it affects whether we can terminate the corresponding number now). When we know the values of V, K and F, the actual digits in the already filled columns don't affect the further execution of the algorithm.

This observation logically leads us to the following Dynamic Programming solution: let's calculate Count[i, V, K, F] which is defined as the number of ways to place the digits in the last i columns in such a way that the sum in those columns matches N, there's a carry of V, the number of summands that have at least i digits is K, and F is 1 when there's a summand that starts with zero, 0 otherwise.

In order to calculate Count[i+1,...] given Count[i,...], we need to consider all possible ways to place up to K digits in the i+1-th rightmost column. K is up to B (since all digits in one column are different, the number of summands doesn't exceed the number of different digits), which can be up to 100 in the large input. From the first glance, this gives us at least 100! (factorial of 100) possibilities, rendering our idea still useless.

But now's when another Dynamic Programming idea comes into play! One can notice that we don't need to know exactly all digits of the i+1-th column. The important thing for us is the amount of those digits, the sum of those digits, and whether one of them is zero. When we know those, we can multiply our answer by an appropriate number (which will be a product of binomial coefficients and factorials) to account for various ways to attach those digits to the already formed numbers in the first i columns.

So we run a separate Dynamic Programming that calculates Count2[K, S, F] which is defined as the number of ways to place K distinct digits in a column such that their sum is S and F denotes whether one of them is zero. K is up to B, S is O(B2), meaning we get O(B3) states, which is small enough.

The main Dynamic Programming has O(B2*number_of_digits) states, and using the Count2 table each state can be processed in O(B2) by looking at the number of digits in the i+1-th column and the carry to the i+2-th column (the required sum in the i+1-th column is uniquely determined by the carry to it, the carry from it, and the corresponding digit of N). The total runtime of this solution is thus O(B4*number_of_digits).

Round 3 2010 - Code Jam 2010

Fence (7pts, 22pts)

The basic scenario here is very similar to the traditional change-making problem, except that the input can be (and actually is guaranteed to be) very large. A condition on the minimum size of the input is very unusual for a programming contest problem, and we didn't add it just for fun. Our solution really, truly does require that the fence length be at least 10^10.

Small Input
Before getting into the real solution though, let's discuss a simpler approach that at least solves the small input. Let's suppose the longest board is of length A ≤ 100. The key idea is that we should never use more than A boards of any size less than A. If we did, we could replace A of those boards with a smaller number of length-A boards. And that would of course be a better solution.

In particular, this means the total length of all shorter boards is at most N * A * A ≤ 1000000. Using a breadth-first search, we can find the optimal way of choosing these boards to get each length in that range. The cost of completing the fence using length-A boards can then be computed with a simple division.

By the way, you can actually replace N * A * A with just A * A in the above solution. Hopefully you will see why after reading the rest of the solution!

Large Input
The small-input solution does not actually take advantage of the minimum length of the fence. So the big question is: how could we possibly do that?

Well, the previous solution offers a bit of a hint. For a really long fence, it makes sense that in the end, we are going to want to make heavy use of the longest board just to cover up as much length as possible. So let's suppose the longest board has length A, and that L is equal to p*A + q for integers p, q with q < A. (Note that the problem statement guarantees p ≥ A.) Then we need to do one of the following things:

Use a number T0,q of shorter boards to create a fence of length 0*A + q, then use p boards of length A to cover the rest.
Use a number T1,q of shorter boards to create a fence of length 1*A + q, then use p-1 boards of length A to cover the rest.
...
Use a number Tp,q of shorter boards to create a fence of length p*A + q, then use 0 boards of length A to cover the rest.
So we need to calculate p + Sp,q where Sp,q is defined to be min(T0,q - 0, T1,q - 1, ..., Tp,q - p). Intuitively, Sp,q can be thought of as measuring the minimal number of boards required to get a fence length of q mod A, subject to two modifications:

Every time the length increases by A, it means one less max-length board in the future, so you can subtract one from the total count.
The total length is not allowed to go over p*A + q.
And now, we can make concrete how the condition that L is very large simplifies things:
Lemma: The second condition in the definition of Sp,q is unnecessary.

Proof: We claim that Ti,q - i is minimized when i ≤ p, which will prove the lemma. So let's consider a minimal Ti,q - i. Then we have a set of boards b1, b2, ..., bm making a length of i*A + q. If i > p, then m > p ≥ A. But then, the set {b1, b1 + b2, ..., b1 + b2 + ... bm} contains at least A+1 numbers, so two of these numbers are congruent modulo A. Subtracting them, we can find a non-empty subset of {b1, b2, ..., bm} whose sum is a multiple of A. Therefore, we can replace that subset with boards of length A to get a strictly better solution, implying Ti,q - i could not have been optimal in the first place!

Okay, that's all very nice, but what's the solution? Well the previous lemma implies we need to calculate the minimum number of boards required to get a fence of length q mod A, subject to the fact that each time the total goes up by A, we will need one fewer board in the future. (For shorter fences, this approach just does not work. Our algorithm would make a very long fence with length correct modulo A, and then try to subtract length-A boards, which of course is not allowed!)

Anyway, once the problem has been reduced in this way, it can be done pretty straightforwardly with a breadth-first search. Our graph has one vertex for each residue modulo A. From each vertex, we add an edge for each possible board length. If adding that board involves wrapping past A, then it has weight 0. Otherwise, it has weight 1. So the final algorithm is: calculate the minimum distance in this graph to vertex q to get Sp,q, and finally add p.

World Finals 2010 - Code Jam 2010

Candy Store (7pts, 20pts)

Summary
At first this problem seems intimidating. There are a huge number of ways customers can order, and even once you've decided which boxes to order, verifying that all possible scenarios are covered is non-trivial.

But as it turns out, a greedy algorithm for choosing the boxes, and a greedy approach to handing them out, ends up working! And as with most greedy problems, the difficulty lies only in convincing yourself that it works. It's actually the easiest problem in the set to implement. The only thing you must overcome is self-doubt!

Proof
Suppose that part of our order is a set of boxes with total weight N grams, with no box larger than X = floor(N/k)+1. Furthermore, suppose that we know these boxes have the property that if the k customers' orders don't total more than N grams, these boxes can be used greedily to satisfy their orders. "Greedily" means that when a customer comes to the store, you simply find the largest box that is less than or equal to his order, and give it to him, repeating until you've filled his order exactly. This is our inductive hypothesis: a guarantee that this strategy works.

Now, suppose we order another box with weight X (ie, floor(N/k)+1) grams, making a new set of boxes with total weight N+X grams (and note that no box is larger than floor((N+X)/k)+1, trivially). Suppose the k customers order no more than N+X grams. What happens if we apply the greedy strategy? Well, as soon as we see an order of ≥ X cents, we will use this new X-gram box on it immediately. If we now pretend that the customer instead ordered X fewer cents, then we know the greedy strategy with the rest of the boxes works. The choices made by the strategy are unchanged aside from the use of the new box. If it turns out that there is NO order of ≥ X cents, then the total is at most k*(X-1) ≤ k*(N/k) = N grams. The new box can't ever be used, so the greedy strategy does the same thing for these orders that it did before, and thus it works.

So using induction, we now have what I'll call "The Algorithm", which constructs sets of boxes that work greedily. We start with 0 boxes, and keep adding new boxes of size floor("total sum so far"/k)+1. eg, first we'll add k boxes of size 1 (which we obviously need), then a box of size 2, and so on.

Ok, now how do we prove that this is the best we can do? Suppose we've ordered some set S of boxes that works. Sort the boxes from smallest to largest, and consider the first box that is larger than would be chosen by "The Algorithm" based on all the boxes smaller than it. In other words, if this box is of size Y, and N is the total of all the boxes smaller than it, Y > X = floor(N/k)+1. (Note that X ≤ C since N < k*C.) What happens if the customers all order X cents? None of the boxes of size Y or greater can be used, and the total is k*X > N, so all the boxes smaller than Y don't add up to enough to handle them. This can't possibly work, regardless of strategy.

Thus, every box in S, considered in sequence, is no larger than would be added by "The Algorithm" based on the previous boxes. But if any box chosen is STRICTLY smaller, then "The Algorithm"'s later selections will all be reduced accordingly (it is monotonic in that sense). Since S must have sum at least k*C, if we ignore S and instead order an equal number of boxes using "The Algorithm", their sum will be at least k*C as well. This proves that "The Algorithm" is the best we can do.

Implementation
"The Algorithm" is really, really simple to code. In fact, this is a complete implementation of a Candy Store solution:

long long T, k, C, prob = 1;
for (cin >> T; T--;) {
  cin >> k >> C;
  long long sum = 0, num_boxes = 0;
  while (sum < k*C) {
    num_boxes++;
    sum += (sum / k) + 1;
  }
  cout << "Case #" << prob++ << ": " << num_boxes << endl;
}
Other Cool Stuff
The observant among you may have noticed that this solution is very general, and remains unchanged even if we vary the problem a little. For instance:

- The bound C is effectively useless. We can throw it out and let customers order any amount, as long as the TOTAL order is no more than k*C.

- We can do no better even if we're told all the customers' orders in advance (at the beginning of the day). The "online" solution is just as good as an "offline" one.

- We can do no better even if we constrain the customers to all order the same amount as each other. This was actually the original proposed form of the problem.

World Finals 2010 - Code Jam 2010

City Tour (4pts, 23pts)

This proved to be the easiest problem in the finals for the experienced competitors.

We get some insight into the problem by noticing the graph resembles a tree. These graphs are actually called partial 2-trees. We can actually build a tree if we follow the way the graph is constructed. We build another graph that associates a node to each new added cycle of length this cycle shares an edge to an old cycle so we add an edge between the two respective nodes in the second graph. This way we have built the tree decomposition of these graphs. As on trees many problems that are hard to solve on general graphs have polynomial algorithms on this type of graphs.

Let's solve the related problem of finding the longest path in a tree. We can use dynamic programming and depth first search. We mark a node as a root. Every path in the tree has exactly one node that is closest to the root and this node splits the path in two downward paths. Now for each node in the tree we compute the longest downwards path that starts in it. To find the largest path in the tree we look at each node and at the two longest paths that start in it's children. This solves the problem in linear time.

The solution for our original problem is pretty similar. For each edge (x,y), we compute the cycle that contains it and and all the other nodes in the cycle have larger indexes. Let's call this a downward cycle since it goes the opposite direction of where the initial three nodes are. To find that number we have to look at all higher indexed nodes that were connected to this edge and try to use them as intermediary points in the cycle. So for a given intermediary point z we can build a cycle by looking at the longest downward cycle that contains the edge (x,z) and the longest downward cycle that contains the edge (z,y), use all the edges, add edge (x,y) and remove the edges (x,z) and (z,y).

We also compute the largest downward cycle which contains these two nodes but doesn't contain this edge, this is a union of the cycle that goes through these nodes and the second largest path from which we remove the edge (x,y).

And here is some code that does implements this solution:

int best_so_far = 0;

int best(int x, int y, int N, int[][] a) {
    int max_len = 2;
    int second_max_len = -1;
    for (int i = Math.max(x, y) + 1; i < N; i++) {
      if (a[x][i] * a[y][i] > 0) {
        int len =  best(x, i, N, a) + best(y, i, N, a) - 1;
        if (len > max_len) {
          second_max_len = max_len;
          max_len = len;
        } else if (len > second_max_len) {
          second_max_len = len;
        }
      }
    }
    best_so_far = Math.max(max_len, best_so_far);
    best_so_far = Math.max(max_len + second_max_len - 2,
                           best_so_far);
    return max_len;
}
Another cool solution is based on the idea of contracting each node of degree two. We replace it with an edge which has the weight equal to the sum of the weights of the two incoming edges. It's a pretty neat idea and we'll let you figure out the details on your own.

World Finals 2010 - Code Jam 2010

Letter Stamper (8pts, 19pts)

The problem idea originated from the report of one of Robert Tarjan's talks, where a quadratic solution for four letters was claimed. We found the easier three-letter version was already very interesting, and presented it to you as the first problem for this Code Jam final.

We note that the general case where the alphabet size is not restricted to a small constant such as 3 can be solved by dynamic programming in O(n^3). It had appeared in several programming contests before.

The current state is characterized by a pair (S, K), where S is the suffix of the input string you still need to print (so the next letter you need to print is the head of S), and K is the stack. For each situation, we need to decide if the next step is a push, a pop, or a print.

We state some rules that are satisfied by an optimal sequence. These will make the solution quite clear. There can be other optimal sequences, but you can always reduce them to one that satisfies these conditions.

Rule 1. If the first letter of S is the same as the top of K, then the next step is a print.
Rule 2. You never need to push the letter that's already on top of the stack. Therefore no letters appears on the stack twice in a row.
Rule 3. Immediately after you push a letter X, the next step is to print X.
The rules above are trivial and we omit the reasoning. But it might worth spending a little time convincing yourself formally.

Rule 4. There should never be three consecutive letters in the stack of the form XYX.
Let us justify this rule. Suppose an optimal solution has XYX on the stack. Let's modify it. At some point the top of the stack is XY and we were going to push another X. Instead, we will pop the Y. Then we continue as before, until we were going to pop the second X that now doesn't exist. Instead, we will push Y, arriving at the same state as before. This way we have shortened the stack while achieving a solution of the same length. By induction you can keep simplifying the solution until all 4 rules are satisfied.

Rule 4, together with Rule 2, implies that the stack always contains a cycle of 3 letters, e.g. ABCABCABC.... The state of the stack then is completely defined by the first two letters and the stack height. There are only 6 patterns, and the height of the stack is never bigger than n. Hence the number of possible states (S, K) is O(n^2). This gives a quadratic dynamic programming solution.

World Finals 2010 - Code Jam 2010

Travel Plan (3pts, 30pts)

As with City Tour, this problem is closely related to Hamiltonian cycles. The good news is that here, the graph consists only of points on a line, and is therefore much easier to analyze. The bad news is we aren't looking for just any Hamiltonian cycle, or even the shortest one; we are looking for one of a particular length. A question this precise requires a complete search of some kind.

The simplest approach is to try all possible travel plans one at a time. This is much too slow for the large input, but it is fine for the small input.

Given that N ≤ 30 throughout, you might next think to try a dynamic programming solution that tracks how far you have gone and which nodes you have visited so far. Unfortunately, this is also too slow - large values of F are a big problem!

A 3N solution
The first key step to solving this problem is to go from N! time to 3N time.

We first divide the line into intervals I1, I2, ..., IN-1. Let tj denote the number of times that the travel plan crosses interval Ij, and let dj denote the length of interval Ij. Then the total length of the travel plan is exactly t1 * d1 + t2 * d2 + ... + tN-1 * dN-1. So this means that all we need to do is figure out what each tj should be.

The big question is what tj values are possible? Well, let's take a look.

Each tj must be positive.
Reason: If tj = 0, then the travel plan never crosses the corresponding interval, and therefore cannot visit every planet.
For each j, tj - tj-1 must be -2, 0, or 2.
Reason: Let P denote the planet between the intervals Ij-1 and Ij. In our travel plan, we can stop at P at most once. Every other time, we must travel through, which contributes 1 to both tj-1 and tj. The one time we do hit the planet, we come in along one interval, and then leave along one interval. (These two intervals may or may not be the same.) This part of the travel plan contributes either (a) 1 to both tj-1 and tj, or (b) 2 to one of these values, and 0 to the other. Regardless, tj - tj-1 must be -2, 0, or 2.
t0 and tN-1 must both be 2.
Reason: Consider the "outside interval" I-1 that is past the furthest planets. The travel plan cannot traverse this interval, so t-1 = 0. The observation here now follows immediately from the previous two.
Every choice of tj satisfying the previous conditions can be achieved by a legal travel plan.
Reason: We can construct such a travel plan by scanning from left to right across the intervals. At any given time during the construction, we will have decided what the travel plan does to the left of some planet P. Specifically, for each trip left of P, we will know what the trip does until it goes right of P again. We will not however know what happens between these visits, or even what order they occur in.
Now, suppose we want to extend our partial travel plan past the next interval Ij. If tj = tj-1 - 2, then we need to merge two of our current trips via a stop to P, and extend the rest along Ij. If tj = tj-1 + 2, then we add a new trip that goes along Ij, stops at P, and then goes back again. In the final case we just extend each trip along Ij, having one stop at P without changing direction.
If you think about this for a while, you should be able to convince yourself that this method does indeed generate a valid travel plan.
Okay! We can now describe a 3N time solution to the original problem. Given tj-1, there are at most 3 possible choices for tj. Trying each one in turn, we can iterate over all possible assignments for the tj's, and then pick the one that leads to the optimal length travel plan.

A 3N/2 solution
Even this 3N time algorithm is too slow however. Fortunately, there is a very handy trick for situations like this.

Let's consider the middle interval IN/2 and some fixed value for tN/2. Using the approach described above, we can enumerate in 3N/2 time all choices for t1, t2, ..., tN/2-1 that are consistent with tN/2. Let A be the set of all possible values t1 * d1 + t2 * d2 + ... + tN/2 * dN/2 that can be achieved in this way.

Similarly, we can calculate B, the set of all possible values of tN/2+1 * dN/2+1 + tN/2+2 * dN/2+2 + ... + tN-1 * dN-1 that are consistent with tN/2. All this takes 3N/2 time. What's left is to find two numbers in A and B whose sum is as close as possible to F without exceeding it. One efficient way to do this is to sort B, and then for each element in A, use binary search to decide which element in B you should try matching it with. That gives a O(3N/2 * N) solution.

This can be improved to O(3N/2) by rearranging the formula a little: t1 * (d1 + ... + dN/2) + (t2 - t1) * (d2 + ... + dN/2) + ... + (tN/2 - tN/2-1) * dN/2. The list of these sums can be generated in sorted order iteratively by repeated merging while adding new terms. The difference tj+1 - tj is always one of -2, 0, 2, so we need to merge 3 sorted lists to add a new term.

When we have the two lists A and B in sorted order, we can process them in linear time with a single scan using two pointers.

World Finals 2010 - Code Jam 2010

Ninjutsu (11pts, 23pts)

Getting something discrete
A fact that jumps out immediately is that this problem is not discrete. The rope length that gives an optimal solution might be a real number, and there does not seem to be a way to ensure that this number is rational or discrete in any other way.

Given any real-valued rope length, we can simulate and count the bends, but this number is not a convex function of the rope length, so ternary search is out.

What we really wish we could do is dynamic programming, where the state is a triple -- the point we are currently swinging around, the direction the end of the rope is pointing in, and the length of the rope. Two of these values are real numbers. However, let's see how many different real values we actually need to care about.

As we are swinging the end of the rope around some point P, nothing interesting happens until the moving segment of the rope touches another point, Q. At that... point, we need to make a decision -- we could either bend the rope and switch to swinging around point Q, or cut the rope at exactly point Q and continue swinging around P. There are at most N2 pairs of points, so we only need to consider at most N2 different directions. That takes care of the second DP parameter.

What about the rope length? Using the same reasoning as above, we can show that there is only a finite number of rope lengths that are interesting. Firstly, note that bending the rope does not change its length. Whenever we swing around point P and hit point Q, there is a binary decision -- is the rotating rope segment longer than the distance from P to Q, or shorter?

In other words, the interval of real numbers between 0 and R can be split into a finite number of of sub-intervals, such that the number-of-bends function has a constant value on each sub-interval. This observation suggests the following, naive dynamic programming (DP) solution. The DP state is a pair of points (P, Q) and a real-valued rope length, r. The answer is the maximum number of bends we can achieve by continuing to spin around point P, if we are currently poining in the direction of point Q and have a swinging segment of the rope of length r. To be a bit more precise, we will actually need to have twice as many states -- one for poining in the direction of Q, and one for poining 180 degrees away from Q. We need the second kind of state for the situation when we decide to bend the rope and continue swinging around Q. At that point, we will start pointing in the direction 180 degrees away from Q. One way to implement this solution is simply to use a point, a 2D vector and a length as the state, instead of two points and a length.

Next, let's get rid of the real-valued length as a DP parameter. Because rope length only changes when we decide to cut it, we can replace the real-valued length parameter with an integer -- the number of bends since the last time we have cut the rope. Let's also replace the first two parameters with the pair of points that caused the cut. We are now interested in all situations of the following sort -- we were swinging around point P, with the end of the rope poining in the direction (dx, dy), at which point we cut the rope, and since then, we have made K bends. This information is enough to identify a state uniquely, and it implies a particular remaining rope length. Our DP state now becomes a triple of P, (dx, dy) and K.

Floating point issues
We now have an almost completely discrete problem. The only place where floating point numbers are necessary is in the testing of whether the current rope length is long enough to hit some given point, or does the rope's end pass underneath that point? Given the guarantee that the optimal solution works for a long range of rope lengths (0.999999), we can avoid floating point rounding trouble by being conservative: only assume that the rope can hit a point if its length is longer by at least, say, 0.5 than the distance to the point. If it is shorter than that, then certainly such a solution doesn't work for a long enough range of rope lengths, so it's not the optimal solution by the guarantee.

Dealing with loops
The naive solution is too slow. Consider, for example, the case where we have a rope of length 109 and two points: (0, 0) and (0, 1). The optimal solution uses the full length of the rope to create 109-1 bends. Of course, we are just going around in loops, so we need a way to detect and handle such loops to have a chance at a polynomial-time solution.

First, let's reorganize the naive DP solution a bit to make loop detection easier. We will have a simpler, three-parameter state and use a memoized recursive function. Each call of the function will correspond to a situation of the following kind -- we are spinning around a point P, with the end of the rope pointing in the direction (dx, dy), and we have just cut the rope because it has touched another point, Q. The total number of such states is O(N^3), but many of them are not possible and will never be visited.

Inside the function, we will simulate the wrapping of the rope and make recursive calls in situations when we decide to cut the rope further. If we simulate the wrapping process naively, we may need to make a huge number of recursive calls. Instead, imagine that we find ourselves in a situation where we have just bent the rope around some point A, and we are about to bend it again around some point B. If this is not the first time we have seen the pair (A, B), then we are looping around the same set of points. The second time we hit this pair, we will have a shorter remaining rope length, r, and we can figure out how much rope one revolution consumes by subtracting the new value of r from the value we had when we first encountered the pair.

Once we detect a loop, we can choose the number of full revolutions we want to make before we cut the rope and enter inside the convex hull of the points we are looping around. Clearly, it never makes sense to cut off more than one whole loop perimeter. We would be throwing away free rope bends. The optimal number of revolutions is, thus, the total remaining rope length, divided by the perimeter of the loop, floored, minus one.

Putting together a complete solution
We now have a DP solution with O(N^3) states and O(N^2) work per state (linear number of recursive calls; linear amount of work to find the next point for each call). We can speed this up by precomputing next points for each state, but this is not necessary. It turns out that the number of reachable states is small enough for this solution to pass.

Remaining difficulty is in dealing with collinear points. Since all coordinates are integers, this should be done exactly, without using floating point computations.

World Finals 2010 - Code Jam 2010

The Paths of Yin Yang (17pts, 35pts)

Plenty and void grow from each other;
hard and easy foster each other;
long and short shadow each other;
elegance and mudanity prosper together;
music and voice complement each other;
back and front stay together;
they last forever.
This version, translated by our colleague Yifan Shi, is certainly closer to the original meaning of the text.
Impressions
One may discover a series of observations in attacking this problem. Some are obvious, while others need some hard work and inspiration. Our algorithm searches all the possible solutions and counts them. Imagine yourself to be a detective facing such an empty rectangular board. Your job is to reveal all the possible solutions, quickly.

For any solution, as the problem stated, almost all the points have degree (number of neighboring squares with the same color as itself) 2; and there are exactly 4 points having degree 1. Let us call the points with degree 1 end points;

Once we have decided all the degrees, we can try to reconstruct the color of each square by brute force search. Suppose in a general case, we have colored a square A, and we also colored all but only one of its neighbor B, then we do not need to try both black and white for B, it is decided by the color of the other squares, and the degree of A.

This gives us a much faster way to recover all the solutions than searching blindly. One can see that if all the colors of the first row are decided, and we proceed from top down, then in each step we always have a square (in fact close to M squares) who is in the same situation as A above. So, once we decided the combination (color of the first row, the position of the 4 end points), all we have to do is to check in O(NM) time if a solution arises.

There are 2M ways to color the first row, and Θ((NM)4) ways to pick the end points. Both numbers are too big. One may easily see most of the 2M colorings of the first row will obviously fail to give a solution, as we will discuss in the next section. After that, we will work on how to reduce the number of positions for the end points.

A bit of Topology
Rather than looking at the first row, let us consider the outer loop of the board. That is, the positions on the boundary. It is easy to see that they cannot be of the same color -- the outer loop must contain both black and white squares.

We can say much more about the outer loop. It must be exactly one segment of black squares, and the rest is exactly one segment of white squares! The reason can be seen from the picture on the left below. If there are at least two black segments (and hence at least two white segments), there is no way one can connected both the black pieces and white pieces from the inside of the board.

The second picture shows a forbidden configuration for a 2 by 2 square. If we have a chessboard-like situation, there is no way to connect both black squares and white squares.

Position for end points
Suppose A is a white end point, and B is its sole white neighbor, as in the picture below.

Because A has no other white neighbors, so the positions labeled with 1 must be black.
To avoid 2 by 2 chessboard, the positions labeled with 2 must be black as well.
Because the first row labeled with 1 and 2 has no other black neighbors, so the positions labeled with 3 must be white.
To avoid 2 by 2 chessboard, the positions labeled with 4 must be white as well.
This deduction can go on until we hit the boundary of the board. In our picture X is a point on the boundary, and it is also the place where black and white squares meet on the boundary. Yet we can continue the deduction along the other direction, until we get a point Y on the boundary as well.
This gives a nice connection between the end points and the outer loop. From an end point, we draw two diagonal rays (both with 135 degrees to its neighbor with the same color), both rays hit the boundary either at a corner, or at a point where the black and white squares meet on the outer loop.

Let us summarize our algorithm. We start by fixing the outer loop as a segment of black squares and the rest white. Then from each corner we draw a diagonal line; also from each point on the outer loop that is next to a different color on the loop, we draw one diagonal line going away from the neighbor with the different color. Thus we have 8 diagonals. These diagonals will produce no more than 16 intersections. Those are all the possible positions for the end points.

There are O((N+M)2) ways to fix the outer loop, and thus O((N+M)2) ways to fix the outer loop and the end points. For each of these, one can check if a solution arises in O(MN) time. So the running time is O(MN(N+M)2). The constant is not very small, but tolerable. And one can reduce it using symmetry.

Below is one of a good solution. We may see how the end points and outer loop are connected by the diagonals.

Qualification Round 2011 - Code Jam 2011

Bot Trust (10pts, 10pts)

BAM! Robots gave us 6 extra seconds of cooperation. Good job, robots!
   -- Cave Johnson (Portal 2)

Hopefully your robots were more focused on teamwork than Cave Johnson's were, or you would never get all the buttons pressed. Just be glad there were no mashy spike plates or potatoes to contend with! Perhaps in the finals...

Anyway, if you think about this problem from the perspective of a single robot, the strategy should be pretty intuitive: always move towards the next button and then push it as soon as it comes up in the sequence.

So the most natural solution to this problem is a straight simulation:

Keep track of which buttons have been pressed, and look ahead in the sequence to figure out which button is next for each robot.
One second at a time, have each robot move towards its next button.
Once it gets to the button, the robot should push it if it's next in the sequence, and just wait otherwise.
At most 100 buttons needs to be pressed altogether and the distance between buttons is at most 100, so this solution will run plenty fast.

If you get stuck on the implementation, remember that you can see other contestants' source code. There is no better way to learn!

Qualification Round 2011 - Code Jam 2011

Candy Splitting (10pts, 15pts)

The main step needed to solve this problem is to understand Patrick's strange addition algorithm. For example, can we describe what happens when Patrick adds up several numbers instead of just two? It turns out we can: we should write all numbers in binary, align them by their least significant bit, and write 1 in those positions where we have an odd number of 1 bits in the summands (the numbers being added).

Consider this example: suppose Patrick needs to add 5, 7 and 9 together. First, he adds up 5 and 7 by writing them in binary and adding digit-by-digit without carry, as described in the problem statement:

  101
+ 111
-----
  010
The result is 010 in binary, which is 2. Now, he adds up 2 and 9:

  0010
+ 1001
------
  1011
The result is 1011 in binary, which is 11. It is most instructive to look at what happened to the least significant bit: after adding up two of the numbers, we had a 0 in the least significant bit since both of the summands had a 1 there. However, we have a 1 in the least significant bit of the overall result since the third number had a 1 there as well. It's not hard to see that this generalizes as described above: for any bit, it will be equal to 1 in the overall sum if and only if this bit is set to 1 in an odd number of summands.

Having established that, we can now understand Sean's task better. He needs to separate the given set of numbers into two parts in such a way that for every bit position, either:

In both parts, an odd number of summands have this bit set to 1, so that the corresponding bit in the sum is 1 for both parts, or
In both parts, an even number of summands have this bit set to 1, so that the corresponding bit in the sum is 0 for both parts.
But saying that we need two numbers to be either both odd, or both even, is equivalent to saying that their sum must be even!

That allows us to reformulate Sean's task simply as: he needs to separate the given set of numbers into two parts in such a way that for every bit position, an even number of summands have the bit set to 1 across both parts put together. Suddenly, we understand that this condition doesn't rely on the way we separate the numbers into two parts at all! Either it is true for every bit position that an even number of all summands have the bit set to 1, in which case any separation into two non-empty piles will make Patrick happy, or there is some bit which is 1 in an odd number of summands, in which case there's no way to make Patrick happy.

For example, suppose Sean has pieces of candy with values 5, 7, 9 and 11. If he separates them into 5 and 7 for himself, and 9 and 11 for Patrick, Patrick will add 5 and 7 as 5+7=1012+1112=0102=2, and 9 and 11 as 9+11=10012+10112=00102=2, so Patrick is happy. But even if Sean takes 7, 9 and 11 and leaves just 5 to Patrick, Patrick will add 7, 9 and 11 as 7+9+11=01112+10012+10112=01012=5, so he is still happy! It's not difficult to verify that in all other cases Patrick is happy as well.

All the above reasoning can be made simpler if you notice that Patrick's strange addition is exactly bitwise exclusive or. The condition for Patrick's happiness can be reformulated as the bitwise exclusive or of all candy values being equal to zero. In many programming languages, bitwise exclusive or is already built in with the "^" operator, making this very easy to check!

Now, how does the overall solution for the problem work? First, we need to check if Patrick will be happy - as shown above, this does not depend on Sean's piles. If not, then we just output "NO" for this testcase. If yes, then Sean should maximize his pile, and this is achieved by taking all pieces of candy except the one with the smallest value.

Qualification Round 2011 - Code Jam 2011

Magicka (10pts, 15pts)

This problem can be solved with a simulation. First, we have to remember what elements combine to make other elements. A map of some sort, like a hash map, is a great way of doing this. Next we have to track the opposed elements, remembering that one element can be opposed to multiple other elements; a set of pairs, while not particularly efficient for this purpose, will do the trick.

Finally, the simulation itself. For each character, first we check to see if it combines with the last item on the element list, and combine it if so. If it doesn't combine, then we iterate through the elements already in the list and see if it's opposed to any of them -- if so, we clear the list. Finally, if neither of those conditions was met, we append it to the list. Here is some Pythonesque pseudocode that solves the problem:

# Let combo_list contain all the combinations as 3-letter strs.
# Let opposed_list contain all the opposed elements as 2-letter strs.
# Let invoke be a str containing the elements to invoke.
combos = dict()
opposed = dict()
for x in combo_list:
  combos[x[0] + x[1]] = x[2]
  combos[x[1] + x[0]] = x[2]
for x in opposed_list:
  opposed.add(x[0] + x[1])
  opposed.add(x[1] + x[0])
# Now combos contains a mapping from each pair to the thing it
# creates.  If one of the combinations was "ABC", then
# combos["AB"] = "C" and combos["BA"] = "C".
# opposed is filled in a similar way.

element_list = []
for element in invoke:
  # If element_list isn't empty, the last element might combine
  # with the element being invoked.
  if element_list:
    last_two = element_list[-1] + element
    if last_two in combos:
      element_list[-1] = combos[last_two]
    continue

  # Now we iterate through element_list to see if anything there
  # is opposed to the element being invoked.
  wipe_list = False
  for e in element_list:
    if (e + element) in opposed:
      wipe_list = True
  if wipe_list:
    element_list = []
    continue

  # There was no combination and no erasing: just append the
  # element to the list.
  element_list.append(element)

Qualification Round 2011 - Code Jam 2011

GoroSort (10pts, 20pts)

The Solution
This problem is very mathematical in nature, requiring a lot of thought and only a little code to solve correctly. We put it as a problem on the Qualifying Round to give you something different to try without having to worry about time pressure or advancement. We hope you enjoyed it!

For an arbitrary array A, let n(A) be the number of elements that are not already in the correct position. It turns out that the answer you are looking for is simply n(A). Once you realize this, it is easy to calculate with a simple loop, but how do you prove it?

The Proof
Let's first show that the expected number of hits is never more than n(A). Suppose Goro always holds down only those elements that are already in position, and then he randomly permutes the rest. Let x(A) be the expected number of hits required for him to sort A using this strategy.

Lemma: x(A) = n(A) for all A.

We prove this by induction on n(A). If n(A) = 0, then the array is already sorted, and we're done. To set up the induction, let's suppose we have proven the lemma already for smaller values of n(A) and we are now trying to prove it for A. Let pt be the probability that exactly t elements are still out of position after the first hit, and let x't be the expected number of hits required in this case. We make three observations:

p0 * 0 + p1 * 1 + p2 * 2 + ... + pN * N = N - 1. In English, this is saying that the expected number of elements that are out of position after the first hit is exactly N - 1, or equivalently, the expected number of elements that are put into position by the first hit is exactly 1. This follows from "linearity of expectation": Goro is permuting N elements; each one has probability exactly 1/N of ending up in the correct position, and hence, the expected number of elements that end up in the correct position is N * 1/N = 1.
x't = t for t ≤ N - 1. This is true by the inductive hypothesis.
x'N = x(A). If no elements are put into the correct position by the first hit, then we will just randomly permute them all again in the next step, so nothing has changed, and hence x'N = x(A).
Now let's write down a formula for x(A):

1 + p0 * x'0 + p1 * x'1 + ... + pN * x'N
= 1 + p0 * 0 + p1 * 1 + ... + pN * N + pN * (x(A) - N)
= N + pN * (x(A) - N),

which simplifies to (N - x(A)) * (1 - pN) = 0. Since pN < 1, we must have x(A) = N, and the lemma is proven.

To complete the proof, we need to calculate y(A), the expected number of hits required for Goro to sort A if he uses the (still unknown) optimal strategy. Since y(A) ≤ x(A) by definition, we have already proven y(A) ≤ n(A).

To conversely prove n(A) ≤ y(A), it would be nice to just extend the proof of the previous lemma. There is one big technical issue though: it is possible for n(A) to go up if Goro doesn't hold down enough elements, and so it is tricky to set up an induction on n(A). We'll resolve this by having a separate proof for this part, and this time use a slightly different induction hypothesis. We can then follow the previous argument very closely.

Lemma 2: Let K be a non-negative integer. Then for any k ≤ K, the statement y(A) = k is equivalent to the statement n(A) = k.

We will prove this by induction on K. Both y(A) = 0 and n(A) = 0 are equivalent to the array already being sorted, so the K = 0 case is clear. Now, let's suppose we have proven the lemma for K already, and are trying to prove it for K+1. Choose A such that y(A) is the smallest possible value larger than K and consider the optimal strategy for Goro. Let T be the number of elements that are either (a) not in the correct position in A, or (b) permuted when Goro hits the table. Define pi and x'i as we did before. Note that T ≥ n(A) ≥ K+1 by the inductive hypothesis.

As in the previous lemma, we can now prove the following:

p0 * 0 + p1 * 1 + p2 * 2 + ... pT * T ≥ T - 1.
x'i = i for i ≤ K. This follows directly from the inductive hypothesis.
x'i ≥ y(A) for i > K. By the inductive hypothesis, n(A') > K implies y(A') > K, which then implies y(A') ≥ y(A).
As before, we can now write y(A) as

1 + p0 * x'0 + p1 * x'1 + ... pT * x'T
≥ 1 + (p0 * 0 + p1 * 1 + ... + pT * T) + (pK+1 + pK+2 + ... + pT) * (y(A) - T)
≥ T + (pK+1 + pK+2 + ... + pT) * (y(A) - T)

which simplifies to (y(A) - T) * (1 - pK+1 - ... - pT) ≥ 0. The second term has to be positive (if not, then y(A) ≥ min(x'K+1, x'K+2, ...) + 1, which contradicts the third bullet point above is therefore impossible), so we must have y(A) ≥ T ≥ n(A) ≥ K+1. Equality holds only if n(A) = K+1. The first lemma guarantees y(A) ≤ x(A) ≤ K+1 in this case, and the proof is complete!

Comments
If you go over the proof carefully, you can see there are two things Goro needs to do in order to be optimal. (1) He needs to always hold down elements that are already in the correct position, and (2) he needs to ensure that for each element x that is permuted, the element in x's correct position is also permuted. This means he actually has some choice about what to do.
On a programming contest, of course you do not need to work through a formal proof to implement a correct solution. The best contestants can solve this kind of problem by looking at small examples to see a pattern, and then using intuitive reasoning to see what's going on without formalizing everything. Mastering this kind of reasoning is a difficult art though!

Round 1A 2011 - Code Jam 2011

FreeCell Statistics (6pts, 14pts)

The first thing to notice in this problem is that both PD and PG can range from 0 to 100, so let’s start by taking care of some easy cases. If our unnamed player has won 100% of his total games (PG = 100) but not won 100% of his games today (PD < 100), then something has clearly gone wrong with the calculator. Similarly, if PG = 0 but PD > 0, something is wonky.

The cases where PD = PG = 0 or PD = PG = 100 are also easy, and the answer for both of them is "Possible" — they mean that all the games ever ended with the same result (loss or win, respectively).

The trick now is that once we ruled out the case of PG being zero or a hundred, we do not need to worry about it any more! Indeed, assume we have a solution that gives the correct daily percentage of wins, and it consists of W games won and L games lost today. To get a global win percentage of PG, we can, for instance, assume we won a total of (W + L) * PG and lost a total of (W + L) * (100 - PG). As 1 ≤ PG ≤ 99, the numbers are greater than W and L, respectively, so they are possible to achieve.

The small data set can now be solved by brute force by simply trying all possible values of D from 1 to N and all possible number of games won from 0 to D, and seeing if any of them results in exactly PD percentage of games being won.

Solving the large date set can require some maths. One way to solve the problem is to directly solve for the minimum number of games we would need to play in order to get a win percentage of PD and simply verify that this number is ≤ N. If we let W be the number of games we have won today, then we want to solve W / D = PD / 100 for the minimum value of D such that W is integral.

From here, it is easy to see D = 100 * W / PD. Thus, if we want to minimize D, then we need to find the smallest value W such that the right hand side is integral. In order to do this, we divide 100 and PD by their greatest common divisor (let's call this value G) so that they are relatively prime and W is minimal when it is PD / G. Plugging this in and cancelling terms tells us that D = 100 / G is the fewest number of games we must be play.

A simpler way to solve this problem is by brute force. We can just try all possible values of D from 1 to N and check if any of them could result in exactly PD percentage of games being won by checking that D * PD = 0 (mod 100), stopping the loop the first time we find a candidate value of D or we exceed N games. While at first this solution appears to be O(N) and would time out for the large data set, this loop will in fact run at most 100 times, regardless of the value of N, so this solution will be plenty fast enough. Coders who noticed this simpler solution early were rewarded with very fast submission times.

Round 1A 2011 - Code Jam 2011

The Killer Word (10pts, 20pts)

Every year, contestants come to us saying they implemented a program correctly but it ran too slowly. Unfortunately, being too slow is the same thing as being wrong. The Google Code Jam is an algorithms contest, and once you get past the early questions, coming up with a fast algorithm is often the whole point of the problem.

Why do we bring this up? Because this problem is a trap. On first glance, it looks just like the first two problems from the Qualification Round. We tell you an algorithm, and you implement it exactly as described:

Loop through every one of Sean's lists.
Loop through every word in the dictionary.
See how many mistakes Sean makes while guessing this word.
Since the last step requires iterating over every word again, the running time here is O(N2M).

We will always give you the hardest inputs we can fit within the stated limits, which means you will see N = 10000 and M = 100. Neither a fast computer nor a fast language will save you here - the whole approach is simply too slow. For example, our C++ implementation takes over 20 minutes for a single test case. To succeed on an algorithms contest, you need to see these issues in advance, either by looking at the Big O complexity or by trying a worst-case input of your own.

Once you do see the issue, it isn't too hard to dramatically speed things up. The main idea is to combine the last two steps:

Loop through every one of Sean's lists.
Divide the words into different classes by length. If you focus on a single class, Sean will make the same first guess for any word in that class. This is because he has exactly the same information in each case.
For each class, figure out what letter Sean will guess, and further divide the class based on each of the different responses you could give to Sean's guess.
Repeat for each of the sub-classes until you are down to a single word, at which point Sean will finish with no mistakes.
The running time here is O(NM), and it will go hundreds of times faster than the more straightforward algorithm.

This technique is similar to Dynamic Programming - the game begins in the same way for many different words, so we should not have to redo all that work every time.

Fun facts: this problem was originally called Bloodthirsty Executioner Man, but was renamed because the hangman is made out of paper, and has no blood. Sean himself answered several of the clarifications for this problem.

Round 1A 2011 - Code Jam 2011

Pseudominion (15pts, 35pts)

Let's denote the bonus numbers of a card c by c.card_draw, c.score and c.turns. Let c.index be the (zero based) index of the card in the ordering from the input file.

After looking at the problem constraints, we can classify a card c as:

a T card, if c.turns > 0.
a C0 card, if c.turns = 0 and c.card_draw = 0
a C1 card, if c.turns = 0 and c.card_draw = 1
a C2 card, if c.turns = 0 and c.card_draw = 2
Let T[i] denote the i-th T card in the sequence of all T cards sorted by the index. We define C0[i], C1[i] and C2[i] analogously.

The first key observation is that it never hurts to play a T card whenever we have one in our hand. We never lose turns or score and we may draw new cards by doing so.

Let's think about some arbitrary sequence of played cards. There are two observations we need to make before we can proceed.

Because C0 cards do not add turns or cards to our hand, we can postpone playing them until we have no other cards we want to play. So we can transform any valid sequence of played cards into another sequence that has all C0 cards at the very end. The total score, being the sum of individual scores of all played cards, will obviously remain the same.
Let's say there are two C1 cards a and b such that we played a before b, but b.index < a.index. Because b.index < a.index, we already had both cards in our hand when we played card a. So we can instead play card b first and card a second. The resulting sequence with a and b swapped will remain valid because a and b both have the same card bonus (1 card) and the same turn bonus (0 turns). Furthermore, the score will remain the same. We can do the same for two C2 cards.
The first observation shows us that we don't care about C0 cards until the very end when we can use our remaining turns on whatever C0 cards we have already drawn. Obviously, we should sort the C0 cards in our hand by decreasing score bonus and then play as many as we can.

The second observation shows us that we can transform any optimal sequence of played cards into another one that has cards of the same type ordered by index. This means that we can play C1 (and C2) cards in increasing order by index.

So, whenever we have more than one C1 (or C2) cards in our hand we can always look at the one with the smallest index and choose whether we will play it right now or never at all.

These observations lead us to construct a directed acyclic weighted graph. A node represents a state of the game and is defined by these properties:

hand - the number of drawn cards.
turns - the number of turns left.
t - T[t] is the first T card that we haven't played yet.
c1 - C1[c1] is the first C1 card which we are not yet sure if we are going to play.
c2 - C2[c2] is the first C2 card which we are not yet sure if we are going to play.
An edge represents a valid transition from one game state to another and is usually associated with playing a card or deciding not to play a certain card at all. The weight of an edge indicates how much your score goes up when you switch between the two game states. For each node (hand, turns, t, c1, c2), we add edges according to these rules:

If we have a T card in our hand, we can play it.
Condition: T[t].index < hand
Weight: T[t].score
Target node: (min(N, hand + T[t].card_draw), min(N, turns + T[t].turns - 1), t + 1, c1, c2)
If we have a C1 card in our hand, we can play the first one.
Condition: C1[c1].index < hand
Weight: C1[c1].score
Target node: (min(N, hand + 1), turns - 1, t, c1 + 1, c2)
If we have a C1 card in our hand, we can throw away the first one.
Condition: C1[c1].index < hand
Weight: 0
Target node: (hand, turns, t, c1 + 1, c2)
If we have a C2 card in our hand, we can play the first one.
Condition: C2[c2].index < hand
Weight: C2[c2].score
Target node: (min(N, hand + 2), turns - 1, t, c1, c2 + 1)
If we have a C2 card in our hand, we can throw away the first one.
Condition: C2[c2].index < hand
Weight: 0
Target node: (hand, turns, t, c1, c2 + 1)
We can always decide to finish the game by spending the remaining turns on C0 cards. This edge leads to the special final node and the weight of the edge is defined by the greedy algorithm that spends all remaining turns picking the best C0 cards.
The answer to our problem is the length of the longest path in the graph described above. The graph is acyclic, so we can use dynamic programming or recursion with memoization to find the length of the longest path.

The number of nodes in the graph is O(n5), so the asymptotic time complexity of the algorithm is O(n5) too. In practice, the runtime of the program is really small because the vast majority of these states are unreachable. You can speed things up even more if you use the preceding observations to their full power. For example, if there is an edge corresponding to a T card, you should always follow it first.

Remark: This problem is inspired by the game Dominion, published by Rio Grande Games.

Round 1B 2011 - Code Jam 2011

RPI (8pts, 12pts)

The problem statement explains exactly what to do here. You just need to follow the instructions and not get too confused! We wanted to give you something to warm up with before the next two problems, which are both quite tricky.

First the winning percentage (WP) of each team needs to be calculated. This is fairly straightforward since the WP of team i only depends on team i's record. To do this part, we need to know the total number of wins for each team, as well as the total number of games played. We can then calculate WP[i] = Wins[i] / Total[i].

Next the OWP of each team needs to be calculated, but the OWP requires a modified WP for each opponent. Let's consider WP'[i][j], the winning percentage of team i if you exclude games against team j. To calculate WP'[i][j], we have to examine three possible cases.

If team i never played versus team j, then WP' has no relevance and can be ignored.
If team i did play versus team j and won the game, then WP'[i][j] = (Wins[i]-1) / (Total[i]-1).
If team i did play versus team j and lost the game, then WP'[i][j] = (Wins[i]) / (Total[i]-1).
All that is left to do to calculate WP' is to try all pairs of teams and calculate the value if the two teams played.

Now that we have WP' for every pair of teams, we can calculate the OWP values. Let S[i] be the set of teams that team i played against. Then we can calculate OWP[i] as follows:

OWPSum[i] = 0
for team j in S[i]:
   OWPSum[i] += WP'[j][i]
OWP[i] = OWPSum[i] / size(S[i]).

Lastly, we need to calculate the OOWP for every team i. The OOWP uses the OWP which we already calculated:

OOWPSum[i] = 0
for team j in S[i]:
   OOWPSum[i] += OWP[j]
OOWP[i] = OOWPSum[i] / size(S[i]).

Finally, we can combine everything with the formula:
RPI[i] = WP[i] * 0.25 + OWP[i] * 0.5 + OOWP[i] * 0.25.

By the way, this formula really is in use. It is not always very good at ranking teams though!

Round 1B 2011 - Code Jam 2011

Revenge of the Hot Dogs (15pts, 20pts)

This problem might look petty tough at first glance. There are lots of hot dog vendors, and each one has a very important choice to make. How can you account for all the possibilities at once?

It turns out there are at least two completely different solutions, one of which uses a classical algorithmic technique, and one of which is purely mathematical. We will present both approaches here.

An Algorithmic Solution
There are two key ideas that motivate the algorithmic solution.

There is no reason to ever have one hot dog vendor walk past another. Instead of doing that, they could walk up to each other and then just go back the way they came. Since everyone moves at the same speed, this is completely equivalent to having the two people cross.
Rather than trying to directly calculate the minimum time required, it suffices to ask the following slightly easier question: Given a time t and a distance D, is it possible to move all the hot dog vendors distance D apart in some fixed time t? If we can answer that question efficiently, we can use a binary search to find the minimum t:
lower_bound = 0
upper_bound = 1012
while upper_bound - lower_bound > 10-8 * upper_bound:
  t = (lower_bound + upper_bound) / 2
  if t is high_enough:
    upper_bound = t
  else:
    lower_bound = t
return lower_bound
So we need to decide if t seconds is enough to move all the hot dogs vendors apart. Let's think about the road as going from left (negative values) to right (positive values), and focus on the leftmost person A. By our first observation, he is still going to be leftmost when everyone is done moving. So we might as well just move him as far left as possible. That way, he will interfere as little as possible with the remaining people. Since we have fixed t, this tells us exactly where A will end up.

Now let's consider the second leftmost person B. He has to end up right of A by at least distance D. Subject to that limit, he should once again go as far left as possible. (Of course once you account for the first guy, "as far left as possible" might actually be to the right!) The reason for doing this is the same as before: the further left this person goes, the easier it will be to place all the remaining people. In fact, this same strategy works for everyone. Once the time is fixed, each person should always go as far left as possible without getting less than distance D from the previous person.

Using this greedy strategy, we can position every single person one by one. If we come up with a valid set of positions in this way, then we know t is enough time. If we do not, then there is nothing better we could have possibly done.

We can now just plug this into the binary search, and the problem is solved!

A Mathematical Solution
On the Google Code Jam, we would expect our contestants to try algorithmic approaches first. After all, you guys are algorithm experts! However, we would like to also present a mathematical solution to this problem. It avoids the binary search, and so it is more efficient than the previous solution if you implement it right.

As above, sort the people from left to right. Let Pi be the position of the ith person, and let xi,j = D*(j - i) - (Pj - Pi). Finally, define X = maxi < j xi,j. We claim max(0, X) / 2 is exactly the amount of time required.

Let's first show that you need at least this much time. Focus on two arbitrary people: i and j. Since nobody should ever cross (as argued above), there must still be j - i - 1 people between these two when everything is done. Therefore, they must end up separated by a distance of at least D*(j - i). They start off separated by only Pj - Pi, and this distance can go up by at most 2 every second, so we do in fact need at least [D*(j - i) - (Pj - Pi)] / 2 seconds altogether.

To prove this much time suffices, we show how X can always be decreased at a rate of 2 meters per second. Let's focus on some single person j. We will say he is "left-limited" if there exists i < j such that xi,j = X, and he is "right-limited" if there exists k > j such that xj,k = X. Suppose we can move every left-limited person to the left at maximum speed, and every right-limited person to the right at maximum speed. Then any term xi,j which is equal to X will be decreasing by the full 2 meters per second, and hence X will also be decreasing by 2 meters per second, as required.

So this strategy works as long as no single person is both left-limited and right-limited. (If that happened, he would not be able to go both left and right at the same time, and the strategy would be impossible.) So let's suppose xi,j = X = xj,k. If you just write down the equation, you'll see xi,k is exactly equal to xi,j + xj,k. But this means xi,k = 2X > X, and we have a contradiction. Therefore, no single person is ever both left-limited and right-limited, and the proof is complete!

Additional Comments
It turns out the answer for this problem is always an integer or an integer plus 0.5. Do you see why? In particular, if you multiply all positions by 2 at the beginning, you can work only with integers. This allows you to avoid worrying about floating point rounding issues, which is always nice!
At first, the mathematical solution looks like it is O(n2), since you are calculating the maximum of O(n2) different numbers. Do you see how to do it linear time?

Round 1B 2011 - Code Jam 2011

House of Kittens (20pts, 25pts)

There are three different tasks you need to work through in order to solve this problem:

How do you transform the input into a more convenient format?
How do you efficiently calculate C?
How do you efficiently find a catnip assignment with exactly C flavors?
An implementation would certainly start with the first task, but when you're just thinking about the problem, it's better to focus on the high-level algorithm first. How could you know what data format is convenient until you know what you want to do with it?

Finding an Optimal Assignment
So let's start with C. The most important observation is also one of the simplest. Let m be the minimum number of vertices in a single room. Kittens in that room have access to at most m flavors of catnip, so it must be that C ≤ m.

In fact, it turns out that C is always equal to m. Proving this is pretty much equivalent to the third sub-task: we need to give a method for assigning flavors that always work with C = m. Here it is:

Choose an arbitrary room. Assign flavors to its vertices in such a way that all C flavors are used, and no two adjacent vertices use the same flavor.
Choose a room adjacent to the starting one. This will have two different flavors fixed for two adjacent vertices. Fill in the remaining vertices as before: all C flavors are used, and no two adjacent vertices use the same flavor.
Choose another room adjacent to one of the rooms previously considered. Again, it will have two different flavors fixed for two adjacent vertices, and nothing else. Proceed as before.
Continue in exactly the same way until all rooms are complete.
There are two keys that make this work.

Key 1: It really is possible to assign valid flavors to the vertices of a single room, even after fixing distinct flavors for two adjacent vertices. Start with those two adjacent vertices, assign the remaining flavors to the next C - 2 vertices, and then just avoid equal neighbors for the remaining vertices. This is always possible since C ≥ 3.

Key 2: When we get to a new room, only two adjacent vertices will ever be fixed. To see why, let's say you just got to a room R by crossing some wall W. Then, W divides the house into two disjoint parts, so this will be the first time you are touching any room on the same side of W as R. In particular, this means the only vertices that will be fixed are the ones that belong to W.

That's it. Just be glad we are asking for an arbitrary assignment of flavors, instead of the lexicographically first one, or something equally evil!

Handling the Input
The main technical challenge in implementing this algorithm is figuring out where all the rooms are in the first place, and how they are connected.

One approach is to maintain a list of lists, representing the vertices in each room. We start off with just a single room: [[1, 2, ..., N]]. For each wall inside the house, we scan through the rooms until we find the one that has both endpoints of the wall, and then split that room into two. We then have to be a little careful about what order we process the rooms in. One option would be to start with an arbitrary room, and then only process future rooms once they have two vertices set. This runs in O(N2) time.

There are also some fancier (almost) linear-time solutions. For each vertex, record the edges coming out of the vertex, sorted by the opposite endpoint. Now you can start with one face, trace along all of its edges, and then recursively proceed to the faces across each edge.

Either method works, so you can use whichever you prefer.

Round 1C 2011 - Code Jam 2011

Square Tiles (10pts, 10pts)

One solution for this problem would be to try to put red tiles in all possible ways over the blue tiles and see if at least one possibility leads to all the blue tiles being covered. However, this will not work in time because there are too many combinations to try.

To optimize the solution, you have to observe that if there is a solution, the top-most, left-most blue tile in the grid (that is, the left-most tile of all the blue tiles in the top row that contains any blue tiles) must be covered by the left-top corner of some red tile. This is because the tiles at its left and top are white (or non-existent), and so the red tile covering our blue tile cannot extend to the left or upwards of it. Based on this observation, we can solve the problem greedily by putting a red tile over the top-most, left-most blue tile in the only way it can be done. If for some blue tile it is impossible to cover it this way (because the red tile would cover some white tiles or extend outside the picture), then it's impossible to cover the whole board.

Note that as we are always sure that any red tile we put down is correct (if a solution exists at all), we can just modify the board on the fly, and thus at the same time check for solution existence and retrieve the answer. To illustrate that, here is a C++ function to cover all the blue tiles in a grid and return whether it was possible or not:

bool CoverTiles(vector<string>& grid) {
  const int m = (int)grid.size(), n = (int)grid[0].size();
  for (int i = 0; i < m; ++i)
    for (int j = 0; j < n; ++j)
      if (grid[i][j] == '#') {
        for (int di = 0; di < 2; ++di)
          for (int dj = 0; dj < 2; ++dj)
            if (i + di < m && j + dj < n &&
                grid[i + di][j + dj] == '#')
              grid[i + di][j + dj] = "/\\"[(di + dj) % 2];
            else
              return false;
      }
  return true;
}
Fun fact: the solution does not change if we drop the requirement that red tiles have to cover 2x2 squares of blue tiles - it is still valid if we allow the rotation and arbitrary positioning of the red tiles; the only condition that matters is that red tiles lie only on blue tiles (do not overlap, stick outside the picture or lie on white tiles) and all the blue tiles are covered in the end.

Round 1C 2011 - Code Jam 2011

Space Emergency (12pts, 25pts)

In this problem, you have a series of edges to follow and you're given the option of shortening up to L of the edges by a factor of two. There's one catch: the shortening takes some time to take effect.

Fortunately, all possible shortenings take the same amount of time, so we can divide our edges into three categories:

Edges that your flagship will pass before any speed boosters can be built.
Edges whose speed boosters would finish while your flagship is moving along the edge.
Edges whose speed boosters can be built before your flagship gets there.
Group 2 only has zero or one members, because we know exactly where your flagship will be when the speed boosters finish building.

Now we can decide how useful it is to build a speed booster on each edge. We never want to build speed boosters on group 1, because they won't help. For each edge in group 3, the benefit is length/2. For the edge in group 2, the benefit is (distance to the end of the edge from where your flagship will be when the speed booster finishes building)/2.

We choose the L most beneficial edges to build on; if there aren't L beneficial edges, we'll stop when we've built on all the beneficial edges. Then we compute the total benefit, subtract it from the total time to traverse the whole path, and voilà! The answer.

The way in which we specify edges in this problem is periodic: if the input is very large, then there must be a lot of edges with the same lengths. A clever solution could take advantage of the periodic nature to run very quickly, but the limits were small enough that this shouldn't be necessary; we actually specified the input in this way because it would make the input file smaller, not because we wanted to test that particular skill.

Round 1C 2011 - Code Jam 2011

Perfect Harmony (8pts, 35pts)

This problem turned out to be very, very tricky, due to a number of cases to consider and the possibility of integer overflow. Thus, we will go over the solution with some care.

To solve the small data set for this problem, it is enough to iterate over all notes that can be played by Jeff, and for each of them check whether it is in harmony with all the other notes. Note that there are two cases to consider for each note - either its frequency has to divide the frequency of Jeff's note, or it has to be divisible by it.

For the large data set, this strategy will not be sufficient - there are too many notes that Jeff can play to check.

We will begin by sorting all the input frequencies. Now assume that the frequency of the note that Jeff will play (let us denote it by F) is somewhere between frequencies fk and fk+1. This means, in particular, that all the frequencies f1, f2, up to fk are no larger than F; so F has to be divisible by all of them. This means that F has to be divisible by their Least Common Multiple (which we will denote LCM(f1, f2, ..., fk)). Similarly, F has to divide the Greatest Common Divisor of fk+1 up to fN.

Calculating the GCDs and LCMs
To make any use of this information, we need to calculate all the LCMs of sets f1, ..., fk up to any k, and also the GCDs of sets fk+1, ..., fN for any k.

Let us recall that the GCD of two numbers a and b can be calculated using the Euclidean algorithm in O(log(a + b)) time. To calculate the LCM of two numbers, we use the formula LCM(a, b) = a * b / GCD(a, b). Using this, we can calculate all the needed GCDs and LCMs in O(N) GCD operations, inductively. For instance, having already calculated the first k-1 LCMs, we calculate the kth as follows: LCM(f1, ..., fk) = LCM(LCM(f1, ..., fk-1), fk), and the first of those numbers is already calculated.

Note that one can also calculate all the GCDs and LCMs directly, in O(N2) GCD operations. With the limit of 104 on GCD this should also run in time.

One final comment to make here is that when calculating the LCMs, we should be careful to avoid overflow. It may turn out that the LCM of some of the input frequencies does not fit into a 64-bit integer (in general, the LCM of 104 numbers, each up to 16 digits, can have even 160,000 digits!). However, note that Jeff cannot play notes with frequency greater than 1016, thus if the LCM of any numbers turns out to be greater than 1016 (or even greater than H) we can safely replace it by 1016 + 1 without changing the answer - Jeff will not be able to play a note with a frequency divisible by this LCM anyway. Thus, when we calculate the formula a * b / GCD(a, b) we should first divide any of the two numbers, say a, by the GCD, then check whether the resulting product exceeds H (e.g. by checking whether (H + 1) / b ≥ a / GCD(a, b), and perform the multiplication only if it does not.

Special cases first
Before going on, we should also consider that there are two special cases when the analysis above does not apply - when F divides all the input frequencies, and when F is divisible by all of them.

There are a number of ways to take care of them. For the first, the easiest is to add another note with frequency 1 to the input. This will not make Jeff's task harder, as any number is divisible by 1, and on the other hand will assure that F is always divisible by at least one of the numbers in the input.

For the upper bound, no analogous trick exists (there is no number that is divisible by any F Jeff might choose; one might consider the LCM of all the numbers that Jeff's instrument can play, but this is usually too large. Thus, if we fail to find a solution F lying between any two of the input frequencies, we have to consider this case separately. Fortunately, it is not complicated - if C is the LCM of all the input numbers, then the result will be the smallest multiple of C that is greater or equal L, provided it is no larger than H.

The standard cases
Notice that as we are looking for the lowest frequency Jeff can play, we can investigate the possibilities one by one - first check if there is a solution between f1 and f2, if yes - return it (recall that f1 is 1, so there is no solution smaller than f1). If no solution was found, look between f2 and f3, and so on. Finally, if no solution is found between fN-1 and fN, we consider the special case analyzed above.

Now for the crux of the problem - how can we check (quickly) whether a solution is to be found between fk and fk+1? Note that this interval can possibly contain up to 1016 numbers, so a brute force check is unsatisfactory.

Recall that if F is between fk and fk+1, then it has to divide GCD(fk+1, ..., fN) (we will denote this number by D), and it has to be divisible by LCM(f1, ..., fk) (we will denote this number by C). Thus, in particular, if C does not divide D, we know there is no solution in this interval.

There are two more easy cases to consider. If the intervals [L, H] and [C, D] are disjoint, there is obviously no solution in this interval. If C lies in the interval [L, H] (and divides D, which we already checked), it is obviously the smallest solution in this interval, so we may safely return it.

Notice that there is at most one interval for which those easy (and in particular, constant time) checks will not suffice. Indeed, if for some k the intervals [L, H] and [C, D] are not disjoint, then for any subsequent interval [C', D'] obtained for a different k' either C' lies in [L, H], or the two intervals are disjoint, as L ≤ D ≤ fk+1 ≤ C'.

Finally, we can concentrate on this one interval. We want to find the smallest number F in the interval [L, H] that divides D and is divisible by C. For this, it is enough to consider all the divisors of D and check them one-by-one. The divisors of D can be enumerated in time proportional to the square root of D - for each divisor d, either d or D/d is no larger than the square root of D, thus to find all the divisors we check all the numbers no larger than the square root, and if a number d divides D, we add both d and D/d to the list of divisors. This algorithm returns the divisors almost sorted, so it is easy to consider them in ascending order and find the first that both is divisible by C and falls into the interval [L, H].

Summary
This was not an easy problem, and required quite a lot of care and attention. Let us enumerate the steps, to wrap it up:

Sort all the input frequencies (in O(N log N) time)
Add 1 to the beginning of the list of inputs (in O(1) time :) )
Calculate the prefix LCMs and suffix GCDs (in O(N) GCD operations, each taking O(log H) time (as we do not consider results greater than H)
For each k from 1 to N-1 check whether the appropriate LCM divides the appropriate GCD (if no, proceed to next interval); if the LCM falls into [L, H] (if yes, return the LCM) and whether the intervals [LCM, GCD] and [L, H] intersect (if no, proceed to the next interval). This takes constant time for each interval, so O(N) time in total.
If we are still analyzing this interval, find all the divisors of the GCD and check them one by one. This takes O(sqrt(H)) time.
If no answer was found as yet, it remains to check the smallest multiple of the LCM of all the inputs that is greater or equal than L. This is done in constant time.
There are other approaches possible to this problem, too. For instance, one may analyze all the divisors of the largest input frequency, and for each of them use binary search to find the interval that contains it and (in constant time, using precalculated LCMs and GCDs) check whether it is a correct solution. We encourage you to analyze the details of this approach yourself.

Round 2 2011 - Code Jam 2011

Airport Walkways (8pts, 10pts)

Some people will tell you that programming contest problems, while interesting to think about, have no practical use in the real world. After solving this problem, you can laugh at these people as you catch your flight and they are left waiting in line at the airport with all the other chumps.

An important thing to notice about this problem is that the location of the walkways does not matter, since you can instantly transition between different speeds. This means that two walkways with speed v of length L1 and L2 can be combined into a single walkway of length L1 + L2 with speed v. By combining this with the observation that any section of corridor with no walkway is equivalent to a walkway with v = 0, you reduce the problem to having 101 different speed walkways of variable length (possibly 0) and deciding for each whether to run or walk (or do some of each).

The small dataset can be solved by brute force. Let W be the set of all positive length walkways (each with a unique speed). Since there are only |W| ≤ 21 different speed walkways to consider, you can simply try choosing each subset S ⊆ W of them to run (and just walk the rest, W - S). This solution is slightly complicated by the fact that once you choose S, you still have to decide which one to only run partially, in case you don’t have time to run them all fully. However, you can again just brute force this decision by iterating over each walkway x ∈ S and only running on walkway x with whatever time is left after completely running all walkways in S - {x}. You simply take the minimum over all choices, discarding any choice which requires more than t seconds of running time. This can easily be implemented in O(N2 * 2N) and will run in time for N at most 20.
Bonus: Implement this algorithm in O(N * 2N) time.

However, this approach will time out for the large data set, where N is at most 1000 and you can have walkways of all 101 different speeds. For this, we need a better way to decide when to run and when to walk. We just need to decide if it’s better to run on slower or faster walkways. It turns out we can solve this with a simple greedy algorithm, and we can prove it is optimal by using a simple exchange argument.

Let’s say we have two arbitrary walkways of speed w1 and w2, with w1 < w2. Now let’s say that in some algorithm we’ve decided to run for r1 and r2 seconds on each of the walkways, respectively. If s1 and s2 are the amount of time we spent walking on each of the two walkways, then our total time spent would be T = r1 + s1 + r2 + s2 seconds. What if instead we decided to run for r1 + ε seconds on the first walkway and r2 - ε seconds on the second walkway, with ε > 0? Then our total time would be T’ = (r1 + ε) + s1’ + (r2 - ε) + s2’ seconds. Solving for T - T’, you will get ε * (w2 - w1) * (R - S) / ((w1 + S) * (w2 + S)) > 0, which says that T > T’, and so the change will always be beneficial. Simply put, it’s always better to run on slower walkways as much as possible. Note that some of the details of these equations have been excluded from this analysis and are left as an exercise to the reader.

Here is some simple Java code which solves the problem:

double res=0;
for (int i=0; i<=100; ++i) {
  double runTime=Math.min(t,W[i]/(i+R));
  double walkDist=W[i]-runTime*(i+R);
  double walkTime=walkDist/(i+S);
  res+=runTime+walkTime;
  t-=runTime;
}
System.out.printf("Case #%d: %.12f%n",TC,res);
Bonus: Solve the problem if the limits changed to 1 ≤ wi ≤ 109.

Round 2 2011 - Code Jam 2011

Spinning Blade (8pts, 12pts)

Let us begin by thinking how can we determine the center of mass of a given blade. The simplest formula for the center of mass (obtained by transforming the formula from the problem statement) is: sum(Massi * Pi) / sum(Massi), where Pi is the position of cell i relative to a static location, such as the upper-left corner of the sheet of metal, Massi is the mass of cell i, and i is iterated over all cells in the blade.

The first thing to note about this problem is that the X and Y coordinates of the center of mass can be calculated independently, which will simplify the calculations significantly. The X and Y coordinates of the center of the blade are also easy to calculate (the X coordinate is the average of the smallest and largest X coordinate of any cell in the blade).

The thing we are interested in is whether the center of the blade and the center of mass of the blade coincide. To avoid floating point calculations (which would induce the need to think about possible precision problems) we can multiply the equality sum(Massi * Xi) / sum(Massi) = (minX + maxX) / 2 by the denominators of both sides. Thus, for each blade we need to check the equality 2 * sum(Massi * Xi) = (minX + maxX) * sum(Massi).

So, we simply need to test this equality for all possible blades. This can be done by iterating over all possible X and Y coordinates of the upper-left corner of the blade, and then over all possible sizes of the blade. Calculating either side of the equality above can be performed by iterating over all cells in the blade (remember to omit the corners!). As there are O(RC) possible upper-left corners, O(min(R,C)) possible sizes and O(min(R,C)^2) cells in a blade, the whole algorithm has a time complexity of O(N5) (where N denotes the common upper bound for R and C), which works for the small input, but we cannot expect to make it work for the large.

Before we attack the large case, let us spend a moment to look at potential overflow problems — we already saw that this can be a serious issue in this competition! The left-hand size of the inequality can be estimated by 2 * N2 * N * maxW — two times the number of cells times the largest possible value of Xi times the largest possible weight of a cell. In the small test cases, this value will easily fit into a 32-bit integer, while in the large case we should use 64-bit integers to be safe. We also considered giving a 1018 bound on D, several approaches of dealing with overflow (that can also handle this obscenely large limit) are given at the end of this editorial, you might also want to think about this problem yourself.

Back to the large case. Notice that in the previous approach we have seen there are O(N3) blades to consider, so one approach to reducing the run time is to attempt to make the center of mass calculation for every blade constant. This requires a bit of precalculation.

To precalculate the center of mass (or rather, the sum(Massi * Xi) and sum(Massi) quantities) for any given blade, we will first calculate the center of mass and total mass for all rectangles with the two corners at (0,0) and (x,y). We will start with the rectangle with corners at (0,0) and (1,1). This is just the first cell of the grid, so we already know its center of mass and total mass. We will store this answer and move on to the next rectangle we want to calculate, which will have corners at (0,0) and (1,2). Since we already know the center of mass and total mass for the rectangle with corners (0,0) and (1,1) we can use this rectangle, and the rectangle with corners at (0,1) and (1,2) to calculate the center of mass and total mass for the rectangle with corners (0,0) and (1,2). As long as we iterate over the Y axis 1 by 1 we can calculate the total mass and center of mass of all possible rectangles in constant time.

Now we have to handle the case where we have to iterate the X axis of the corner, so we need to know the center of mass and total mass of the rectangle with corners at (0,0) and (2,1). This is the same as the first case for the Y axis, so we just calculate the center of mass and total mass using the rectangle with corners at (0,0) and (1,1) as well as the rectangle with corners at (1,0) and (2,1). In the next step we run into a problem. We have a grid that looks something like the image below.

If it is not clear in the image, the rectangles marked B and C overlap with the rectangle D.

We know the appropriate sums for the rectangles marked A, B, C, and D, but we need to know the sums for the entire rectangle, which we will call R. This can be done in constant time using:

R = A + B + C - D

We subtract D because it is added twice when we add both B and C.

This can be used to calculate center of mass and total mass both for all rectangles with corners (0,0) and (2,2) on to (X,Y). The total run time for this is O(N2) since we only have to look at each cell once. Now, how can this be used to calculate the sums for a square with corners at (x1,y1) and (x2,y2) (or any other square we might be interested in)? This is very similar to the way we calculated the center of mass and total mass during the precalculations. Assume we label a few of the rectangles we have already precalculated as:

A = Square we are looking for, with corners at (x1,y1) and (x2,y2)

B = Rectangle with corners at (0,0) and (x1,y2)

C = Rectangle with corners at (0,0) and (x2,y1)

R = Rectangle with corners at (0,0) and (x2,y2)

D = Rectangle with corners at (0,0) and (x1,y1)

The same picture, reasoning and equation as before gives us R = A + B + C - D, which transforms to A = R + D - B - C. Thus, having all the precalculations done, we can compute all the needed quantities for any square (and thus, by subtracting the values in the corners, for any blade) in constant time!

With a constant center of mass calculation, this brings the total run time to O(N2 + N3). This will easily work with N <= 500.

Side Note
Another approach to this problem is to try all possible upper-left corners for the blade, and then slowly expand the size of the blade by 1 unit to the bottom-right at a time until it cannot be expanded any further, calculating the required sums using O(N) calculations at each step to increment the previous values. That brings the total run time to O(N4). While this may seem incredibly large when N can be up to 500, in practice this is really a lot less calculations than 5004 (as for most corners we cannot expand up to size N), while the input file size limit guarantees there are at most two max-cases in the input. Thus, this approach will run in time on most computers and in most languages.

Dealing with overflow
Let us go back to the question what would we do if the limit on D was larger.
We can try to use BigIntegers of some sort. This solution costs us in terms of efficiency, although if we go for the O(N3) solution we should still be able to make it.
We can realize that the value of D is irrelevant. On an intuitive level - adding a constant mass D to each cell is the same as putting a new sheet with each cell of mass D on top of our sheet. The center of mass is going to be somewhere in between the centers of mass for the two sheets — and so it is going to be in the middle if and only if our original sheet had the center of mass in the middle. On a formal level, if we add D to each Massi, both sides of the equation increase by 2 * AvgX * NumberOfCells, where AvgX is the average X coordinate of a cell in the blade. Thus, we can set D to, say, 1 and use 32-bit integers
We can also set D to zero. This is somewhat scary, as it would result in a division by zero in the original equation for the center of mass, we encourage you to consider why this works.
Finally, we may ignore the overflow problem totally, if only our compiler guarantees that overflow results in modulo arithmetics. As D cancels out on both sides of the equation anyway, it also cancels out modulo MAXINT, so if only the rest of the equation will not overflow, we will be fine. This requires some knowledge of your tools, though — for instance modular arithmetic for integers in C++ is guaranteed for unsigned integers, but not for signed integers, and the compiler can make optimizations that assume overflow does not happen for signed integers (and thus break code depending on the modular arithmetic).

Round 2 2011 - Code Jam 2011

Expensive Dinner (13pts, 17pts)

This problem might look pretty intimidating at first:

There are N! possible orders in which your friends could arrive.
Not only is O(N!) too slow for this problem, even O(N) is too slow!
The actual price your friends are paying will quickly exceed the bounds of a 64-bit integer.
When O(N) is too slow, it means you need to forget about coding for a while and think. You will need some insight to even get started.

Let's begin by fixing an ordering (x1, x2, ..., xN) of your friends. Also let yi be the total price that your group is paying after the first i friends enter, the waiter has been called if necessary, and everyone has become happy.

Observation 1: yi = LCM(x1, x2, ..., xi). This does not depend on the order your friends buy things after the waiter has been called.

Explanation: LCM(x1, x2, ..., xi) is, by definition, the smallest multiple of x1, x2, ..., xi. Since yi must be a multiple of all these numbers for your friends to be happy, it is certainly true that yi ≥ LCM(x1, x2, ..., xi). Conversely, a friend xj would never buy something that skips over a multiple of xj. In particular, none of the friends here would buy something that would skip over LCM(x1, x2, ..., xi). Since yi-1 ≤ LCM(x1, x2, ..., xi), you will eventually reach this price and then stop.

Let M be the number of times the waiter is called over. Then, M is equal to the number of values i in {1, 2, ..., N} such that yi != yi-1. (We define y0 = 0, because the waiter is always called over by the first friend who enters.) So the question becomes: how do we make M as big as possible according to this definition, and how do we make it as small as possible?

Least common multiples are closely related to prime numbers and factorizations, so let's define p1, p2, ..., pP to be the primes less than or equal to N. Also let ei be the largest integer such that piei is less than or equal to N.

Observation 2: Let P' be 1 + sum ei (i.e., the number of prime powers less than or equal to N, including 1). Then the maximum value of M is exactly equal to P'.

Explanation: Suppose your friends arrive in the order 1, 2, ..., N. Then each friend with a prime power index will cause the price to increase, and so the maximum value of M is at least P'.

On the other hand, the price is always a least-common multiple by the first observation, and it always increases to a multiple of itself. In particular, the sum of the exponents in its prime factorization must always go up every time the waiter is called. After the first friend arrives, this sum is at least 0. At the very end, it is equal to P' - 1. Therefore, the waiter can be called at most P' - 1 times after the first person arrives. Combining that with the initial increase proves that M ≤ P'.

Observation 3: If N > 1, then the minimum value of M is equal to P.

Explanation: Suppose the first friends to arrive are numbered p1e1, p2e2, ..., pPeP. Then the price is already equal to LCM(p1e1, p2e2, ..., pPeP) = LCM(1, 2, ..., N) after these friends have arrived. This means no subsequent friend will change the total price, and hence M = P in this case.

On the other hand, notice there is no number less than N that is divisible by both piei and pjej. This is because piei and piej are relatively prime and larger than sqrt(N). (If one of them was at most sqrt(N), then its exponent could be increased, which is a contradiction.) Therefore, no single friend can make the total price divisible by two different entries out of {p1e1, p2e2, ..., pPeP}. And so the waiter must be called at least P times, as claimed.

To solve the small input, you can just calculate P and P' directly and go from there. The large input requires one last clever trick. The number of primes less than 1012 is quite large, and you probably cannot afford to just count them all. However, calculating P - P' is actually easier than this!

If you go back to the original definitions, you can see P - P' is precisely the number of integers less than or equal to N that can be written in the form pe for e != 1. When e = 0, you just get 1. When e > 1, then p ≤ 106, and you can easily enumerate all primes of this size! One good way is to use the Sieve of Eratosthenes. As always, you can look at solutions from other contestants to see a full implementation.

Bonus Comment: The Sieve of Eratosthenes runs in O(M * log log M) time, so the simplest implementation of the method described here runs in O(sqrt(N) * T * log log N) time altogether. However, it's worth pointing out that you can do even better. If you pre-compute and sort all prime powers less than 1012 with exponent other than one, you can then use a binary search to very efficiently count how many are less than N for each test case. This is not necessary to solve the problem, but it lets you speed things up even more to a blazing O(sqrt(N) * log log N + T * log N). Proving this running time is a little tricky though!

Round 2 2011 - Code Jam 2011

A.I. War (10pts, 22pts)

The computer game A.I. War hides at least a few good algorithmic problems. The author is anything but an expert, but musing about the game brought both this problem and Space Emergency (which originally took place on a graph like this one) to life. The game presents a trickier version of this problem: there are two A.I. homeworlds, multiple other planets that you might want to visit, and you don't know where to find any of them at the start of the game. Fortunately, here we're dealing with a simplified version of the problem and you didn't have to know anything about the game.

Let's first state the problem in graph theory terms. We are given an undirected graph with P vertices and W edges. We are looking for a sequence of vertices, starting with vertex 0 (our home planet) and ending with a neighbor of 1 (A.I.'s home planet), such that:

every vertex in the sequence is adjacent to one of the previous vertices
the sequence is as short as possible
given the above, the number of distinct neighbors of the vertices in the sequence, but outside the sequence, must be as large as possible
Let D be the distance from vertex 0 to vertex 1. It's clear that every such sequence must have at least D elements. We also note that we will achieve exactly D elements if and only if the sequence forms a shortest path from vertex 0 to vertex 1. Hence we're looking for a shortest path. Unfortunately, there may be many shortest paths and we have to pick the one that optimizes the last requirement.

It simplifies things to include among the "threatened" planets those planets that we do conquer. Given that we already know that we have to conquer exactly D planets, we just have to subtract D at the end.

Crucial observation
The crucial observation for solving the problem is the following: if a vertex is at distance d from 0, it can only be threatened by a vertex at distance d - 1, d or d + 1. This is true because the distances of two adjacent vertices differ by at most 1. Therefore every vertex in the graph is only threatened (or conquered) within at most three consecutive vertices in the path. As we will see, this observation allows us to use dynamic programming to compute best paths of larger and larger lengths.

The algorithm
The first step in our algorithm is to run breadth-first search to compute for every vertex v its distance from 0: dist[v]. dist[0] = 0 and dist[1] = D. Every shortest path will start with vertex 0, then continue with vertices at distance 1, distance 2, ..., distance D-1.

For any two adjacent vertices a, b such that dist[b] = dist[a] + 1, define F(a, b) = the maximum number of planets threatened or conquered by a shortest path 0→...→a→b. We will compute this using dynamic programming for increasing distances from 0. The answer to the problem is the maximum value of F(a, b) - D, where a, b are adjacent, dist[a] = D-2, dist[b] = D-1, and b is adjacent to 1.

F(0, a) can be computed directly (there is only one path possible). It remains to compute the values of F for a given distance, given the values of F at a previous distance. To compute F(b, c), dist[b]=d, dist[c]=d + 1, try all vertices a adjacent to b such that dist[a] = d - 1. In other words, we are looking for paths ending with a→b→c. We have already computed the value of F(a, b) in the previous iteration, the question is: how many new unique threatened vertices does extending the path to c add?

This is where our "crucial observation" becomes useful. If a neighbor of c was already threatened (or conquered) before, it must have been a neighbor of either a or b. Therefore we must add the number of neighbors of c that are not neighbors of either a or b. Let's call this value G(a, b, c). So we have the recursive formula that we can use for dynamic programming:
F(b, c) = maxa F(a, b) + G(a, b, c).

Computing G: four algorithms
We are almost done, but how do we compute the values of G, and what is the total run-time of the algorithm? There are O(W) values of F to compute (at most one for each edge), and for each one we look at O(P) other values of F (one for each a). So if we already knew all the values of G, it would take O(PW) time to compute all the values of F and solve the problem. Computing the values of G turns out to be the most time-consuming part though.

We may have to compute G(a, b, c) for every sub-path a→b→c of increasing dist. How do we compute these values? We have thought of at least 4 ways. In the actual contest it didn't really matter which you chose, all would easily run in time, but we will mention them just for fun.

Approach 1. We need to compute the number of vertices v such that v is a neighbor of c, but not of a or b. The simplest approach here is to just check this condition for every v. This computation takes O(P) time. How many values do we need to compute? At most O(P3), which gives a total run-time of O(P4) for the whole algorithm. We can give a better estimate. Since a and b are neighbors, there are at most W such pairs. This gives the total run-time of O(P2 W).
Approach 2. Modify approach 1 slightly. Instead of checking every vertex v, we only need to check every neighbor of c. This way there are O(W) pairs a, b and O(W) pairs c, v, which gives the total run-time of O(W2).
Approach 3. Modify approach 1 in another way. Precompute the set of neighbors of each vertex as a bitmask neighbors[v]. Now we are simply interested in the number of bits set in (neighbors[c] and not (neighbors[a] or neighbors[b]). This is a speedy computation due to bit-level parallelism. If our computer has machine words of size w (this is typically 32 or 64), we cut our work by a factor of w. This assumes that we can count the number of bits set in a word in a single step, which modern processors support in a single machine instruction. The final runtime: O(P2 W / w).
Approach 4. Finally, we come to a theoretically interesting, but rather complex to implement aproach. Define two matrices, A of size W x P, and B of size P x P, as follows. Aij = 1 if vertex number j does not neighbor any of the two endpoints of the edge number i, 0 otherwise. Bij = 1 if vertices number i, j are adjacent (or equal), 0 otherwise. Now compute the matrix product C = A * B. If we apply the definition of matrix product and do the math, we will see that C is a W x P matrix and that the entries are exactly the values of G. Specifically, Cij = G(a, b, j) if the i-th edge is a↔b.
If we evaluate the matrix product in the natural way, we get the same complexity as in approach 1: O(P2 W). We have basically expressed approach 1 in matrix notation.
The trick is that there are faster matrix multiplication algorithms. The fastest currently known is the Coppersmith-Winograd algorithm. To make a long story short, it gives us a theoretical asymptotic run-time of O(P1.376 W).
We do not recommend the last approach in the contest. Not only is this unnecessarily complicated, it also wouldn't actually run any faster for the graph sizes we are considering. The asymptotic advantage would only materialize for impractically enormous, dense graphs.

Round 3 2011 - Code Jam 2011

Irregular Cakes (7pts, 7pts)

Before we can start solving the problem, we first need to know how to calculate the area of the cake, so we know how much cake each party-goer is going to eat. We are guaranteed that this is a non-intersecting polygon in the limits, so we can use the equation:

2*Area = sum(Xi * Yi+1 - Xi+1 * Yi) for i = 0 to N-1.

For this to work the first and last points in the polygon must be the same point. That is, X0 = XN and Y0 = YN. The list of points must also be in counterclockwise order. This can be achieved by iterating over all points in L, and then iterating over all points in U in reverse order.

Once we know the total area of the cake, we need to determine how much cake to give each guest. Since there are G guests, we can evenly calculate this number as AreaPerGuest = Area / G.

Finally we are ready to determine where to cut the cake. This can be solved a variety of ways, but here we will discuss using a binary search as this is often the simplest algorithm to code. To obtain a piece of cake with an area of AreaPerGuest, we know that the vertical cut will have an X coordinate between 0 and W, so we do a binary search over this range. For each cut point we try during the search, we compute the area of the cake to the left of that cut. If it produces a piece of cake with area greater than AreaPerGuest, we update the upper bound of our search. If we choose a cut that produces a piece of cake with area less than or equal to AreaPerGuest, we update the lower bound of our search. This search will eventually converge to the correct cut point with sufficient accuracy.

If G=2, then we are done. If G > 2, then the binary search can be repeated to find the other cuts. The second cut point should be placed such that the area of cake to the left of that cut is AreaPerGuest * 2, the third cut point should be placed so that the area of cake to the left of that cut is AreaPerGuest * 3, and so on.

At each iteration of the binary search, we need to calculate the area of a polygon using part of L, U, and a line on an arbitrary X coordinate, which requires more work than computing the area of the whole cake. The first thing to note is that we can use all points in L and U that have X coordinates such that 0 <= Xi <= Xcut, where Xi is the X coordinate of point i, and Xcut is the X coordinate of the cut. Next we need to determine the Y coordinates of our Xcut points on L and U. If we find two consecutive points A and B in L such that Ax <= Xcut and Bx >= Xcut, we can use line intersection to find the intersection of line AB and the vertical line defined by Xcut. Once we know these two new points the area of the cut piece of cake can be calculated.

This problem can also be solved with a linear time solution in O(N+G). The basic premise is to split the cake into trapezoids, and iterate from left to right accumulating the total area. Any time a trapezoid needs to be split for a cut, a quadratic equation is used to determine where to cut the cake. This approach only requires the basic formula for the area of a trapezoid, which is (a+b)/2 * h, where a and b are the lengths of the bases, and h is the height of the trapezoid.

More information:
Polygon area
Binary search

Round 3 2011 - Code Jam 2011

Dire Straights (4pts, 12pts)

Dire Straights had one of the easiest small datasets for round 3. Several brute force or backtracking solutions that followed the rules could come up with a valid answer in the allotted time, so rather than look at the small dataset, we will instead examine the large. The large dataset requires some insight into a greedy approach.

For a problem like this, a good strategy is to think of how you might try to solve this problem by hand. A very intuitive strategy is to first put all the cards in order, then start setting the cards down on the table, creating a new straight whenever necessary. Since our goal is to make the length of the shortest straight as long as possible, then one idea that seems like it might work is to always increase the length of the shortest straight when we have a choice. Now we need only to prove that this choice is optimal.

Suppose we have two straights, one from a to b, another from c to d, such that a < c <= d < b.

   a-------b
    c-----d
Notice that we can replace these two straights with straights from a to d and c to b (this change is illustrated below), and this does not decrease the score. In fact, this change has the potential to increase the score.

   a------d
    c------b
This shows that we can always make sure that a straight that started later never ends before one that started earlier. Hence, attaching the next card to the shortest straight is optimal.

Finally, the size of every straight is examined and the length of the shortest straight is the total score achieved in Dire Straights.

Round 3 2011 - Code Jam 2011

Perpetual Motion (5pts, 24pts)

The first thing we need to notice is that if two lemmings do not end up in the same square after one second, they will never end up in the same square. That is because after one second there will be exactly one lemming in each square, and the state is exactly the same as the second before.

The number of combinations for the conveyor belt directions is 2R·C, so we can afford to try all of them to solve the easy input and count how many combinations lead to all lemmings being in different squares after one second.

To solve the hard input, we'll have to take a look at the problem from a graph theory perspective. Suppose we created a bipartite graph like this:

For each cell (r, c), create two nodes startr, c and endr, c.
Create an edge between startr1, c1 to endr2, c2 if there is a way to choose the direction of the conveyor belt in cell (r1, c1) such that the lemming ends up in (r2, c2) after one second.
All start nodes will end up being incident to exactly two edges because there are exactly two different cells where a lemming can end up in one second starting from any given cell. End nodes, on the other hand, can be incident to 0-8 edges, depending on the conveyor belt orientations of the neighbouring cells.

Let's observe the graph some more. There are two rules that apply:

If there are no edges incident to a node endr, c then no lemming can end up in cell (r, c) in one second, so there will be two lemmings in some other cell no matter how we direct the conveyor belts. In that case the answer is simply 0, so we can proceed to the next test case.
If there is a node endr2, c2 incident to only one edge leading to startr1, c1 then we have no other choice but to direct the conveyor belt on the cell (r1, c1) to lead to the cell (r2, c2). Then we can simply remove both endr2, c2 and startr1, c1 from the graph along with all the incident edges.
We can apply these rules iteratively until they can not be applied anymore.

Let N be the number of start nodes left after the above process is done. The number of end nodes is also equal to N, because we removed them in pairs.

The number of edges incident to each start node is still equal to two, so total the number of edges is equal to 2·N because the graph is bipartite. We also know that each end node is now incident to at least two edges, because the above rules do not apply anymore.

But, if any of the end nodes had more than two incident edges then the number of edges incident to all the end nodes combined would be greater than 2·N. This would contradict the fact that the number of edges is equal to 2·N. Therefore, the number of edges incident to each end node is also equal to two.

Any graph having all node degrees equal to two is in fact a set of cycles. For bipartite graphs, the length of each cycle is even. So, we are left with K cycles of even length which we can solve independently and multiply the individual numbers to get the final number.

To solve the circle, select any cell (r1, c1) and pick a conveyor belt direction. The lemming ends up in (r2, c2). Now remove startr1, c1 and endr2, c2 from the graph along with incident edges. This will break up the cycle, and we can proceed to apply rule #2 until we decide the conveyor belt for all the remaining cells. Because the cycle has even length and we always remove nodes in pairs, the rule #1 will never apply.

We can pick the direction of the cell (r1, c1) in two ways. So the answer for any cycle is always 2. So the final answer is equal to 2K (modulo 1000003).

Round 3 2011 - Code Jam 2011

Mystery Square (10pts, 31pts)

Overview
This problem may have a simple statement, but make no mistake: it is really hard. Unless you have a small army of computers working in parallel, there is no way to try all 240 possible values. You need a way to limit your search.

The key observation is that for the most part, a perfect square is uniquely determined by both the first half of its digits, and by the second half. Either one suffices. So the high-level idea is to pick the half that has fewer question marks in it, try all possible ways of filling the question marks, deduce what the rest of the number has to be, and then see if it works. Before we get into the details though, let's talk about one nasty little implementation detail that you can't help but notice.

Dealing with big integers
If a binary number has 120 digits, there is obviously no way to fit it into a standard 64-bit integer! And that means arithmetic can be a nuisance. Here are a few ways you can deal with this extra complication:

Use Java and take advantage of the BigInteger class.
In g++, you can use the little known __uint128_t type.
Roll your own BigInteger functions for 128 bits. In practice you only need Square and SquareRoot. The first is doable with grade-school long multiplication formulas and some care. The second can be done with a binary search.
Use an external library such as GMP.
Hopefully you remembered your Fair Warning from last year, and were prepared! Even so, we would have loved to let you work on only 64-bit integers if we could, but it turns out computers are so fast today that you can simply loop over ALL 64-bit perfect squares in a few seconds. The problem becomes pretty boring in that case.

Filling in a perfect square top-down
All right, so with that detail of the way, let's get down to the solution. If you know the first half of the digits in a perfect square, how can you easily figure out the rest? For example, let's look at 10110????? (in binary).

Notice the square root has to be at least sqrt(1011000000) and at most sqrt(1011011111). In fact, there is only one integer that is between these two, and it is 11011! So we can just see if 110112 matches 10110?????, and then we're done. And this always works. If a number X has 2t digits, then its square root Y has t digits, and (Y+1)2 = Y2 + 2Y + 1, which already differs from Y2 in more than the last t digits.

So in summary: once we know the first half of the digits in N, we can just replace the ? characters with 1, take the square root, round down, and that is the only possible option.

Filling in a perfect square bottom-up
The other half of the solution is not much harder conceptually, but the devil is in the details. If you know the second half of the digits in a perfect square, how can you easily figure out the rest? For example, let's look at ????011001.

Getting started is actually tricky, but let's suppose we have figured out the last two binary digits of the square root are 01.

The square root must then be 4A + 1 for some integer A. Its square is 16A2 + 8A + 1 = 8A + 1 mod 16. However, we know that the square is 9 mod 16, and hence A must be odd. Therefore, the square root must end in 101.
We now know the square root must be 8B + 5 for some integer B. Its square is then 64B2 + 80B + 25 = 16B + 25 mod 32. However, we know that the square is 25 mod 32, and hence B must be even. Therefore, the square root must end in 0101.
Continuing in this way, we can use the last k+1 digits of N to calculate the last k digits of its square root. If you know just over half of the digits in N, this is enough to completely determine the square root. As above, we can now just check if it works, and then we're done.
This technique always works, subject to two condition: (a) N must be odd, and (b) you must already know the last two digits of the square root. You might enjoy writing down the formula in both failure cases, and seeing what goes wrong.

Let's think about (b) first. If N is odd, then the square root must also be odd, and so the last digit must be 1. There is no easy way to determine what the second last digit has to be in advance, but who cares? Just try both of them, and see which one works!

Next let's suppose N is even. Since it is a perfect square, it must actually be a multiple of 4, and N/4 is also a perfect square. So we can just cut the last two digits off of N, repeat until N becomes odd, and then solve as above. In fact, this trick is pretty much required. If N is odd, then the last k digits are enough to find k-1 digits in the square root. If N is even though, then the last k digits may only be enough to find k/2 digits in the square root.

Of course, we might not know whether N is even or odd in advance. If the last digit is a '?' character, then we just try both possibilities and see what happens.

Remark: This whole approach works only because 2 is prime. If we were working base-10, then an even number that is not a multiple of 10 would be pretty nasty to deal with!

Putting it all together
Here is the full solution:

Assume that N is odd, if possible.
If N has more question marks in its bottom half than in its top half, then iterate over all possible ways of filling in the top half question marks, deduce the whole number, and see if it works.
If N has more question marks in its top half than in its bottom half, then iterate over all possible ways of filling in the bottom half question marks, deduce the whole number, and see if it works.
Now assume that N is even, if possible. Fill in the last two digits as zero, and repeat from the very beginning (potentially solving either top-down or bottom-up) for N/4.
The parenthetical comment in the last part is actually quite important! Consider the following input for example: "10010000010000011100000110110010001???????????????????????????????????000000000??000000000000000?0000000000000000000000??00". Most of the '?' characters are in the first half, so it is tempting to just start from the back and never re-evaluate your decision. However, there are only '0' characters back there, and they will not give you much information. To run fast enough in this case, you need to start from the front after eliminating some of the 0's.

World Finals 2011 - Code Jam 2011

Rains Over Atlantis (7pts, 23pts)

One obvious solution (which works for small input) is to simulate. The trouble starts when the heights are large, and M is small. Thus, we need to be able to do multiple steps at once.

Begin by noting that, given the current map of heights, we can determine the water levels (i.e., what areas are submerged and what areas are not) using a single run of Dijkstra's shortest path algorithm, in RC log(RC) time.

We will have a single step procedure. What it does is:

Calculate water levels for all fields.
For each field find out if it is submerged; if not, find the lowest water level in the adjacent fields.
Create a new map in which all the heights are lessened by the appropriate erosion, and increase the day counter by one.
This is one step of the simulation, which is also needed for the naive solution.

We will also have a multistep procedure, which tries to perform multiple single steps at once, as long as all of the following are true:

all non-submerged fields erode at maximum speed,
the water levels of all submerged fields decrease at the same speed, and
no submerged field becomes un-submerged during these steps.
The multistep procedure will work as follows:

Calculate the water levels for all fields.
For each field find out whether it is submerged. If not, find out how much it will erode. If this is not M, break the multistep. Note that we can choose to erode fields to below sea level for simplicity without changing the result.
If all fields erode at speed M, then the water levels of all "lakes" also decrease at this speed. Note that each lake has at least one field on the border which is not submerged, but determines the water level of the lake, and this level will decrease by M. Thus, we can continue repeating steps until some field that was submerged surfaces.
Thus, we find the submerged field with the smallest amount of water on top, and batch the appropriate number of steps together.
If there are no submerged fields, then we increment the day counter by ceil(maxheight / M) and finish the algorithm.
Note that in each multistep one field that was submerged becomes uncovered, and it's easy to see that a field that is not submerged will never become submerged in the future - thus there are at most RC multisteps in the algorithm (actually, fewer because the fields on the edge of the board are never submerged).

Now we want to know how many steps are possible before we can perform a multistep. Define an auxilliary graph that has a node for each un-submerged field. For each lake we join all the fields in this lake to an arbitrary field on the boundary of the lake which defines the water level for the lake (the outlet of the lake). We define natural edges in the graph -- two nodes have an edge if any of the two fields merged into the two nodes were adjacent. For each nodes we can define its "parent" to be one of the nodes with the lowest level adjacent to it. Thus, we have a tree, rooted in the external sea. We define any path going upward in the tree to be "fixed" if all of the edges along the path have a height difference of at least M.

We can now prove that each day either

a submerged becomes uncovered (thus decreasing the number of multisteps available), or
the number of vertices that have fixed paths to the root increases, or
all vertices lie on fixed paths.
If all vertices lie on fixed paths, then we can perform a multistep. As there can be at most RC increases before all vertices are on fixed paths, this means we get at most RC steps and at most one multistep before some field is uncovered, meaning at most (RC)2 steps and RC multisteps in total, giving a running time of (RC)3 log(RC). The constants are very small here, so this will easily run in time.

Now consider any fixed path. We assume the node distribution remains the same (i.e. the lakes remain lakes, and nothing gets uncovered). First notice that it remains a fixed path after one day - all of the fields on the path decrease by the same amount, so the differences remain the same. Now take any vertex that is not on a fixed path, but its parent is. Note that the sea level is on a fixed path by definition, so if no such vertex exists, it means that all vertices are on a fixed path. Then after one day the height of this vertex decreases to the height of the parent, while the height of the parent decreases by M. So the new difference is M, meaning that the vertex has joined the fixed path. This ends the proof.

Note that the upper bound can actually be (more or less) hit in the following map with M = 2: take a winding path of length that is on the order of RC, that does not touch itself, like so:

X X X X X X X
P P P P P P X
X X X X X P X
X P P P P P X
X P X X X X X
X P P P P P X
X X X X X X X
Where X represent very large values, and P represents the path. Thus, for the foreseeable future, the tree described above is actually the single path marked by Ps.

Now take the following values on the path: A, A-2L-1, A+1, A - 4L - 1, A+1, A - 6L - 1, A+1, A - 8L - 1, etc., where L is the length of the path. It is not difficult to check that "off by one" changes get propagated up the path with speed one, and each change gets to be fully propagated before the next one kicks in (due to the uncovering of another lake).

World Finals 2011 - Code Jam 2011

Runs (14pts, 16pts)

On a contest made up of unconventional problems, this one stands out as being a little more "normal" than the rest. That doesn't make it easy though! The main techniques here are good old-fashioned dynamic programming and counting, but you need to be very good at them to stand a chance.

The most important observation is that you cannot afford to build strings from left to right. With 450,000 characters, there is just far too much state to keep track of. The correct approach turns out to be placing all of one character type, then all of the next, and so on. Towards that end, let's define Nc to be the number of characters of type c, and define Xc,r to be the number of ways of arranging all characters of type 1, 2, ..., c so that there are exactly r runs. We will compute the X values iteratively using dynamic programming.

Consider a string S using only the first c - 1 character types, and having r0 runs. Note that S has exactly M = N1 + N2 + ... + Nc-1 characters altogether. Additionally, it has a few different types of "character boundaries":

There is the boundary before the first character and the boundary after the last character. If we add a run of type-c characters in either one of these locations, the total number of runs will increase by 1.
There are r0 - 1 boundaries between distinct characters. If we add a run of type-c characters in any one of these locations, the total number of runs will increase by 1.
There are M - r0 boundaries between identical characters. If we add a run of type-c characters in any one of these locations, the total number of runs will increase by 2.
So let's suppose we add x runs of type-c characters in any one of the r0 + 1 boundaries from the first two groups, and we add y runs of type-c characters in any of the M - r0 boundaries from the the third group. There are exactly (r0 + 1 choose x) * (M - r0 choose y) ways of choosing these locations, and we will end up with r0 + x + 2y runs this way. Finally, we need to divide up the Nc characters of type c into these x + y runs. This can be done in exactly (Nc - 1 choose x + y - 1) ways. To see why, imagine placing all the runs together. Then we need to choose x + y - 1 run boundaries from Nc - 1 possible locations. See here for more information.
Therefore, the number of ways of adding all Nc type-c characters to S so as to get a string with exactly r runs can be calculated as follows:

Loop over all non-negative integers x, y such that r0 + x + 2y = r.
Add (r0 + 1 choose x) * (M - r0 choose y) * (Nc - 1 choose x + y - 1) to a running total.
After looping over all x, y, this running total will contain the answer we want.
Note that the answer here depends only on r0. Therefore, the total contribution from all strings with r0 runs is exactly Xc-1,r0 times this quantity. Iterating over all r0 gives us the recurrence we need for X!
This method is actually quite fast. We can use O(450,000 * 100) time to pre-compute all the choose values. Everything else runs in O(26 * 1003) time.

By the way, you probably need to calculate the choose values iteratively rather than recursively. 450,000 recursive calls will cause most programs to run out of stack space and crash! Here is a pseudo-code with a sample implementation (modulo operations removed for clarity):

def CountTransitions(M, Nc, r0, r):
  # Special case: If adding the first batch of characters the
  # only possible result is to have one run, and there is only
  # one way to achieve that.
  if r0 == 0:
    return r == 1 ? 1 : 0
  result = 0
  dr = r - r0
  for (y = 0; r0 + 2 * y <= r; ++y):
    x = r - (r0 + 2 * y)
    nways_select_x = Choose(r0 + 1, x)
    nways_select_y = Choose(M - r0, y)
    nways_split = Choose(Nc - 1, x + y - 1)
    result += nways_select_x * nways_select_y * nways_split
  return result

def Solve(freq, runs_goal):
  runs_count = [0 for i in range(0, runs_goal + 1)]
  runs_count[0] = 1
  M = 0
  for i, Nc in enumerate(freq):
    if Nc > 0:
      old_runs_count = list(runs_count)
      runs_count = [0 for i in range(0, runs_goal + 1)]
      for (r0 = 0; r0 <= runs_goal; ++r0):
        for (int r = r0 + 1; r <= runs_goal; ++r):
          nways = CountTransitions(M, Nc, r0, r)
          runs_count[r] += nways * old_runs_count[r0]
      M += Nc
  return runs_count[runs_goal]

World Finals 2011 - Code Jam 2011

Program within a Program (15pts, 23pts)

Here at the Google Code Jam, we have always tried to encourage a variety of programming languages. But in this problem, we took things a little further by forcing you to program in the language that started them all -- the abstract Turing Machine! Unfortunately, abstract Turing Machines are pretty unwieldy, and that makes even the simple task here quite difficult.

With so few rules allowed, it is important to take advantage of the numbers you can mark onto the lampposts. The obvious approach is to write down, perhaps in binary, the number of steps you need to make, and then have the robot makes decisions based on that. Unfortunately, there is also a rather tight limit on the number of moves, which means you cannot afford to keep going back to the start to check on your number. You will need to take the data with you as you move.

Here is one effective way of doing this:

First write down the distance you want to go forward in binary, using the values 1 and 2 to represent the binary digits. This information will now be available on the starting lampposts.
Now repeat the following:
Trace through the number from right to left, and subtract 1 as you go.
If the number was 0, then drop the cake right now.
Otherwise trace through the number from left to right, copying everything one position to the right.
Once you have the high level algorithm, each piece is relatively straightforward to implement. For example, subtracting 1 comes down to formalizing the subtraction algorithm you learned in grade school:
Start in state 1, which we'll you use to mean you have not yet done the subtraction. If the last digit is 1, you can replace it with 0 and the subtraction is done, so switch to state 2. If the last digit is 0, replace it with 1 but you now need to borrow 1 from the previous digit, so stay in state 1. Either way, move to the previous digit.
Once you are in state 2, you are just moving left through the number without changing anything.
If you reach the left end of the number in state 1, then the number must have been 0, so you should drop the cake. Otherwise, you should move onto the copy phase.
And the copy phase is similar but conceptually simpler.

Almost any implementation of this algorithm should be good enough to solve the problem, but there is room for more optimization if you want a challenge! With the 30-rule limit, we were able to bring the number of steps down to about 95,000. Here is the full program to move 5,000 lampposts:

0 0 -> E 1 1
1 0 -> E 2 3
2 0 -> E 3 1
3 0 -> E 4 3
4 0 -> E 5 2
5 0 -> E 6 3
6 0 -> E 7 1
7 0 -> E 8 3
8 0 -> W 9 3
9 0 -> R
10 0 -> E 11 0
9 1 -> W 9 3
10 1 -> W 10 1
9 2 -> W 10 1
10 2 -> W 10 2
9 3 -> W 10 2
10 3 -> W 10 3
11 1 -> E 12 0
12 0 -> W 9 3
12 1 -> E 12 1
12 2 -> E 13 1
12 3 -> E 14 1
13 0 -> W 10 1
13 1 -> E 12 2
13 2 -> E 13 2
13 3 -> E 14 2
14 0 -> W 10 2
14 1 -> E 12 3
14 2 -> E 13 3
14 3 -> E 14 3
Can you do better? We'd love to hear about it if so!

World Finals 2011 - Code Jam 2011

Ace in the Hole (20pts, 22pts)

This is a rather unconventional problem that requires a lot of thought up-front and relatively little implementation.

Let's think about the problem as a competitive game. Ben chooses a card position, and then Amy chooses the value of that card. Amy is not allowed (a) to re-use values, (b) to create a decreasing subsequence of length 3, or (c) to place the 1 before the final turn. Our task it to understand what decisions Amy could have made during the game. We will present the answer first, and then prove it is correct. Define an "adversarial strategy" for Amy as follows:

At any point, consider the cards that Ben has not yet looked at. Suppose they are in positions p1 < p2 < ... < pm, and have values v1 < v2 < ... < vm.
If Ben looks at the card in position pk with k < m, then Amy assigns it value vk+1.
If Ben looks at the card in position pm and either (a) m ≤ 2, or (b) there is a previously revealed card in position less than pm with value between vm-1 and vm, then Amy assigns it value vm. Otherwise, she assigns it value either vm-1 or vm.
We claim that the problem conditions are equivalent to saying that Amy chooses the deck values according to an adversarial strategy. Once that is established, it will be pretty straightforward to find the lexicographically largest solution.

Adversarial Strategies Cannot Be Exploited
The main technical challenge is to prove that adversarial strategies really do require Ben to look at every card. On a contest of course, you would not need to make this argument quite so rigorously.

Lemma: If the deck values are assigned according to an adversarial strategy, then the deck contains no decreasing subsequence of length 3.

Proof: We follow along as Ben looks at the cards one at a time. At each step, we will say a card is "safe" if Ben has looked at it, and either (a) all cards with a higher position have also been looked at, or (b) all cards with a higher value have also been looked at. Let's suppose the remaining cards are in positions p1 < p2 < ... < pm, and have values v1 < v2 < ... < vm. These are the "unsafe" cards.

We claim the following is true:

There is no way of assigning values to the remaining cards that will make a decreasing subsequence of length 3 with a safe card.
If an unsafe card in position pi has been looked at, then it has value vi+1.
We prove this claim by induction on the number of cards that Ben has looked at. Initially, all cards are unsafe and the claim obvious.
Now let's suppose Ben looks at a card C in some position pk.

Case 1: k < m - 1. Since Ben has not looked at the card in position pm (or else it would be safe), the value of card C will be set according to the first adversarial strategy rule. Specifically, if there are q cards in positions less than pk that have not been looked at, then C will be assigned the (q+1)th smallest unrevealed value. As all safe cards have been revealed, we can restrict our attention to unsafe cards, from which the inductive hypothesis makes it clear that the (q+1)th smallest unrevealed value is vk+1. Since k + 1 < m, the set of safe cards will not change because of this step, and the inductive hypothesis will still be satisfied afterwards.

Case 2: k = m - 1. By the same reasoning as in Case 1, C will be assigned value vm. However, the set of safe cards will change in this case. Suppose Ben has already looked at the cards in positions pu, pu+1, ..., pm-2, but he has not looked at the card in position pu-1. These cards will all be labeled as safe set because they have high values, while all other cards will remain unsafe. We need to show that they cannot be part of a decreasing subsequence of length 3. By the inductive hypothesis, we can ignore cards that had been previously marked as safe for this purpose. Now, the newly labeled safe cards are arranged in increasing order, there is only one unsafe card positioned after them, and no preceding unsafe card can have higher value, so indeed, they cannot be part of a decreasing subsequence of length 3. Therefore, the inductive hypothesis is once again satisfied.

Case 3: k = m. Suppose C is assigned value vt. We know Ben has not looked at the card with value vm, or else that card would already be safe. If Ben has also looked at the card with value vm-1, then the adversarial condition forbids C from having value less than vm-1. Otherwise, vm-1 and vm are both still unrevealed, and the adversarial condition demands that C have value vm-1 or vm.
If t = m-1, then C will be marked as safe but no other cards will. (Ben cannot have looked at the card in position pm-1 because it would have value vm, and would therefore be safe already.) Furthermore, it cannot be part of a length-3 decreasing subsequence because no unsafe card has a larger position and at most one unsafe card can have higher value.
If t = m, then some additional cards pu, pu+1, ..., pm-2 might also get marked safe. The newly labeled safe cards are arranged in increasing order, there is only one unsafe card positioned after any of them, and no preceding unsafe card can have higher value, so no decreasing subsequence of length 3 can include these cards. Therefore, the inductive hypothesis is once again satisfied.
In any case, the inductive hypothesis holds at each step, and therefore the lemma is proven.

It now follows immediately that Ben requires all N guesses if the cards are assigned according to an adversarial strategy. No matter what cards Ben looks at, Amy can continue her adversarial strategy and avoid revealing either a decreasing subsequence of length 3 or the card with value 1. In particular, Ben can never finish early.

Non-Adversarial Orders Can Be Exploited
The previous argument is important from a technical perspective, but in practice, you would begin solving this problem from the other side: first showing how Ben can exploit poor strategies. Towards that end, we show that if Amy ever deviates from an adversarial strategy, then Ben can find the 1-card without requiring all N turns.

Let's begin by focusing on the first card Ben looks at.

Observation 1: Suppose Ben looks at card i < N. Then Amy must assign it value i + 1.

Reason: Suppose he sees value j ≠ i + 1. We claim that the value-1 card cannot be in position N. Indeed, if it was in that position, then the condition that the deck has no decreasing subsequence of length 3 implies that (a) all cards with position less than i have value in the range [2, j-1], and (b) all cards with position between i + 1 and N - 1 have value in the range [j+1, N]. If j < i + 1, then there are not enough cards in the range [2, j-1] for this to be possible, and if j > i + 1, then there are not enough cards in the range [j+1, N] to be possible.

Therefore, if Amy assigns this card a value other than i + 1, then Ben need never look at card N. But we know he did indeed have to look at all the cards, so it must be that Amy assigned value i + 1.

Observation 2: Suppose Ben looks at card N. Then Amy must assign it value N - 1 or N.

Reason: Suppose he sees value j < N - 1. If N = 3, then Ben just found the value-1 card, which is an immediate contradiction. Otherwise, N ≥ 4, and we show that Ben can find the 1-value card in less than N - 1 further card checks, which is another contradiction.

Forgetting the information Ben has already obtained, we know the remaining N - 1 card values are placed in the N - 1 preceding positions. By Observation 1, if Ben checks the card in position i, then it must have the (i+1)th smallest value of the remaining cards. (Otherwise, we already know he can find the 1-value card without checking everything.) So if Ben checks the cards in all positions except N - 3 and N - 1, he must see the following values:

2, 3, ..., j-1, j+1, j+2, ..., N-2, ???, N, ???, j.

The two unknown cards have value 1 and N - 1 in some order. However, we know the second-last card cannot have value N - 1, or we would have a decreasing subsequence: N, N - 1, j. Therefore, the 1 must be in that position, and so Ben never needs to check the fourth-last card, and the proof is complete.

There is only one tricky case left. At a given point, let the remaining unrevealed cards be in positions p1 < p2 < ... < pm, and have values v1 < v2 < ... < vm. Suppose that there is a previously revealed card in position less than pm with value between vm-1 and vm. We need to show that if Ben looks at the card in position pm, then Amy must assign it value vm.

Reason: Suppose Amy does not do this. Consider the first time where she does not. Let C be the card in position pm, and let C' be the revealed card that is positioned before C and that has value between vm-1 and vm.

Up until this point, Amy has followed an adversarial strategy, so we can use the inductive statement proven in our first lemma to divide the deck into safe and unsafe cards. We know there is an unrevealed card positioned after C' (namely C) and there is an unrevealed card value that is higher than the value of C' (namely vm), so C' must be an unsafe card.

Let the unsafe cards have positions q1, q2, ..., qr and values w1, w2, ..., wr. Since vm-1 is an unrevealed value, it belongs to an unsafe card and we have vm-1 = wu for some u. Also C' has value wt for some t > u and position pt-1.

Now suppose Amy assigns C a value of vm-1 = wu. Then the card in position qu-1 must be unrevealed, or it would have had the same value. Also, as argued earlier, the card in position qr-1 must be unrevealed or it would have value wr and therefore be safe already. Ben can now exploit Amy's strategy by looking at every card other than qu-1 and qr-1. It is easy to check this will leave only the values 1 and wu+1. But the card in position qr-1 cannot have value wu+1 or we would get a decreasing subsequence: wt, wu+1, wu. Therefore, this card must have value 1, and Ben can avoid checking the card with position qu-1. This proves the final part!

Finding The Lexicographically Largest Solution
We are now finally ready to discuss the solution. At each step, Amy's strategy is almost completely determined. The only question is whether she should assign vm or vm-1 to the card in position pm when she has the choice. In fact, the two choices lead to identical deck orderings except with vm and vm-1 swapped. In order to make the ordering as large lexicographically as possible, we want vm early on, and so we should always prefer using vm-1.

That's it! We just need to implement the adversarial strategy, always preferring vm-1 over vm. But of course it took a lot of reasoning to get to this stage!

World Finals 2011 - Code Jam 2011

Google Royale (20pts, 40pts)

Google Royale is perhaps the most unapproachable problem we have ever posed for the Google Code Jam. It requires a great deal of understanding before any progress at all can be made on the large input. This is because the interaction between winning probability and starting money is rather complicated, and you cannot possibly afford to do a complete search through all 1016 states. Although the scenario here might seem similar to Millionaire, a previous Code Jam problem, you simply cannot afford the kind of computation that worked there.

First, let's try to understand a betting round. Let the initial bet be y. If you lose k times and then win, your total earnings will be y * 2k-1 - y * 2k-2 - y * 2k-3 - ... - y, which equals y no matter what k is. If you lose, your total earnings will be negative and the exact value will depend on how many times you doubled.

The key to the whole problem is that your expected winnings from a betting round is exactly 0, no matter what your initial bet is and no matter how many times you are willing to double if you keep losing. (Recall that the expected winnings is your "average" winnings -- formally it is defined as sum pi * i, where pi is the probability that you win exactly i dollars.) The expected value is 0 because at each step of each round, you have a 50% chance of winning some money, and a 50% chance of losing the same amount of money. The average winnings is always 0 in each step, and therefore 0 overall.

Now let's think about strategy a little bit. In general, there will be several different strategies that maximize your probability of winning. For now, we will work on identifying only one of them. We will come back to the question of finding the maximum bet later.

Observation 1: If you have x dollars, there is no reason to start a betting round with more than V - x.

Reason: If you bet more than V - x, then winning will always put you over V. If you decrease the bet slightly, then winning is just as good, but losing is no worse. Therefore, you might as well bet less.

From now on, we will always restrict our attention to strategies that follow Observation 1, and therefore, we will never end up with more than V dollars, no matter how lucky or unlucky we are.

Observation 2: Fix a strategy. Let P be the probability of reaching V dollars, and let L be the expected number of dollars you end up with if you lose. Then P = 1 - (V - A) / (A - L). Since V and A are fixed, this means maximizing P is equivalent to minimizing L.

Reason: This follows from the fact that your expected winnings after any number of betting rounds is always equal to 0, or equivalently, your total expected money is always equal to A. Let's see what this tells us at the end of all betting rounds. At that point, you will have won with probability P, in which case your money is exactly V by Observation 1. Or you will have lost with probability 1 - P, in which case your expected money is exactly L. Therefore, we get:
P * V + (1 - P) * L = A
=> P * (V - L) = A - L
=> P = 1 - (V - A) / (A - L).

So now the question reduces to making L as small as possible. Using these ideas, we can make a couple major deductions about the optimal strategy.

Observation 3: If you have x dollars, you might as well do one of two things: (1) bet exactly x and double until you win or until it is no longer possible to double, or (2) bet exactly 1 and do not double even if you lose.

Reason: An optimal strategy will bet some number y and double up to k times. If one of the betting steps results in a win, you will end up with x + y dollars. Otherwise, you will end up with some number x - z dollars. As always, your expected number of dollars is constant, and so is equal to x. We will show how to replace the strategy of betting y and doubling up to k times with a new strategy (possibly requiring multiple betting rounds) that follows the rules we want, and that is at least as effective at reaching x + y dollars.

Case 1: x - z ≥ 0. Instead of betting y, repeatedly bet 1 and do not double. Stop only when your total amount of money increases to x + y or decreases to x - z. Since x - z ≥ 0, it is legal to continue betting until one of these outcomes happens. Like the original strategy, this strategy will result in the same two outcomes (x + y dollars or x - z dollars), and the expected money at the end will still be x for the exact same reason. However, as Observation 2 shows, if two strategies have identical winning and losing outcomes and identical expected values at the end, they have identical probability of getting the better outcome. Therefore, this strategy is identical to the original strategy, and we can just do this instead.

Case 2: x - z < 0. Instead of betting y, repeatedly bet 1 and do not double. Stop only when your total amount of money increases to x + y or decreases to y. If you end up at y, then bet y, and keep doubling until you win or go broke. If you win, go back to the first step and continue betting 1 until you reach x + y or return back down to y. If you end up at y again, bet y, and repeat. Like the original strategy, this will end with you having exactly x + y dollars or with you being broke. However, it is easy to check that L is strictly smaller here than in the original strategy while the expected money at the end, as always, stays equal to x. Therefore Observation 2 guarantees that this new strategy is strictly better than the original strategy.

We have shown that any strategy can switch to betting 1 or x without becoming any worse, and that proves Observation 3.

This is a very powerful observation, but there is still more to understand! With 1016 possible money amounts, we cannot afford to try both strategies in every situation. There is one more idea that drastically cuts down the search space.

Observation 4: For each non-negative integer i, let Mi be the largest integer such that Mi * 2i ≤ M. We will call these numbers "inflection points". Then you should never bet all your money unless the amount you have is an inflection point.

Reason: The proof here is very similar to Observation 3. Consider a strategy that bets everything for x satisfying Mi < x < Mi-1. Let y equal x/2, rounded up. Note that 2y ≤ x + 1 ≤ Mi-1, so if we bet y, then we can double i times.

The current strategy either gives you 2x dollars or x * (1 - 1 - 2 - ... - 2i-1) = -2x * (2i-1 - 1). However, y / x ≥ 1/2 > (2i-1 - 1) / (2i - 1). Therefore, the losing amount of money for this strategy is more than -2y * (2i - 1).

Now we consider an alternate strategy. Instead of going all in at x, repeatedly bet 1 and do not double. Stop only when your total amount of money increases to 2x or decreases to y. In the latter case, go all in and double up to i times, or until you win. As noted above, your bet will never exceed M, so this is a legal strategy. If you win, start over from the beginning. Like the original strategy, this will either leave you with 2x dollars or broke. However, if you are broke, you end up with y * (1 - 1 - 2 - ... - 2i) = -2y * (2i - 1). As argued above, this losing value is less than it was for the original strategy, and so Observation 2 implies the new strategy has a higher probability of reaching 2x. Therefore, the original strategy could not have been optimal.

When to go All-In: Let's say a dollar amount is an "all-in" point if we should bet everything when we have that amount of money. Observation 4 guarantees that all-in points are a subset of the inflection points, but it is not true that every inflection point is an all-in point.

Consider the inflection points in increasing order. The smallest inflection point will be 1, and certainly that is an all-in point. Let's now consider the second smallest inflection point. Observation 2 says that our goal is to minimize L, so we calculate what we end up with if we go all in from this inflection point and lose. If it is our lowest total yet, we should go all in there. Otherwise, we can do better by betting 1 until we get to the lower inflection point. The same argument applies for the third inflection point, and then the fourth and so on. In this way, we can very quickly calculate an optimal strategy at every dollar amount.

There are two tricky cases to watch out for here:

By Observation 1, we should only consider inflection points that are at most V / 2.
It might be that going all-in at a point x is equally effective as betting 1 and waiting until the next all-in point. In this case, we will say x is an "optional all-in point". The other all-in points are called "strict all-in points".

Calculating the Winning Probability: Let Px denote the probability of reaching V if your current amount of money is x. If x is not a strict all-in point, then one optimal strategy is to bet 1 until we reach the strict all-in point directly above x or directly below x, or until we reach V itself. For any x between these all-in points, we have the recurrence: Px = (Px-1 + Px+1)/2, or in other words, Px - Px-1 = Px+1 - Px. This implies Px is linear in this range, and therefore we can calculate Px if we know the value of P at both endpoints.

We can now calculate P by starting from the largest all-in point and working down. For example, suppose the largest all-in point is y and the probability of losing immediately from going all in there is 1 - p. Then Py = p * P2y. Also, by linearity, P2y = Py * (V - 2y) / (V - y) + 1 * y / (V - 2y). We know p, V, and y, so it is easy to calculate Py from here. Once we have Py, we can use the same trick to calculate P for the second-largest all-in point, then for the third-largest all-in point, and so on, until eventually we have calculated all probabilities.

Calculating the Largest Optimal Bet: It remains only to calculate the largest optimal bet. If A is an all-in point, either strict or optional, then certainly we can and should bet A there.

Otherwise, consider a dollar amount x. If x is not a strict all-in point, then we already saw that Px - Px-1 = Px+1 - Px. This means Px is completely linear between strict all-in points, and so we can certainly increase the bet until either winning or losing would take us to a strict all-in point on either side. If x is equal to a strict all-in point however, then 1 is not an optimal bet, and we have Px > (Px-1 + Px+1) / 2, or equivalently, Px - Px-1 > Px+1 - Px. In particular, this means that increasing the bet further would hurt us. Therefore, the largest possible bet is the distance between A and the nearest strict all-in point.

Qualification Round 2012 - Code Jam 2012

Speaking in Tongues (15pts)

In most Google Code Jam problems, each test case is completely separate and nothing you learn from one will be useful in another. This problem was different however:

"Googlerese is based on the best possible replacement mapping, and we will never change it. It will always be the same. In every test case."

We meant it when we said this! There really is just one mapping, and the main challenge here is figuring out what it is. Fortunately, there is a lot we can learn from the sample input and output. For example, by looking at the first word in the first line, we know that "our" becomes "ejp" in Googlerese, so 'o' -> 'e', 'u' -> 'j', and 'r' -> 'p'. If you go through the entire text, you will that there is almost enough information to reconstruct the entire mapping:

'a' -> 'y'
'b' -> 'n'
'c' -> 'f'
'd' -> 'i'
'e' -> 'c'
'f' -> 'w'
'g' -> 'l'
'h' -> 'b'
'i' -> 'k'
'j' -> 'u'
'k' -> 'o'
'l' -> 'm'
'm' -> 'x'
'n' -> 's'
'o' -> 'e'
'p' -> 'v'
'q' -> ???
'r' -> 'p'
's' -> 'd'
't' -> 'r'
'u' -> 'j'
'v' -> 'g'
'w' -> 't'
'x' -> 'h'
'y' -> 'a'
'z' -> ???
We just need to figure out how to translate 'q' and 'z'. But if you read the problem statement carefully, you will notice there was one more example we gave you! "a zoo" gets translated to "y qee". This means that 'z' gets mapped to 'q'.

Next we have to figure out what 'q' gets mapped to. For this part, you need to remember that every letter gets mapped to something different. And if you look carefully, there is already a letter getting mapped to everything except for 'z'. This leaves only one possibity: 'q' must get mapped to 'z'.

And now we have the full translation mapping, and all we need to do is write a program to apply it to a bunch of text. Here is a solution in Python:

translate_to_english = {
    ' ': ' ', 'a': 'y', 'b': 'h', 'c': 'e', 'd': 's',
    'e': 'o', 'f': 'c', 'g': 'v', 'h': 'x', 'i': 'd',
    'j': 'u', 'k': 'i', 'l': 'g', 'm': 'l', 'n': 'b',
    'o': 'k', 'p': 'r', 'q': 'z', 'r': 't', 's': 'n',
    't': 'w', 'u': 'j', 'v': 'p', 'w': 'f', 'x': 'm',
    'y': 'a', 'z': 'q'}

for tc in xrange(1, int(raw_input()) + 1):
  english = ''.join(
      [translate_to_english[ch] for ch in raw_input()])
  print 'Case #%d: %s' % (tc, english)

Qualification Round 2012 - Code Jam 2012

Dancing With the Googlers (10pts, 10pts)

Clever contestants will have noticed that this problem might be based on an actual television show. The author of the problem found himself watching an episode of Dancing With the Stars (the show and its distributor, BBC Worldwide, do not endorse and are not involved with Google Code Jam), and having forgotten whether any of the contestants had earned a score of at least 8 from any of the judges.

The judges on that show very often agree within 1 point of each other, and it really is surprising when they disagree by so much. There are a few key observations that help us solve this problem:

Any non-surprising score is expressed uniquely by an unordered triplet of judges' scores. 21 must be 7 7 7; 22 must be 7 7 8; 23 must be 7 8 8.
There only a few kinds of surprising scores. 21 could be 6 7 8; 22 could be 6 8 8; 23 could be 9 7 7. There's a repeating pattern of surprising scores that extends from 2 (0, 0, 2) to 28 (8, 10, 10).
The repeating pattern stops at 2 and 28 because 1 and 29 can't be surprising: we can't use (-1, 1, 1) for 1: -1 is not allowed as a score. We also can't use (9, 9, 11) for 29.
Putting all of these facts together, we can easily construct a mapping from each total score to its best result if it's surprising, and to its best result if it isn't surprising:

unsurprising(0) = 0
unsurprising(1) = 1
unsurprising(2) = 1
unsurprising(n) = unsuprising(n-3) + 1
unsurprising(n) = ceiling(n/3), for 0 <= n <= 30

surprising(2) = 2
surprising(3) = 2
surprising(4) = 2
surprising(5) = 3
surprising(n) = surprising(n-3) + 1
surprising(n) = ceiling((n-1)/3) + 1, for 2 <= n <= 28
Many contestants wrote an initial solution that didn't take into account that 0, 1, 29 and 30 can't be surprising, and so failed one of our sample inputs. That's what they're there for!

One way to build the tables of unsurprising(n) and surprising(n) that doesn't require so much thought could involve writing three loops to go through all possible sets of judges' scores, checking whether each combination of three was valid or surprising, and building the maps that way.

Now, for each value ti we have one of three cases:

unsurprising(ti) >= p. This Googler can be "good" (i.e. have a maximum score of at least p) even with an unsurprising triplet.
2 <= ti <= 28 and surprising(ti) >= p > unsurprising(ti). This Googler can be "good" only by using a surprising triplet.
Otherwise, the Googler can't be "good".
A simple greedy algorithm works: take all of the Googlers of the first type, and as many as possible (at most S) of the second type.

Qualification Round 2012 - Code Jam 2012

Recycled Numbers (10pts, 15pts)

Many contestants got stuck in this problem because of the sample test case number 4. Let's say n is 1212, then after moving 1 or 3 digits you will get 2121, hence the pair (1212, 2121) will be counted twice if you count all possible moves. You can avoid this by breaking out of the loop once you reach the original number again, which will happen after moving 2 digits in the above example.

For the small input you can simply check for each pair (n, m), A ≤ n < m ≤ B, whether it satisfies the conditions mentioned in the problem statement, and whether you can obtain m by moving some digits from the back of n to the front without changing their order. To check if you can obtain m from n simply try to move all possible groups of digits from n and check if what you get is m. The digit shifting can be done using string manipulation or mathematical expressions, both will run within the time limit.

But the previous solution will not run within the time limit for the large test cases. So here is another solution which should run within the time limit. For each number n, A ≤ n ≤ B, try to move all possible groups of digits from its back to its front and check if the resulting number satisfies the conditions or not. If it does satisfy the conditions then increment the result. Don't forget to avoid counting the same number twice.

Here is a sample solution:

int solve(int A, int B) {
    int power = 1, temp = A;
    while (temp >= 10) {
        power *= 10;
        temp /= 10;
    }
    int result = 0;
    for (int n = A; n <= B; ++n) {
        temp = n;
        while (true) {
            temp = (temp / 10) + ((temp % 10) * power);
            if (temp == n)
                break;
            if (temp > n && temp <= B)
                result++;
        }
    }
    return result;
}

Qualification Round 2012 - Code Jam 2012

Hall of Mirrors (15pts, 25pts)

To keep things exciting, we ended the qualification round with Hall of Mirrors: a truly challenging problem. It is difficult to see how to get started, and then it is even more difficult to actually get the implementation right!

How To Understand Mirrors

The first step in solving this problem is reducing the set of all light ray angles to something that you can actually check. And to do that, it is helpful to use a famous trick for thinking about mirrors. Look at the two diagrams below. Do you see why they are really illustrating the exact same thing?

In the left picture, there is a light ray in a 2x1 room that reflects off three mirrors, and then ends back where it started. Let's focus on where the light ray reflects off the bottom mirror. Instead of drawing the reflected light ray, let's take the entire room, and draw it reflected about this mirror. We can then draw the rest of the light ray's path in this picture. This is illustrated in the left diagram below:

Next, once the light ray hits the right mirror, we can again draw the entire room reflected about that mirror, and then continue drawing the rest of the path there. Finally, we can do the same thing for the one remaining mirror to get the original picture from above. This kind of drawing has two really nice properties for us:

The light ray is perfectly straight, and it never bends or turns.
No matter how many reflections we do, and no matter where they are, the reflected position of your starting point will always be in the middle of some square.
Together, these imply the key fact you need to get started: for a light ray to even have a chance of reflecting back to you in distance at most D, it must be directed towards the center of a square that is distance at most D from you.

The Small Input

If you get this far, you can either try to solve just the small input or both the small and the large. The small input isn't that much smaller really, but it is simpler because you cannot have mirrors in the middle of rooms. This means that we can take the original room and repeatedly reflect it about each of the four walls to cover the whole plane. This is shown below:

Now let's look at your position in each of these reflected rooms. We claim that these give precisely the directions you can look in order to see a reflection of yourself. Try tracing through a few cases and you will see why.

So to solve the problem, you can iterate over all of these positions that are within distance D of you, and count how many unique directions they give. (Remember that the light ray stops once it returns to you, so if two reflections are in precisely the same direction, only one is visible.)

The Large Input

The large input is not any harder conceptually, but you will have to do some more implementation. The idea is to iterate over all squares within distance D of your starting location, and to follow light rays that are directed towards each of these squares. In particular, how many of these light rays return to the starting position after distance D? The challenge of following rays through a 2-dimensional grid is called "ray casting", and solving it efficiently was the key to Wolfenstein 3D and other games of that era.

This isn't too hard in theory, but the implementation can get nasty if you do not set things up carefully. One approach that helps is to first focus on rays that are moving more vertically than they are horizontally. Then, iterate over each horizontal line the ray touches. Between each of these steps, it will touch 0 or 1 vertical lines, making the processing fairly straightforward (except for corners!).

If you are stuck, try looking through the submitted solutions.

Round 1A 2012 - Code Jam 2012

Password Problem (10pts, 10pts)

The first challenge with Password Problem is wrapping your head around expected values. These things come up all the time and are extremely useful, so they are well worth learning. (For example, expected value is key to solving our last problem on the 2011 finals!)

Once you understand what the problem is asking, you mainly need to evaluate the expected number of keystrokes for each strategy:

Strategy 1: Finish typing the password, then press "enter".

The probability that this works on the first try is x = p1 * p2 * ... * pA. In this case, you need B - A + 1 keystrokes. In the other case, you need 2B - A + 2 keystrokes. Therefore, the expected number of keystrokes for this strategy is:

B - A + 1 + (B + 1) * (1 - x).

Strategy 2: Hit backspace k times, then proceed as in Strategy 1.

The probability that this works on the first try is xk = p1 * p2 * ... * pA-k. In this case, you need B - A + 2k + 1 keystrokes. In the other case, you need 2B - A + 2k + 2 keystrokes. Therefore, the expected number of keystrokes for this strategy is

B - A + 2k + 1 + (B + 1) * (1 - xk).

Strategy 3: Press enter immediately, and retype the whole password.

This always takes B + 2 keystrokes.
The problem is asking you to calculate the minimum of all these values. There is one more catch though: if you compute each xi from scratch, your program will probably be too slow. It might take 99999 multiplications to calculate x, 99998 multiplications to calculate x1, 99997 multiplications to calculate x2, and so on. Instead, you should calculate them all together:

xA = 1
xA-1 = xA * p1
xA-2 = xA-1 * p2
etc.
Making sure your solution is fast enough to complete in the time limit is an important part of the Google Code Jam!
Here is a short Python solution:

import sys

for tc in xrange(1, int(sys.stdin.readline())+1):
  A, B = [int(w) for w in sys.stdin.readline().split()]
  p = [float(w) for w in sys.stdin.readline().split()]
  best, x = B + 2.0, 1
  for i in xrange(A):
    x *= p[i]
    best = min(best, (B - i) + (A - i - 1) + (B + 1) * (1 - x))
  print 'Case #%d: %f' % (tc, best)
The Large Input and Underflow

There is one trick on this problem that caught quite a few contestants, and that involves a subtlety with how floating point numbers work. Let's suppose you calculate xi in a slightly different way:

x = p1 * p2 * ... * pA
x1 = x / pA
x2 = x1 / pA-1
etc.
At first glance, this looks completely equivalent to the solution above. Unfortunately, it is wrong to do this for two reasons. First, if one of the pi values is 0, you are in trouble. Second, it turns out that even a 64-bit floating point number reserves only 11 bits for the exponent. This means that it cannot store values much less than 2-1000. These very small values get rounded down to 0. Normally, you wouldn't care about values this small, but if you are multiplying 100,000 probabilities together, it becomes an issue. After rounding to 0, you will end up with every x value, including xA, being 0, and then you are in trouble!

Some people reported failing tests due to this bug, and were then able to fix it by switching to the "long double" data type in C++. However, they were lucky! Even long double has the same issues.

If you ever need to combine a lot of multiplications and divisions in the future, where intermediate values might get very small, it helps to work in log space:

A * B / C = exp( log(A) + log(B) - log(C) )
Do you see why this approach avoids the problem?

Round 1A 2012 - Code Jam 2012

Kingdom Rush (15pts, 18pts)

This problem was an interesting one to create. In the actual game Kingdom Rush, there are three stars per level, "challenge" levels, and you can't try level 2 until you've beaten level 1 with at least one star. Coming up with a problem that was solvable, while maintaining the same feeling as the game that inspired it, was a balancing act.

We solved this problem with a greedy algorithm. At every step of the algorithm, Ryan will make a decision about which level to play, and his decision will be based simply on the properties of the levels available, and what he's done so far.

First, let's observe that Ryan should only complete a level if he's never completed it before, or if he can go from a one-star rating to a two-star rating. There's simply no point in beating a level otherwise. When we're talking about levels below, we'll ignore levels that he shouldn't complete for this reason.

Second, if Ryan ever reaches a state where he can't complete any of the remaining levels, then he is "TOO BAD" to beat the game. This will happen independent of the order in which he completes the levels.

Third, if Ryan can complete a level with a two-star rating, he should do it immediately. There's no reason for him to wait: he can earn those two stars (or one star) with one level completion either now or later. If there are multiple levels with two-star ratings that Ryan could complete, he should choose one arbitrarily; he can do the other one next.

Now we've covered all situations except for one: when the only levels Ryan can complete are levels that he can complete with a one-star rating. Consider two levels like that, level 0 and level 1:
a0 b0
a1 b1
The values of a0 and a1 don't matter: by assumption, Ryan has at least that many stars already. Let's assume without loss of generality that b0 < b1. Which level should Ryan complete first?

Let's remember that Ryan's objective is to complete levels the minimum number of times. In the worst case, it will take Ryan 4 completions to finish those two levels: two to get him a one-star rating in both levels, and two more to get him a two-star rating in both levels. But earning stars from these levels (or other levels) might allow him to complete one of them with a two-star rating without having to complete it with a one-star rating first.

Here's a possible series of events. Assume Ryan starts with S stars. We'll decide later whether k is 0 or 1:

Ryan completes level k with a 1-star rating and earns 1 star.
Ryan completes other levels and earns s stars.
Ryan completes level 1-k with a 2-star rating.
Which choice of k makes this scenario possible? If k=0, then this is possible iff S + 1 + s ≥ b1. If k=1, then this is possible iff S + 1 + s ≥ b0. Since b0 < b1, then this is possible with k=0 only if it's possible with k=1. So we might as well simply choose k=1, and have Ryan choose the level with the highest value of b.
So to summarize, Ryan's strategy should be:

While there is any level remaining that Ryan hasn't completed yet, or any level for which he can earn a higher rating than he previously had:
If he can earn a two-star rating on any of those levels, he should complete one of those levels (chosen arbitrarily).
Otherwise, if there is some set of levels on which he can earn a one-star rating, he should complete the one of those levels with the highest value of b.
If Ryan has beaten all levels with a two-star rating, he's done. Otherwise he's TOO BAD.
By simulating this strategy, we can see whether Ryan can beat Kingdom Rush, and the smallest number of level completions he can do it in.

Round 1A 2012 - Code Jam 2012

Cruise Control (17pts, 30pts)

This is a challenging problem requiring some insights and a careful implementation, making it a really tough nut to crack!

Solving the small - simulation

Let's look at some car A. If there is no car that overlaps with A (that is - no car that is less than five meters ahead or behind of A), then it does not matter which lane A currently is in, as it can change lanes instantaneously with impunity. Thus, the crucial moment when we have to make a decision is when A overtakes some other car B in front of it (that is, at the moment when A is five meters behind B and getting closer), or when some other car C overtakes A.

Since all cars go at a constant speed, after any car A overtakes a car B, car B will never overtake A. This means that cars will not overtake each other very many times in the small case - the total number will be at most the number of pairs of cars, i.e. 15. Also notice that when one car overtakes another, there are only two possibilities that we need to explore: either the faster car takes the right lane and the slower car takes the left lane, or the reverse happens. If neither possibility is viable (because one of the cars is not able to take the lane we want it to take), then someone has to turn off cruise control. These two observations allow us to do a direct simulation of all possibilities.

To do this, we begin by finding out all moments in time when two cars would intercept one another, and we then look at them in order starting from the earliest. Whenever two cars meet, we check which lane they are in right now, and whether they can change to the other lane. If both cars can change lanes, we have two possibilities, and we branch out to explore them both. If one of the cars is blocked, we have only one possibility - the car which is free to switch lanes has to take the free lane, and we continue without branching. The same thing happens if both cars are blocked but are in different lanes. If the two cars are blocked in the same lane, we know someone has to turn off cruise control, and we return the current time from this branch. Finally, if we process all the overtaking events and still nobody needs to turn cruise control off, we have found a way for everybody to drive on cruise control indefinitely - we now already know the answer for the whole test case!

Since we want everybody to continue without turning off cruise control as long as possible, in the case with two branches, we should choose the branch which returned the higher value. As we branch at most 15 times, and we are able to check whether a car can change lanes simply by examining all other cars, our solution will easily run in time.

Solving the large - postponing choice

The previous strategy will obviously not cut it for the large test case. With 50 cars, there could be 1225 interceptions, and there is no way you can try 21225 different possibilities! We will have to postpone making choices for as long as possible to avoid branching.

Suppose we have two cars: A and B, and A overtakes B at some point. They now drive side by side with A gaining on B over time. One of them is occupying the right lane and one is occupying the left lane, but we do not know which is which. If A manages to move a full five meters ahead of B, it is again free to change lanes, and the choice - which side it passed B on - is irrelevant.

On the other hand, let's see what happens when a third car, C, comes along and tries to overtake whoever is in the back (let's say it's still A); moreover assume that C is driving in the right lane, and cannot switch to the left. This means that if A is in the right lane, we will have to turn cruise control off now; on the other hand if A is in the left lane, we will be able to drive on for a while. This means that putting A in the left lane was a strictly better choice.

This leads to the idea of postponed choices. Although the choice of which lane A takes had to be made some time ago, it becomes relevant only now - so let's say we reveal our choice only now. Before C came along, we think of A and B as being in an indeterminate state, with either car possibly being in the left lane, and the exact choice is forced on us only with the arrival of C (one of the Googlers working on this problem said it reminded him strongly of Schroedinger's cat).

Solving the large - undetermined lanes

To formalise this approach, we will say that at any given moment of time, a car either is in a fixed lane or in an undetermined lane. For instance, in the situation described before, car C was fixed in the right lane, while cars A and B were initially in undetermined lanes. When C overtook A, the lanes of A and B became fixed (to the left and right lane, respectively).

Notice that we also had additional information - although A and B were in undetermined lanes, we knew that they were nonetheless in different lanes. In fact, the state of the whole system can be described at any given time by the following information:

for each car, whether it is fixed in the right lane, fixed in the left lane, or in an undetermined lane,
for each pair of cars in undetermined lanes, whether they are necessarily in the same lane, in different lanes, or whether they are independent of each other.
At this point, you might wonder how two undetermined-lane cars could be forced to be in the same lane. The way this happens is that there is a third car (also undetermined) that blocks them both from changing lanes. If you examine the fourth sample case thoroughly, you will find that this is exactly what happens at the twelfth second. The cars with speeds 2 and 4 are in undetermined lanes but they are necessarily the same lane, and so one of them has to turn off cruise control.
The initial state of the system is easy to calculate. Any car that is initially adjacent to another one is in a fixed lane (the lane it starts in). Any other car is in an undetermined lane, and independent from all others. The tricky question is how to update this state over time.

Solving the large - updating the state

The state changes in two situations - when two cars get close to each other (and their states stop being independent), and when two cars stop being close to each other. We can calculate all these events up front and order them by time, just as in the small case. This takes O(N2 log N) time.

When two cars become close, they become interdependent - they have to be in different lanes. If one of them has a fixed lane, the other one now also has its lane fixed; if both were undetermined and independent, they are now still undetermined, but they have to be in different lanes. In other cases either nothing happens - like when one of the cars was fixed in the right lane, and the other one was fixed in the left lane; or someone has to turn off cruise control and we have solved the problem - like when both cars are forced in the left lane, or both are undetermined but they are forced to be in the same lane.

Moreover, when an undetermined car becomes fixed lane, it impacts all the other cars that are dependent on it - they also become fixed lane. Similarly, if two independent cars A and B become interdependent and we had a car C in the same lane as A and a car D in a different lane from B, we gain the information that C and D must be in the same lane. More generally, whenever we gain information due to a pair of cars A and B becoming close, we have to update information about any other pair of cars C and D with C dependent on A and D dependent on B. Fortunately, updating the state is very straightforward! A nice trick is to use +1 and -1 to denote the left and right lanes; and to use -1 to mean two cars are in a different lane, and +1 to mean they are in the same lane (and 0 to mean "undetermined" and "independent"). Then if, for example, we learn that A is in the right lane (A = -1), and we know that A and B are in different lanes (AB = -1), then B is in lane A * AB = 1 - the left lane. Try this out and see how it works!

What happens when two cars stop being close? Well, if neither of them can change lanes (due to some other adjacent cars), nothing happens. They remain dependent, just as they were. The only moment when something does change is when a car is free to change lanes - its state immediately becomes undetermined and independent from all other cars.

Solving the large - putting it together

So let's see how the solution will work. We first determine the list of events (two cars becoming close or becoming far away), ordered by time. We also determine the initial state of each car (initially each car is either fixed-lane, or undetermined and independent from all others). We keep the state of each car in an array, and the dependency information for each pair of cars in a two-dimensional array.
For each event when two cars become close, we check if they are able to take opposite lanes (that is - they are not both fixed to the same lane, and they are not dependent to be in the same lane). If yes, we update their dependency (and possibly lane-fixedness), and update the dependencies between all their dependents). This takes O(N2) time.

For each event when two cars stop being close, we check whether either of them can change lanes freely. If yes, we mark that car as undetermined and independent from all others. This takes O(N) time.

As we have at most O(N2) events to process, the whole solution will run in O(N4) time, which is fast enough for the limit of 50 we have on N.

Solving the large - optimizing

It's not hard to see that the solution we have could be optimized. To do this, let's notice that instead of keeping (and updating) the full dependency matrix, we can think in terms of groups of cars, since if A and B are dependent and B and C are dependent, then A and C are dependent as well. The exact nature of the dependency can be deduced from the other two dependencies. Thus, we need only keep one representative from each group of co-dependent cars, and for each car in the group remember whether it is in the same lane as the representative, or in a different lane. When two groups merge, we can merge them in O(N) time now: first we check whether the representatives are in the same lane, and then we switch everybody in one of the groups to use the representative from the other group. This makes our solution run in O(N3) time. For extra credit, you might want to consider how to make the solution run in O(N2 log N).

Fun fact: This problem was conceived when the author was driving on US interstate highways, and was annoyed by having to turn off cruise control frequently due to sub-optimal choices by other drivers who were unable to solve this problem.

Round 1B 2012 - Code Jam 2012

Safety in Numbers (10pts, 11pts)

Introduction
Like some earlier problems in Code Jam 2012, this one was inspired by the author watching significant amounts of Dancing With the Stars. Unlike previous problems, this one needed no modification to be used in Code Jam (although we don't know what they do in case of a tie): this is exactly how their scoring system works.

Sample Cases
Sometimes it's useful when solving Code Jam problems to start by working to understand the provided sample cases. Let's start there.

Case #1
In the first case, if the first dancer (err, contestant) gets 1/3 of the audience votes, then his score is 30. There's only one way for the rest of the votes to be distributed, which is for 2/3 of the votes to go to the second contestant. That contestant's score would then be 30, which is a tie, and would lead to contestant #1 being safe. Any lower score would result in that contestant being eliminated. We can do similar math on the second contestant.

Case #4
Now let's think about a case where there are more than two contestants. In the fourth sample case, there are three contestants, and their judges' scores are 24, 30 and 21. The sample output claims that the first contestant is safe if she gets 34.666667% of the vote. Is that really true?

That proportion of the votes would give contestant #1 a total of 50 points. For her to be eliminated, all other contestants would need more than 50 points. We can do a little math and find out that for the second contestant to get more than 50 points, he would need more than 26.66666...% of the vote. For the third contestant to get more than 50 points, she would need more than 38.66666...% of the vote. Adding those numbers up gives us 100%.

That means the other contestants would need more votes than the 65.333333% that remain in order to all beat the first contestant, so the first contestant can't be eliminated. If we lowered the first contestant's percentage at all, then the other contestants would all be able to beat her, and she'd end up being eliminated; so 34.666667% is correct.

N Binary Searches
Considering the sample cases has led us very naturally to an algorithm. When we're trying to find the minimum score that will make a particular contestant safe, first we'll choose that contestant's audience vote percentage, and then we'll allocate the rest of the audience votes to other contestants in the worst possible way.

This lends itself very naturally to binary search. First we pick a percentage. Is the contestant safe at that percentage? If so, the minimum safe percentage is lower. If not, the minimum safe percentage is higher. Repeat that N times, and you have the answer for each contestant.

Those numbers add up to 1!
It's a bit surprising that, when you independently calculate the minimum safe percentage for each contestant, you apparently end up with percentages that add up to 1. A little thought shows us why.

If each contestant has a minimum "safe" percentage, we can say that contestant has a minimum "safe" score (50 for contestant #1 in case #4). Thinking about the last example, we might say the minimum safe score is the value x such that we use up 100% of the audience votes if all contestants have a score equal to x. The percentages that contestants need to reach that score will naturally add up to 100%.

This is almost true, but there is one exception. Consider the following case: 4 30 0 0 0. The safe score clearly won't be more than 30, since not all contestants can get to 30, so we have to deal specially with contestants whose score is greater than the safe score. With that in mind, the safe score is the value x such that we use up 100% of the audience votes if all contestants have either a score equal to x, or 0% of the audience votes and a score greater than x.

A Different Binary Search
Now we know how to write a single binary search to find the "safe" score. The safe score is the score for which all contestants with a higher number of judge points get 0% of the audience votes; the other contestants get a percentage that gives them that total score; and the total audience vote percentages add up to 100%.

So to find the safe score, we can try a candidate score, and see if the total audience vote percentage adds up to more than 100%. If so, we need a lower score. If not, we need a higher score. As one of our preparers put it, "Binary search for the number such that it's impossible for all contestants to have that many points, and you're done!"

A Linear Solution
After this editorial was first published, a number of contestants wrote in to tell us that they didn't use binary search at all. One way to avoid it is to use a single loop, plus some math, to compute the "safe" score we talked about before. First, create a sorted list of judges' points. Then, for i from 0 to N-1, repeatedly determine the following number: if we allocated all the audience votes to the first i people to give them the same score, what would that score be? If it's less than the score for contestant i+1, or i+1=N, then that's the "safe" score.

Precision
The output of this problem had to be a floating-point number, and any number would be fine as long as it was within 1e-5 of the correct number, absolute or relative. What does that mean?

Absolute: If the correct answer is y and you output x, you will be judged correct if |y−x| < 10-5.
Relative: If the correct answer is y and you output x, you will be judged correct if |1−min(y/x,x/y)| < 10-5.
We confused some of our contestants by outputting an inconsistent number of decimal places in our sample output. The reason we did that was to illustrate that the number of decimal places you output isn't important, as long as the answer was right; unfortunately, although a number of contestants got an impromptu education on this rule, many more were confused. We'll think about careful ways of presenting problems with floating-point output in the future.

Solving the Small
There's actually an algorithmic approach for this problem that isn't good enough to solve the Large, but does work for the Small, if you aren't up on your binary search. Because each answer you come up with only has to be within 1e-5 of the correct answer, and we know the right answer is between 0 and 100, there are only a couple million numbers you have to check for each user's minimum score that avoids elimination.

Check 0, 0.00001, 0.00002, 0.00003, ... 9.99999, 10. That's 1 million numbers. Then, because the right answer can be within 1e-5 multiplicatively, and the answers we're checking now are more than 10, you can check 10.0001, 10.0002, ..., 99.9999, 100. That's another 900,000 numbers. So by checking 1.9 million numbers, we avoid having to implement binary search; though to be honest, it's probably easier to implement binary search than do that -- and binary search only has to check log2(105)=17 numbers!

Also, note that this method will only work to replace the first binary search we talked about here, not the second one. It isn't enough that the safe score is within 1e-5 of the correct value: the numbers we output have to be that close.

Why did people get it wrong?
5608 people downloaded the Small input for this problem, but only 2687 of them got it right. That's an unusually low success rate. So what did those people do wrong?

Some of them outputted a negative percentage for users with a score above the "safe score". Presumably that would have moved the safe score for them as well. Users who did that, and were told that they'd gotten the wrong answer, could have checked their output to see that they'd printed a negative number. In Code Jam you have access to the input and output file you're being tested on; use it!

Others made assumptions that ended up not being true about certain derived values being integers. One contestant who did that got our sample cases right, but outputted 0.0 33.0 33.0 33.0 for the case 4 10 0 0 0. Still others had problems with integer division: in many programming languages an integer divided by an integer will always return an integer. If you want to avoid that, you have to "cast" one of the integers to a floating-point number -- or just add 0.0 to it, which amounts to the same thing.

Some users might have made a mistake early, then fixed it, but submitted the output for the wrong input file. Whenever you retry a submission, it's important to run your code on the input you just downloaded, or you'll get the wrong answer! We're working on ways to make it easier for users to catch this problem.

Round 1B 2012 - Code Jam 2012

Tide Goes In, Tide Goes Out (18pts, 18pts)

There is a lot going on in this problem, and it is difficult to keep track of everything. Only seven hundred contestants managed to get it right, so you know it can't have been easy! Let's go over it.

The first thing you should notice is that this is a shortest-path problem. We want to get from point A (the start) to point B (the exit) as fast as possible. There are a number of classical shortest-path algorithms available; we will try to adapt Dijkstra's algorithm here.

Dijkstra's algorithm works on a weighted graph, so we need to identify a set of vertices and edges between them to use it. This is pretty easy if you have done graph theory before - we take the squares to be the vertices, and we put an edge between every two adjacent cells. We encounter problems when trying to assign weights to the edges: the cost (in seconds) of travelling from one square to another is not fixed - it can be one or ten seconds (and we haven't touched on the issue of moving around before the tide starts going down yet).

A fact that might be somewhat surprising is that this is not a problem for Dijkstra's algorithm. Let us briefly recall how it works - it maintains a tentative cost for each vertex, and at each step it chooses the vertex with the smallest cost, fixes that to be the real cost of visiting that vertex, and updates the neighbors accordingly. Of course in our case the cost of reaching a vertex is simply the time it takes.

This means that we need to consider the time of going from some square A to an adjacent square B only once, when we are recalculating the time of reaching B due to having fixed the time for A. But since we fixed the time of reaching A, we can easily calculate the water level at this moment - and so we will know the cost of travelling this edge. (Note that the earlier we can start moving from A, the faster the move will be. Therefore, we really should be moving as soon as possible instead of waiting for a better moment.)

There are also other constraints that we have. Let's name two of them - the water level needs to be at least 50 centimeters below the ceiling of the square we want to enter, and the "gap condition" needs to be satisfied. For the latter, we can just check - this does not depend on the time we want to travel - and remove the edge from the graph if the condition is not satisfied (we can either do this up-front, or lazily at the moment we try to use this edge). For the former, we need to look at what Dijkstra's algorithm needs again. The cost of travelling the edge AB is used to determine the shortest path to B that goes through A and then directly through the edge AB to B. Well, if we really want to take this path, and the water level is too high to enter B when we arrive at A, we have only one choice - we have to wait until the water level drops to CB-50 before moving. Remember that this may cause us to have to drag the kayak!

All this means that we can calculate the cost of each edge at the moment in which we need it, and we would have a complete solution for the problem if not for the extra possibility of moving around before the tide starts going down. One might be tempted to solve this issue by a preprocessing phase, where we use a breadth-first search to find all the squares that are reachable from the start in time zero.

A simpler solution to code, however, is to insert this phase into the Dijkstra's algorithm that works so well for us so far. All this extra movement means is that if we want to traverse the edge AB, we are at A in time zero, and B is accessible from A in time zero, then the cost of making this move is zero - as we can make it before the tide starts going down. This means that we insert one extra condition in our function that calculates the cost, and we are done!

Notice that the outcome here is just a standard implementation of Dijkstra's algorithm and nothing more, with all the problem-specific logic inserted into the function that calculates the weight of a given edge at the time it is needed. To convince oneself that this works, however, an understanding of the inner workings of the algorithm is needed.

Round 1B 2012 - Code Jam 2012

Equal Sums (6pts, 37pts)

The Small Input
If you want to forget about the large input and go straight for the small, then Equal Sums might look like a classic dynamic programming problem. Here is a sketch of one possible solution in Python:

def GetSetWithSum(x, target):
  if target == 0: return []
  return GetSetWithSum(x, target - x[target]) + [x[target]]

def FindEqualSumSubsets(S):
  x = [0] + [None] * (50000 * 20)
  for s in S:
    for base_sum in xrange(50000 * 20, -1, -1):
      if x[base_sum] is not None:
        if x[base_sum + s] is None:
          x[base_sum + s] = s
        else:
          subset1 = GetSetWithSum(x, base_sum + s)
          subset2 = GetSetWithSum(x, base_sum) + [s]
          return subset1, subset2
  return None
The idea is that, in x, we store a way of obtaining every possible subset sum. If we reach a sum in two different ways, then we can construct two subsets with the same sum.

For the small input, this approach should work fine. However, x would have to have size 500 * 1012 for the large input. That is too big to work with, and you will need a different approach there.

The Large Input
The first step towards solving the large input is realizing that the statement is very misleading! Let's suppose you have just 50 integers less than 1012. There are 250 ways of choosing a subset of them, and the sum of the integers in any such subset is at most 50 * 1012 < 250. By the Pigeonhole Principle, this means that there are two different subsets with the same sum.

So what does that mean for this problem? Well, even if you had just 50 integers to choose from instead of 500, the answer would never be "Impossible". You need to find some pair of subsets that have the same sum, and there is a lot of slack to work with. The trick is figuring out how to take advantage of that slack.

The Birthday "Paradox" Method
The simplest solution is based on a classic mathematical puzzle: the birthday paradox.

The birthday paradox says that if you have just 23 people in a room, then some two of them probably have the same birthday. This can be surprising because there are 365 possible days, and 23 is much smaller than 365. But it is true! One good way to look at is this: there are 23 choose 2 = 253 pairs of people, and each pair of people has a 1/365 chance of having the same birthday. In particular, the expected (aka average) number of pairs of people with the same birthday is 253 / 365 = 0.693... Once you write it that way, it is not too surprising that the probability of having at least one pair matching is about 0.5.

It turns out this exact reasoning applies to the Equals Sums problem just as well. Here is a simple algorithm:

Choose 6 random integers from S, add them up, and store the result in a hash set. (Why 6? We'll come back to that later...)
Repeat until two sets of 6 integers have the same sum, then stop.
After choosing t sets, there will be t choose 2 pairs, and each set will have sum at most 6 * 1012. Therefore, the expected number of collisions would be approximately t2 / (12 * 1012). When t is around 106, this expectation will be near 1, and the reasoning from the birthday paradox says that we'll likely have our collision.
That's it! Since we can quickly generate 106 small subsets and put their sums into a hash-set, this simple algorithm will absolutely solve the problem. You may be worried about the randomness, but for this problem at least, you should not be. This algorithm is extremely reliable. By the way, this method will work on the small input as well, so you don't need to do the dynamic programming solution if you do not want to.

Note: There are many similar approaches here that can work. For example, in our internal tests, one person solved the problem by only focusing on the first 50 integers, and trying random subsets from there. The proof below works for this algorithm, as well as many others.

A Rigorous Proof
If you have some mathematical background, we can actually prove rigorously that the randomness is nothing to worry about. It all comes down to the following version of the birthday theorem:

Lemma: Let X be an arbitrary collection of N integers with values in the range [1, R]. Suppose you choose t + 1 of these integers independently at random. If N ≥ 2R, then the probability that these randomly chosen integers are all different is less than e-t2 / 4R.

Proof: Let xi denote the number of integers in X with value equal to i. The number of ways of choosing t + 1 distinct integers from X is precisely

sum(1 ≤ i1 < i2 < ... < it+1 ≤ R) [ xi1 * xi2 * ... * xit+1 ].

For example, if t=1 and R=3, the sum would be x1 * x2 + x1 * x3 + x2 * x3. Each term here represents the number of ways of choosing integers with a specific set of values. Unfortunately, this sum is pretty hard to work with directly, but Maclaurin's inequality states that it is at most the following:

(R choose t+1) * [ (x1 + x2 + ... + xR) / R ]t+1
= (R choose t+1) * (N/R)t+1.

On the other hand, the number of ways of choosing any t + 1 integers out of X is equal to N choose t+1. Therefore, the probability p that we are looking for is at most:

[ (R choose t+1) / (N choose t+1) ] * (N/R)t+1
= R/N * (R-1)/(N-1) * (R-2)/(N-2) * ... * (R-t)/(N-t) * (N/R)t+1.

Now, since N ≥ 2R, we know the following is true for all a ≤ t:

(R-a) / (N-a)
< (R - a/2)2 / [ R * (N-a) ]     (this is because (R - a/2)2 ≥ R(R-a))
≤ (R - a/2)2 / [ N * (R-a/2) ]
= (R - a/2) / N.

Therefore, p is less than:

R * (R - 1/2) * (R - 2/2) * ... * (R - t/2) / Rt+1.

It is now easy to check that (R-a/2) * (R-t/2+a/2) ≤ (R-t/4)2, from which it follows that p is also less than:

(R - t/4)t+1 / Rt+1
= (1 - t/4R)t+1
< (1 - t/4R)t.

And finally, we use the very useful fact that 1 - x ≤ e-x for all x. This gives us p < e-t2 / 4R as required.

In our case, X represents the sums of all 6-integer subsets. We have N = 500 choose 6 and R = 6 * 1012. You can check that N ≥ 2R, so we can apply the lemma to estimate the probability that the algorithm will still be going after t+1 steps:

If t = 106, the still-going probability is at most 0.972604477.
If t = 5*106, the still-going probability is at most 0.499351789.
If t = 107, the still-going probability is at most 0.062176524.
If t = 2*107, the still-going probability is at most 0.000014945.
If t = 3*107, the still-going probability is at most 0.00000000001.
In other words, we have a good chance of being done after 5,000,000 steps, and we will certainly be done after 30,000,000 steps. Note this is a mathematical fact, regardless of what the original 500 integers were.

Comment: What happens if you look at 5-element subsets instead of 6-element subsets? The mathematical proof fails completely because N < R. In practice though, it worked fine on all test data we could come up with.

A Deterministic Approach

The randomized method discussed above is certainly the simplest way of solving this problem. However, it is not the only way. Here is another way, this time with no randomness:

Take 7,000,000 3-integer subsets of S, sort them by their sum, and let the difference between the two closest sums be d1. Let X1 and Y1 be the corresponding subsets.
Remove X1 and Y1 from S, and repeat 25 times to get Xi, Yi and di for i from 1 to 25.
Let Z = {d1, d2, ..., d25}. Calculate all 225 subset sums of Z.
Two of these subset sums are guaranteed to be equal. Find them and trace back through Xi and Yi to find two corresponding subsets of S with equal sum.
There are two things to show here, in order to justify that this algorithm works:

S will always have at least 7,000,000 3-integer subsets. This is because, even after removing Xi and Yi, S will have at least 350 integers, and 350 choose 3 > 7,000,000.
Z will always have two subsets with the same sum. First notice that the subsets in the first step have sum at most 3 * 1012, and so two of the sums must differ by at most 3 * 1012 / 7,000,000 < 500,000. Therefore, each di is at most 500,000. Now, Z has 225 subsets, and each has sum at most 25 * 500,000 < 225, and so it follows from the Pigeonhole Principle that two subsets have the same sum.
Other similar methods are possible too. This is a pretty open-ended problem!

Round 1C 2012 - Code Jam 2012

Diamond Inheritance (14pts, 14pts)

We are given a directed graph, and have to determine if there is a pair of nodes (X,Y) such that there are two or more paths from X to Y.

For each node, we do a depth-first search with that node as the root. If during the depth-first search we reach the same node twice, then we must have followed two different paths to get to that node from the root node, so we have found a pair (X,Y). Conversely, if there are two paths between X and Y, we will reach Y at least twice when doing a DFS from X. So if this algorithm finds no pair (X,Y), then none exists in the graph.

If there are V nodes and E edges in a graph, then a DFS is O(V+E) in general. But each DFS will never follow more than V edges, because after following that many edges, some node will have been reached twice, so we can stop at that point. Therefore this algorithm is O(V2).

We can also just use a variation of Floyd's algorithm, which is O(V3), but very simple to write. With only 1000 nodes in the graph, a fast implementation will finish within the time limit.

Round 1C 2012 - Code Jam 2012

Box Factory (12pts, 23pts)

This problem is a variation of the Longest Common Subsequence problem, which is to find the longest string of characters which occurs as a subsequence of each of two strings (a string S is a subsequence of a string T if S occurs in T, possibly with more characters inserted between the elements of S.) In this case, the first "string" is the sequence of types of each box produced by the factory, and the second "string" is the sequence of types of each toy produced by the factory.

A dynamic programming algorithm for this problem is to find the maximum number of toys that could be placed in boxes using the first x boxes and the first y toys. Let this value be f[x][y]. Then f[x][y] is equal to the maximum of:

f[x-1][y]
f[x][y-1]
f[x-1][y-1]+1
with the last case only applying if the xth letter of the first string is equal to the yth letter of the second string. These three cases correspond to the last action taken being to drop a box, to drop a toy, and to put a toy in a matching box, respectively.
Unfortunately, even though the number of runs of boxes and toys is small, the total number of boxes and toys can be very large, so this algorithm is not practical. But we can modify it so that f[x][y] will be the maximum number of toys that could be placed in boxes using the first x runs of boxes and the first y runs of toys. Now f[x][y] is the maximum of:

f[x-1][y]
f[x][y-1]
f[a][b]+g(a,b,x,y), for all a<x, b<y
Similarly to before, the last case only applies if the type of box run x matches the type of toy run y. It corresponds to only using boxes of that type between runs a+1 and x, and toys of that type between runs b+1 and y. g(a,b,x,y) is the minimum of the number of those toys and the number of those boxes, which is the number of toys that can be placed in boxes in that range.
This algorithm is O(n4). Another improvement can reduce this again to O(n3), which we leave as an exercise to the reader!

Round 1C 2012 - Code Jam 2012

Out of Gas (10pts, 27pts)

This was a hard problem to wrap your head around, as evidenced by the results. The final solution was, however, surprisingly simple (although the argumentation for it is not).

Let us first consider a single case, with a single acceleration value a, and an arbitrarily large N. Suppose some strategy S1 takes you home in time T. Obviously a*T2 / 2 ≥ D, as a*T2 / 2 is the distance we travel if we start accelerating immediately and never brake.

We can propose an alternate strategy S2: first stop and wait for time T - sqrt(2 D/ A), and then start accelerating full speed and never brake. This will also bring you home in time T.

We have to prove that if S1 did not collide with the other car, neither will S2. We prove this by checking that S2 arrives at any point between the top of the hill and your home no earlier than S1.

Assume S1 was at some point X later than S2. Notice that the speed of S1 coming into X had to be larger than the speed of S2 — otherwise even accelerating full speed will not let S1 catch up with S2. But it is impossible to achieve a faster speed at X than S2: strategy S2 accelerated all the way from the top of the hill to X.

So, now we only need to consider strategies such as S1. We now just need to determine how long do we need to wait on the top of the hill.

One last thing to notice is that if the other car moves at constant speed between xi and xi+1, and our strategy does not put us in front of the other car at xi or at xi+1, then it does not intercept the other car in any intermediate point either. We know that because if we did intercept it in some intermediate point, it would mean that we were moving faster than the other car at the time of interception; and since we're accelerating, and the other car's speed is constant, we would end up in front of it at xi+1. Therefore passing the other car between xi and xi+1 leads to a contradiction, and it must be the case that we don't pass it.

With this reasoning complete, we now have two possible algorithms. One is a binary search: To check if a given waiting time T is long enough, we just need to check all the N points at which the other car can change its speed. To achieve a precision of 10-6, we will need around 40 iterations in the binary search. This means we solve each test case in O(N A) time, with the constant 40 hidden in the big-O notation — fast enough.

If we are worried about the constant factor of 40, we can also iterate over all points xi, and for each calculate the minimum waiting time needed not to intercept the other car at this point: ti - sqrt(2xi/a) for all i such that xi < D (you also have to include the moment where the other car reaches your house). This will make our solution run in O(N A) time without the pesky constant.

Round 2 2012 - Code Jam 2012

Swinging Wild (5pts, 9pts)

Translating to a graph problem
To analyse the problem, let us begin with describing the state we can be in at any particular point during traversing the swamp. It is easy to see that we need two items of information to describe this state - which vine are we currently holding on to, and how far away from the root are we holding it.

Once we have such a description, we can frame the problem as a path-finding problem in a graph. We start in the state (0, d0), and we want to any state (i, p) with p + di ≥ D (to simplify, we can also add an artificial N+1st vine where our true love stands and demand that we reach the state (N, p) for any p).

The transitions between states (or, in other words, edges in the graph) are allowed swings we can make. If we currently hold vine i, at p units away from the root, we can swing to any vine j rooted between di - p and di + p, and the new length will be the minimum of |di - dj| and lj. Note that the only use of climbing up a vine is to catch a vine that would be too short to be within our swing path - so we implicitly include it in the transitions described above (and thus do not need extra transitions from (i, p) to (i, p-1)).

Limiting the number of nodes
As described, we could have a large number of states (as there are very many possible vine lengths. A crucial fact to notice is that the second part of the state (the length away from root) is uniquely determined by the previous vine we got here from; and it is independent of where did we hold it (assuming we held it far enough to actually reach this state). This means that there are at most N2 states we will ever consider, as each is determined by a pair of vine indices.

We can now solve the small input. We have a graph with N2 nodes and at most N edges from each node, and we want to verify whether a path between some two nodes exists (we can merge all the target nodes into one node for simplicity). As we have at most N3 edges in total, any standard graph traversal algorithm (like BFS or DFS) will allow us to solve the problem.

Limiting the number of edges
For the large input, a O(N3) solution will be unsatisfactory, and we need a bit more subtlety. There is a number of tricks one can use to beat down the complexity. One is to make use again of the fact that the target node depends only on the vine we start from, and not on the position at which we hold it. This means that there are in fact at most N edges from a given vine - if we manage to reach some vine j from a given vine i when holding it at position A, we do not need to check this edge when considering moves from vine i held at position B (because even if we reach vine j, we will arrive in a state that we have already analysed). Thus, we need to make at most N2 edge traversals to visit all reachable nodes. There are various techniques to actually code this (for instance, for each vine, we could order the other vines by distance from this vine, and each time process this list from the closest vine and remove all traversed edges), we encourage you to explore the options.

An alternative for the large problem
An alternative is to notice another fact - the position part of the state (that is, the distance away from the root that you hold the vine at) never increases. This is because if you swing from vine i to vine j, it means you were holding i at least |di - dj| away from the root, while this quantity is at the same time an upper bound on the position you will hold vine j at.

This means that we can use Dijkstra's algorithm to find, for each vine, the maximum position we can hold this vine at - we treat the decreasing vine position as increasing time, and in each step we analyse what lengths could we obtain for each vine by moving from the current vine, and then choose the vine with the largest length to analyse. This will give us a O(N2logN) solution, which should be fast enough.

Going even faster
A fun fact is that this problem can be solved faster than O(N2), although you didn't need to notice this to solve our data sets. The key fact here is that if you can pass the swamp at all, you can always do it without going backwards (that is, you always catch a vine that's in front of you, you never swing back). An easy way to use this observation is to modify the Dijkstra's algorithm mentioned above to process the vines from first to last, which will turn O(N2logN) into O(N2).

To go down to O(N), we need one more trick. Notice that if we can reach a vine, we will get the largest (meaning best) position if we swing to it from a vine that is as far away as possible. As we move only forward, this means that as soon as we can reach any particular vine, we should note the position achieved and we never need to check any other way of reaching it. This means we can get an O(N) solution by keeping track of the vine we are currently processing and farthest we have reached so far, and from each vine trying to update only vines that we have not reached as yet. As each vine will be updated at most once, and read at most once, we will do O(N) operations.

We encourage you to flesh out the details and try to prove the "never go back" lemma - it's not trivial!

Collapse right panel

Round 2 2012 - Code Jam 2012

Aerobics (6pts, 15pts)

The key to this problem lies in realizing that there really is a lot of space on the mat to take advantage of, and a number of different approaches will work.

For the small input, putting the circles along one of the longer edge, and - if it runs out - along the opposite edge will work. The precise analysis of why they fit is somewhat tedious, so we will skip it in favor of describing two solutions that can also deal with the large input.

A randomized solution
The large input is a more interesting case. We will describe two solutions that allow one to solve this problem. The first one will be randomized. (If you solved the Equal Sums problem in Round 1B, you should already realize how helpful randomized algorithms can be!).

We will order the circles by decreasing radius. We will then take the circles one by one, and for each circle try to place it on the mat at a random point (that is, put the center of the circle at a random point on the mat). We then check whether it collides with any circle we have placed previously (by directly checking all of them). If it does collide, we try a different random point and repeat until we find one that works. When we succeed in placing all the circles, we're done.

Of course, if we manage to place all the circles, we have found a correct solution. The tricky part is why we will always find a good place to put the new circle in a reasonable time. To see this, let's consider the set of "bad points" - the points where we cannot put the center of a new circle because it would cause a collision. If we are now placing circle j with radius rj, then it will collide with a previously placed circle i if and only if the distance from the new center to the center of circle i is less than ri + rj. This means that the "bad points" are simply a set of circles with radii ri + rj.

What is the total area of the set of bad points? Well, since the set is a group of circles, the area is at most the sum of the areas of the circles. (It can be less because the circles can overlap, but it cannot be more). As we are placing the circles ordered by decreasing radius, we know rj ≤ ri, so the area of the ith "bad" circle is at most π * (2ri)2 = 4π ri2. Here is where we will use that the mat is so large - we see that the total bad area is always at most 80 percent of the mat. Therefore, we have at least a 1 in 5 chance of choosing a good center on every attempt. In particular, it is always possible to find a good center, and it will take us only a few tries.

For each attempt, we have to make O(N) simple checks, and we expect to make at most 5N attempts, so the expected time complexity of this algorithm is O(N2) - easily fast enough.

A deterministic solution
As usual, if we are willing to deal with a bit more complexity in the code, we can get rid of randomness in the solution, taking advantage of all the extra space we have in a different way. One sample way to do this follows.
To each circle of radius R we attach a 4R * 2R rectangle as illustrated below:

The top edge of the rectangle passes through the center of the circle. The bottom edge, the left edge, and the right edge are all at a distance of 2R from the center of the circle.

We now place circles one at a time, starting from the ones with the larger radius. We always place each circle in the topmost (and if we have a choice, leftmost) point not covered by any of the rectangles we have already drawn.

An argument similar to the one used in the randomized solution proves that if we place a circle like that, it does not collide with any of the previously placed circles. As we place each point in the topmost available point, the centers of the previously placed circles are necessarily above the last one placed, and so if our new circle would collide with one of the previous ones, it would have to be in the rectangle we associated with it.

Now notice that the areas of all the rectangles we place is the sum of 8R2 = 8 / π times the total area of the circles - which is easily less than the area of the mat. This means we always can place the next circle within the mat.

This solution is somewhat more tricky to code than the previous one (because we have to find the topmost free point, instead of just choosing a random one), but still easier than if we tried to actually stack circles (instead of replacing them by rectangles).

Round 2 2012 - Code Jam 2012

Mountain View (13pts, 14pts)

This problem gives you a lot of freedom in how you approach it, as it is OK to output any sequence of peak heights matching the input data. What follows is one of the possible solutions, but others are possible.

First notice that if from peak 3 we perceive peak 7 as highest, then from peaks 4, 5 and 6 I can surely see no farther than 7, as the peaks have to lie below the line connecting peaks 3 and 7, and no peak after 7 lies above that line (or it would appear higher from peak 3). If this condition is not satisfied, we can safely return "impossible".

So, begin with peak 1, then go to the one appearing to be highest from it, then the apparent highest from it, and so on. Give all these peaks height of 109. Note this sequence necessarily ends on peak N (as it is strictly increasing).

Now sequentially take the first peak A that we haven't assigned a height to yet. As it's the first non-assigned, the previous peak (A-1) has necessarily been assigned, and we can look at the peak that appears highest from A-1. Suppose it's B. We know that when we start a sequence of apparently highest peaks from A, it has to contain at B - it cannot skip B, because the whole sequence has to lie below the [(A-1), B] line), and it is strictly increasing. If the sequence jumps over B, we return "impossible".

Assume the line [(A-1), B] has slope T; our construction will guarantee that T is an integer (notice the slope of the first line is zero). We will want all the peaks on the sequence starting at A to lie on a line passing through peak B and with slope T+1; this determines the line uniquely. If we do it this way, we will not spoil visibilities built before (because the whole line is below the line connecting A-1 and B), and in the sequence for each peak the apparently highest peak will be the next peak in the sequence (as they are all in one line, no peaks with constructed heights lie between A and B, and all the peaks after B are invisible, because they lie below the line connecting (A-1) and B, and thus, even more so, below the line connecting A and B (which has larger slope).

We continue in this fashion. We can check that at any moment of our construction:

If we assume the peaks that we have not constructed yet do not stand in the way (have, say, a height of zero), then for each peak we already constructed the peak appearing to be the highest is the one we expect to appear highest.
For any constructed peak A, either A+1 is constructed as well, or no peak between A and the peak visible from A has its height constructed.
Adding a new sequence upholds these invariants, which proves that when we end, the visibilities are all OK.
Thus, the construction works.
Notice that we increase the slope by one for each sequence, so it is at most N at the end, and so the lowest we can get is 109 - N2; thus the heights are all positive.

Round 2 2012 - Code Jam 2012

Descending in the Dark (8pts, 30pts)

The main challenge in this problem is determining whether a cave is lucky. The set of all possible states is huge, so a complete search over all plans is simply not going to work. Fortunately, there is one observation we can use to greatly simplify the task.

Eliminating Backtracking
Fix a cave C, and recall that SC denotes the set of squares from which C can be reached. We will build up our plan one instruction at a time.

Let X be the set of squares that you can be in if you start in SC and follow the plan so far. If X contains a square not in SC, we know the plan cannot work for every starting position. Otherwise, we can be 100% sure that the plan so far is fine! This is because the set of possible squares we are in has either stayed the same or gotten strictly smaller. In particular, if there was a plan that worked from the starting position, we can append it to what we have done so far, and it will still work.

So what does this mean? As long as we don't do a move that adds a square not in SC to the set of possible squares X, we can go ahead and do that move, and it will still be possible to finish. In particular, we can always move left and right whenever we want, since moving left or right can never move you out of SC.

All You Need Is Down!
Even with the previous observation, we still have work to do. We now know all the moves that can be done safely, but the state space is still huge. We can't find just any move; we need one that makes progress.

Here is where it is important that you cannot go up the mountain. Suppose you can add a Down move to the plan, satisfying the following two properties:

There is at least one position in X from which you can actually move down. (Without this, the Down move will never do anything, and so is useless!)
There is no position in X from which a down move will take you outside of XC.
If you add this Down move to the plan, then the sum of the heights over all squares in X will have gone down, and it can never go up again, because you can never climb the mountain!
Since the sum of the heights is a positive integer, you will eventually have to stop making Down moves. At that point, you are stuck in one or more horizontal intervals. If there is just one interval and it contains the cave, the plan can be finished successfully. Otherwise, you're screwed! And remember, since this set of squares is a subset of what you started with, you were in fact screwed from the start.

Can You Go Down?
Only one question remains: given a set of positions X reachable from the start, can you come up with a valid plan that includes at least one Down move?

X must be contained in a set of horizontal intervals, bounded by impassable squares to the left and right. Since it is always safe to move left and right, we can keep moving left until X is actually just the leftmost square in each of these intervals. If we cannot make progress from that situation, we have already shown we are lost.

As we perform left and right moves from there, our position within each interval might change. However, note that if two intervals have the same length, our relative horizontal positions within them will always be the same. Therefore, let's define xj to be our relative horizontal position within all intervals of length j. (In particular, xj = 0 if we are in the leftmost position, and xj = j - 1 if we are in the rightmost position.)

Lemma: It is possible to reach position (x1, x2, ...) using left and right moves if and only if xi ≤ xj ≤ xi + j - i for all i < j.

Proof: First we show xi ≤ xj. This is true initially. If it ever failed after some sequence of moves, it would be because xi and xj were equal, and then either:

We moved left, and only xj was able to move.
We moved right, and only xi was able to move.
However, both of these scenarios are impossible. Therefore, xi is indeed at most xj, or in other words, the distance from xi to the left wall is no larger than the distance from xj to the left wall. The same argument can be applied for the right wall, which gives us the other half of the inequality: xj ≤ xi + j - i.
Conversely, any set of positions with xi ≤ xj ≤ xi + j - i really can be reached via the following algorithm:

Start with each xi = 0.
Loop from i = largest interval length down to 2.
Move i-2 + xi - xi-1 times to the right, and then i-2 times to the left.
Try it yourself and you will see why it works!

We are now essentially done. We need to determine if there is set of positions {xi} that can be reached for which it is safe to move down. Once you have gotten this far, you can finish it off with dynamic programming. Here is some pseudo-code that determines whether there is a set of positions from which it is possible to move down and make progress:

old_safety = [SAFE] * (n+1)
for length in {n, n-1, ..., 1}:
  for i in [0, length-1]:
    pos_safety[i] = best(old_safety[i], old_safety[i+1])
    if moving down leaves SC:
      pos_safety[i] = UNSAFE
    elif moving down is legal and pos_safety[i] == SAFE:
      pos_safety[i] = SAFE_WITH_PROGRESS
  old_safety = pos_safety
return (pos_safety[0] == SAFE_WITH_PROGRESS)
This is a good starting point, but you would still have to tweak it to actually record what the right xi is for all i.
Putting it all together, we repeatedly use the above algorithm to see if it is possible to make a down move. If so, we do it and repeat. Otherwise, we stop. If the only remaining interval is the one containing the cave, then that cave is lucky. Otherwise, it is not!

Since there are no correct submissions for the large input during the contest, we provide here a full java solution (by Petr Mitrichev) so everyone can use it to generate correct outputs for various inputs.

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;

public class Dark {
 static class Segment {
  int len;
  long goodExitMask;
  long badExitMask;
 }

 public static void main(String[] args) throws IOException {
  BufferedReader reader = new BufferedReader(new InputStreamReader(System.in));
  int numTests = Integer.parseInt(reader.readLine());
  for (int testId = 0; testId < numTests; ++testId) {
   String[] parts = reader.readLine().split(" ", -1);
   if (parts.length != 2) throw new RuntimeException();
   int rows = Integer.parseInt(parts[0]);
   int cols = Integer.parseInt(parts[1]);
   String[] field = new String[rows];
   for (int r = 0; r < rows; ++r) {
    field[r] = reader.readLine();
    if (field[r].length() != cols) throw new RuntimeException();
   }
   System.out.println("Case #" + (testId + 1) + ":");
   for (char caveId = '0'; caveId <= '9'; ++caveId) {
    int cr = -1;
    int cc = -1;
    for (int r = 0; r < rows; ++r)
     for (int c = 0; c < cols; ++c)
      if (field[r].charAt(c) == caveId) {
       cr = r;
       cc = c;
      }
    if (cr < 0) continue;
    boolean[][] reach = new boolean[rows][cols];
    reach[cr][cc] = true;
    int nc = 1;
    while (true) {
     boolean updated = false;
     for (int r = 0; r < rows; ++r)
      for (int c = 0; c < cols; ++c)
       if (reach[r][c]) {
        if (r > 0 && field[r - 1].charAt(c) != '#' && !reach[r - 1][c]) {
         reach[r - 1][c] = true;
         ++nc;
         updated = true;
        }
        if (c > 0 && field[r].charAt(c - 1) != '#' && !reach[r][c - 1]) {
         reach[r][c - 1] = true;
         ++nc;
         updated = true;
        }
        if (c + 1 < cols && field[r].charAt(c + 1) != '#' && !reach[r][c + 1]) {
         reach[r][c + 1] = true;
         ++nc;
         updated = true;
        }
       }
     if (!updated) break;
    }
    List<Segment> segments = new ArrayList<Segment>();
    for (int r = 0; r <= cr; ++r)
     for (int c = 0; c < cols; ++c)
      if (reach[r][c] && (c == 0 || !reach[r][c - 1])) {
       int c1 = c;
       while (reach[r][c1 + 1]) ++c1;
       Segment s = new Segment();
       s.len = c1 - c + 1;
       for (int pos = c; pos <= c1; ++pos) {
        if (r + 1 < rows && field[r + 1].charAt(pos) != '#') {
         if (reach[r + 1][pos])
          s.goodExitMask |= 1L << (pos - c);
         else
          s.badExitMask |= 1L << (pos - c);
        }
       }
       segments.add(s);
      }
    while (true) {
     int maxLen = 0;
     for (Segment s : segments)
      maxLen = Math.max(maxLen, s.len);
     long[] badByLen = new long[maxLen + 1];
     for (Segment s : segments) {
      badByLen[s.len] |= s.badExitMask;
     }
     long[] possible = new long[maxLen + 1];
     possible[1] = 1;
     for (int len = 1; len <= maxLen; ++len) {
      possible[len] &= ~badByLen[len];
      if (len < maxLen) {
       possible[len + 1] = possible[len] | (possible[len] << 1);
      }
     }
     for (int len = maxLen; len > 1; --len) {
      possible[len - 1] &= possible[len] | (possible[len] >> 1);
     }
     List<Segment> remaining = new ArrayList<Segment>();
     for (Segment s : segments)
      if ((s.goodExitMask & possible[s.len]) == 0) {
       remaining.add(s);
      }
     if (remaining.size() == segments.size()) break;
     segments = remaining;
    }
    System.out.println(caveId + ": " + nc + " " + (segments.size() == 1 ? "Lucky" : "Unlucky"));

   }
  }
 }
}

Round 3 2012 - Code Jam 2012

Perfect Game (3pts, 7pts)

Preamble
It will come as no great surprise that this problem was inspired by a video game that the author played: Super Meat Boy. Turning the real achievement challenge into a Code Jam problem involved a little simplifying: it doesn't take the same amount of time to lose a level as to win it, for example.

Total Expected Time and Number of Attempts
Your total expected time to complete the achievement will be the expected number of attempts, times the expected time per attempt.

First, let's figure out how many attempts it's going to take us to complete the achievement. The probability of success in any given attempt is easy to compute, since it's simply the probability of succeeding on every level once without failing any of them: (1-P1)*(1-P2)*...*(1-PN). The expected number of attempts won't depend on anything other than this probability, and this probability doesn't depend on the order of the levels; so the expected number of attempts doesn't depend on the order of the levels, and we can ignore it when figuring out what order to attempt them in.

In case you're curious about what the expected number of attempts is anyway, read on. Let's say that we're going to try to do something that might fail, like complete this achievement, and that we're going to keep trying until we succeed. If the probability of success is P, then what we're doing is called a Bernoulli Trial, repeated until we achieve success. The number of attempts that will take falls into a random distribution called the Geometric Distribution. The expected value of this distribution–the number of attempts it will take–is 1/P.

Knowing about the Geometric Distribution lets us compute the expected number of attempts easily. The chance that we'll complete every level successfully, one after another, is the product of the probabilities of success, which we'll call P; the number of attempts it will take us to complete that is 1/P. As promised, this number doesn't depend on the order in which we attempt the levels, and since what we're trying to do is compute the best order, we're going to ignore it.

Time Per Attempt
Let's choose an arbitrary order for the levels. Suppose the ith of those levels takes ti time, and you die in that level with probability pi. In any attempt, you'll definitely reach level 1; you'll reach level 2 with probability 1-p1; you'll reach level 3 with probability (1-p1)*(1-p2); and so on.

Based on those calculations, the amount of time it takes you to make one attempt will be expected time = t1 + (1-p1)t2 + (1-p1)(1-p2)t3 + .... This is because you will try level i only if you pass the first i-1 levels.

Now, let's consider what would happen to that expected time if we swapped levels i and i+1. Only two of the terms of the expected time equation would be affected—the others would simply reverse the order of (1-pi) and (1-pi+1), which doesn't matter. Those two terms themselves have several multiplicative terms in common:

Pre-swap:
(1-p1)(1-p2)...(1-pi-1)ti +
(1-p1)(1-p2)...(1-pi-1)(1-pi)ti+1.
Post-swap:
(1-p1)(1-p2)...(1-pi-1)ti+1 +
(1-p1)(1-p2)...(1-pi-1)(1-pi+1)ti.

So we're better off post-swap iff:
ti + (1-pi)ti+1 > ti+1 + (1-pi+1)ti
tipi+1 > ti+1pi
Now we can compare two adjacent levels to see whether they should be swapped. Doing this greedily results in a stable sort of the levels. With a little more work, you can prove that non-adjacent levels should be swapped under the same conditions, and that the stable sort is thus optimal.

Implementation Details
Our last inequality above said we should swap iff tipi+1 > ti+1pi. It's tempting to go one step further, to ti/pi > ti+1/pi+1, but we can't: pi or pi+1 could be zero, and dividing by zero isn't a mathematically valid thing to do.

Another reason not to make that final step–though the final step actually kind of works if you don't mind special-casing pi=0 to give an infinite score–is because division almost inevitably makes us deal with floating-point numbers. In a problem like this where we're trying to make a decision based on numbers, we want to make sure we're doing exact comparisons. We wouldn't want to reverse two equivalent levels because ti/pi = 10.0000000001 and ti+1/pi+1 = 10.0.

Round 3 2012 - Code Jam 2012

Havannah (8pts, 12pts)

This problem probably looked a little scary to many competitors at first. (I, for one, remember some nasty problems involving hexagons in other contests). However, this one wasn't as bad as it looked.

Let's first see what kind of cells there are and how to distinguish between them:

Inner Cells: Those were the cells that are not part of the border of the board. They had 6 neighboring cells.
Corner Cells: Those were the cells where two edges of the board overlap. They had 3 neighboring cells.
Edge Cells: Those were the cells that were neither inner cells, nor corner cells. They had 4 neighboring cells.
Note that it was a bit counter-intuitive that the corner cells were *not* edge cells (as they, in fact, lie on the edges), however that was clearly defined in the statement: "Corners do not count as edges.".
We can enumerate the edges and the corners of the board. Each edge will have a distinct integer index between 0 and 5, inclusive, and each corner will also have a distinct index between 0 and 5, inclusive. This is okay, but for simplicity later we will give the indices from 0 to 5 to the corners and add 6 to the indices of the edges, to obtain 6 to 11 for the edges. If a cell neither is a corner, nor belongs to any of the edges, we can mark it with -1. Thus, we create a function int checkType(row, col) that checks if the cell at (row, col) is a corner (and which one if it is), part of an edge (and which edge, if it is) or neither.

Corners are the cells: {(1, 1), (1, S), (S, S * 2 - 1), (S * 2 - 1, S * 2 - 1), (S * 2 - 1, S), (S, 1)}
Edges are the cells: {(1, X), (X, 1), (X, S * 2 - 1), (S * 2 - 1, X)} where X is any integer, together with all cells for which |row - col| == S - 1.
Don't forget that corners are not considered edges, so exclude them when checking for edges.
Now let's examine in more detail the winning figures:

Bridge: This was probably the simplest of the three: just a set of stones that connects any two corner cells. The minimal number of stones to achieve it was 2 on a board with S = 2 (which was the minimal possible board).
Fork: This was a bit more complicated. We need a set of stones, that connects 3 distinct edges (i.e. cells from three distinct edges). The minimal number of stones to achieve it was 5 on a board with S = 3 (note that the board with S = 2 does not have any edge cells).
Ring: This was the most complicated figure. We need a set of stones that encircles an empty cell. As it might have been a bit unclear what "encircles" in this case means, the problem statement contained a detailed explanation and an example. The minimal number of stones to achieve it was 6 on a board with S = 3. Note that a ring on a board with S = 2 is impossible, as it inevitably would lead to a bridge first.
As many of the competitors might have realized, the Bridge and Fork cases are relatively simple to detect. One way to solve them would be to use binary search over the move in which they are created and check if a Bridge and/or a Fork exists. That was okay, however it didn't work for Rings. Some competitors used this solution for Forks and Bridges, and another one for Rings. However, this required some extra code, which is rarely a good idea in a speed contest.

Some competitors probably realized that even an O(M2) solution would pass. However, we will describe an O(M) solution, which was very fast and not much harder to implement.

First, it's a good idea to store the board in a both memory and time efficient way. The thing to notice was that when the board is really big, then it is also really sparse (only 10000 stones in a board with over 9000000 cells). So the way to go was to use any kind of set data structure our programming language of choice provided. In C++, a hashset is fastest, but tree sets are also fine. Putting a stone is a constant operation if we are using a hashset and logarithmic one if we're using balanced tree set (for example C++ STL set).

Now we start putting stones on the board in the order they are given. We need a fast way to check if putting a stone created some of the winning figures. The fork and the bridge needed some cells to be connected (i.e. to belong to the same connected component). In fact, the ring also requires the same thing. As many competitors in this round probably know, these are usually handled using the union find algorithm. As a reminder union-find supports the following basic operationss:

Create a new node, belonging to a group of size 1.
Merge two new groups into one.
For a given node, find the group it is.
Implemented correctly, all of these operations are blazingly fast.
We will use union-find here with a minor addition. After putting a stone, we see if some of the neighboring cells contains a stone already. If none do, we add the new stone to its own connected component. If there are stones in adjacent squares, we merge them all to a single connected component that also contains the new stone. Additionally to the straight-forward union-find algorithm, we will store what type of cells each component contains. Since the number of different types of cells we are interested in is relatively small (only 12 types) we can use a mask with 12 bits (corresponding to the indices from 0 to 11 we mentioned earlier). We do this in order to have an easy way to see if a component contains corner or edge cells. When merging the component A to component B, we change the mask of B to contain the bits of the mask of A as well (since now they are the same component). This can be done really easily with the bitwise operation OR. If, after merging, a component ends up with 2 set bits in the first 6, or 3 set bits in the second 6 positions, then we've just created a bridge or a fork, respectively.

Everything's fine with that, but we still haven't answered the question how do we handle rings. Well, having the stones and their respective components makes it easy to check for them. In order for a ring to be created, we must have just added a stone that connects a component to itself. But if it does, it still does not necessarily create a ring. What we can do is check all neighbors of the cell where we are putting the stone, and search for two cells with stones with the same component.

We can represent the neighbors of the given cell as the following:

# 1 2
6 * 3
5 4 #
Going clockwise twice (or counter-clockwise, if you prefer, it doesn't matter) and looking at the neighbors there should be one of the following sequences (as a subsequence of the created sequence of 12):
{C, X1, C, Y1, Y2, Y3}
{C, X1, X2, C, Y1, Y2}
{C, X1, X2, X3, C, Y1}
where the cells C belong to the same component, and each of the cells X and Y are either empty or belong to some component (not necessarily the same as C, and not necessarily the same as other Xs and Ys).
After this move a ring is formed if and only if:

At least one of the Xs is an empty cell
At least one of the Ys is an empty cell
(Note that if there is a {C, X1, C, Y1, Y2, Y3} sub-sequence, then there will be a {C, X1, X2, X3, C, Y1} one, however we've included them both for clarity).
Why is this true? Well, if none of the Xs or none of the Ys is an empty cell, then the two Cs were already connected "from this side" and adding the stone doesn't change it into a ring (obviously). If both some of the X cells and some of the Y cells contain an empty cell, then we've just encircled at least one of them! Imagine it this way - a circle has two sides - inside and outside. We know that we created a circle (since we are connecting a component to itself), but we don't know which side is the inner one and which side is the outer one. Thus, if both contain an empty cell, then we have an empty cell in the inner side for sure.

What is the time complexity of the given algorithm? Since OR is a constant operation, we have a constant number of bits to check when looking for a Fork or a Bridge, and checking for a ring involves also a constant number of operations (equal to the number of neighbors), the complexity is dominated by the speed of the union-find operations. Thus, the total complexity of the algorithm is O(M * RACK(M)), where RACK() is the inverse of the Ackerman function. However, RACK() is so slowly growing, that you can assume the algorithm will run in nearly O(M) for the given constraints.
Remark: Another nice trick for dealing with rings is to start with all pieces played on the board and work backwards, always considering which empty squares are connected to each other. Removing a stone possibly connects two of these components, and so we can again use union-find to track the state quickly. This is conceptually simpler, but it is slower because we need to first find all the connected components after the pieces are played.

Round 3 2012 - Code Jam 2012

Quality Food (9pts, 18pts)

Multiple Solutions
Sometimes our problemsetters like to come up with big, complicated solutions to their problems, and sometimes it turns out that there are simpler alternatives. We'll present two solutions here, including the simple one that many problems solvers used, and the more complex one used by the author.

How much does a single delivery cost?
Let's look at the cost of a delivery containing K meals. It's easy to see that we should order the cheapest meal with a time-to-stale of at least 0, the cheapest meal with a time-to-stale of at least 1, the cheapest meal with a time-to-stale of at least 2, and so on until we order the cheapest meal with a time-to-stale of at least K. Since K could be very large, we'll solve this problem in O(N log(N)).

First we'll sort the kinds of food by price, and construct an array that contains [(0, 0), (d1, c1), (d2, c2), ...], where we should buy food that costs ci for the meals between di-1 and di after the delivery arrives. We can then process that into another array containing (di, Ci), where Ci is the total cost of a single delivery that lasts di days, and (Ci-Ci-1) / (di-di-1) = ci.

We only had to do those above steps once. Now that we have that array, it's an O(log(N)) binary search (or a simpler linear search, since we'll find that we don't need the efficiency for either of our solutions) to find out how much it costs to have a delivery that lasts K days: find i such that di-1 ≤ K < di, and charge Ci-1 + (K-di-1)*ci-1.

We'll call that function SingleDeliveryCost(days), and for convenience later in this editorial we'll also define SingleDayCost(day) to be the cost to buy the cheapest meal with a time-to-stale of at least day. Observe that SingleDayCost(a) ≤ SingleDayCost(b) if a ≤ b, and that SingleDeliveryCost(days+1) = SingleDeliveryCost(days) + SingleDayCost(days+1).

Deliveries should be (almost) the same size
Let's show that all of your deliveries should be almost the same size. Let's suppose that we have a solution that has a delivery, A, containing a meals, and another delivery, B, that contains b meals, with b ≥ a+2. Then the cost for those two deliveries is SingleDeliveryCost(a) + SingleDeliveryCost(b). If instead we increased delivery A's size to a+1 and decreased delivery B's size to b-1, we'd have the same total number of days, but the delivery cost would be:
SingleDeliveryCost(a+1) + SingleDeliveryCost(b-1)
= SingleDeliveryCost(a) + SingleDayCost(a+1) + SingleDeliveryCost(b) - SingleDayCost(b-1)
≤ SingleDeliveryCost(a) + SingleDeliveryCost(b).

This shows that all our deliveries should be of size a or a+1 for some value of a.

Deliveries should be of an "interesting" size
Let's say that we want to buy a total of D days' worth of food, and we want to know how big our deliveries should be. We'll start by considering a number of deliveries X that we're going to show is "uninteresting": one such that if we look the size of the largest delivery, ceil(D/X), then ceil(D/(X-1)) = ceil(D/X) = ceil(D/(X+1)). In such a case, adding a delivery changes the cost by SingleDeliveryCost(ceil(D/X)) - ceil(D/X)*SingleDayCost(ceil(D/X)). If we add a delivery, we'll change the cost by the negation of that amount. One of those two quantities has to be non-negative, which means that we never need to consider making X deliveries.

That means that we only need to look at numbers of deliveries such that if we changed the number of deliveries, we'd be changing SingleDayCost(ceil(D/X)). There are only 2N such delivery sizes, and trying all of them solves the problem in O(N log(N)).

The other approach
A competitor who couldn't convince himself or herself of the above still has a good chance of solving the problem. We'll start by presenting a trick ("Changing the Question") that you can use whether or not you have the above realization–it might make the math easier–and a totally different approach that, admittedly, requires some math of its own.
Changing the Question
An observation we can make is that instead of solving the problem that was asked, we can play a trick to let ourselves solve a simpler problem: "Can I eat quality food for D days?" We do this by binary searching on the answer. We know the answer is between 0 and M, so we can start by solving that problem O(log(M)) times.

How many deliveries should there be?
If the logic in the previous solution was too complicated, here's an alternative that probably isn't going to make you any happier. On the other hand, if you were thinking that there weren't enough logarithmic factors in the complexity analysis, or enough -ary searches, this could make you happy. This was our original solution for the problem, and it makes the I/O author perversely proud to have been part of making a problem to which a binary search in a ternary search in a binary search is a perfectly reasonable solution.

The argument here is that we can ternary search for the number of deliveries that minimizes the cost to buy quality food for D days. In order for a ternary search to be possible, the function has to be strictly decreasing, then strictly increasing (or vice versa). In this case, the domain of the function is from where the number of deliveries X is the minimum possible number at which we could order D meals (regardless of how expensive they are), to X=D. We'll show that the function decreases, then increases over this domain.

First let's take the SingleDayCost function and extend it over the real numbers. We already know what it is for the integers; let's just linearly interpolate for the real numbers in between, and call the result G. This has the nice result that the cost for X deliveries and D days, which we'll call H(X), is H(X) = G(D/X)*X + X*F.

Now, we're about to start taking derivatives, and although G is continuous, it isn't differentiable. There are a couple of ways of getting around this, and we're going to take a physicists' route. We'll define a function G'' to be a sum of delta functions such that the double-integral of G'' over X is our original G. Then we'll define G' to be G'''s integral, and G to be G''s integral. That lets us say that the first derivative of G is non-negative everywhere, and so is its second derivative. Don't try this at home, folks, especially if you live in a math class.

What we're really interested in doing here is proving that H(X) is decreasing and then increasing, which we can do by proving that it has a positive second derivative:
H(X) = G(D/X)*X + X*F
H'(X) = G(D/X) - X*G'(D/X)*X-2 + F
H'(X) = G(D/X) - G'(D/X)/X + F
H''(X) = -X-2G'(D/X) + X-2G'(D/X) + G''(D/X)X-3
H''(X) = G''(D/X)X-3 ≥ 0
Therefore H(X) is decreasing and then increasing (or just decreasing or increasing, which is also fine), and can be ternary searched on for the minimum cost of surviving D days. This algorithm's complexity works out to O(Nlog(N) + log2(M)log(N)).

Round 3 2012 - Code Jam 2012

Lost Password (7pts, 36pts)

This is certainly an intimidating problem. Even small strings, like the example from The Fellowship of the Ring, are difficult to work through. When you are trying to combine passwords of length at most 500 into a password string, there are so many possibilities to consider that optimization quickly becomes overwhelming.

The Small Input

Let's begin with the small input. In this case, we need to connect up pairs of letters. The key insight here is to imagine everything as a graph. If you have heard of De Bruijn sequences before, that is the perfect thing to build from. We will make a vertex for each letter (the 26 normal letters as well as all the 133t variations), and add an edge between every pair of letters. Each 2-letter word that we need in our password string corresponds to an edge on this graph. For example, "t0" corresponds to the edge 't' -> '0'. Let's call these candidate edges.

Next let's consider a password string. We can think of this as a path on the graph. We start at the first letter, and then continually move to the next letter in the string. For example, "abc0" corresponds to the path 'a' -> 'b' -> 'c' -> '0'. Therefore, the problem can be re-stated as follows: what is the length of the shortest path on this graph that includes all the candidate edges?

Here is it is helpful to think back to another classic algorithm problem: Eulerian paths. The problem could also be re-stated as: if we start with just the candidate edges, what is the minimum number of edges we need to add so that the resulting graph has an Eulerian path? Fortunately for us, the Eulerian path problem has been completely solved!

Fact: A directed graph has an Eulerian path if and only if (1) every vertex has in-degree equal to its out-degree except possibly for two vertices that are off by one, and (2) every vertex with positive degree is connected in the underlying undirected graph.

If you play with some examples, you will see that you will always be connected here, but let's come back to a formal proof when we talk about the large input. The fact that connectivity comes for free is really what makes this problem solvable, and so it is a good thing to think about!

The remaining condition says that all we need to do is add edges to balance in-degrees and out-degrees. We can add any edge we want at any time, so the method is simple: choose a vertex u with higher in-degree than out-degree and a vertex v with higher out-degree than in-degree, and then add an edge from u to v. Repeat until there are only two vertices left that are only slightly imbalanced, and we're done! After all that talk, we end up with a very simple greedy algorithm.

The Large Input

In fact, this solution already has most of the key ideas that go into solving the large as well, but we just need to take them further. There are three main challenges:

When passwords are length greater than 2, how do we interpret them as edges on a graph?
When it comes time to balancing degrees, some edges will be impossible to add. How do we adapt the greedy algorithm from before?
The output password could be huge! Is it really possible to solve the problem in time less than the size of the output password?
The first challenge is the most important, and again, De Bruijn sequences provide a good model to build from. If we are trying to construct all passwords of length k, we will create a graph with vertices corresponding to all lengh-(k-1) phrases. Each candidate password corresponds to the edge between its prefix phrase and its suffix phrase. Let's begin by making this nice and rigorous, although the intuition is exactly the same as in the small input.

A Minimum-Cost Flow Interpretation

Let us call any length-k substring of S (and any of its "l33tspeak" equivalents) a "candidate", and any length-(k-1) string composed of digits and lowercase letters a "phrase" (whether it is contained in a candidate or not). For each phrase s, define its weight w(s) to be the number of candidates that end with s minus the number of candidates that begin with s. Note that the sum of w(s) is 0. (The weight of a phrase will measure how far the corresponding vertex is from supporting an Eulerian path.)

Form a directed graph G with vertices corresponding to the phrases. For phrases a and b, add an edge from a to b if you can make a into b by adding a letter to the end of a and removing the first letter of a. Now we set up a flow problem: if a phrase s has positive weight, it is a source with capacity |w(s)|; otherwise it is a sink with capacity |w(s)|. All edges have infinite capacity and cost one.

Let ANSWER denote the shortest possible length of a password string for S, let C denote the set of all candidates, and let FLOW_EDGES denote the minimum number of edges (i.e. minimum cost) in an "almost-maximum" flow. (Specifically, this is a flow that leaves at most 1 capacity unused in some source and at most one 1 capacity unused in some sink).

Lemma 1: ANSWER = FLOW_EDGES + |C| + k - 1.

Proof Part 1: ANSWER ≤ FLOW_EDGES + |C| + k - 1

Let G' be the directed graph formed as follows:

Begin with the directed multi-graph formed by the minimum-cost almost-maximum flow on G.
For each candidate c, add an edge from the prefix phrase of c to the suffix phrase of c.
For a given phrase s, let's calculate its out-degree minus its in-degree in G'. After the first step, this value is exactly w(s), except for two vertices that are off by 1. (This is due to the fact that our flow is only almost maximum.) After the second step, this value becomes exactly 0, again except for two vertices that are off by 1.
Therefore, we know G' satisfies the condition on in-degrees and out-degrees for it to have an Eulerian path. (See the "Fact" in the Small Input discussion.) We now show it also satisfies the connectivity condition. This is where we use the specific nature of the problem and the fact that |S| ≥ 2k. Actually, we suspect the last condition is unnecessary in the end, but the proof becomes much harder if this is not true!

Let's say a vertex s is a core vertex if it corresponds to a phrase in S, or to a 133tspeak variation of such a phrase.

If s a core vertex, then s is adjacent in G' to its predecessor and successor phrases within S. (Note that there may be multiple predecessors or successors if the new letter has a leet variation or if S appears multiple times.) Therefore, we can certainly follow the edges of G' to go from s to some phrase a(s) that starts at the first letter of S. We can then walk from there to a phrase b(s) that ends at the last letter of S. Since a(s) and b(s) are completely disjoint, we can choose b(s) to be completely non-leet by always adding on non-leet successor letters. This means b(s) does not depend on s, and hence we have demonstrated every core vertex is connected to a single vertex in G'.
Now consider a non-core vertex t with positive degree in G'. This can only happen if t has positive degree in G, and therefore it must be connected via the flow to some core vertex s. Since we just showed all core vertices are connected, we conclude the non-core vertices are connected as well.
Therefore, the underlying undirected graph of G' is indeed connected, and hence G' has an Eulerian path consisting of some vertices s1, s2, ... sFLOW_EDGES + |C| + 1. We can now construct a password string as follows:

Begin the password string with the k - 1 letters in s1.
For each subsequent vertex si, append to the password string the one letter at the end of si that is not in si-1.
This string has length exactly k - 1 + FLOW_EDGES + |C|, as required. Notice that after appending the letter for si, the final k-1 letters in the password string are always precisely equal to si. (This invariant can easily be proven by induction.) Now consider an arbitrary candidate c. Because of how we constructed G', c has prefix si-1 and suffix si for some i. But then after appending the letter for si, the last k letters precisely spell out c. Hence, every candidate is in this password string, and the inequality is proven.
Proof Part 2: ANSWER ≥ FLOW_EDGES + |C| + k - 1

For the second part of the proof, we just need to reverse the previous argument. It is actually a little easier because we do not have to worry about connectivity.

Consider a password string P of length ANSWER. By definition, we know each candidate must appear somewhere in P. Therefore, for each candidate c, we can define pos(c) to the be the character in P where the first occurrence of c begins. Now let's order the candidates c1, c2, ..., c|C| by pos(c).

For each i, consider the substring of P starting with character pos(ci) + 1 and ending with character pos(ci+1) + k-2. Note that this substring begins with the suffix phrase of ci and ends with the prefix phrase of ci+1.

By reversing the construction from the previous section, we can interpret this substring as a path in the graph G from the suffix phrase of ci to the prefix phrase of ci+1. This path has pos(ci+1) - pos(ci) - 1 edges in it. Now let's think about what happens when we combine all these paths. We get a flow with a source at every suffix phrase except c|C| and a sink at every prefix phrase except c1. When we add all these sources and sinks together, we get precisely an almost max-flow on G. Furthermore, this flow uses exactly sumi [pos(ci+1) - pos(ci) - 1] = pos(c|C|) - pos(c1) - |C| + 1 edges.

It follows that FLOW_EDGES ≤ pos(c|C|) - pos(c1) - |C| + 1. Finally, we know pos(c|C|) ≤ |P| - k = ANSWER - k, and also pos(c1) ≥ 0. Therefore, ANSWER ≥ FLOW_EDGES + k + |C| - 1, as required.

A Greedy Solution

At this point, we still have not solved the whole problem, but we have reduced it to something more tractable. Minimum-cost flow is a complicated problem but it is well known, and at least it can be solved in polynomial time. We can either try to optimize here, or we can use more clever idea to make our life much simpler.

Lemma 2: Fix the graph G and assign arbitrary source and sink capacities to arbitrary nodes. Then, a minimum-cost almost-max flow can be achieved by repeatedly taking the shortest path from an unused source to an unused sink, and pushing flow along there (without ever reverting any flow that's already been pushed).

Proof: Let F denote a minimum-cost almost-max flow on G, and suppose the shortest path in G between a source and a sink goes from source u to sink x. Furthermore, suppose that F contains a path from u to a different sink y, and a path from a different source v to x. We claim that we could replace these two paths with a path from u to x and with a path from v to y to achieve a flow with no more edges. (Since F is only an almost-max flow, it might also be that u and/or x is simply not used in F, but that case is a lot simpler.) Recall that every edge in G has infinite capacity, so the only challenge here is making sure the path lengths work out.

Given two phrases p and q, let's define A(p, q) to be the longest string that is both a suffix of p and a prefix of q. Then the distance from p to q in G is precisely equal to k - 1 - |A(p, q)|. This means we can reformulate the claim as follows: given that |A(u, x)| ≥ max(|A(u, y)|, |A(v, x)|), prove that |A(u, x)| + |A(v, y)| ≥ |A(u, y)| + |A(v, x)|.

Now, notice that A(u, x) and A(u, y) are both suffixes of u, but A(u, x) is at least as long. Therefore, A(u, y) is a suffix of A(u, x). Similarly, A(v, x) is a prefix of A(u, x). Let t = |A(u, y)| + |A(v, x)| - |A(u, x)|. If t ≤ 0, the claim is trivial. Otherwise, there must be a length-t string z that is a suffix of A(v, x) and a prefix of A(u, y). Then z is also a suffix of v and a prefix of y. Therefore, |A(v, y)| ≥ |z| = t, and the claim is proven.

We have shown that F can be modified to contain the path from u to x without increasing the number of edges. Now consider the rest of F. It defines a smaller flow, and we can repeat the same argument to show this residual flow can also be modified to contain the shortest path between a remaining source and a remaining sink, and that this modification will not increase the number of edges. Applying this argument repeatedly gives us the flow proposed in the lemma without ever increasing the number of edges, and so we have shown that this flow is indeed optimal.

The Implementation

Almost there! We have outlined a greedy algorithm, but what does it actually mean when it is put back into the context of the original problem, and how can it be implemented quickly?

The first thing to do is to construct the prefix phrases and the suffix phrases of all candidates. We need to construct an almost max-flow from the suffixes to the prefixes. If a phrase is both a suffix and a prefix, then these two instances cancel out.
First we should look for a source u and a sink x that are separated by distance 1 in the underlying graph. This is equivalent to saying that there is a length k-2 string that is a suffix of u and a prefix of x.
Next we should look for a source u and a sink x that are separated by distance 2 in the underlying graph. This is equivalent to saying that there is a length k-3 string is a suffix of u and a prefix of x.
And so on...
This can be summarized as follows:

Let P = the multi-set of prefix phrases of all candidates.
Let S = the multi-set of suffix phrases of all candidates.
Let x = k + |C| and i = 0.
(*) While |P| ≥ 2 and |P intersect S| ≥ 1: delete one copy of a common element from both P and S and increment x by i.
Remove the last letter from every element in P and the first letter from every element in S.
Increment i by 1, and repeat again from (*) until P and S have just one element.
Output x.
Unfortunately, this is still a little too slow. It will run in time proportional to the output password string length, which could be 1018. The final optimization is that if you look at any element in P (or S) in the algorithm above, then all 133tspeak variations of that element should also be in P (or S). You can always treat all variations as one big batch:

Let P = the map from phrase to the number of times a 133tspeak variation of that phrase appears as the prefix of a candidate.
Define S similarly for suffixes.
Let x = k + |C| and i = 0.
(*) While P and S are non-empty:
While P and S have a common element t: delete min(P[t], S[t]) from P[t] and S[t] and increment x by i * min(P[t], S[t]).
Remove the last letter from every element in P and the first letter from every element in S.
Increment i by 1 and repeat again from (*) until P and S are empty.
Output x-i+1.

The Misof Implementation

Only one contestant solved this problem during the contest, and he used a variation on the method approached here. Instead of using the greedy algorithm, he implemented the flow directly, grouping together 133t variations in the same way that we did to achieve sub-linear running time in the password string size. It is a little slower and a little harder to implement, but it works just fine. Congratulations to misof for coming up with this!

World Finals 2012 - Code Jam 2012

Zombie Smash (7pts, 18pts)

Firstly, it is worth noting that the small dataset with only 8 zombies can be solved simply by evaluating each possible permutation for the order in which to smash zombies and keeping the best one. For each permutation, simply attempt to smash the zombies in the order given, skipping any zombies that cannot be reached on time. This simple approach has exponential time complexity and clearly will not scale for the large data set.

For the more efficient approach, let us start by considering the game state represented in an inefficient way and see how it can be made more efficient. We can represent the game state at any point in time with the following tuple:

(Time, Location, Set of zombies already smashed)
This certainly works - given this state, we essentially have the snapshot of the game at any point in time, but this is very verbose. Since we want to smash as many zombies as possible, we want to smash zombies as soon as possible, i.e. we want to arrive at the grave they pop out of as soon as possible. With that in mind, we can make the following assumption: we will arrive at the grave of the next zombie as soon as possible, and potentially stand around waiting until that zombie can be smashed. Once smashed, we will move to the next grave as quickly as we can and repeat the process. That way, it becomes unnecessary to keep track of any states where we are transiting between two graves because those states can be derived from the states at the origin and destination graves. We can change the state to be:
(Time last zombie was smashed, Last zombie smashed, Set of zombies already smashed)
This is an improvement, but now the problem is that we keep track of all possible sets of zombies already smashed. Let’s see what we can do about that.
Consider the state (T1, Z1, {Z1 …}) where we have just smashed zombie Z1 at time T1. The set of zombies already smashed contains Z1, and possibly a bunch of other zombies. Suppose that Z0 is a zombie that we have smashed earlier. There are two cases: either Z0 appears at an interval overlapping with Z1, or it has already appeared before Z1.

If Z0 has already appeared before Z1 then by T1 it can no longer be at its grave (even if we haven’t smashed it) and explicitly tracking that it has already been smashed is unnecessary.
Otherwise, if Z0 appears in an interval overlapping with Z1, is it possible that we will attempt to smash Z0 again, if we don’t keep track of it? Suppose that Z0 was smashed at T0, since the Zombie Smasher needs to recharge twice, Z0 will be already gone because it stands around for 1000ms and it takes 1500ms for the Zombie Smasher to recharge twice. Again, it is not necessary to explicitly track the set of zombies smashed to avoid smashing the same zombie twice.
In light of the above observations we can simplify the state to be:

(Time last zombie was smashed, Last zombie smashed, Number of zombies already smashed)
It is easy to see that we prefer earlier times over later times for smashing a zombie - the sooner we smash a zombie, the sooner we can move on to the next one, so we are only interested in states with minimal time possible. Let us model the state transitions as a graph and minimize on time.
The game starts at time 0 and location (0, 0). Based on this information, we can generate the initial frontier of zombies that can be reached and smashed on time. Given this frontier, the times and the locations at which zombies will appear, we can apply a modified Dijkstra’s Algorithm to find the set of game states that are reachable. Once we know those, we can simply return the maximum number of zombies smashed in a reachable state. Here is the pseudo-code:

solve():
  all_states = Q = generateStates()
  while Q is not empty:
    s = Q.popMin()
    if s.time = infinity:
      break;

    for each zombie z such that z ≠ s.zombie:
      earliest_arrival_time = s.time + max(750,
                                           dist(s.zombie, z))
      if earliest_arrival_time ≤ z.appearance + 1000:
        earliest_smash_time = max(z.appearance,
                                  earliest_arrival_time)
        Q.update(earliest_smash_time, z, s.smashed + 1)

  // Scan for states with time < infinity, keeping the maximum
  // number of zombies smashed to get the final answer.
  return best_reachable_state(all_states)

generateStates():
  states = {}
  states.Add(0, nil, 0) // Include the initial state.
  for each zombie z:
    for zombies_killed from 1 to Z:
      // For other reachable states this will be revised later.
      earliest_smash_time = infinity
      if zombiles_killed = 1:
        earliest_arrival_time = dist((0, 0), z)
        if earliest_arrival_time ≤ z.appearance + 1000:
          earliest_smash_time = max(z.appearance,
                                    earliest_arrival_time)
      states.Add(earliest_smash_time, z, zombies_killed)
  return states
Crude worst-case complexity analysis of the above: generateStates() will produce O(Z2) states as each element of the pair (Last zombie smashed, Number of zombies already smashed) can vary from 0 to Z independently. Each state will be iterated over at most once by the outer while loop of solve(), and the inner for loop of solve() will run over all zombies, costing another O(Z), assuming an efficient heap is used, giving O(Z3), which is fast enough for the large dataset where Z = 100. Lastly, a few contestants solved this problem with dynamic programming, keeping a 2D table with zombie index in one dimension and time since that zombie has popped up in the other dimension, maximizing on the total number of zombies smashed.

World Finals 2012 - Code Jam 2012

Upstairs/Downstairs (13pts, 17pts)

Upstairs/Downstairs was a last-minute addition to the Finals, replacing a year-old problem proposal that had appeared on a separate contest two months before. This problem loosely mirrors an experience the author had once while on vacation. He was downstairs.

Solving this problem involves two observations and an algorithm. Before making our observations, let's start by writing out the formula for the quantity we want to minimize. pi will represent the probability that the ith activity Konstantin performs will result in Ilia being awake:
P(woken up) =
(1-p0) * (1 - (1-p1)(1-p2)*...* (1 - pK)) +
p0(1-p1) * (1 - (1-p2)(1-p3)*...*(1 - pK)) +
p0p1(1-p2) * (1 - (1-p3)(1-p4)*...*(1 - pK)) +
...

Observation 1: Noisy First
For our first observation, we'll look for a reason to prefer that Konstantin perform noisier or quieter activities earlier. Intuitively, we suspect that noisier activities should come first: a strategy that keeps Ilia awake and then tries to keep him asleep seems pretty reasonable.

Looking at the structure of the formula above, we can look for differences between how pi and pi+1 are used. Here are the terms in which swapping them would lead to a difference in the final result:
p0p1...pi-1(1-pi) * (1 - (1-pi+1)(1-pi+2)*...*(1 - pK)) +
p0p1...pi-1pi(1-pi+1) * (1 - (1-pi+2)*...*(1 - pK))
Simplifying these two terms, we have:
p0p1...pi-1 * [
  (1-pi) + pi(1-pi+1) -
  (1-pi)(1-pi+1)...(1-pK) -
  pi(1-pi+1)...(1-pK)]
= p0p1...pi-1 * [1 - pipi+1 - (1 - pi+1)(1 - pi+2)...(1 - pK)]
This quantity is minimized by choosing pi+1 < pi, which confirms the intuition that noisier activities should happen earlier. Repeatedly swapping adjacent activities like this shows that in an optimal solution, activities should be performed from noisiest to quietest.

Observation 2: Go Extreme!
Our original formula for P(woken up) is clearly linear in pi for all i. This quantity therefore must be minimized by taking either the largest possible value or the smallest possible value for pi.

Putting this observation together with the previous one, we can conclude that Konstantin should start by performing the K-q noisiest activities in order from noisiest to quietest, and then should perform the q quietest activities, also in order from noisiest to quietest. We'll call the first set of activities the prefix, and the others the suffix. Note that here we're considering an activity that can be repeated c times as c different activities.

Possible Algorithms
For the Small, it's sufficient to try all values of q. There are O(K) possible values, and evaluating each one naively takes O(sum(ci)) time, so the running time is O(sum(ci)2).

There are only two numbers we really need to keep track of for each possible suffix: the probability that Ilia will end up being woken up if he starts the suffix awake, and the probability that he will end up being woken up if he starts the suffix asleep. After precomputing those 2 values for each of the K possible suffixes, which we can do in O(K) time, we can simulate each possible prefix, also in O(K) time, and do a quick lookup for the corresponding suffix for each one. A little math produces the correct answer, in O(sum(ci)) time.

Another algorithm that boils down to the same thing involves treating Ilia's three states—awake, asleep, and woken up—as a 3x1 vector that can be operated on by matrices. For any activity, we can build a 3x3 transition matrix, looking like this:
[[p    0    0]
 [1-p  1-p  0]
 [0    p    1]]
Ilia's initial state is:
[1
 0
 0]
To build the transition matrix for a series of activities, we can multiply the activities' matrices together, with the noisiest activity on the right. In this way, we can compute a 3x3 matrix corresponding to each prefix of length up to K, and one for each suffix of length up to K, in O(K) time. Then it's an O(1) operation to check the result for any given prefix/suffix pair: multiplying those two matrices by each other, then by Ilia's initial state. The algorithm ends up taking O(sum(ci)) time.
Note that the bolded entries of the transition matrix, above, correspond to the two numbers we cared about for each suffix in the previous algorithm.

Ternary Search Passed Test Cases, But...
Because this problem was prepared at the last minute, it didn't occur to us until during the contest to wonder whether ternary search would work for selecting q, the length of the suffix. We would have been happy with the answer either way, but we would have wanted test data that would break ternary search if the answer turned out to be "no".

It turned out that the answer was "no", and that none of our existing cases broke any ternary search that we found. Because of this, some contestants—most notably misof, who came third partly on the strength of the points he got from this problem—managed to submit ternary search solutions that passed our test data.

Sometimes it happens in Code Jam that contestants will come up with algorithms that don't work in general given the limits we've provided, but do work on our test data. We try to avoid situations of that sort, but they do happen. We think the consequences here weren't significant, for a few reasons: Implementing the ternary search is about as hard as implementing a correct solution, if not a little harder; the "hard part" of the problem was already done by that point; and if we'd come up with a breaking case, we'd have put it in the Small data as well as the Large. Contestants who implemented ternary search would have seen it was wrong in the Small, and taken the time to fix it. misof had plenty of time separating him from the fourth-place contestant, so the top three likely would not have changed.

Why Ternary Search Fails
Ternary search seems like it should work. Indeed, the function that we're minimizing, P(woken up), is strictly non-increasing and then strictly non-decreasing in q. Unfortunately, that isn't quite enough: it needs to be strictly decreasing and then strictly increasing; otherwise a large constant patch will leave the ternary search unable to tell which direction it should go in.

Here's an example case that breaks ternary search in principle, and breaks a few contestants' submissions in practice:
2
2 200
1/2 40
1/100 400
2 200
1/2 40
99/100 400

The correct answer is:
Case #1: 0.863976521
Case #2: 0.863976521

In the first case, the "1/2" activities are best not performed; but because they are at the very start of the list of activities, outnumbered severely by the "1/100" activities, a standard ternary search will find no difference between the two suffix lengths it tries. It will merely consider a different number of "1/100" activities to be part of the suffix as opposed to the prefix, and leave all the "1/2" activities in place. The second case is identical, but with "99/100" activities instead of "1/100" activities, which should defeat a ternary search that happens to choose the right direction in the first case.

Paweł i Gaweł
At dinner after the finals, several Polish contestants were shocked to discover that the problem was not based on a poem called Paweł i Gaweł. The similarity was entirely coincidental, as the contestants should have known: Gaweł, who (according to Google Translate's translation of that page) "invented the wildest frolics", lived downstairs. Still, in retrospect, we wish we'd named the problem after that poem, and perhaps had Gaweł minimize the probability that Paweł would go fishing.

World Finals 2012 - Code Jam 2012

Xeno-archaeology (12pts, 33pts)

To begin with, let's try to formalize the rules of forming the pattern of tiles. If the center is at some position x, y, then the red tiles are those in positions x', y' for which the number max(|x - x'|, |y - y'|) is odd, while the blue tiles are those for which this number is even. This is because the formula max(|x - x'|, |y - y'|) = C for any C describes a square ring around x, y, and the rings alternate color with the parity of C

For the small problem, we can prove that if there exists a solution, there exists one with |X| + |Y| < 202. Thus, we can check all candidates for the center, for each one check whether all the tiles have the right colors, and output the best candidate. This will not run in time for the large data, of course, as we will have over 1030 candidate centers to check.

Single tile analysis

We may now assume we know the parity of x and y. We will simply check all four possibilities, finding the best possible choice of the center for each of the four assumptions, and then pick the one specified by the tie-breaking rules (or output "Too damaged" if none of the four assumptions led to finding a viable center for the pattern). This makes it easier to analyse the information gained by knowing the color of a single tile. Suppose the tile at some position x', y' is, say, red. This means max(|x - x'|, |y - y'|) has to be odd. Now, we know the parities of x, y, x' and y', and so:

if both x - x' and y - y' are odd, then any choice of a center (satisfying the parity requirements for x and y) is going to fit our knowledge;
if both x - x' and y - y' are even, then there is no solution satifying the parity requirements;
if, say, x - x' is odd, while y - y' is even, we have to have |x - x'| ≥ |y - y'|
If there is any tile of the second type, we can immediately return "Too damaged" for these parity assumptions. We can ignore of tiles of the first type, and now we are left only with tiles of the second type.
Note that in the third case, since the parities of x - x' and y - y' differ, it doesn't matter whether we use a strict inequality, as the equality case is eliminated from consideration by the parity assumptions. Thus, when considering regions defined by these inequalities, we can ignore problems related to "what happens on the edges of these regions", as - by the reasoning above - the edges will necessarily be eliminated from consideration by the parity assumptions.

The first and second cases are easy to analyse; the trick is to find out whether a solution exists (and if yes, find the best one) satisfying the set of conditions of the type |x - x'| ≥/≤ |y - y'| for various x' and y'. Transforming the condition |x - x'| ≥ |y - y'| we see it is equivalent to saying that one of the following has to hold:

x + y ≥ x' + y' and x - y ≥ x' - y', or
x + y ≤ x' + y' and x - y ≤ x' - y'.

Dividing the plane

The lines x + y = xi + yi and x - y = xi - yi (which are the boundaries of the constraint-satisfaction region for the input tiles) divide the plane into at most (N + 1)2 rectangles. The idea of our algorithm will be as follows:

Iterate over the four sets of parity assumptions about the center
Iterate over all rectangles formed by the boundary lines, and for each of them check whether it satisfies the constraints posed by all input tiles
for each rectangle satisfying the constraints, find the best (according to the tie-resolution conditions) center candidate within it (if any)
output the best of all center candidates found.
A fun fact is that there will be at most N+1 rectangles that satisfy the constraints; so we need not worry overly about the performance of the "find the optimal within the rectangle" phase (as long as it is independent of the size of the rectangle). The naive approach to the second phase is O(N3) (for each rectangle check all tiles), which with N up to a thousand and 50 testcases risks being too slow, so we'll need to speed it up a bit.
There are many ways to trim down the runtime of the constraint-checking phase for rectangles. One sample way is to process the rectangles "row-by-row", as follows: Take the set of rectangles with A ≤ x+y ≤ B, with A and B being some two adjacent boundary values. For each input tile (out of those that set any constraints on the center position), we have two areas of constraint satisfaction; but only one of them is compatible with A ≤ x+y ≤ B, because one of the areas satisfies the constraint x+y ≥ C, while the other has x+y ≤ C. This means that we know which area is the interesting one for this row; so we obtain a constraint on x - y that has to be satisfied by all the rectangles in this row. This will be either of the form x - y ≤ D, or x - y ≥ D. We take the largest of the lower bounds, the largest of the upper bounds, and obtain a rectangle that we have to check. This algorithm runs in O(N2) time, which will be easily fast enough.

A more advanced algorithm (using the sweep line technique) can be used to obtain a runtime of O(N logN) runtime. We will not describe it (as it is not necessary to obtain a fast enough program with the constraints given), but we encourage the reader to figure it out.

Finding the best point within a rectangle

This was the part of the problem that seemed to cause most difficulties for our contestants. There are two cases to consider here. Let's assume our rectangle is defined by A ≤ x+y ≤ B and C ≤ x-y ≤ D.

Let us define
g(k, l) = min(|k|, |l|) if k and l are of the same sign, 0 otherwise.
If g(A, B) = 0 and g(C, D) = 0, then the point (0, 0) is within our rectangle. In this case it suffices to check the near vicinity of the origin. Specifically:

If both x and y are supposed to be even, (0, 0) is obviously the optimal solution.
If both x and y are supposed to be odd, then the best four points, in order, are (1, 1), (1, -1), (-1, 1) and (-1, -1). If B ≥ 2 we can take (1, 1) and we're done. Otherwise, if D ≥ 2, we take (1, -1); and so on. If all the four points are infeasible, the rectangle contains no points satisfying the parity constraints.
If , say, x is supposed to be odd, while y is even, the first eight candidates are (1, 0), (-1, 0), (3, 0), (1, 2), (1, -2), (-1, 2), (-1, -2), (-3, 0). Again, one can check that if none of them is feasible, the rectangle contains no points satisfying the parity constraints. The same happens when x is even and y is odd.
Thus, if (0, 0) is within the rectangle, we can check a constant number of points and take the best feasible one of them.
When (0, 0) is not within the rectangle, we first look for the smallest Manhattan distance of any point within the rectangle. It is equal to M := max(g(A, B), g(C, D)). As all the boundaries have parities disagreeing with the parity assumptions, the smallest Manhattan distance we can hope for is M + 1. We now have an interval of points with Manhattan distance M + 1 in our rectangle, the best one of them is the one with the highest X coordinate (out of the ones fulfilling the parity conditions). The one last special case to deal with here is when the interval contains only one point, and it has the wrong parities - in this case we need to look at distance M + 3 (the fact that one never needs to look at M + 5 is left as an exercise).

It was also possible to solve this problem in a number of other ways. A pretty standard one was to identify a number of "suspicious points" within a rectangle (the vicinity of (0, 0), the vicinity of the corners, and the vicinity of places where the coordinate axes intersect the edges of the rectangle) and check them all, taking the best solution.

World Finals 2012 - Code Jam 2012

Twirling Towards Freedom (10pts, 39pts)

The Small Input

On most Google Code Jam problems, the small input can be solved without worrying about the running time. That was not the case here though. Even if N = 10 and M = 10, there are still 1010 different rotation patterns you could try. That is a lot!

There are various ways you could try to bring the running time down to a more manageable amount, but here is one fact that makes it easy:

The 1st star you rotate around, the 5th star you rotate around, and the 9th star you rotate around should all be the same.
The 2nd star you rotate around, the 6th star you rotate around, and the 10th star you rotate around should all be the same.
The 3rd star you rotate around and the 7th star you rotate around should be the same.
The 4th star you rotate around and the 8th star you rotate around should be the same.
If you realize this, there are only 104 possibilities to try. But why is this fact true? Read on and you'll find out!

Understanding Rotations

One of the biggest challenges here is just wrapping your head around the problem. How can you intuitively understand what it means to rotate something 100,000 times? The best way to do this is to roll up your sleeves and write down a couple formulas.

When dealing with rotations and translations of points on a plane, complex numbers provide an excellent notation:

A point (x, y) can be represented as x + iy.
Complex numbers can be added and subtracted just like vectors.
Complex numbers can be rotated clockwise 90 degrees about the origin simply by multiplying them by -i.
It is this last property that makes them so clean to work with, and it is what we will use for the analysis. If you prefer, you could also imagine replacing everything with matrices.
We know that rotating a point P0 by 90 degrees about the origin will send it to -i * P0. However, what happens if we rotate it about a different point Q0? There is a standard formula for this situation that you may have already seen. The resulting point P1 satisfies the following:

P1 - Q0 = -i * (P0 - Q0)
This is our previous formula applied to the fact that rotating P0 - Q0 by 90 degrees about the origin must give you P1 - Q0. (Do you see why that is true? It's really just a coordinate change.) In our case, it will be helpful to group things a little differently in the formula:

P1 = -iP0 + Q0 * (1 - i)
Now, suppose we rotate P1 by 90 degrees about another point Q1, then by 90 degrees about another point Q2, and so on. What would happen to this formula? Let's write out a few examples:

P2 = -iP1 + Q1 * (1 - i) = -P0 + Q1 * (1 - i) + Q0 * (-1 - i).
P3 = -iP2 + Q2 * (1 - i) = iP0 + Q2 * (1 - i) + Q1 * (-1 - i) + Q0 * (-1 + i).
P4 = -iP3 + Q3 * (1 - i) = P0 + Q3 * (1 - i) + Q2 * (-1 - i) + Q1 * (-1 + i) + Q0 * (1 + i).
P5 = -iP4 + Q4 * (1 - i) = -iP0 + (Q4 + Q0) * (1 - i) + Q3 * (-1 - i) + Q2 * (-1 + i) + Q1 * (1 + i).
etc.
From here, it's not too hard to guess the general formula. If the origin is rotated by 90 degrees about Q0, then Q1, and so on, all the way through Qm-1, then the final resulting point Pm is given by:

(1 - i) * (Qm-1 + Qm-5 + Qm-9 + ...) + (-1 - i) * (Qm-2 + Qm-6 + Qm-10 + ...) + (-1 + i) * (Qm-3 + Qm-7 + Qm-11 + ...) + (1 + i) * (Qm-4 + Qm-8 + Qm-12 + ...)
And once the formula is written down, it's not too hard to check that it always works. So anyway, is this simpler than the original problem formulation? It may look pretty complicated, but for the most part, you're now just adding points together here, and addition is easier than rotation!

Pick a Direction

We want to choose stars Q0, Q1, ..., Qm-1 in such a way as to make the following point as far away from the origin as possible:

(1 - i) * (Qm-1 + Qm-5 + Qm-9 + ...) + (-1 - i) * (Qm-2 + Qm-6 + Qm-10 + ...) + (-1 + i) * (Qm-3 + Qm-7 + Qm-11 + ...) + (1 + i) * (Qm-4 + Qm-8 + Qm-12 + ...)
There are different ways of thinking about things from here, but the key is always convex hulls.

Let X be the point furthest from the origin that we can attain, and more generally, let Xv be the point that is furthest in the direction of v that we can attain. Certainly, X = Xv for some v. (This is because X is surely the point that is furthest in the direction of X.) Therefore, it suffices to calculate Xv for all v, and we can then measure which of these is furthest from the origin to get our final answer.

So how do we calculate Xv for some given v? We want (1 - i) * Qm-1 to be as far as possible in the v direction, or equivalently, we want Qm-1 to be the star that is furthest in the (1 + i)*v direction. Of course, the same thing is true for Qm-5, Qm-9, etc. We should choose the same star for all of them. (This is the fact that we used for the small input solution!) Similarly, for Qm-2, Qm-6, etc., we want to choose the star is furthest in the (-1 + i)*v direction. In general, we want to choose stars from the original set that are as far as possible in the following directions: (1 + i)*v, (-1 + i)*v, (-1 - i)*v, (1 - i)*v.

We are now almost done (at least conceptually). First we find the convex hull of the stars and solve for one particular v. Now what happens when we rotate v? For a while, nothing will change, but eventually, one of the four directions we are trying to optimize will be perpendicular to an edge of the convex hull, and as a result, the optimal star will switch to the next point on the convex hull. This simulation can be done in constant time at each step, and there will be only O(N) steps since each star choice will rotate once around the convex hull.

At this point, we are done! We have found every Xv, and we can manually check each of them to see which one is best. The implementation here is tricky, but it is an example of a general technique called rotating calipers. You can find plenty more examples of it online. Or you can always download solutions from our finalists!

Bonus Question!

When we generated test data for this problem, we were surprised to find that we could not come up with a case to break solutions which misread "clockwise" as "counter-clockwise" in the problem statement. In fact, there is no such case! Can you see why?

World Finals 2012 - Code Jam 2012

Shifting Paths (5pts, 46pts)

For the small dataset, we can directly simulate the process of walking through the forest. How many steps can this take? There are 210 states the trees in the clearings can be in, and 10 possible states for the clearing we are standing in. So if we reach the final clearing, it can't take more than 10 × 210 steps, and if we take that many steps without reaching the final clearing we know the answer is Infinity.

For the large dataset, 40 × 240 steps is too many to simulate directly, but can an input case approach this many?

A test case can be designed to make the clearings simulate an N-1 bit counter. The first clearing has both paths leading to the second clearing. Each clearing after that forms a chain where one path leads to the next clearing, and one path leads back to the first clearing. Whenever the trail leads back to the first clearing, the states of the clearings give the next N-1 bit number, and after all of those numbers have been produced, it can reach the final clearing. This will take at least 239 steps.

So we need a solution that will not simulate every step individually. In the previous example, we spend 221-2 steps in the first 20 clearings between each time we visit any of the last 20 clearings. If our program could detect that this always happens, it could simulate those 221-2 steps without having to do them all individually. We will only take 220-3 steps from any of the second twenty clearings, so the total runtime would be reasonable.

An implementation of this approach is to split the clearings into two sets A and B of roughly equal size. We then use dynamic programming to compute, for each location in A and each of the 2|A| states of the clearings in A, how many steps it will take to leave A, which clearing in B we will be leaving towards, and what state the clearings in A will be left in.

After this DP is done, our solution will take an amount of time proportional to the number of steps we take from clearings in B -- for each of those, we simulate that step, and if that takes us to a clearing in A, we look up the appropriate result of the DP from a table. So to make this solution efficient, we need to choose A and B so that only a small number of steps are taken from clearings in B.

Find the distance from each clearing to the final clearing, where distance is defined as the number of paths you need to take, assuming the state of the clearings is optimal. If the final clearing cannot be reached from some clearings, put those in a separate set. If we ever reach one of those, the answer is Infinity.

Choose B to be the closest half of the remaining clearings. There can be at most B × 2|B| steps taken from clearings in B, because we cannot repeat a state of B without reaching the final clearing. This can be only at most 20 × 220, so this solution is sufficiently fast.

Qualification Round 2013 - Code Jam 2013

Tic-Tac-Toe-Tomek (10pts, 20pts)

In this problem, you had to classify the state of a Tic-Tac-Toe game with a twist. The board is 4x4, and an extra symbol can appear on the board - a "T", which either player can use for victory.

Note that you are guaranteed by the problem description the input will always describe a board that was obtained by a correct sequence of moves. As the game ends when one player wins, this guarantees that only one player can have four symbols (or 3 symbols and a T) in a completed line.

Thus, the simplest way to check whether a player, say "X", won is to check all rows, columns and diagonals whether they contain only "T"s and "X"s. Do not forget to check both diagonals! If there is a row, column or diagonal containing no "."s or "O"s, we know that X won. Similarly, if there is a row, column or diagonal contianing no "X"s or "."s, O won.

If none of the players won, we only have to distinguish between a draw and a game not completed. This is relatively simple - if the board contains even a single ".", the game has not completed yet; otherwise it's a draw.

Note that your solutions are checked automatically, by a program. This means that your output has to be exactly matching the specification. A number of contestants had problems due to returning "O Won" instead of "O won" or "The game has not been completed" instead of "Game has not completed". In a programming competition it is important to follow the specification of the output as exactly as possible.

Here is a complete solution in Python for reference:

import sys

def solve(b):
  for c in ['X', 'O']:
    wind1 = True
    wind2 = True
    for x in range(4):
      winh = True
      winv = True
      for y in range(4):
        if b[y][x]!=c and b[y][x]!='T': winv = False
        if b[x][y]!=c and b[x][y]!='T': winh = False
      if winh or winv: return c + ' won'
      if b[x][x]!=c and b[x][x]!='T': wind1 = False
      if b[3-x][x]!=c and b[3-x][x]!='T': wind2 = False
    if wind1 or wind2: return c + ' won'

  for x in range(4):
    for y in range(4):
      if b[y][x]=='.': return 'Game has not completed'

  return 'Draw'

numcases = int(sys.stdin.readline())
for casenum in range(1,numcases+1):
  board = []
  for i in range(0,5):
    board.append(sys.stdin.readline().strip())
  print 'Case #' + repr(casenum) + ': ' + solve(board)

Qualification Round 2013 - Code Jam 2013

Lawnmower (10pts, 30pts)

The Small Input
For a problem like this, it can be helpful to think through a few cases. Let's look at the first two examples from the problem statement:

   2 2 2 2 2
   2 1 1 1 2    2 1 2
   2 1 2 1 2    1 1 1
   2 1 1 1 2    2 1 2
   2 2 2 2 2
All the grass needs to be cut to either height 1 or 2. Thus, we can begin by cutting the whole lawn to height 2. The question that remains is which rows and which columns to cut to height 1. Note that if we cut a row (or column) to height 1, all the squares in this row or column will be height 1 in the final pattern since we can't grow grass back.

In the left example, we must at some point do a cut with the lawnmower at height 1. Otherwise, we would never get any grass that low. However, there is nowhere that it is safe to make a cut like that. Every row and every column has at least one square where we want the final grass height to be 2, so we can never run the lawnmower through that row or column while at height 1. This pattern is impossible.

In the second example, we also must do some cuts with the lawnmower at height 1. However, in this case, there are two places where we can safely make that cut: the middle row and the middle column. If we do both, we get the desired pattern.

More generally, there are some rows and columns we cannot cut to height 1. By avoiding those rows and columns, we ensure nothing will be made too low. What remains is to check if it is still possible to get all the grass low enough. Well, if our only goal is to get the grass low, we should do all the cuts we can!

This suggests the following approach:

Determine which rows and columns it is safe to cut at height 1 (meaning the pattern has no square with height > 1 in that row or column).
Do a cut on each of these rows and columns at height 1.
Check if we got every square low enough. If so, the pattern is possible. Otherwise, it is not.
The Large Input
For the large input, we can use almost the exact same strategy. You just have to think through what that means!

We can cut any row or column at a height that is equal to the maximum height appearing in this row (or column). As long as we follow this rule, we will never cut a square too low, and then as above, we just need to try to get everything low enough. For that purpose, we want to use all the cuts we can. The full algorithm is then:

Iterate over every row and find the largest height that the pattern has in this row. Cut the row at this height.
Do the same thing for every column.
Output "YES" if this achieved the desired pattern, and "NO" if not.

Qualification Round 2013 - Code Jam 2013

Treasure (20pts, 60pts)

Lexicographically Smallest?
This problem asks you to open the chests in the lexicographically smallest order. As if it's not hard enough to find just any old way to open the chests, we make you find a particular way! If you have not tried similar problems before, this part might seem especially daunting.

However, it's a red herring. Suppose you can answer the following simpler question: "Is it possible at all to open all the chests?". Then you can use that as a black box to find the lexicographically smallest way of opening them. Check whether you have the key to open chest #1 first, and whether the black box says it is possible to open all the remaining chests after doing so. If the answer to both questions is yes, then you should definitely start by opening chest #1. Otherwise, you have no choice but to try a different chest. Repeating this logic for every chest at every time, you can get the lexicographically smallest solution.

This is not very fast, but there aren't too many chests to worry about here, so that's okay. And that means, from now on, we can focus on the slightly simpler question: "Is it possible to open all the chests?".

Eulerian Path
Unfortunately, even this question is still pretty hard!

When dealing with a really tricky problem, it can sometimes be helpful to look at special cases to build up your intuition. Towards that end, let's suppose that you begin with exactly one key, that one chest is empty, and that all remaining chests also contain exactly one key. Why look at this case in particular? You'll see!

In this version of the problem, you will always have exactly one key at any given time. When you open a chest of type A containing a key of type B, you are switching from a key of type A to a key of type B. This suggests that we can represent the problem as a graph. We will have one vertex for each chest/key type, and for every chest, we will add a directed edge from the chest type to the contained key type.

You begin with a single key, and then you must choose a chest of the matching type, giving you a key of perhaps a different type. From there, you must choose a chest of the new type, and so on, eventually choosing every chest exactly once. In the graph formulation, you can think of this as beginning at the vertex corresponding to your starting key, and then repeatedly following edges, using each edge exactly once. In other words, you are looking for an Eulerian Path on the graph!

The good news here is that Eulerian Path is a famous problem and you can look up on the internet how to tell if one exists:

At most one vertex (namely the start vertex) has OutDegree - InDegree = 1.
At most one vertex has InDegree - OutDegree = 1.
All other vertices have InDegree = OutDegree.
There is a path from the start vertex to every other vertex in the graph.
The bad news is that if this special case is already as hard as Eulerian Path, the full problem is going to be even harder!

Generalizing the Eulerian Path
If you come up with the previous observation, the first thing you might try is to reduce the full problem directly to Eulerian paths. Unfortunately, this is probably doomed to fail. Once you have multiple keys in a single chest, there is no graph structure to use. (Nothing we have found anyway!)

The better plan is to generalize only the conditions required for an Eulerian path to exist. In fact, those conditions can be described very naturally in terms of chests and keys:

For each type, there must be at least as many keys of that type as there are chests of that type.
It must be possible to get at least one key of any single type.
Let's say a chest/key configuration is connected if it satisfies the second condition, and good if it satisfies both conditions. It turns out that it is possible to open all the chests if and only if the configuration is good!

And it is not too hard to check if a configuration is good - the first condition is just a count, and the second can be checked with a Breadth First Search or a Depth First Search. So all that remains for a complete solution is convincing ourselves that checking goodness is indeed equivalent to the original problem:

Claim: It is possible to open all the chests if and only if the configuration is good.

Proof: One direction is easy. If the configuration is not good, then we will never be able to open enough chests of one type, either because there are not enough keys, or because we can never reach even one of those keys.

For the other direction, let's suppose we have a good configuration. Nothing we do from this point on will change whether there are enough keys of each type. We will show there is always at least one chest we can open that also maintains the connectivity property. The resulting configuration would then also be good, so there would also be a chest we could open there that would maintain connectivity, and so on. Repeating, we can keep opening chests until there is nothing left.

So all we need to do is prove that there is at least one chest that can be opened without breaking connectivity. We know the configuration is connected initially. For each type T, there is a sequence of chests we can open to get a key of type T. To be precise, here exists a sequence of types T1, T2, ..., Tn, with the following properties:

You already have at least one key of type T1.
Tn = T.
For each i, there is a chest of type Ti which you would be able to open to get a key of type Ti+1.
Suppose you have a key of some arbitrary type A. If you already have all keys of type A, then you can open all the chests of type A, and doing so will certainly not break connectivity. So that case is easy.

Otherwise, there must exist some chest of type B containing a key of type A. Let T1, T2, ..., Tn-1=B, Tn=A denote a sequence of key types that you can go through to get a key of type B and then use that to get another key of type A. As mentioned above, we know such a sequence must exist. We can also assume that Ti != A for 1 < i < n. Otherwise, we could just chop off part of the sequence to get a faster method! Now we consider two cases:

Suppose that T1 != A. Then you can use your key of type A to open anything you want, and we claim the resulting configuration will still be connected.

To prove this, pick an arbitrary key type C (possibly equal to A). Before opening any chests, we know there is some sequence of key types S1, S2, ..., Sm=C that will give you a key of type C. If A is not part of this list, then opening a chest of type A does not interfere with getting a key of type C. Otherwise, the S list and T list intersect somewhere, so we can let j be the largest integer such that Sj equals some Ti. Then after using our A key, we can still get a key of type C in the following way: T1, T2, ..., Ti=Sj, Sj+1, ... Sm=C.

Since this is true for every C, we know the configuration is still connected!

Suppose that T1 = A. In this case, you should use your key to open a chest of type T2. You can now still obtain a key of type B, and so the rest of the argument follows exactly as before.
Therefore, no matter what happens, you can always open a chest without breaking connectivity, and the claim is proven!

Qualification Round 2013 - Code Jam 2013

Fair and Square (10pts, 35pts, 55pts)

The first thing to do in this problem (as in many other problems) is to make sure you read it carefully. Many contestants thought that 676 should be a fair and square number - after all, it is a square and a palindrome. It is not, however, a square of a palindrome, and this example is actually mentioned specifically in the problem statement!

The small input
Once you realize this, you can approach the small testcase by iterating over all the numbers Little John considers, and checking each one of them. You have to check for each number X whether it is a palindrome, and whether it is the square of a palindrome.

To check whether X is a palindrome, you can simply convert it to string format (the details here depend on the programming language you are using) and compare the first character to the last, the second to the second last, and so on.

To check whether X is the square of a palindrome, there are multiple options. One is to calculate the square root, and if the square root is an integer, check if it is a palindrome as described above. Another is simply to iterate over all numbers up to X, and for each palindrome, square it and see if the square is X. That's a perfectly good solution for the small input, but it will be too slow for the larger ones.

The first large input
For the first large input, we need to deal with numbers up to 1014, and also with 10,000 test cases. A linear search of all numbers up to 1014 is not going to be fast enough, so we have to be smarter - we can't afford to check each number in the interval individually.

We don't really need to go all the way up to 1014 though! We are interested in numbers whose squares are Fair and Square and between A and B - and that means we have to check up to the square root of B only. That's only 107 numbers to check in the worst case.

We are not done though. While 107 numbers can be processed within the time limit, processing 10,000 cases like this is somewhat risky. There are two tricks you can notice to make your solution faster.

One trick is that we are interested not in all the numbers up to 107, but only in palindromes. We can generate all the palindromes much faster. Start by taking all the numbers up to 104, and then taking their mirror reflections (either duplicating the last number or not) to generate all palindromes of length up to 8 (and then square each and check whether it is a Fair and Square number in the interesting interval). This will cause us to evaluate only around 10,000 numbers for each test case, which is small enough that even a slow machine can deal with all the test cases in four minutes. You would need to use a reasonably efficient language however.

An alternative is to simply generate all the fair and square numbers up to 1014 before processing the test cases. There are relatively few of them — it turns out only 39. Thus, if you find all of them (in any fashion) before downloading the input file, you can easily give the correct answers to all the input cases.

Note that if you do this, you have to include the code you used to generate Fair and Square numbers - not just the code that includes the full list!

The second large data set
Now we come to the largest data set. Even combining both the tricks above is not enough - we need to go over 1025 palindromes to precompute everything. This will take a very long time in any language on any computer! A good idea here is to generate the first few Fair and Square numbers (and their square roots) to try and get an idea of what they look like. There are two things you can notice:

All the Fair and Square numbers have an odd number of digits
All the digits are rather small. In particular, with one exception, every square root of a Fair and Square number consists only of digits 0, 1 and 2.
Let's try to understand why these things would be true.

Let's begin with the "odd number of digits". A square of a number with N digits will have either 2N - 1 or 2N digits, depending on whether there is a carry on the last position. Let's try to prove a carry never happens. Let X be Fair and Square, and let its square root be Y. Let the first digit of Y be c - then the first two digits of X are between c2 and (c+1)2. In particular:

If the first digit of Y is 1, the first digit of X is between 1 and 4 - and thus no carry.
If the first digit of Y is 2, the first digit of X is between 4 and 9 - and thus no carry.
If the first digit of Y is 3, the first digits of X are between 9 and 16, so the first digit is 9 or 1. As Y is a palindrome, the last digit of Y is 3 as well, and thus the last digit of X is 9 - meaning the first digit of X is 9 as well, meaning no carry.
If the first and last digit of Y is 4, the last digit of X is 6, while the first is either 1 or 2 - so X can't be Fair and Square.
Similarly, if the first and last digit of Y is 5 (last digit of X is 5, first is 2 or 3), 6 (last digit of X is 6, first is 3 or 4), 7 (last digit of X is 9, first is 4, 5 or 6), 8 (last digit of X is 4, first is 6, 7 or 8) and 9 (last digit of X is 1, first is 8 or 9), then X also can't be Fair and Square.
This means there is no carry in the first digit.

Now since all the digits seem so small, maybe this means there is no carry at all? Note that if you take a palindrome and square it, and there's no carry, the result is a palindrome as well - so that would give us a nice characterization of Fair and Square numbers. Indeed, it turns out to be the case, and the proof follows.

Let Y have digits (ad)(ad-1)...(a0). Let bk = a0 * ak + a1 * ak-1 + ... + ak * a0. Note that bi is exactly the ith digit of X = Y2 when performing long multiplication, before carries are performed. Since aj = ad-j, we also have bi = b2d-i.
Now suppose there's a carry in the long multiplication (meaning some bj is greater than 9), and that we take a carry into digit i but no larger digits. We know digits i and 2d-i in X are equal, and are equal to bi plus whatever we carried into digit i. Since we carry nothing to digit i+1, bi is no larger than 9.
Now we will see that digit 2d-i of X has to equal b2d-i (which is equal to bi, and thus no larger than 9). For it to be different, we would have to carry something into digit 2d-i - but this would mean that bj is larger than 9 for some j < 2d-i, and hence b2d-j is also greater than 9 and we would have a carry after digit i.
Since X is a palindrome, this tells us that digit i of X is equal to bi, which means that no carry entered digit i, and we have a contradiction.
We conclude that no carries were performed in the long multiplication at all.

Thus, the Fair and Square numbers are exactly the palindromes with no carries inside. In particular, the middle digit of X is the sum of squares of all the digits of Y, so this sum has to be no larger than nine. We conclude that only 0, 1, 2, and 3 can appear in Y.

To find all Fair and Square numbers, it therefore suffices to consider only palindromes consisting of these four digits, with the sum of squares of digits at most 9. It turns out this is a small enough set that it can be directly searched, allowing us to generate the full list of all Fair and Square numbers up to 10100 in a few seconds - and thus allowing us to solve the largest dataset.

Lessons learned
There are a few things we want to remind you of in the context of this problem:

It is really important to read the problem statement carefully.
We can sometimes have problems in which we don't have the standard combination of one small and one large input. The rules for dealing with small and large inputs are still the same (unless explicitly stated otherwise in the problem statement).
We can also sometimes give problems that require very large integers. We did give Fair Warning about this some time ago, but it's always worth reminding.
Finally, if you use precomputation in your solution, remember that you are required to provide us not only with the code that you actually used to solve the problem (containing the precomputed values), but also the code that you used for precomputation.

Round 1A 2013 - Code Jam 2013

Bullseye (11pts, 13pts)

First, we need to know the area of the first black ring. It can be calculated by subtracting the area of a white circle with radius r from a black circle with radius r+1 cm. That is, the area is (r+1)2π-r2π cm2. Expanding we get (2r+1)π cm2. Since 1mL of paint can cover area π cm2, we need exactly 2r+1 mL of paint to draw the first black ring. For the second black ring, we do the same: subtracting the area of a white circle with radius r+2 cm from a black circle with radius r+3 cm, thus we need 2r+5 mL to draw. In general, we need exactly (r+2k-1)2-(r+2k-2)2 = 2r+4k-3 mL of paint to print the k-th black ring.

For small we know that the answer is less than t = 1000 anyway, so that we can try adding black rings one by one until the total amount of paint used including the next black ring exceeds t, then we stop adding and output the number of black rings we have included so far.

However, this does not work well with the large input as the answer can be much bigger! This can be verified by the fourth sample (we intended to be kind to contestants, but turns out many still failed the large due to integer overflow :-(). How can we improve the algorithm?

The key is the following: if you can paint at most k black rings using t mL of paint, for sure you can also use it to draw 1 black ring, 2 black rings, ... up to k-1 black rings. So instead of searching for the answers linearly, we can perform binary search on "Is it possible to use t mL of paint to draw k black rings?" to find the answer more efficiently. Now the remaining question is how much paint is used to draw k black rings.

Looking back about the total amount of paint to draw individual black rings, it is easily observed that they form an arithmetic progression: 2r+1, 2r+5, 2r+9, ... 2r+4k-3. Then the total amount is simply the sum of the progression, which is (2r+1+2r+4k-3)×k÷2 = (2r+2k-1)k mL. This is sufficient to completely solve the problem.

Here is a complete solution in Python for reference:

num_cases = int(raw_input())
for casenum in range(1, num_cases+1):
  r, t = [int(z) for z in raw_input().split()]
  res, lo, hi = 0, 1, t
  while lo <= hi:
    mid = (lo + hi) / 2
    if mid * (2 * r + 2 * mid - 1) > t:
      hi = mid - 1
    else:
      lo, res = mid + 1, mid
  print "Case #%d: %d" % (casenum, res)
You might think that you need big integer libraries to solve the large input, in fact you don't. For example, double precision floating point numbers suffice to check the condition correctly. A more elegant way is to find the first range with exponential growth in length so that there exists a value in the range which fails to satisfy the condition. It takes logarithmic time to find the required range. Then we perform binary search on it and compute the answer. Here is a C++ snippet that describes the algorithm without using any big integer library.

// Check if we can draw k black rings.
bool Check(long long r, long long t, long long k) {
  return 2 * k * k + (2 * r - 1) * k <= t;
}

// Find the maximum number of black rings that can be drawn.
long long Solve(long long r, long long t) {
  long long left = 0, right = 1;
  // Find the range that the answer lies in.
  while(Check(r,t,right)) {
    left = right;
    right *= 2;
  }

  // Binary search on the range [left, right) for the answer.
  while(right - left > 1) {
    long long k = (left + right) / 2;
    if (Check(r, t , k))
      left = k;
    else
      right = k;
  }
  return left;
}

Round 1A 2013 - Code Jam 2013

Manage your Energy (12pts, 23pts)

The small dataset
The limits in the small dataset are very small for this problem, and they allow brute-force approaches. With a fast enough language, even an algorithm that for each activity in the row tries each possible amount of energy usage (from 0 to current energy) will run in time.

The large dataset obviously does not allow such a solution — both the number of activities and the number of possible amounts of energy to consider are significantly too large.

The highest-valued activity
We will begin solving this problem by looking at the highest-value activity (if there is more than one, pick any of them), let this be activity number a. We will prove that there exists an optimal solution that uses full E joules for this activity.

Consider any optimal solution. First assume that we had less than E joules at the start of activity a. This means we spent energy on some other activity before (since we started with E energy), let b be the number of the last activity we spent non-zero energy on before a. Now consider a solution in which we spend one joule less on b. Since our energy wasn't E in the original solution at the beginning of a, if we spend one less joule on b, we will still have at most E energy at the beginning of a, and so it won't go to waste. Thus, we can spend this extra joule on a, and then proceed as in the original solution, since after a we have exactly the same amount of energy as in the original. The difference in values of these two solutions is -vb + va, which is non-negative (since a was one of the highest-value solutions). We can repeat this procedure until we obtain a solution which is no worse than the original (and thus still optimal), and has E joules at the beginning of a.

Now assume that we have an optimal solution that enters a with E joules, but does not use all of them on a. We can change it in a similar fashion. Find the first activity after a on which we spend non-zero energy, call it c. If it doesn't exist, or if any energy is wasted between a and c, we can simply expend one more joule on a and obtain a strictly better solution. Otherwise, we can spend one joule less on c and one more on a, to obtain a solution that's no worse.

After repeating these procedures as long as we can, we obtain an optimal solution in which, indeed, we spend full E joules on a.

Other activities
Notice that the fact we are spending E energy on a has some implications on the nearby activites. For instance, we will have at most R energy available at a+1, at most 2R at a+2, etc. At the same time, we need to leave a-1 with at least E-R energy, leave a-2 with at least E-2R energy, and so on.

Consider the activity d with the next-highest value after a. We have a limit on how much energy we have at most when entering d (it's the lower of two numbers — E and whatever limit the spending on a imposed); and we know how much energy we have left unspent (the higher of 0 and the limit imposed by a). We will spend anything between these two limits on activity d — a reasoning identical to the one for a proves that this doesn't prevent us from getting an optimal solution.

We will continue in this fashion. At each step, we will have for each activity an amount of energy that we have to leave after ending it for activities we already considered, and the maximum amount of energy we can have beginning this activity. At each step, we will take the highest-valued activity we haven't considered yet, and assign to it all the energy we can, updating the limits afterwards.

The time complexity of this solution, as formulated, is O(N2) — in each of the N steps we find the highest-valued activity not considered yet, assign energy to it, and update limits for all other activities. Due to the constraints on input size that we have this will be fast enough (at least if written reasonably efficiently). It is possible to do better, though.

An O(N logN) solution
First note that finding the highest-valued activity not considered yet can be done faster — it's enough to just sort the activities by value up front, and then consider them in descending order.

The more difficult part is updating the limits. To do this, we look at how the limits are set again. Each activity we assign imposes a limit on how much energy do we have in later activities, and how much do we have to leave behind in the previous activities. We would like to prove that for each activity a the limit we have to consider comes from the latest activity in the day we considered before a for how much energy we have, and the first activity we considered later in the day than a for how much we need to leave unspent.

This is not surprising. Consider, for instance, the activity b, which is the latest activity considered before a, and some activity c which comes even earlier in the day. If we considered c before b, the loss of energy on activity c is already taken into account when we consider b, so the limits imposed by b will be no less stringent. On the other hand, if we considered c after b, it means that the energy spent on c was already limited so that it would all be recovered by the time we reach b — and so it does not impact the amount of energy we have available at a. A similar reasoning works for activities later in the day than a.

A solution in which each step takes logarithmic time will take each considered activity and insert it (along with the limits it imposes) into a binary search tree allowing logarithmic-time insertions and lower/upper-bound operations (like the "set" structure of many languages). Then, at each subsequent activity, we find the nearest activity already considered before and after it, compute the limits they impose on the current activity, spend all the energy we can (updating the total value) and insert this activity (along with new limits) into the tree.

Note that if R ≥ E, you can just assume R = E, since any energy you regain above E will necessarily go to waste. We observed this and changed the problem statement to avoid this unnecessary case; but we missed some of the inputs in which this should have been corrected, due to which we had to change the statement back and issue a clarification during the contest. We apologize for the confusion this caused.

An O(N) solution
One nice thing about running a contest for a group of very smart people is that they come up with solutions for problems that the authors didn't even think of! This was the case with this problem — after the contest we learned some contestants came up with a O(N) solution.

The key to this solution is observing that we can actually know up front how much energy we will want to spend at a given day. As seen above, if there is no more valuable activity ahead of us, we should just spend all the energy we have at the moment. On the other hand, if there is something more valuable, it's always a good idea to save up energy if we will be able to use it for the more valuable activity. To make use of this we need to know, for each activity, the nearest activity in the future that's more valuable.

Calculating such an array is a classic problem, it is described, e.g., as the "Stock Span Problem" in the Wikipedia article on stacks. Once we have this array, we can, for each activity in chronological order, do the following:

If there is no more valuable activity coming up, spend all the energy we have.
Consider the nearest more valuable activity X. If we can spend any energy, and still have E energy when X comes (assuming we don't spend any between now and X), spend as much as we can while still having E when X comes. Notice the alternative to spending it now would be to spend it between now and X — but there's no activity more valuable than the current one in the period to make it worthwhile.
If we can't spend any energy and still have E when X comes, we shouldn't spend any — all our energy will be better spent at X.
Thanks to for pointing this out to us! The credit for this solution goes to pedrosorio; other O(N) solutions are possible as well (see, e.g., misof's submission).

Round 1A 2013 - Code Jam 2013

Good Luck (10pts, 31pts)

The luck factor
This problem is unusual in that you do not have full information and are forced to make guesses as part of the solution. We thought it would be a fun change from the usual deterministic setting.

Nevertheless, luck did not play a huge role in this problem. The first dataset was easy enough for many approaches to work. The second dataset was harder, but we estimated that an optimal solution would have very good chances: the probability of an optimal solution failing is only on the order of 1 in a million! This is because 8000 is a lot of independent guesses, and the limit X is rather conservative: about 5 standard deviations below the expected number of correct guesses.

The optimal strategy
One may be tempted to apply various heuristics to try to reason about what kind of hidden numbers are likely. In this case, it is best to approach the problem scientifically and simply always go for the highest probability of success!

In order to do that, we compute the probability of each of the 18564 possibilities (for the larger dataset) and pick the largest one.

Why 18564? There are 7 choices for each of the 12 hidden numbers. That seems to give 712 = 13841287201 possibilities, which is a lot. But, the order of hidden numbers doesn't matter, which reduced the number of different possibilities to (12+7-1) choose (7-1) = 18564. Try to derive this formula! Or just generate them all and count.

A priori probabilities: K=0
What if K=0, so we have no information about the hidden numbers at all? It may seem like then it doesn't matter what we guess, since all possibilities are equally likely. Many contestants made this mistake. Some possibilities are more likely a priori than others, even without any additional information!

For example, for the small dataset: 333 is less likely than 234. 6 times less likely, to be exact. Why? Because 234 may have been generated in 6 different ways (234, 243, 324, 342, 423, 432) while 333 can be generated in only 1 way.

In general, if digit d appears Cd times among hidden cards, the probability of that set is N! / (C2! * ... * CM! * (M-1)N).

K=1 and Bayes' theorem
So we have computed the a priori probability of every set of hidden cards, but that does not use the crucial available information: the K products of random subsets. How do we use that information? Conditional probabilities are the right tool for the job.

Let's start with K=1. For each set of hidden numbers, A, we already know the probability of that set happening, Pr(A). We also know a product p of a random subset of these numbers. What we are trying to compute is the conditional probability that the hidden set is A given that the product of a random subset is p. Let's write that as Pr(A | p).

How to compute that? Use the definition of conditional probabilities:
Pr(A | p) = Pr(A ∩ p) / Pr(p) = Pr(A) * Pr(p | A) / Pr(p)
This derivation is called Bayes' theorem.

We already know Pr(A), so we only need to know Pr(p | A). We can pre-compute these values for every A. Simply try every possible subset of each possible set of hidden numbers, see what the products are in each case, and build a large table of all these probabilities. There are 18564 * 212 ≈ 76 million such subsets.

Pr(p) can then be computed as the sum of Pr(A) * Pr(p | A) over all A.

The complete solution
K is greater than 1, but that's not a problem: we iterate the above reasoning for each of the K products, adjusting the probabilities of the hidden combinations in the process.

The full solution is then:

Some precomputation:
Generate all possible combinations of hidden numbers, ignoring order.
Compute the initial probability of each of these hidden sets.
For each possible hidden set, find all possible products of subsets and compute Pr(p | A). Index these values by p for easy lookup later.
For each hidden set:
Start with the pre-computed initial probability distribution over possible hidden sets.
Read one product p at a time, and adjust the probability distribution by using Bayes' theorem and the pre-computed conditional probabilities Pr(p | A).
Output the most probable possibility.

Round 1B 2013 - Code Jam 2013

Osmos (10pts, 12pts)

We can simplify the problem by making the following observation:
There is an optimal solution where Armin chooses to absorb motes in order from smallest to largest (skipping removed motes).
If Armin's solution absorbs mote X before mote Y, and mote X is larger than mote Y, then he could change his solution to absorb Y right before X, without needing to perform any extra "add" or "remove" operations. So if Armin has an optimal solution, we can always change it into an optimal solution that absorbs motes in order of size.

Now we can limit our search to solutions that absorb motes in order of size. We could use a dynamic programming algorithm where the state is the number of motes considered, and Armin's mote's current size or the number of operations performed, but there is a simpler algorithm based on the following observation:

In an optimal solution, if Armin removes a mote, he also removes all motes of equal or greater size.
To see this, consider a solution where there exist motes X and Y where X is smaller than or equal in size to Y, X is removed, and Y is absorbed. X could instead be absorbed immediately before Y is absorbed, which would save an operation by not removing X. So the solution cannot be optimal.
So to find an optimal solution, we only need to consider N+1 cases -- those where we try to absorb 0, 1, ... N of the original motes and remove the remainder.

To find how many operations are needed for each of these cases, we simulate Armin trying to absorb each mote in turn. If Armin's mote is not yet large enough to absorb the next mote, we add motes of size one less than Armin's mote's current size and absorb them, until Armin's mote is large enough.

This solution takes O(N2) time to run as written, which is fast enough given the input size limits. There is a small adjustment to it that will make it linear, though. Can you see it?

One final case to handle is when Armin's mote is of size 1, and so is unable to absorb any motes at all. We were generous and added this as a case in the sample input!

This mechanics for this problem were inspired by Osmos by Hemisphere Games.

Round 1B 2013 - Code Jam 2013

Garbled Email (12pts, 24pts)

There are multiple approaches possible to this problem. For the small test case, one can use a dynamic programming approach to calculate for each prefix of the given word what's the smallest number of substitutions needed to form this word so that the last substitution was k characters ago, for k = 1, 2, 3, ...

For example, for the word "codejam", we will find that "c" cannot be formed without a substitution, but can be formed (for instance from "a") by a substitution 1 character away. We find this by going over all dictionary words. Then, we go over all dictionary words to try and form "co" (we can do this, for instance, from "do" with one substitution 2 characters ago). We can also consider one letter words to extend the "c" we already know how to form, but this won't work, since "o" isn't a word, and we're too near to the last substitution. Next goes "cod", which actually is a word, so can be formed with zero subsitutions. Next goes "code" — for this we have a number of choice, like combining the "c" we know how to form and "ode", or the "cod" and a one substitution to form "e" from "a", or — the best one, since requiring no subsitutions — just using the word "code".

In this fashion for each prefix and each distance of the last substitution we can find out what's the least number of substitutions needed to form this prefix by looking at all smaller prefixes (including the empty one), all smaller dictionary words, and figuring out whether we can combine them.

For the large test case, we can't afford to go over the whole dictionary that often. So, we start by building a hash table. For each dictionary word, we insert that word into the hash table, and also insert the word with each possible set of changed letters in the word replaced by '*' characters.

For example, for the word "coders", we store in the hash table:

coders
*oders
c*ders
co*ers
cod*rs
code*s
coder*
*oder*
Next we use dynamic programming to build a table that contains, for each prefix of the email and location within that prefix of the last changed letter, the minimum number of changes required to transform a sequence of dictionary words into the prefix, if it is possible. (To save time, we can merge together all the states for a prefix where the last changed letter was 5 or more positions from the end of the prefix, because if the last changed letter is more than 4 positions back, it doesn't matter how much more.)
Each of the states where this is possible corresponds to a partial solution. We consider each possible way of adding one more word to create a longer partial solution. To do this, we try each combination of:

The length of the next word, L (1 ≤ L ≤ 10).
Each possible set of positions of changed letters in the next word, S.
For each of these combinations, we construct a string by taking the next L letters of the original email, and changing the positions in S to '*' characters. Then we search for this string in the hash table. If we find it, we can update the table to reflect a partial solution formed by appending that word.
The answer is the minimum number of changes found to produce the prefix that is the entire email.

Round 1B 2013 - Code Jam 2013

Falling Diamonds (14pts, 28pts)

This was a tricky problem, and solving the small often made the difference between advancing and not - in particular, solving all of this problem was enough to advance; as was solving the small of this problem and the large of Osmos.

The small case
For the small case, we had only twenty diamonds to deal with in each test case. Note that every diamond will do something non-deterministic at most once, when it falls point down on the point of another diamond (when it can slide in one of two directions). This means that we will have at most 220 different things that can happen when the diamonds fall, so we can simply try to enumerate all of them (note that there is actually less possible paths - for instance the first, fifth and sixth will never have a choice).

This is actually a bit tricky to do, since in each branch we have to keep track of what is the probability of reaching this branch — it is not only dependent on how many diamonds we processed so far, but rather on how many diamonds had a choice so far. After this is done, we need to figure out in how many of all the options the place we're interested in did get a diamond and add them all up. All this is not easy to get right, but it is doable, as over 900 of our contestants proved!

The large case
For the large test case, we will need to be smarter — simulating all the options is obviously not a choice for 106 diamonds. We will begin with the following observation:

The diamonds fall in layers. First a diamond falls at (0, 0), then diamonds fall into positions with |X| + |Y| = 2, only after all five of these are filled the positions with |X| + |Y| = 4 start filling, only after them the |X| + |Y| = 6 start filling, and so on.
Indeed, note that the diamond sliding to one side does not change the layer it is in, since it always starts sliding in some (0, 2k) position, and the (0, 2k) position is always the last in a layer to be filled.
Thus, when N diamonds fall, the only uncertainty as to how they shape up is in the last layer, and this is what we have to calculate. If the place we are considering is not in this layer, we can respond immediately. Thus, we have only to figure out probabilities in the last layer.

The dynamic programming approach
First let's estimate how large the last layer can be. If we have at most a million diamonds, one can calculate there will be no more than 710 layers. When diamonds fall, the state of the layer can be described by two numbers — how many diamonds are on the left of the center (with negative X), and how many are to the right (we assume here there aren't enough diamonds to fill this layer, so the top spot with X = 0 will stay empty). This means that when the diamonds drop, there are roughly 500,000 different states to consider.

One can approach to this problem is dynamic programming. For each of the states possible for the last layer, we calculate the probability of reaching this state when the appropriate number of diamonds has dropped (each state determines the number of diamonds uniquely).

A formulaic approach
One can also notice that what matters is how many diamonds of the ones that hit the top decide to go left, and how many to go right. Which diamonds exactly are those does not matter for the final state. Thus, we can precalculate binomial coefficients (or rather binomial coefficients divided by 2D, where D is the number of diamonds falling into the layer), and — once we know which layer we're looking — sum up the options that lead to a diamond falling into the right place.

Round 1C 2013 - Code Jam 2013

Consonants (8pts, 20pts)

Solving the small
It cannot be simpler than trying each possible substring given the name. For a given substring, we just check if there exists n consecutive consonants. If it is true, we count this substring into part of the n-value. There are O(L2) substrings, and it takes O(L) time to check for at least n consecutive consonants. In total each case takes O(L3) time to solve, which is acceptable to solve the small input. This approach is, of course, not fast enough to solve the large input.

Improving the naive algorithm
In fact we can skip the linear time checking for all possible substrings. Here we assume the index is zero based. Suppose we start from the i-th character. We also have c that starts as zero. When we iterate up to the j-th character, if it is a consonant, we increase c by 1, otherwise reset it to zero. Actually c is the number of consecutive consonants that starts after the i-th character and ends at the j-th character. If we meet the first instance such that c ≥ n, we can conclude that every substring which starts at the i-th character and ends at the k-th character, where k ≥ j, is the desired substring. Then we know that we can add L - j to the answer, and proceed to the next starting character. This algorithm runs in O(L2) time, which is still not sufficient in solving the large input. But the concept of computing c is the key to solve the problem completely.

Further improving
Let us extend the definition of c to every character, call it ci: the number of consecutive consonants that ends at the i-th character. For example, suppose the string is quartz, then c0 = 1, c2 = 0, and c5 = 3. We can use similar approach mentioned in the last section to compute every ci in O(L) time. Also define a pair (x, y) to be the substring that starts at the x-th character and ends at the y-th character.

Knowing from the previous section, if we know that ci ≥ n, then we know that substrings (i - ci + p, i + q), where 1 ≤ p ≤ ci - n + 1 and 0 ≤ q ≤ L - i - 1, are the desired substrings. It implies that there are (ci - n + 1) × (L - i) substrings. If you proceed like this, you missed some substrings. Consider the string axb with n = 1. We see that c1 = 1 but we only count 2 substrings, namely x and xb. We miss the prefix options, namely ax and axb. It looks like we can consider the substrings (p, i + q), where 0 ≤ p ≤ i - n + 1 and 0 ≤ q ≤ L - i - 1. Unfortunately, in this case we may count certain substrings multiple times. Consider the string xaxb with n = 1, where we count xax and xaxb twice since c0 = c2 = 1.

To correctly count the substrings, we need to choose the appropriate range of p. In fact, we just need one more value: the last j < i such that cj ≥ n. Let r = j - n + 2 if there is such j, or r = 0 otherwise. Then we have the right set of substrings (p, i + q), where r ≤ p ≤ i - n + 1 and 0 ≤ q ≤ L - i - 1. In fact, r means the longest possible prefix so that (r, i - n) contains at most n - 1 consecutive consonants and therefore we avoid repeated counting. Hence for each ci ≥ n we count (i - n - r + 2) × (L - i). Summing up we have the answer. r is updated whenever we see that ci ≥ n before iterating the next position. Therefore it takes constant time to update the value. Overall the running time is O(L), which is enough to solve the large input.

Despite the complications, the algorithm is extremely simple. The following is a sample solution:

def Solve(s, n):
  L = len(s)
  cnt, r, c = 0, 0, 0
  for i in range(L):
    c = c + 1 if s[i] not in "aeiou" else 0
    if c >= n:
      cnt += (i - n - r + 2) * (L - i)
      r = i - n + 2
  return cnt

Round 1C 2013 - Code Jam 2013

Pogo (10pts, 25pts)

The small dataset
The small dataset did not require finding the optimal solution, instead accepting any solution that solved the problem within 500 moves. This allowed for a variety of approaches. One such approach is as follows: note that with two subsequent jumps in two opposite directions you can move one unit in a chosen direction. By a sequence of at most 200 such pairs (so in total at most 400 moves) you can reach any point with |X|, |Y| ≤ 100.

There were also other approaches possible, for instance a shortest-path search, if one could prove or guess that it's OK to limit the search space somehow. It turns out that one can actually just search for a path within the points with |x|, |y| ≤ 100, we will see why in the next section. Thus, Dijkstra's algorithm or just breadth-first search will provide a short enough path to the target.

The large dataset
For the large dataset, we not only need to return the best possible solution, but also deal with more distant targets, so neither of the approaches above will work. We will begin with a few easy observations:

First, if we want to reach the target in N moves, we have to have 1 + 2 + ... + N ≥ |X| + |Y|.
Moreover, if we want to reach the target in N moves, the parity of the numbers 1 + 2 + ... + N and |X| + |Y| has to be the same. This is because the parity of the sum of the lengths of jumps we make in the North-South direction has to match the parity of |Y|, and the sum of lengths of West-East jumps has to match the parity of |X|.
It turns out that if N satisfies these two conditions, it is possible to reach (X, Y) with N jumps.
Let's consider a point (X, Y), and any N satisfying the two conditions above. For the sake of brevity, assume |X| ≥ |Y|, and X ≥ 0 (it's easy to provide symmetric arguments for the other four cases). In this case, we will assume the last move was East. This means the first N-1 moves have to reach (X-N, Y). We will proceed recursively, so we just have to prove that (X-N, Y) and N-1 satisfy the conditions above.

For N = 1 or 2, it's easy to enumerate all the possible X and Y. For N = 1, there are four possibilities, and in each case our strategy produces the correct move. For N = 2, if we assume X is positive and greater than |Y|, the only possibilities are (3, 0), (2, 1), (2, -1) and (1, 0); after the move they turn to (1, 0), (0, 1), (0, -1) and (-1, 0) — all of which satify the conditions for N = 1.

For larger N, the parity condition is trivial — both considered parities stay the same if N was even, and both change if N is odd. For the inequality condition, if N ≤ X, both sides just decrease by N, so the only interesting case is if X < N. However, in this case, we move to (X - N, Y), and the sum of absolute values is N - X + |Y| ≤ N - X + X = N ≤ 1 + ... + N - 1. Thus, the conditions are satisfied after one move, and we can continue with our strategy.

This logic again translates to simple code:

def Solve(x, y):
  N = 0
  sum = 0
  while sum < abs(x) + abs(y) or (sum + x + y) % 2 == 1:
    N += 1
    sum += N
  result = ""
  while N > 0:
    if abs(x) > abs(y):
      if x > 0: 
        result += 'E'
        x -= N
      else:
        result += 'W'
        x += N
    else:
      if y > 0:
        result += 'N'
        y -= N
      else:
        result += 'S'
        y += N
    N -= 1
  return result.reversed()

Round 1C 2013 - Code Jam 2013

The Great Wall (9pts, 28pts)

The small input
Despite the very long statement, solving the small input wasn't actually that hard. Still, the long statement scared many contestants off, which is probably why we saw the first submission only after half an hour of the contest, and relatively few submissions to the problem in general. With at most 10 tribes, at most 10 attacks and all the attacks happening on a short section of the Wall, we can just simulate all that happens. Let's look at it a bit more carefully.

Since delta_p is limited by 10, a tribe attacks at most 10 times, and the initial attack is between -100 and 100, all the attacks will occur between -200 and 200. Thus, we can afford to remember the height of the wall at each interesting point. This brings us to the first trick of this problem — what are the points we should be interested in?

Note that since the edges of attacked areas are always integers, the height of the wall in each open interval (x, x+1) for integral x is always constant. Moreover, the height at the integral points is never lower than at any of the two neighboring open intervals, since any attack that affects any of these intervals will also affect the integral point next to it. As wi < ei, any attack always affects at least one whole interval, and so the success of the attack depends only on the height of the wall in the intervals, and not on the edges. Thus, it is enough to keep information about the height of the wall in points of the form x + 0.5 for integral x. There are 400 such points to consider in the small input, and the height of each is initially zero.

There are a 100 attacks to consider. We can begin by generating all of them explicitly (noting the beginning and end point, day and strength for each of them), and sorting them by time of occurrence. For each day on which at least one attack occurs, we first check for each attack whether it succeeds (by examining the wall height at each attacked interval). Afterwards, for all attacks we go over all affected intervals and increase the height of the wall if necessary. Note that it is important to increase the wall height only after checking all the attacks that occur on a given day.

The large input
The numbers are much bigger for the large input. We can have 106 attacks, and they can range over an interval of length over 108. Let's analyse which parts of the previous approach will work, and which will not.

We can still generate all the attacks explicitly, and sort them by time. We probably need a more concise way to represent the Wall, though, and we surely need a faster way to check whether an attack succeeds and updating wall heights.

The problem of concise representation can be solved by noticing that since we have only 106 attacks, we will have around 106 interesting points. A sample way to take advantage of this it to "compress" all attack coordinates — sort all the coordinates that are beginnings or ends of attacks, and consider as interesting only the points in the middles of intervals of adjacent endpoints. We will end up with at most 2 x 106 points, and each will represent an interval such that the height of the wall on this interval is always the same. Using this tric to compress the attack coordinates, we can assume all attacks happen in a space of at most 2 x 106 points. We can rename these points to be consecutive for convenience.

To attack the problem of checking attack success and updating the wall, we will need some variant of an interval tree. We will present two interval-tree based approaches below.

An interval tree is a tree, in which each node represents an interval [m x 2k, m x 2k] for some m,k. The parent of a node containing an interval I will be the node representing a twice longer interval containing I (so, if I is [m x 2k, m x 2k], the parent is [(m / 2) x 2k+1, (m/2 + 1) x 2k+1]). This is the common pattern for interval trees, the trick is in what to store in nodes.

High and low
In the first approach, we will try to answer the questions directly by the means of using a modified interval tree. We will store two values in each node — hi and lo. The "hi" value will be pretty standard, and will be defined so that the height of the wall at any given point is the maximum "hi" value of all the intervals containing this point. This can be updated in logarithmic time when any interval of the wall is attacked - we can split any interval into a logarithmic number of intervals represented by nodes, and update the hi value in each of them. This will allow us to update the wall height, and to figure out what the height of the wall at a given point is, each in logarithmic time. We still need a way to figure out whether an attack will succeed in logarithmic time, though.

We will use the "lo" values for that. For a given node X and a path to a leaf from X we can define the maximum "hi" value on this path as the "partial height" of the leaf node. This is what would be the height, if we disregarded all the nodes above X (in particular, "partial heights" measured from the root node are simply wall heights). We now define the "lo" value of X as the smallest partial height of a descendant of X. We need to see how this is useful, and how to update it in logarithmic time when updating the "hi" values.

Note that if we have a "lo" value for a node calculated, we can easily figure out the height of the lowest wall point in this interval - it's the maximum of the "lo" value of this node and the "hi" values of all the ancestors of this node. Thus, to figure out whether an attack will succeed on a general interval we split it into a logarithmic number of intervals represented by nodes, and figure out the lowest wall segment in each of these sub-intervals. If any of these is lower than the strength of the attack, it will succeed. This is logarithmic-squared as described, but it's easy to implement it to actually be logarithmic.

Now note that the "lo" values have a simple recursive definition - take the minimum of the "lo" values of the children, or the "hi" value of the node itself, whichever is higher. This means that when updating the "hi" value for a node, we only need to update the "lo" values for this node and its ancestors - meaning we can update "lo" values in logarithmic-squared time for each attack (and, again, it's simple to update them in logarithmic time).

Order by strength
Another approach that allows us to solve this problem with an interval tree is to order the attacks by strength, descending, and not by chronology. In this approach, for each point we know what was the earliest time at which it was attacked. Note that since we process from the strongest attack, any section of the wall that was attacked earlier by an attack we already processed is immune to attacks that come later and are processed later. Thus, to learn whether the attack is successful we need to find what's the latest attack time in the whole interval this attack covers; and subsequently we need to update the attack times to the minimum of the time that was stored so far, and the time of the currently processed attack.

This is called a min-max interval tree (we update with the minimum, and we query for the maximum). We encourage you to figure out what to store in the nodes to make this work in logarithmic time!

Round 2 2013 - Code Jam 2013

Ticket Swapping (8pts, 11pts)

The small dataset
Note that in this problem we treat the passengers as one "player" in a game, and assume they all cooperate to pay as little as possible in total to the city. This means it doesn't matter who actually pays for a given entry card. In particular, when the train leaves a station, the charge on each entry card in the train increases (by N - i, where i is the number of stations this card traveled so far). Since all passengers want to exit the subway eventually, all entry cards will have to be paid — so we might just as well immediately subtract this cash from the passenger "total" and move on.

As long as nobody exists the train, there's no need to exchange entry cards. Only once someone needs to exits, the passengers (along with the ones who just entered) need to gather and figure out which entry cards do the passengers who are just leaving take with them. They should choose the entry cards that have been on the train for the shortest amount of time so far, since on each subsequent stop the price for such an entry card will be larger than the price for any card that has been on the train longer (as the price is N - i, where i is the number of stations the card traveled so far). This means that at every station, the passengers should pool all the cards together, and then whoever wants to exit takes the entry cards with the smallest distance.

For the small dataset, a naive implementation of this algorithm will work. We can process station by station, holding a priority queue (or even just any collection) of the entry cards present. When anyone wants to exit, we iterate over the collection to find the card that has been on the train for the shortest amount, add its cost to the total passengers cost and remove it from the set. We also need to figure out how much the passengers should pay, but fortunately that's easy.

The large dataset
The large data set needs a bit more subtlety. With the insane amounts of passengers and stations we will need to avoid processing unnecessary events. First of all, we should process only the stations at which someone wants to enter or exit. This will make us process only O(M) stations, much less than the N we would process otherwise. Moreover, we should avoid processing passengers one by one.

To this end, notice that the order in which we want to give exit cards to passengers is actually a LIFO stack — whichever card came in last will go out first. So, we can keep the information about entry cards present in the train in a stack. Whenever a new group of passengers comes in, we take their entry cards and put them onto the stack (as one entry, storing the number of cards and the entry station). Whenever any group wants to leave, we go through the stack. If the topmost group of cards is big enough, we simply decrease its size, pay for what we took, and continue. If not, we take the whole group of cards, pay for it, decrease the amount of cards we need by the size of the group, and proceed through the stack.

This algorithm will take only O(M) time in total to process all the passengers — we will put on the stack at most M times, so we will take a whole group from the stack at most M times, and each group of passengers will decrease a group size (not take the whole group of cards) at most once, so in total — at most M such operations in the whole algorithm. Additionally, we need to sort all the events (a group of passengers entering or leaving) up front, so we are able to solve the whole problem in O(M logM).

Finally, when implementing, one needs to be careful. Due to the large amounts of stations and passengers involved, we have to use modular arithmetic carefully, because — as always with modular arithmetic — we risk overflow. In particular, whenever we multiply three numbers (which we do when calculating how much to pay for a group of tickets), we need to take care to apply the modulo after multiplying the first two.

Round 2 2013 - Code Jam 2013

Many Prizes (7pts, 13pts)

Let's begin by an observation that will make our life a bit easier: if we reverse all the team numbers (that is, team 0 becomes team 2N-1, team 1 becomes team 2N-2, etc.), without changing the tournament list. This will result in the final ranks of the teams also being reversed, since it's relatively easy to see that all the records will be reversed (wins becoming losses and vice versa). This means that the problem of having a team rank as low as possible to get into the first P is equivalent to the problem of a team ranked as high as possible to get into the bottom P (or, in other words, not get into the top 2N - P). Thus, if we are able to answer the question what is the lowest rank that can possibly get into the top P, we can run the same code to see what's the lowest rank that can possibly be in the top 2N - P, subtract one, and reverse, and this will be the lowest rank that will always get a prize. This means we only have to solve one problem — lowest ranked team guaranteed to be in the top P

For the small dataset we have at most 1024 teams, which means we can try to figure out the tournament tree that will give a given team the best position, do this for all teams, and see which team was the lowest-ranked one to get a prize. Let's skip this, however, and jump straight to the solution for the large dataset, where 250 teams clearly disallow any direct approaches.

The key observation we can make here is that if we have a team we want to be as high as possible, we can do it with a record that includes a string of wins followed by a string of losses, and nothing else. This sounds surprising, but is actually true. Imagine, to the contrary, that the team (let's call them A) has a loss (against some team B) followed by a win against C, who played D in the previous round. Note that up to the round where A plays B the records of the four teams were identical. Also note that all the tournament trees of the four teams so far are disjoint, and so we can swap them around. In particular, we can swap team C and all its tree with team B and all its tree. Since we swap whole trees, the records of teams don't change, so now team A will play C in the first match and win — and so its record is going to be strictly better than it was, no matter what happens next. Thus, any ordering in which team A has a loss followed by a win is suboptimal.

This allows us to solve the problem of getting the highest possible rank for a given team. We simply need to greedily win as much as we can. If we're the worst team, we can't win anything. If we're not, we can certainly win the first match. The second match will be played against the winner of some match, so in order to win it we need to be better than three teams. To win two matches, we need to be better than seven teams, and so on.

We can also reverse the reasoning to get the lowest-ranked team that can win the prize. First, let's ask how many matches does one need to win in order to win a prize. If you win no matches, you are in the top 2N (not a huge achievement!). If you win one, you are in the top 2N - 1. And so on. Once we know how many matches you need to win, you directly know how many teams you need to be better than. Simple python code follows:

def LowRankCanWin(N, P):
  if P == 0:
    return -1
  matches_won = 0
  size_of_group = 2 ** N
  while size_of_group > P:
    matches_won += 1
    size_of_group /= 2
  return 2 ** N - 2 ** matches_won

def ManyPrizes(N, P):
  print 2 ** N - LowRankCanWin(N, 2 ** N - P) - 2, LowRankCanWin(N, P)

Round 2 2013 - Code Jam 2013

Erdős–Szekeres (9pts, 15pts)

Get the information
The real trick to this problem is squeezing as much information as possible from the sequences A[i] and B[i]. We will get information out in the form of inequalities between various elements of the sequence X.

First notice that if we have two indices i < j, and A[i] ≥ A[j], then X[i] > X[j]. Indeed, if it were otherwise, then the increasing sequence of length A[i] with X[i] as its largest element could be extended by adding X[j] at the end to obtain an increasing sequence with X[j] at the end, and we would have A[j] ≥ A[i] + 1. This allows us to add some inequalities X has to satisfy. We can add symmetric ones for B — if i < j and B[i] ≤ B[j], then X[i] < X[j].

These inequalities are not enough, however, to reconstruct X. Indeed, if we take A[i] = i + 1 and B[i] = N - i, we get no inequalities to consider, but at the same time not all permutations X are going to work. The problem is that while we know what we need to do with X so that no longer subsequences exist, we still need to guarantee that long enough sequences exist.

This is relatively simple to accomplish, though. If A[i] > 1, then the increasing subsequence ending at X[i] is formed by the extension of some sequence of length A[i] - 1. This means that X[i] has to be larger than X[j] for some j < i with A[j] = A[i] - 1. The previous set of inequalities guarantees that of the set of all such j (that is, j which are smaller than i and have A[j] = A[i] - 1) the one with the smallest X[j] is the largest j. Thus, it is enough to find the largest j with j < i and A[j] = A[i]-1 and add X[j] < X[i] to our set of inequalities. We again do the symmetric thing for B.

Use the information
Notice that the inequalities we have are indeed enough to guarantee that any sequence X satisfying them will lead to the A and B we want. It's relatively easy to check the first set of inequalities guarantees the numbers A and B will not be larger than we want, while the second set of inequalities guarantee they will be large enough.

We now reduced the problem to finding the lexicographically smaller permutation satisfying a given set of inequalities. To find the lexicographically smallest result we are foremost interested in minimizing the first value in X. To this end, we will simply traverse all the inequalities that the first value in X has to satisfy (that is, iterate over all the elements we know to be smaller, then all elements we know to be smaller than these elements, etc.). We can do this, e.g., through a DFS. After learning how many elements of X have to be smaller than X[1], we can assign the smallest possible value (this number + 1) to X[1]. Now we need to assign numbers smaller than X[1] to all these elements, in the lexicographically smallest way.

Note that how we do this assignment will not affect the other elements (since they are all going to be larger than X[1], and so also larger than everything we assign right now). Thus, we can assign this set of elements so that it is lexicographically smallest. This means taking the earliest of these elements, and repeating the same procedure recursively (find all elements smaller than it, assign the appropriate value, recurse). Note that once some values have been assigned, we need to take that into account when assigning new values (so, if we already assigned 1, 3 and 10; and now we know that an element we're looking at is larger than 4 others, the smallest value we can assign to it is 7).

Such a recursive procedure will allow us to solve the problem. For each element, we are going to traverse the graph of inequalities to find all the elements smaller than it is (O(M time if we do a DFS, where M is the number of inequalities we have), then see what's the smallest value we can assign (O(N) with a linear search, we can also go down to O(logN), but don't need to), and recurse. This gives us, pessimistically, O(N3) time — easily good enough for the small testcase, but risky for the large.

Compress the information
Following the exact procedure above we can end up with O(N2) inequalities. This will be too much for us (at least for the large dataset), so we will compress a bit.

The problem is with inequalities of the first type — for indices smaller than a given i there can be many with A larger or equal to A[i]. A trick we can use, though, is to take only one inequality — find the largest j < i with A[j] = A[i], and insert only the inequality X[j] &gt X[i].

Any other k < j with A[k] = A[i] will follow from transitivity — there will be a sequence of indices with A equal to A[i] connecting k to i. Any k with A[k] > A[i] will also follow, since X[k] will have to be greater than some X[l] for A[l] = A[i] and l < k (and thus < i). This means we can reduce our set of inequalities down to a set of O(N), which means that each DFS traversal step will take only O(N) time, and the solution to the whole problem will run in O(N2).

Interesting fact: It is also possible to solve this problem in O(N logN). Can you figure it out?

Round 2 2013 - Code Jam 2013

Multiplayer Pong (12pts, 25pts)

Preliminaries
We hope you took our Fair Warning, which we fairly repeated in Fair and Square this year — we consider Big Integers to be fair game, so if your language doesn't natively support them, you'd better have a library to handle them ready.

This problem seems to be about fractions initially, since the ball can hit the walls at fractional positions. There's a way to move it to integers (man, Big Fractions would be too much). The way to go about it is to scale things. Let's scale the time, so now there are VX units to a second, and scale vertical distances, so that there are VX units to the old unit. This means the vertical speeds stay the same, horizontal speeds get VX times smaller, horizontal distances stay the same, and vertical distances grow VX times larger. In implementation terms, this means we shrink VX to 1 and grow A times VX — and now suddenly the ball moves a integral number of units upwards and one unit to the side in each unit of time (and so will hit the vertical walls in integral moments of time).

The above assumes VX is positive. If it's zero, then the ball will never hit the walls, and so the game ends in a draw. If it's negative, we can flip the whole board across the Y axis and swap the teams to make VX change signs. Similarly, we can assume VY positive — if it's zero, putting all paddles in the single impact point will guarantee a draw, and if it's negative, we can flip the board vertically.

So now the ball will hit a given vertical wall every 2B units of time. It's also relatively easy to figure out what is the position of the impact. If there were no vertical walls, the ball would hit at the initial hit position (Y + (B - X) VY), and then at 2BVY intervals from there. To calculate the positions in real life, notice that every 2A upwards the ball is in the same position again (so we can take impact points module 2A), and if that number is larger than A, the ball goes in the other direction, and the impact position is 2A minus whatever we calculated.

Many bounces
So now we know how to calculate hit positions, so we can just simulate and see who loses first, right? Well, wrong, because the ball can bounce very many times. Even in the small dataset, the number of bounces can go up to 1011, too much for simulation.

Notice that the positions of the paddles are pretty much predetermined. The paddle of a given player has to be exactly at the point of impact when it is the player's turn to bounce it, and then the only question is whether the player will have enough time to reach the next point of impact before the ball. With N paddles on a team, and V being the speed of the paddle, the player can move the paddle by 2BVN before the next impact. The ball, in the same time, will move by 2BVYN, but while the paddle moves in absolute numbers, the ball moves "modulo 2A with wrapping", as described above.

If the distance the ball moves (modulo A) is smaller or equal to the distance the paddle moves, the paddle will always be there in time. The interesting case is when it is not so. In this case, there is still a chance for the paddle to be in the right place on time due to the "wrapping effect". For instance, if the ball moves by A+1 with each bounce, and the first bounce happens at A / 2, the next one will happen at A / 2 - 1, the next one at A / 2 + 2, and so on — so the initial bounces will be pretty close by. We can calculate exactly the set of positions where the ball hitting the wall will allow the paddle to catch up, and it turns out that it is two intervals modulo 2A.

So now the question becomes "how long can the ball bounce without hitting a prohibited set of two intervals", which is easy to reduce to "without hitting the given interval". This is a pure number-theoretic question: we have an arithmetic sequence I + KS, modulo 2A, and we are interested in the first element of this sequence to fall into a given interval.

Euclid strikes again
There is a number of approaches one can take to solving this problem. We will take an approach similar to the Euclidean algorithm. First, we can shift everything modulo 2A so that I is zero, and we are dealing with the sequence KS. Also, we can shift the problem so that S ≤ A — if not, we can (once again) flip the problem vertically. Thus, the ball will bounce at least twice before wrapping around the edge of 2A.

We can calculate when is the first time the ball will pass the beginning of the forbidden interval (by integral division). If at this point the ball hits the forbidden interval, we are done. Otherwise, the ball will travel all the way to 2A, and then wrap around and hit the wall again at some position P smaller than S. Notice that in this case the interval obviously is shorter than S.

Now, the crucial question is what P is. It's relatively easy to calculate for what values of P will the next iteration land in the interval (if the interval is [kS + a, kS + b] for some k, a, b, then the interesting set of values of P is [a, b]). If P happens to be in this interval, we can again calculate the answer fast. If not, however, we will do another cycle, and then hit the wall (after cycling) at the point 2P, mod S. Notice that this is very similar to the original problem! We operate modulo S, we increment by P with each iteration, and we are interested in when we enter the interval [a,b].

Thus, we can apply recursion to learn after how many cycles will we finally be in a position to fall into the original interval we were interested in. So we make that many full cycles, and then we finish with a part of the last cycle to finally hit the interval.

Complexity analysis
All the numbers we will be operating on will have at most D = 200 digits in the large dataset, so operations on them will take at most O(D2 logD) time (assuming a quadratic multiplication implementation and a binary-search implementation of division). Each recursion step involves a constant number of arithmetic operations plus a recursive call, for which the number A (the modulo) decreases at least twice. This means we make at most O(D) recursion steps — so the whole algorithm will run in O(D3 logD) time, fast enough allowing even some room for inefficiency.

Round 3 2013 - Code Jam 2013

Cheaters (7pts, 10pts)

Background
This problem challenges us with an 'intelligent' roulette game where the ball always lands on one of the numbers that has the least total money bet on it. Armed with this knowledge, we go to calculate the maximum expected profit. We know the current bets (need to be integers) and our budget. We now place bets (also integers) to maximize our expected profit.

Examples and insights
Before we delve into the solution, let's go through a few examples to get an intuition for the solution strategy. First off, we describe the convention used in the figures.

a square in the figure represents a single unit of money
a column in the figure represents the current bet (pile of money) on some number
colors of squares:
a red square represents the existing bets by other betters
a white square means there are no bets
a green square represents the bets we have already placed
a yellow square represents a bet we are considering
a blue square represents our bets that we are pointing out
In our figures, we show only 8 columns (different numbers) even though in our roulette game we have 37 different numbers, we assume all other numbers have much higher bets on them already
We sort the piles of money (red columns) by increasing height going from left to right
Let's start out with a toy example (see figure a). Here, we have piles of heights 0, 0, 0, 2, 2, 3, 4, 4. Let's say we have 3 units to bet: which piles should we bet on and how much should we bet per pile? In this case, we can bet on the three piles with height 0 (see figure b) which gives us an expected profit of 33.

Observation #1: As only the piles with minimum-height have a chance to win, we want to try and place our bets on piles resulting in a minimum-height.

What if we had 6 units to bet? We can again place bets on piles with height 0 (see figure c). The result is 5 piles with height 2 (3 green piles and 2 red piles). Our expected profit is 37.2. By the way, if the red piles of height 2 were instead of height 3, our expected profit would have been 66!

What if we had 7 units to bet? We can place 6 units as described above. But where should we place the 7th one? We could try placing it in the blue locations in figure d, but that does not help at all. Or we could try placing it in any of the blue locations in figure e, which will change our expected profit as it will decrease the total number of piles with minimum-height 2. If we placed the 7th unit in the yellow position as in figure f, we will reduce our expected profit! But if we placed it as in figure g, our expected profit will increase to 47! It increases as we have decreased the number of piles with minimum-height that weren't contributing to our expected profit.

 
Observation #2: We can possibly increase the expected profit by decreasing the number of piles with minimum-height.

What if we had 8 units to bet? You guessed it. We can place it as in figure h and get an expected profit of 64. In general, our optimal set of bets will have a step like figure of minimum-height h, and height h+1 or higher (as shown by the blue squares in figure i).

Simple strategy
Our simple strategy (for the small input) is to lay the bets one by one as shown in figure j. Each time we put a bet (e.g. in 1), we calculate the expected profit then place the next bet (i.e. 1 and 2) then calculate the expected profit, then repeat for the next one (1, 2 and 3). We retain the maximum expected profit.

This strategy works fine when the bet we have to place is small, but amount of money we can bet can get as large as 1012! Therefore we need to use a strategy that will run faster.

Advanced strategy
Observation #3: For the optimal solution, the step from minimum-height h to h+1 (or higher) will occur at some vertical location (see figure k). At that vertical location, we want the highest minimum-height h possible with the amount of money we have.

Armed with this observation, we will essentially fix a vertical location and try to build as tall a step as possible. For example if we had 7 units of money and we fixed the vertical step location after the first column as shown in figure k, then the highest step we could build is shown in figure l, which uses 5 units of money (2 units remain unused). For the vertical step location after the fourth block as shown in figure m, we use all 7 units of money.

 
Therefore the strategy is to try all possible vertical locations, calculate the highest 'step' we can generate for each vertical location and calculate the expected profit. As usual, we retain the maximum one.

So how do we calculate the highest possible step given a vertical location? Doing it by adding squares one-by-one is going to be too slow, therefore we turn to our trusty friend binary search. We want to determine the highest minimum-height step for that vertical location. Notice that since the amount of money (squares) required to build each 'step' is monotonically increasing, we can perform binary search.

Round 3 2013 - Code Jam 2013

Rural Planning (9pts, 13pts)

Introduction
In this problem, we are given a set of points on a 2D plane. The goal is to construct a single simple polygon that uses all of the points. To make things interesting, the area of the resulting polygon has an additional constraint; it must be strictly more than half of the area of the largest polygon constructed from any subset of points. A polygon that takes up maximum area but can use a subset of the points is always the convex hull of the points.

Small
For the small data case, we choose an exhaustive approach that finds the polygon with maximal area. Assuming that it is possible to construct a polygon with a large enough area (see the explanation of the large case for justification), the maximal area polygon will be a solution. The question now becomes how to construct a polygon of largest area. We permute the order of the fence posts. For each ordering, we verify the polygon has no intersecting edges (being careful of edges that pass through fence posts). We keep the polygon with the largest area. With N=10 fence posts, this takes no more than O(N! * T), where T is how long it takes to validate a polygon and compute its area. Verification can be implemented easily in O(N2) time. Polygon area can be done in O(N) time. So the resulting runtime is O(N! * N2). For N=10, this is around 500 million basic operations, which is relatively quick.

Large
For the large case, N = 1000, exhausting even a subset of fence post permutations it too costly. An algorithm over O(N^3) may take too long to compute. We explore a more direct method.

As mentioned earlier, the polygon with the largest possible area using a subset of the fence posts is the convex hull. Suppose we break the convex hull into two polygons, one of these polygons must contain at least half of the convex hull's area. Notice that regardless of how the convex hull is split, one of the resulting polygons is always at least half of the area.

Suppose we have a way to split the convex hull using the interior fence posts. This results in a polygon with at least half of the area of the convex hull, and at least one isolated vertex on the exterior of the polygon. We just need to connect the isolated vertices to the polygon without introducing intersecting edges. This process could only possibly add more area to the polygon, hence the final polygon is strictly more than half of the convex hull's area.

Making the above observation as the basis of our solution, we can arbitrarily split the convex hull into the upper half and lower half.

Note that the union of upper and lower polygon contains the full area of the convex hull. Furthermore the interior path cannot cause intersecting edges, because we chose to sort the points from left to right (note, special care must be taken when ties are involved, see special cases). One of these polygons has at least half the area of the convex hull, as explained above.

As mentioned, the larger of the two polygons will not contain all of the points. However, we can extend the polygon to use all the points. This is done by iteratively adding a point to the polygon. Because all exterior points are on the convex hull, the polygon must increase in area as we do this. The points are added to the polygon by taking an exterior point and "connecting" it to the point left and right of it on the polyline dividing the convex hull. For an optimization, the exterior points can be added at the same time as forming the polyline if you know the exterior points when constructing the polyline.

Special cases

On the left example, the leftmost and rightmost points are connected. In this case, the convex hull is split into the full polygon and a line segment. The full polygon, a.k.a. the convex hull, can simply be used as the final answer.

The second special case, above on the right, can occur when points share the same x coordinate and "left to right" is not well defined. An elegant way to handle this case is to project all of the points onto a "nearly horizontal" line and retain the scalar project values. If we pick a line that is not parallel or orthogonal to the line formed by any two points in our input, then each projected point will have a unique position on the "horizontal" line. A line that passes through points (0,0) and (200000,1) will work.

Once all points are projected onto the line, the sort order of the points is determined by the projected value. If we take a collection of points and go through them in this order we form a polyline.

We can show that running this algorithm including the two endpoints of one of our half hulls and all the middle points will create a closed polygon. Notice that when you also expand the middle set to also contain all points that are in the half hulls that the polygon grows in size. This is because the points added are outside our current polygon as a property of the half hull. A line coming from the middle points will move away from the first polygon towards the hull and back to the middle points when following our order. This technique ensures that the area of our polygon grows bigger than the polygon with just the middle points.

Putting it all together
To sum up the solution, we will reiterate the main steps of our algorithm:

Take the input and turn it into three sets of points cut using line (0,0) to (200000,1):
upper hull
lower hull
middle points
Run the closing algorithm on using upper hull and combining lower hull and middle points.
Run the closing algorithm on using lower hull and combining upper hull and middle points.
Print the solution with the larger polygon.

Round 3 2013 - Code Jam 2013

Are We Lost Yet? (12pts, 18pts)

Introduction
The problem asks you to determine which part of a path E1, E2, ..., Ek can be a prefix of some shortest path from 1 to 2. This would be easy if not for the fact we don't know the exact edge costs - we only know ranges into which they fall.

Small dataset
Let's concentrate on any fixed prefix E1, E2, ..., Ep of the proposed path, and try to make it a prefix of some shortest path. If we decide on a shortest path candidate (that includes our prefix), it's obivously advantageous to assume the edges on our path are as short as possible, and the other edges are as long as possible.

Notice that a consequence of this is that we can just restrict ourselves to looking at graphs in which each of the edges is either as long or as short as possible. Thus, since in the small dataset the number of edges is only 20, we can simply check all possibilities - make some of the edges short, the rest long, find the shortest path in the resulting graph (preferring the proposed path by, for instance, decreasing the cost of the edges on this path by a small epsilon), and then out of all the possibilities pick the one that takes the most steps along the proposed path.

Large dataset
This is obviously not going to fly for the large dataset. Again, we fix a prefix E1, E2, ..., Ep of the proposed path and try to make it a prefix of the shortest path. Assume Ep ends in Tokyo. Thus, we will try to get from Tokyo to London as fast as possible, while still not allowing for a shorter path from Mountain View to London that doesn't begin with our prefix. If we want to optimize for speed, we can do a binary search for the longest prefix that can be a start of some shortest path; if we optimize for simplicity, we can just iterate over all prefixes.

Imagine two robots that try to reach London as fast as possible. One starts from Tokyo, and has a handicap of the cost to travel from the Mountain View to Tokyo along the proposed path (we call this the "good" robot). When this robot goes across an edge, it will always take the minimal cost - this robot represents the shortest path we hope to construct. The other robot starts from Mountain View, and tries to reach London via some other path. We call this the "bad" robot and will try to force it to take as long as possible. The question is whether we can make the good robot be at least as fast as the bad robot (the bad robot can obviously be equally fast simply by following the good robot).

We already set the costs for the edges E1 to Ep to the low values. Since our aim is to make the good robot move fast and the bad robot move slow, a naive approach is to simply have the good robot pay the low cost for all other edges, and the bad robot pay the high cost. However, we will still encounter a problem in this model. If two robots go across a same edge, they are actually taking different costs, which is not possible in a fixed configuration.

Notice, however, that the two robots are walking the same graph. Thus, if they reach a same node at a different time, the one that arrived earlier will always beat the other one if the shortest path goes through that node, because it can always follow the other one's route. This means the later robot does not have any purpose in visiting this node at all. Since ties are resolved in favor of the good robot, we can simply decrease the good robot's handicap by 0.5 to avoid any ties.

Thus, we can solve the problem with a single run of Dijkstra's algorithm. We begin with two starting points - one in Mountain View at time 0, and the other in Tokyo with time equal to the cost of travel along the proposed path to Tokyo minus 0.5. When processing a node, we calculate the costs of outgoing edges as follows:

If the edge is one of E1, ..., Ep, we take the low cost.
If the current node is processed because the good robot reached it (which we know because the cost of the current node is not an integer, it ends with 0.5), the cost is the low cost.
Otherwise, we are processing the node because the bad robot reached it, and the cost is the high cost.
As Dijkstra's algorithm visits a node only once to process outgoing edges, we will never have a robot visit a node that the other robot reached earlier.

Round 3 2013 - Code Jam 2013

Observation Wheel (8pts, 23pts)

Small dataset
In this problem we are interested in calculating the average amount of money we will make from filling up every gondola on our observation wheel. Because of linearity of expectation, this is equivalent to summing up the expected amount of money paid by each person. Given the fact that the amount a person pays doesn't depend on the order in which gondolas got occupied, we can represent the current free/occupied state of gondolas as a bitmask and use dynamic programming to solve the small case.

Let E(mask) be the expected amount of money we make starting from the configuration represented by mask, where 0 represents an empty gondola, and 1 represents an occupied gondola. A person has a probability of 1/N of starting at any given position. Once that starting position is fixed, we simply find the first 0 following it (in cyclic order), and that's where the person will eventually end up.

We'll define f(i) as the index of the gondola occupied by a person who starts at position i. Similarly, let c(i) be the amount of money this person pays, as per the problem statement. Then we have the following recurrence:

E(mask)=1/N*∑i=0..N-1(E(mask | (1 << f(i)) ) + c(i))

The bitwise operation here simply sets the bit corresponding to the gondola the user occupied. The base case is when there are no empty positions, in which case the expected amount of money is 0.

There are 2N states, each of which can be computed in linear time, so our time complexity is O(N*2N). This is pretty easy for the small data set, but unfortunately it's far too slow for the large case.

Large dataset
Let's analyze the problem from the end: one of our gondolas will be the last to become occupied, so we have several cases, one per each free gondola at the beginning. The expected amount of money we will get is equal to the sum of the expected amount of money we make in each of those cases multiplied by the probability of that case.

At first, it doesn't seem we've reduced the complexity of the problem: instead of having to find just the expected amount for the whole process, we now have to find both the expected amount and the probability for several cases! However, we can repeat the above trick. Take one of those cases, let's say that gondola i is the last to become occupied. Let's look at which gondola will be occupied directly before it. Let's say it's gondola j. And here's the breakthrough: as soon as we've fixed that gondolas i and j are the last two to become occupied, the observation wheel has been separated into two independent parts that don't affect each other: those between i and j, and those between j and i. They don't affect each other since i and j stay empty, and thus no person approaching one part of the wheel will end up in the other part.

Our approach in general will be to compute E(i, j), the expected amount of money we get from all gondolas from i-th until (j-1)-th, excluding j-th gondola itself which will stay empty. It is possible to have i > j since we're dealing with a cyclic problem, so keep this in mind when implementing things. We basically start at i and proceed around the circle, stopping just short of j.

To compute expectations, we're going to need probabilities, so first let's look at P(i, j), the probability that j-th gondola will stay empty while we fill up all gondolas from the interval [i, j) assuming each coming person approaches some gondola in inteval [i, j] (note that j is included here). We can develop a recurrence to compute this.

Suppose we know that the last person enters the gondola at position (i + k). This splits the interval into two parts, with a empty squares on the left, b empty squares on the right, and 1 more empty square at (i+k).

The probability that gondola j stays empty while we fill interval [i, j) and that gondola at position (i+k) is filled last is P(i, j, k) and can be computed as:

P(i, j, k) = C(a+b, a)*((k+1)/(j-i+1))a+1*((j-i-k)/(j-i+1))b*P(i, i+k)*P(i+k+1, j)

Here C(n, k) is the binomial coefficient representing the number of ways to choose k objects from a set of n. The equation above amounts to choosing a people from (a+b) to go on the left side of the final empty space, and then making sure that (a+1) people go to the left side (including the person to fill up gondola i+k) and b people go to the right side. The probability that gondola i+k will stay empty is P(i, i+k), and the probability that gondola j will stay empty is P(i+k+1, j).

This assumes that (i+k) is empty initially, otherwise we define P(i, j, k) = 0.

Of course, we can't actually fix the final person, but since every way to fill up the interval has some final person, we can just compute the probability that gondola j will stay empty as the sum of P(i, j, k) over all possible final positions k, giving:

P(i, j)=∑k=0..j-i-1P(i, j, k)

For our base case, we have P(i, j, k) = 1 if the interval [i, j) contains no free gondolas. This also includes the case where the interval is of size 0. Don't forget, we're still in a cyclic situation!

On to computing expectations! We'll use the same trick of splitting around the last person. The expected money we get while filling out the interval [i, j) so that the last filled gondola is at position (i+k) is:

E(i, j, k) = E(i, i+k) + E(i+k+1, j) + N - k/2

Summing over all possible k to get the expectation, we get:

E(i, j)=(∑k=0..j-i-1P(i, j, k)*E(i, j, k))/P(i, j)

The way the first equation works is to combine the expectations from the left interval and the right interval, and then we need the expected number of skips to place the final person. There are (k+1) starting positions, corresponding to 0 skips, 1 skip, …, k skips. Each of these is equally likely, so the expectation is N - 1/(k+1)*(0 + 1 + ... + k)=N - k/2.

As before E(i, j, k) = 0 if gondola at position (i+k) is occupied.

To compute the final answer, we'll repeat the same trick in the final step. We try all possible empty positions as the last gondola to be filled and compute the expected number of skips. If the last empty position is i, then the expected money we get is:

P(i+1, i)*(E(i+1, i) + (N+1)/2),

and the total expected amount of money is just the sum of this quantity over all empty positions.

This algorithm is O(N3), which is easily within the time limit for the large case.

World Finals 2013 - Code Jam 2013

Graduation Requirements (7pts, 18pts)

In this problem we are interested in finding the maximum time we can drive against the direction in a traffic circle. For small test cases, the size of the input is small enough to try all possible intersections, time to start, and check the length to drive without touching any other car.

For large test cases, the above approach is not good enough due to the extremely large N and X (up to 1010). Therefore in order to solve the large test cases, we transform the problem into the 2-dimensional plane with intersection and time as axes. Then, each car can be represented as line segments; your car will be represented as a line segment orthogonal to other line segments formed by other cars. Figure 1 shows the second sample test case. Lined segments corresponding to different cars are denoted with different colors while the black dotted segment corresponds to your car. Note that as the intersections are in a traffic circle, we show it by repeating intersection 5 and 1 on the two ends in Figure 1.

Figure 1

So the problem can be reformulated as follows: given a set of line segments, find the maximum possible length of a perpendicular segment which does not touch any other segment. Note that if two segments have a point in common the corresponding cars touch each other.

To solve the large test case one needs to notice that a line segment of maximum possible length can be chosen so that it goes near one of the endpoints of another segment. By “near” we mean distance = 1 in one of the axes. Indeed, if we have a segment of maximum possible length, that does not go near one of the endpoints, we can always move it so that it does, see Figure 2 for an example.

Figure 2

Figure 3

The statement is also true in examples such as in Figure 3 because intersections are arranged in a circular manner and the solution segment is near the top right endpoint which is shown with an arrow.

Figure 4

Thus we note that changes in the length of segments happen near these endpoints. Therefore to solve the problem we need to go over all endpoints of car segments and consider its neighbourhood (-1 and +1 in both of the axes). Note that we also need to consider segments that go through endpoints +/-(1,1), an example is shown in Figure 4. Then we can compute all segments that pass through these near points and pick the longest segment as the answer.

For each such candidate point, we need to check how far up and down a segment that go through the candidate point can reach without touching other segments. This can be done simply by going over all car segments and checking if and where our candidate segment intersect them. The complexity of this algorithm is O(C2), which is enough to solve the large case.

World Finals 2013 - Code Jam 2013

Drummer (9pts, 20pts)

This problem can be viewed as a modified version of finding the minimum width of a convex hull [1] which can be solved using a modified version of the rotating calipers technique [2,3]. We present below the intuition on why the problem is similar to finding the modified minimum width of the convex hull.

First off, we can plot the sequence of drum strikes as (i, Ti )on the 2-dimensional plane. If the drummer performs perfectly, then let's call the sequence Pi and (i, Pi) falls on the same line L (see Figure 1 and Figure 2).

Figure 1

Figure 2

Unfortunately the drummer does not perform perfectly and has error Ei (which is |Ti - Pi|) (see Figure 3).

Figure 3

So in the example in Figure 3, we have our line L, and it is clear that our answer is the maximum among all Ei, which in this case is E1 (or E2 or E4). If we draw two lines parallel to L shifted in the y-axis by +E1 and -E1, denoted as L+E1 and L-E1 (see Figure 3), you will notice that the region between the two lines contains all the points (i, Ti); this example fits the definition of the problem which is a drum rhythm where each strike differs by at most E from some perfect rhythm. On the other hand, if we choose E5 as the error candidate, the region between the two lines L+E5 and L-E5 will not contain all the points (i, Ti) (see Figure 4).

Figure 4

In essence, we want to find a line denoting the perfect rhythm and two parallel lines that are shifted in the y-axis by +E and -E such that the two parallel lines contain all the points (i, Ti) and also that E is the minimum possible value. We would like to point out that instead of trying to find the line with the perfect rhythm which can be a hard problem due to the error associated with each (i, Ti), we can instead focus on the equivalent problem of finding the two parallel lines. Note that each of these two lines must touch at least one point otherwise we can always reduce the distance between these two lines, and further reduce the distance on y-axis to get a smaller error E.

In fact, all candidate parallel lines touch (without intersecting) the convex hull of the points (i, Ti). Therefore we transform the problem to finding the minimum distance between two parallel lines in y-axis touching (without intersecting) the convex hull. For example, the convex hull of the points in Figure 2 are shown in Figure 5.

Figure 5

Therefore to solve this problem, we first compute the convex hull of the points (i, Ti). Then we go through all the line segments on the boundary of the convex hull and find the corresponding parallel line on the opposite side of the convex hull (see Figure 5 for some examples), then get the y-distance between these two parallel lines, and report the minimum among all such y-distances.

World Finals 2013 - Code Jam 2013

X Marks the Spot (10pts, 29pts)

Median choice
Since the four quarters formed by the two lines have to contain the same number of mines, each of the two lines has to split the set in half. Thus, if we know the inclination of the two lines (given as, e.g., the directed angle the first line forms with the horizontal axis), we can position the each line in any place such that it splits the points in half (by, for instance, sorting the points by the value of the cross-product with the direction of the line). Let's begin by choosing any angle alpha as our initial angle, and draw the two lines according to the procedure above. 

Suppose that the first quarter contains X points. The second quarter contains 2N - X, since there have to be 2N points above the red line. The third quarter will again contain X points (because there are 2N points to the right of the green line), and the fourth will contain 2N - X. So if X = N, our two lines are a correct solution. This doesn't need to be the case, though, as we can see on the figure above.

Rotating the lines
If the angle alpha we chose happens not to be a correct solution to the problem, we will try rotating the lines (by increasing alpha) until it becomes valid.

Let's consider what happens when we rotate the lines. At some moment, one or both of the lines will rotate to a point where instead of splitting the set neatly in half, the "splitting line" passes through two mines, with 2N-1 mines on either side (note that we are taking advantage of the fact that no three points are collinear, so we know the line passes through exactly two points, and not, say, four). We will call the moment at which there are two points on at least one of the splitting lines a "discontinuity". After we rotate a tiny bit more, the lines split the set neatly again.

We will prove in a moment that at any discontinuity, X changes at most by one (it can also happen to stay the same). Notice, however, that when we increase alpha by 90 degrees, the red and green lines will exchange places, which means that the first quarter (which also rotated by 90 degrees) now contains 2N-X points. Thus, somewhere in between X had to be exactly equal to N!

Discontinuity
Let's analyze what happened after the discontinuity. Obviously, only the points that were on the dividing lines could have changed quarters at the discontinuity. One of the points that were on the red line crosses from one of quarters (1, 2) to one of quarters (4, 3), and the other point crosses in the other direction. Thus, if the discontinuity had points on one line only, X changes by at most one.

We will see that even if there were points on both lines at the discontinuity, X will still change only by one. The points on the red line go from quarters (1, 4) to quarters (2, 3). So, for X to, say, grow by two, we would have to have a point from 2 go to 3 and point from 4 go to 1 on the red line, while a point from 4 went to 3 and a point from 2 went to 1 on the green line. However, the red line (and the green line as well, but for now it's the red line that matters) is rotating clockwise - thus, it's the more leftward point that will go down, and the more rightward point that will go up — so the situation described above is impossible.

Finding the solution
We can now use binary search to find a solution to the problem. Begin with an arbitrary angle alpha as the left bound, and alpha + 90 degrees as the right bound, assume that for the left bound X is smaller than N (if it's equal, we're done, and if it's larger, take alpha + 90 as left and alpha + 180 as right). We know somewhere between the two there is a point in which X = N. So we pick an angle midway between left and right, and check how big is X for this median angle. If it's equal to N, we are done. If it's smaller, the median angle is our new left, if it's larger, it's the new right. Proceed until success.

A single iteration, with the standard implementation, takes O(N logN) time - sort the points twice, assign each to the appropriate quarter, find X. If somebody cared enough (one doesn't need to in this problem) it can be done in O(N) time, using a faster algorithm to find the median values (either a randomized one, like quicksort, but recursing only into the bigger of two halves, or even deterministically with the median of medians algorithm.

The key question is "how many iterations of the binary search algorithm will we need to find the angle we are looking for?". This depends on the size of the interval of angles that we will try to hit. This interval will occupy the space between some two discontinuities, and each discontinuity is defined by a line connecting two of the input points - thus, the size of the interval can be expressed as an angle between two lines, each crossing two points with integral coordinates no larger than 10-6. Such an angle can be on the order of 10-12, which means we will need roughly 40 steps of the binary search. Thus, our algorithm will easily run in time.

Precision issues
There are three types of precision issues that can hit one on the solving of this problem.

First, it can happen that one of the lines we choose happens to be a discontinuity. This can be either worked around (by choosing a median angle a bit to the left or right - there are only finitely many discontinuities, after all), or avoided by choosing a random angle to begin with - since there are finitely many discontinuities, it's very unlikely to hit one. It's also possible to choose a deterministic angles to avoid discontinuities.

Second, the interval of angles that we are trying to find can happen to be rather small, and so we will need many iterations of the binary search. This means that we need to use relatively high precision numbers to deal with the quantities involved. It's a pretty standard issue in geometry problems, though, and shouldn't surprise anyone.

Third, there are limits on the precision with which we can output the result, and the checker for the problem will check whether the output values are correct. Since there is a finite precision of the output, there will be some rounding happening. This means that if we were unlucky, and our chosen angle happened to be quite close to the boundary of the "good" interval, the rounding can push it out of the interval. There are two things we can do to mitigate this. First, we can add a few more steps of the binary search to find a few more points within the good interval, and then choose for our answer a point that we know is relatively far from the edge. Second, we should use all the precision that we are allowed in the input. In particular, when we know the line we want to draw, we should choose the second point we output (the one other than the crossing) to be as far away from the crossing as the limits on the output allow us, to minimize the error in the angle of the line resulting from rounding.

World Finals 2013 - Code Jam 2013

Can't Stop (11pts, 32pts)

Divide and Conquer
There is a nice divide-and-conquer solution. Split the input down the middle; the best interval is either entirely in the left half, entirely in the right half, or crosses the middle. We handle the right and left cases recursively; if we can handle the "crosses" case in linear time, we will have an O(N lg N) algorithm, which is good enough.

If the best interval crosses the middle, we know it must use the middle element, so we must pick one of the D numbers in that roll to use. Now go out as far as we can to the left and right with that number. Once we stop, we can extend either to the left or the right, so there are 2D different numbers to try. Expand again, and get 2D more numbers to try for a final expansion. In total, we only tried D*2D*2D different choices, so the whole thing runs in 256N time, and we get our O(N lg N) time algorithm.

Note: surprisingly for a divide and conquer algorithm, we don't use any information from the left and right halves to solve the middle case; the fact that it crosses is enough.

Slow and Simple

There's another set of approaches to this problem. Try every starting position, expanding out to the right and only making choices when necessary (just as in the divide-and-conquer). This is O(DK * N) for each starting position, or O(DK * N2) overall, which is too slow. However, there are several different improvements (some small, some large) one could make in order to make this linear in N.

Linear Time

One improvement works in this way: Every time you pick a number, check if the roll before your starting position contains that number. If so, we can safely ignore that choice (because we would have done better starting one roll earlier and choosing the same set of numbers).

It turns out that this runs in linear time, but the reason why isn't obvious. We want to prove that a particular index i is only visited from O(1) starting positions. Imagine we have reached i from start. Now imagine starting at i and going leftward (as usual, we expand left as far as possible before picking each new number). If we only choose numbers from the set that got us from start to i, we must get back to start (and no further, since the optimization above guaranteed that start - 1 doesn't contain any of our numbers). But there are only 1 + D + D2 + D3 different choices of numbers starting from i, so there are only that many possible positions for start, so i will only be visited O(D3) times.

World Finals 2013 - Code Jam 2013

Let Me Tell You a Story (14pts, 50pts)

Dissecting the problem
The problem statement asks for the number of ways to remove elements from a sequence until we obtain a non-increasing sequence. Looking at the problem from the end, suppose we know the final non-increasing sequence, and it's of length K. Certain elements have been eliminated, and we removed N-K elements in total. There are (N-K)! (factorial of N-K) ways to remove those elements. So the first approximation seems to be to sum those factorials over all non-increasing subsequences of the the original sequence.

However, this is not entirely correct. Some non-increasing subsequences are not even reachable since we stop as soon as our sequence becomes non-increasing. For example, in the second example from the problem statement the sequence is non-increasing from the start, so no proper subsequence is reachable at all. For subsequences that are reachable, not every way of reaching them might be possible. For example, in the first example from the problem statement we can reach the '7 <first 6>' subsequence (note, it should be considered different from '7 <second 6>' subsequence) by eliminating the second 6, and then 4. However, it can't be reached by doing those operations in reverse order, since we'd stop right after eliminating 4.

Now instead of all (N-K)! ways to reach a certain subsequence, we need to count just the possible ways. The main trick in solving this problem is: let's count the impossible ways instead, and then subtract. The impossible ways are simply those when before the last removal, the subsequence was already a non-increasing sequence of length K+1. And for each such subsequence there are exactly K+1 ways to make an impossible removal and arrive at a subsequence of length K. That means the sum of numbers of impossible ways to reach all non-increasing subsequences of length K is equal to the total number of non-increasing subsequences of length K+1 times (N-K-1)! (the number of ways to reach the longer subsequence) times K+1.

To summarize, suppose AK is the number of non-increasing subsequences of length K. Then the answer to this problem is sum over K of AK*(N-K)!-AK+1*(N-K-1)!*(K+1).

Solving the sub-problem
We've now reduced our problem to a much simpler one: find AK. This problem can be solved used a somewhat standard dynamic programming approach, with a twist to make it run faster.

First, let's assume that all input numbers are different, and between 0 and N-1. It's not hard to transform them in this way without changing the answer. If there are equal numbers, we'll slightly reduce the number to the right, so that non-increasingness of all subsequences is preserved.

Our dynamic programming problem will now be: what is the number of non-increasing subsequences of length P that end with number Q? Let's call that number BP,Q. We will find them in the order Qs appear in the sequence.

It's not hard to see that BP,Q is just the sum of BP-1,Q' for all numbers Q' that are greater than Q and appear before Q in the sequence. Since we process the states in the order Qs appear in the sequence, we just need to take the sum over all Q' that are greater than Q.

This is already a working solution for our problem, but it is a bit too slow: it runs in O(N3) which is a bit too much for N=8000. We have O(N2) states in our dynamic programming, and we need to find a sum of O(N) numbers to process each state. However, we can compute such sum faster! We just need an array-like data structure that supports changing elements and finding the sum of its suffix (over all Q' that are greater than Q), and the Fenwick tree is a data structure that does exactly that, performing each operation in O(logN) time, for a total running time of O(N2*logN).

Qualification Round 2014 - Code Jam 2014

Magic Trick (6pts)

In the first arrangement, when the volunteer tells the magician the row that contains her card, the magician is able to see a set of four cards in that row. Similarly, in the second arrangement, the magician is able to see another set of four cards in the selected row.

Therefore, after hearing the two volunteer’s answers, the magician will have two sets of cards:
- the first set of four cards that lie in the selected row in the first arrangement, and
- the second set of four cards that lie in the selected row in the second arrangement.

To know which card the volunteer chose, the magician must be able to find exactly one card that is in both sets (i.e., the intersection size of the two sets must be equal to one). If there is more than one card that is in both sets (i.e., the intersection size is bigger than one), then it means the magician can not determine which card the volunteer chose since there are more than one possible card that could have been chosen by the volunteer (the magician did a bad job). If none of the card in the first set is in the second set (i.e., the intersection size is zero), then there is no card consistent with the volunteer's answers (the volunteer cheated).

The sample input covers all three possible cases:

In Case #1, the two sets are {5, 6, 7, 8} and {9, 10, 7, 12}. There is exactly one card (i.e., card 7) that is in both sets, and thus the volunteer chosen card must be 7.

In Case #2, the two sets are {5, 6, 7, 8} and {5, 6, 7, 8}. The card chosen by the volunteer can be any of {5, 6, 7, 8} because all of them contains in both sets. Bad magician!

Lastly, in Case #3, the two sets are {5, 6, 7, 8} and {9, 10, 11, 12}. None of the cards in the first set is in the second set. The volunteer must have cheated!

Qualification Round 2014 - Code Jam 2014

Cookie Clicker Alpha (8pts, 11pts)

In this problem, we need to decide on the number of cookie farms to buy and also need to decide on when to buy the farms.

The strategy is perhaps surprisingly simple: first, collect enough cookies to buy a farm. Then figure out whether it's faster to buy one farm and then collect X cookies, or simply to collect X cookies now. If it's faster to collect X cookies now, you should do that. If it's faster to buy a farm first, buy that farm and then repeat this process (collect enough cookies to buy another farm...).

It's very easy to say that, but it's not as easy to prove it works. How many farms might you end up buying? If it's in the billions, your program might be too slow, and we never proved it wouldn't be. We also didn't prove that it's best to buy a farm right away as soon as you have enough cookies. The rest of this editorial will go into those questions in detail.

We build the intuition for the solution by using geometry. We represent the problem in the 2d plane. Let the x-axis represent time (in seconds) and the y-axis represent the number of cookies. Initially, we gain cookies at the rate of 2 cookies per second which is shown by line L0 in Figure 1. Let’s say the target number of cookies (X) is 16. We can represent it with line y=16 (LX). This means that if we do not buy any cookie farm then the time it takes to get 16 cookies is given by the intersection between LX and L0. See Figure 1.

Figure 1

Now, let’s delve into what happens (geometrically speaking) when we buy a cookie farm. Let’s say the cost for buying a cookie farm (C) is 6, and the extra cookies per second (F) is 2. In Figure 2, we buy a farm as soon as we have 6 cookies. This means at time = 3, we go from having 6 cookies to 0 cookies (to pay for the cookie farm), and our cookies per second increases to 4. This information is represented by L1 in Figure 2. Note that the dashed lines represent the drop of current cookies when we buy a cookie farm. Notice that L0 and L1 intersect at the 6 second mark (and correspondingly, X=12 cookies). It means that, if our target number of cookies X is anywhere between 0 and 12 then it is not advantageous to buy a cookie farm! Why? Let’s look at an example line LXa which is in that range. In Figure 2, we see that L0 intersects LXa earlier (at 4 second mark) than L1 intersects LXa (at 5 second mark). At X = 12 (represented by LXb), it does not matter if we buy a cookie farm or not. But if X is higher than 12, for example X = 16 (represented by LX), we should buy a cookie farm since L1 intersect LX earlier (at 7 second mark) than L0 (at 8 second mark). Jumping ahead briefly, we notice a similar behavior for the intersection between L1, L2 and LX in Figure 4 (we'll describe how we compute L2 in subsequent paragraphs). If we choose X = 18, it doesn't matter if we choose L1 or L2, but if X is below 18 then L1 intersection is better than L2 intersection, but if X is above 18 then L2 intersection is better than L1 intersection.

Figure 2

Now we discuss the strategy for how early we should buy a cookie farm i.e. should we buy a cookie farm as soon as we have C cookies, or should we wait a little longer before buying a cookie farm? We claim that we should buy a farm as soon as we have C cookies (and not wait any second longer).

Figure 3

In Figure 3 as before, L1 represents buying a cookie farm as soon as we have 6 cookies (at 3 second), while L1a represents delaying buying a cookie farm by a second (at 4 second). Note that L1 and L1a are going to be parallel to each other (i.e. they have the same rate: 4 cookies for second) but L1 is located to the left of L1a. What does this mean? It means that the intersection between any line LX and L1 will always be at an earlier time than the intersection between LX and L1a. Therefore we should not wait to buy cookie farms any more than needed. This means that if buying a cookie farm contributes to your winning strategy, then we should buy a cookie farm as soon as possible, i.e. as soon as we have C cookies.

In Figure 4, we can observe that the earliest time to buy the first cookie farm is on the intersection of line L0 with line LC (y=C). Then, the earliest time to buy the second cookie farm is on the intersection of line L1 with LC.

Figure 4

Now, we are ready to describe our solution strategy. We first determine the time t0 it takes to get X cookies without buying a cookie farm (i.e. intersection between L0 and LX). Then we try to buy 1 cookie farm and figure out the time t1 it takes to get X cookies (i.e. intersection between L1 and LX). Then compute t2 for buying another cookie farm (i.e. intersection between L2 and LX), and so on. We stop when tn+1 is greater than tn (i.e. we do worse, in terms of time, by buying an additional cookie farm). For example, in Figure 4, we do worse with L2 than with L1 (intersections with LX). We finally report tn as our winning time.

A note on doing the actual line intersection computation follows. We want to compute the line intersections between lines L0, L1, L2, etc and y = C or y = X. Let our current line be Ln starting at (Sn, 0) and have a slope of m (i.e. cookies per second after buying n cookie farms). Note that s0 is 0, and m = 2 + n * F. Then the time required to get A cookies is given as: Sn + A / m.

Our solution strategy mentioned above iterates until a winning condition is achieved. But you might be wondering about total iterations needed before we are done. We want to point out that the number of iterations is bounded. In the solution strategy, we noted that the stopping condition for iteration is when we do worse (in terms of time) when buying an additional farm. Let’s formulate that as an equation. Let’s say our current iteration is i with line Li with the next line being Li+1. The intersection between line y = X and Li is given as ti = si + X / (2 + i * F), and similarly intersection between line y = X and Li+1 is given as ti+1 = si+1 + X / (2 + (i + 1) * F). We stop when ti+1 > ti. Note that si+1 - si = C / (2 + i * F). After going through some math, we get i > (X / C) - 1 - (2 / F), which is the iteration when ti+1 becomes bigger than ti. Therefore the iteration should terminate around X / C.

Qualification Round 2014 - Code Jam 2014

Deceitful War (14pts, 16pts)

Ken's best possible game of War
First, let's think about the best possible outcome for Ken in a game of War. Let's suppose the maximum number of points Ken can get is k; then after a little work convincing yourself, you should see that Ken can achieve that outcome if Ken's k heaviest blocks are played in decreasing order against Naomi's k lightest blocks, also in decreasing order.

That exact pairing likely won't happen, and without knowing Naomi's blocks' weights, Ken doesn't even know what the pairing would be; but Ken can follow a simple strategy to score k points anyway.

Ken's strategy
Ken's strategy is simple: when Naomi plays a block, Ken beats it with the lightest possible block if he can; and if he can't beat it, he plays his lightest block. It is clear that using such strategy will maximize Ken's winning potential for the following rounds because Ken not only wins one point for this round, but also preserves his heavier blocks for the following rounds.

The following block of text proves that this strategy will earn Ken the maximum possible number of points, k, no matter what Naomi does. If that's obvious to you, or you aren't into formal proofs, go ahead and skip it.

Proof
In this strategy, to win k points Ken wants to maintain an invariant: after Ken has scored i points, Ken's k-i heaviest blocks will still beat Naomi's k-i lightest blocks. When k-i is zero, Ken cannot win any more points because his heaviest block is lighter than any of Naomi's blocks. Therefore, if that invariant is maintained Ken must have k points when there are no more blocks left.

Proof of invariant:
Suppose Ken has i points. We'll refer to "pairs" of blocks later: Ken has k - i heaviest blocks remaining and his j-th heaviest remaining block is "paired" with Naomi's (k-i-j)-th lightest remaining block. There are three types of blocks Naomi might play:

If Naomi plays a block that's heavier than all of Ken's, it isn't from Naomi's lightest k-i blocks. Ken will play his lightest block which isn't from his heaviest k-i blocks, and the invariant is maintained.
If Naomi plays a block that's lighter than one of Ken's blocks but isn't from Naomi's lightest k-i blocks, Ken will either beat it with his heaviest block—in which case Naomi's lightest k-i-1 blocks will lose to Ken's heaviest remaining k-i-1 blocks, since nothing has changed about how the remaining blocks are paired—or Ken will beat it with something lighter, in which case his position is obviously no worse. Either way Ken has i+1 points and the invariant is maintained.
If Naomi plays a block from her lightest k-i blocks, Ken will either beat it with the block it's paired with, in which case Ken now has i+1 points and the remaining k-i-1 pairs are maintained; or Ken will beat it with something lighter, in which case Ken is clearly no worse off. Either way Ken has i+1 points and the invariant is maintained.
Naomi's best possible game of War
As we saw in the proof above, Ken can force his maximum possible number of points in War no matter what Naomi does. No wonder Naomi is tired of playing it!

Naomi's best possible game of Deceitful War
In Deceitful War, however, it turns out that the situation is exactly reversed: as we'll show in the following paragraphs. In War, Ken got the best possible pairings for himself but Naomi will get the best possible pairings for herself in Deceitful War.

Naomi's strategy for Deceitful War
There are several strategies for Naomi to play Deceitful War optimally. We present one here. Naomi's strategy for that is almost trivial. Assume k is the best possible score Naomi could achieve. Naomi can do score k points by pairing her k heaviest blocks with Ken's k lightest blocks.

Naomi will take her i-th heaviest block and tell Ken that it is heavier than all of Ken's blocks. Ken will believe her and play his lightest block, which is what Naomi wanted. Now Naomi simply has to repeat the process of playing her remaining heaviest blocks from lightest to heaviest and inciting Ken to play his lightest block from lightest to heaviest. After Naomi scores k points, her heaviest block will be lighter than any of Ken's available blocks. Now, since Naomi cannot lie anymore Naomi plays her remaining blocks without lying about their mass.

Conclusion
It's a particularly beautiful piece of symmetry that Ken can achieve his optimal pairing by being reactive, despite being honest and without information; and Naomi can reverse Ken's advantage by beating Ken's reactiveness with dishonesty and perfect information.

Qualification Round 2014 - Code Jam 2014

Minesweeper Master (11pts, 24pts)

There are many ways to generate a valid mine configuration. In this analysis, we try to enumerate all possible cases and try to generate a valid configuration for each case (if exists). Later, after having some insight, we provide an easier to implement algorithm to generate a valid mine configuration (if exists).

Enumerating all possible cases
We start by checking the trivial cases:

If there is only one empty cell, then we can just fill all cells with mines except the cell where you click.
If R = 1 or C = 1, the mines can be placed from left to right or top to bottom respectively and click on the right-most or the bottom-most cell respectively.
If the board is not in the two trivial cases above, it means the board has at least 2 x 2 size. Then, we can manually check that:

If the number of empty cells is 2 or 3, it is Impossible to have a valid configuration.
If R = 2 or C = 2, valid configurations exists only if M is even. For example, if R = 2, C = 7 and M = 5, it is Impossible since M is odd. However, if M = 6, we can place the mines on the left part of the board and click on the bottom right, like this:
            ***....
            ***...c
If the board is not in any of the above case, it means the board is at least 3 x 3 size. In this case, we can always find a valid mine configuration if the number of empty cells is bigger than 9. Here is one way to do it:

If the number of empty cells is equal or bigger than 3 * C, then the mines can be placed row by row starting from top to bottom. If the number of remaining mines can entirely fill the row or is less than C - 2 then place the mines from left to right in that row. Otherwise, the number of remaining mines is exactly C - 1, place the last mine in the next row. For example:
            ******        ******
            *****.        ****..
            ......   ->   *.....
            ......        ......
            .....c        .....c
If the number of empty cells is less than 3 * C but at least 9, we first fill all rows with mines except the last 3 rows. For the last 3 rows, we fill the remaining mines column by column from the left most column. If the remaining mines on the last column is two, then last mine must be put in the next column. For example:
            ******        ******
            **....   ->   ***...
            **....        *.....
            *....c        *....c
Now, we are left with at most 9 empty cells which are located in the 3 x 3 square cells at the bottom right corner. In this case, we can check by hand that if the number of empty cells is 5 or 7, it is Impossible to have a valid mine configuration. Otherwise, we can hard-coded a valid configuration for each number of empty cell in that 3 x 3 square cells.

Sigh... that was a lot of cases to cover! How do we convince ourselves that when we code the solution, we do not miss any corner case?

Brute-force approach
For the small input, the board size is at most 5 x 5. We can check all (25 choose M) possible mine configurations and find one that is valid (i.e., clicking an empty cell in the configuration reveal all other empty cells). To check whether a mine configuration is valid, we can run a flood-fill algorithm (or a simple breath-first search) from the clicked empty cell and verify that all other empty cells are reachable (i.e., they are in one connected component). Note that we should also check all possible click positions. This brute-force approach is fast enough for the small input.

The brute-force approach can be used to check (for small values of R, C, M) whether there is a false-negative in our enumeration strategy above. A false-negative is found when there exist a valid mine configuration, but the enumeration strategy above yields Impossible. Once we are confident that our enumeration strategy does not produce any false-negative, we can use it to solve the large input.

An easier to implement approach
After playing around with several valid mine configurations using the enumeration strategy above, you may notice a pattern: in a valid mine configuration, the number of mines in a particular row is always equal or larger than the number of mines of the rows below it and all the mines are left-aligned in a row. With this insight, we can implement a simpler backtracking algorithm that places mines row by row from top to bottom with non-increasing number of mines as we proceed to fill in the next row and prune if the configuration for the current row is invalid (it can be checked by clicking at the bottom right cell). This backtracking with pruning can handle up to 50 x 50 sized board in reasonable time and is simpler to implement (i.e., no need to enumerate corner / tricky cases).

If the contest time were shorter, we may not have enough time to enumerate all possible cases. In this case, betting on the backtracking algorithm (or any other algorithm that is easier to implement) may be a good idea. Finding such algorithms is an art :).

Round 1A 2014 - Code Jam 2014

Charging Chaos (8pts, 17pts)

This problem is looking for the minimum number of switches that need to be flipped so that all devices can be charged at the same time. All devices can be charged at the same time if each of the outlets can be paired with exactly one device and vice versa. An outlet can be paired with a device if both have the same electric flow after flipping some switches.

We observe that flipping the same switch two times (or an even number of times) is equivalent to not flipping the switch at all. Also, flipping the same switch an odd number of times is equivalent to flipping the switch exactly once. Thus, we only need to consider flipping the i-th switch once or not at all.

Let’s define a flip-string of 0s and 1s of length L as a way to flip the switches. If the i-th character of the string is 1, it means we flip the i-th switch (otherwise we do not flip the i-th switch). A flip-string is good if the resulting outlets’ electric flows (after the flips) allow all the devices to be charged at the same time. We can check whether a flip-string is good by flipping the bits in the outlets’ electric flow according to the flip-string and then checking whether each device’s electric flow can be matched exactly to one outlet with the same electric flow and vice versa. We can use hashing to perform the check and thus the complexity to check whether a flip-string is good is O(LN). Note that we can (optionally) encode the flip-string into a 64-bit number and reduce the complexity of the check to O(N).

Brute force algorithm:
A naive brute force solution is to try all the 2L possible flip-strings and check whether it is a good flip-string and keep the one that has the least number of 1s in the string (i.e., it has the minimum number of flips). The time complexity of this algorithm is O(2L * LN). For the small dataset where the maximum for L is 10, this brute force algorithm is sufficient. However, it is really slow for the large dataset where L can be as large as 40.

Better algorithm:
To improve the naive brute force algorithm we need two more observations. First is that there are only a few good flip-strings. If we can efficiently generate only the good flip-strings, we can improve the algorithm complexity significantly. The second observation is that given a device’s electric flow (of L bits) and an outlet’s electric flow (of L bits), we can generate a flip-string that will flip the outlet’s electric flow such that it matches the device’s electric flow. There are only N2 possible pairs of devices and outlets. Thus, only N2 flip-strings need to be generated. Note that the generated flip-strings may or may not be a good flip-string. However, all other flip-strings that are not generated by any pair is guaranteed to not be a good flip-string. With these observations, we can reduce the complexity of the naive algorithm down to O(N2 * LN), which is fast enough for the large input.

The last (optional) observation we can make is that since a device must be plugged in to exactly one outlet, we can further reduce the number of possible flip-strings from N2 down to N. The N possible flip-strings can be generated by pairing any one device with the N outlets.

Round 1A 2014 - Code Jam 2014

Full Binary Tree (9pts, 21pts)

Brute force algorithm:
Given a tree with N nodes, there are 2N subsets of nodes that can be deleted from the tree. The brute force algorithm is to simply check all possible node deletions and pick the one where the remaining nodes form a full binary tree and the number of nodes deleted is minimum. Note that when a set of nodes are deleted from the tree, the remaining nodes may not be a tree anymore (since the tree may become disconnected).

Let’s say there are M remaining nodes after deletion. One way to check whether the remaining nodes form a full binary tree is to first check whether it is still a tree. This can be done by performing a simple breadth-first or depth-first search to check that the remaining nodes are in one connected component. Next, we need to find a root in that tree where all its subtrees have either zero or two children. This leads to an O(M2) algorithm for checking if the remaining nodes form a full binary tree.

Another way to check whether a graph is a full binary tree is via counting which is linear O(M):

For the trivial case where M equals to 1, it is a full binary tree.
For M > 1, it must satisfy that there is exactly one node having degree two and the rest of the nodes must have degree either one or three. Also, the number of remaining edges must be equal to M - 1 to signify that it is a tree, and not a disconnected graph.
Therefore, using different methods for checking whether the resulting graph is full binary tree or not you end up with a brute force algorithm that runs in O(2N * N2) or O(2N * N), which is sufficient to solve the small dataset. But it is not sufficient to solve the large dataset where N = 1000. In the following sections, we address the large dataset.

Quadratic algorithm:
The insight to the quadratic algorithm is that minimizing the number of deleted nodes is equivalent to maximizing the number of nodes that are not deleted.

Let’s pick a root for the tree. The parent-child relationship is defined based on the chosen root. For each node, we will make a decision to delete it or keep it with the goal of maximizing the size of the full binary subtree. We now investigate the various cases for each node. If a node has only one child, then we have to remove that child (as it violates the full binary tree condition of having 0 or 2 children). Therefore, the size of that subtree is only one node (i.e. we only count the node itself). If it has more than one candidate children, we have to pick two of them as the children of the node. But which two? We pick the two which retain the maximum number of nodes in their own subtrees, which we will demonstrate later.

We can implement the above idea via a greedy post-order depth-first-traversal from a chosen root. During the traversal, we determine the maximum number of nodes we can retain in each subtree to form a full binary tree. The function maxSubtreeNodes(node, parent) is as follows:

The function’s parameters are the current node and its parent.
If the current node has 0 or 1 children then the number of nodes in this subtree is 1.
Otherwise, we have at least two subtrees and we should recursively call the function for each child of the current node. Remember, we want to maximize the number of nodes retained in the current node’s subtree, therefore we keep the two children that have the largest subtrees rooted at those children. We return the size of the two largest subtrees + 1 (1 to account the current node).
Finally, given this function the minimum number of nodes to be deleted for a given root node is:

N - maxSubtreeNodes(root, 0)  // 0 as the root does not have a parent.
We can run the maxSubtreeNodes(root, 0) function by picking each of the N nodes as the root. We pick the root node that minimizes N - maxSubtreeNodes(root, 0). As maxSubtreeNodes(root, 0) is a depth-first-traversal on a tree, it runs in O(N) time. Also, we run the traversal N times picking each node as the root. Therefore, the time complexity is O(N2).

Figure 1 shows an example where we demonstrate running maxSubtreeNodes(1, 0). Node 8 is deleted as it is the only child for node 7. Then node 7 is deleted for the same reason. After that, node 1 will have to choose only two children among nodes 2, 3, and 4. These 3 subtrees have sizes 1, 3, and 1, respectively. So node 1 will choose the maximum two subtrees which are nodes 3 and 4 (note we could choose node 2 instead of node 4 too). So for maxSubtreeNodes(1, 0), the maximum number of nodes to keep is 5 (nodes 1, 3, 4, 5, and 6). Equivalently, the minimum number of nodes to be deleted is 3 (nodes 2, 7 and 8).

Here is the pseudo-code for this algorithm:

minDeletions = infinity
for root = 1 to N:
  minDeletions = min(minDeletions, N - maxSubtreeNodes(root, 0))

def maxSubtreeNodes(currentNode, parent):
  maximumTwoNumbers = {}  // Structure that keeps track of 
  // the maximum two numbers.
  for x in neighbors of currentNode:
     if x == parent:
       continue
     update maximumTwoNumbers with maxSubtreeNodes(x, currentNode)
  if size of maximumTwoNumbers == 2:
    return 1 + sum(maximumTwoNumbers)
  return 1
Linear algorithm:
The previous quadratic algorithm is sufficient to solve the large dataset but you might be interested in a solution with better time complexity. We present here a linear time algorithm which is based on the quadratic time algorithm.

In the quadratic algorithm, we take O(N) to compute the size of the largest two subtrees for any root node (remember, we brute force over all N possible root nodes). So our goal in the linear time algorithm is to compute the size of the largest two subtrees for any node in constant time. To do so, we will precompute the three largest children subtrees (not only two unlike in the quadratic algorithm). We will explain why we need three largest children in a subsequent paragraph.

So let's define our precomputation table structure. It is a one-dimensional array of objects called top3, which is defined as follows:

  class top3:
    class pair:
      int size
      int subtreeRoot
    pair children[3]  // children is sorted by the size value.
    int parent
In the greedy DFS function maxSubtreeNodes(node, parent), there are only 2 * (N-1) different parameter pairs for the function (a parameter pair is the node, parent pair, and also there are exactly N-1 edges in any tree). The key to our linear algorithm is to run DFS once on the given tree from some root node (say from node 1), and during this traversal for every node, to store the three largest subtrees among its children while also keeping track of the parent of the node. We will explain later why we need to keep track of the parent. This DFS traversal will be O(N) as it visits every node exactly once.

Figure 2 is an illustration figure for the stored top3 objects for every node after calling maxSubtreeNodes(1, 0). For node 2, {1, 5} means the subtree rooted at the child node 5 has maximum size 1, and “Parent: 1” means the parent of node 2 is 1.

But still after doing this precomputation there is something missing in our calculation. We assumed that node 1 is the root but in the original quadratic algorithm, we need to try all nodes as root. We now show how we can avoid having to try all nodes as root. In the function described above, for any node its parent is not considered as a candidate to be put in that node’s top3 children array. We now describe an example to show how we account for the parent of a particular node. In the example, we will use the parent’s information to update the top3 array for the current node.

We describe a local update process to fix the top3 array for node 2. Remember that node 2’s parent is node 1 (see Figure 2). Let us pretend that node 2 is picked as the root (shown in Figure 3). We then look at its parent’s (node 1’s) top3 array. Note that the top element in the top3 array for node 1 is in fact node 2! We update node 2’s top3 array using node 1’s top3 array, therefore we exclude the result for node 2 from node 1’s top3 array. After excluding node 2 from the top3 array of node 1, the resulting pair describing node 1 is {5, 1} (i.e. size of subtree rooted at node 1 excluding node 2 is 5: nodes 1, 3, 4, 7 and 8). We now update the top3 array for node 2 with {5,1}. Therefore, now node 2’s top3 array is: {5,1}, {1,5}, {1,6}, and the size of the largest full binary tree rooted at node 2 is 7 (5 + 1 from the first two children, and 1 to count node 2 itself).

We perform the local update process described in the previous paragraph in a pre-order depth-first-traversal starting from node 1. Note that when doing the local update, excluding the current node from its parent’s top3 array might result in an array with only one element. In such cases, the parent’s subtree (excluding the current node) should be of size 1.

Well, you might be still wondering why we need the three largest subtrees and not two. Observe in Figure 3, if we had stored only the top 2 instead of the top 3 subtrees in node 1 (meaning only {3,2} and {3,4}) and we excluded node 2 during the local update, then we will only have the pair {3,4} which describes node 4 but ignores node 3! This would be incorrect as node 3 is part of a full binary tree rooted at node 1. Therefore we keep information for the largest three subtrees.

Round 1A 2014 - Code Jam 2014

Proper Shuffle (45pts)

This problem is a bit unusual for a programming contest problem since it does not require the solution to be exactly correct. In fact, even the best solutions may get wrong answer due to random chance. That being said, there exist solutions that can correctly classify, with high probability of success, whether a permutation was generated using the BAD algorithm or the GOOD algorithm. One such solution is to generate many samples of BAD and GOOD permutations and to look at their frequency distribution according to some scoring functions. Knowing the distributions of the scores, we can create a simple classifier to distinguish the GOOD and the BAD permutations. Note that while the solution seems to be very simple, the amount of analysis needed and the number of trial and errors to produce a good scoring function are not trivial. The rest of the analysis explains the intuition to construct a good scoring function.

What makes a GOOD permutation?
If a permutation of a sequence of N numbers is GOOD, then the probability of each number ending up in a certain position in the permutation is exactly 1 / N. In the GOOD algorithm, this is true for every step. In the first step, the probability of each number ending up in the first position is exactly 1 / N since the length of the sequence is initially N. Now the number in the first position is fixed (i.e., it will not be swapped again in the next steps), we can ignore it in the second step and only consider the remaining sequence of N - 1 numbers. Whichever number that gets chosen in the second step did not get chosen in the first step ((N - 1) / N chance), and got chosen in the second step (1 / (N - 1) chance), so every number has a 1 / N chance of ending in the second position. Continuing this logic, in the end, every number has a uniform 1 / N probability to end up in any position in the permutation sequence.

Why is the BAD algorithm BAD?
Let’s examine the BAD algorithm. At first, the BAD algorithm may look “innocent” that in step i it picks an element at random position and swaps it to the element at position i. However, a deeper look will reveal that the direction in which i is progressing makes the resulting permutation become BAD (i.e., it is not uniform). To see this, imagine we are currently at step i and we pick a random number X at position j where j happens to be less than i. Then X can never be swapped to lesser position than i in the next steps. The number X can only be swapped to higher positions in the permutation. This creates some bias in the permutation.

Let’s examine the frequency of seeing the number i that ends up in position j in the permutation for all i,j. We illustrate the frequency in the figures below. The x-axis (from left-to-right) is the original position of the numbers (from 0 to N - 1). The y-axis (from bottom-to-top) is the position of numbers in the permutation generated by the algorithm (from 0 to N - 1). For this illustration, we use N = 100. We run each algorithm (the GOOD and the BAD separately) 20K times and record the event that number i (in the original position) ends up in position j (in the permutation). The intensity of the pixel at position x=i, y=j represent the frequency of the events. The darker the color represent the higher frequency of occurrence.

We can observe that the GOOD algorithm uniformly distributes the numbers from the original positions to the resulting permutation positions. While the BAD algorithm tends to distribute the lower numbers in the original positions to the higher positions in the permutation (notice the darker region on the top left corner).

A Simple Classifier
With the above insights, we can devise a simple scoring function to classify the BAD permutation by counting the number of elements in the permutation that move to lower positions compared to its original position. See the following pseudo-code for such a scoring function:

def f(S):
  score = 0
  for i in 0 .. N-1:
    if S[i] <= i:
      score++
  return score
The scoring function f takes in a permutation sequence S of length N and returns how many numbers in the permutation sequence that ends up at position that is less than or equal to its original position. If the permutation sequence S is generated by the GOOD algorithm, we expect that the score should be close to 500 for N = 1000. If S is generated by the BAD algorithm, we expect that the score should be significantly lower than 500 (since the lower numbered elements tends to move to higher position). If we run 10K samples for each algorithm and plot the frequency distribution of the scores, we will get the following graph:

The GOOD algorithm produces samples with scores clustered near 500 while the BAD algorithm produces samples with scores near 472. Knowing these scores, we can build a very simple classifier that decides whether a given permutation sequence S is generated by the GOOD or the BAD algorithm with high probability. We can simply check the score of f(S) whether it is near 472 (BAD) or 500 (GOOD), as depicted in the following pseudo-code:

if f(S) < (472 + 500) / 2:
  S is produced by the BAD algorithm
else:
  S is produced by the GOOD algorithm
How accurate is the classifier?
The good thing about being a programmer is that we do not always need formal proofs. We can roughly guess the accuracy of the classifier by generating a number of GOOD / BAD permutations each with 50% probability and check how many permutations are correctly classified using our simple classifier. According to our simulation, the simple classifier achieves around 94.05% accuracy. The simple classifier is good enough to correctly solve 109 test cases out of 120 test cases: this will happen in roughly 94.58% of all inputs. In the unlucky situation that your input is in the other 5.42%, you can download another one. 5.42% seems to be a lot to be given to chance though. What if you were only allowed one submission? In the next section we'll explore one idea out of many that offer higher chances of success.

Naive Bayes Classifier
Let S be the input permutation. We want to find P(GOOD | S), the probability that the GOOD algorithm was used, given that we saw the permutation S. By Bayes’ theorem it follows that:
P(GOOD | S) = P(S | GOOD) * P(GOOD) / (P(S | GOOD) * P(GOOD) + P(S | BAD) * P(BAD))
where:

P(S | GOOD) is the probability that S is generated by the GOOD algorithm
P(S | BAD) is the probability that S is generated by the BAD algorithm
P(GOOD) is the probability that the permutation is chosen from the GOOD set
P(BAD) is the probability that the permutation is chosen from the BAD set
Since we know that P(GOOD) and P(BAD) is equally likely since each algorithm has the same probability to be used, then we can simplify the rule to:

P(GOOD | S) = P(S | GOOD) / (P(S | GOOD) + P(S | BAD))
If P(GOOD | S) > 0.5, it means that the permutation S is more likely generated by the GOOD algorithm. By substituting P(GOOD | S) with the right hand side of the rule in P(GOOD | S) > 0.5 and performing some algebraic manipulation, we can further simplify the expression to P(S | GOOD) > P(S | BAD).

We know the exact value for P(S | GOOD) is 1/N! since there are N! possible permutations. Hence, we only need to find P(S | BAD). If we can find P(S | BAD), we will have an optimal algorithm. Unfortunately, we do not know how to efficiently compute P(S | BAD) precisely. The best algorithm we know is intractable for large N = 1000.

Nevertheless, we can borrow an idea from machine learning and make a simplifying Naive Bayes assumption: let’s assume that the movement of each element is independent. Now we can approximate P(S | BAD):

P(S | BAD)  ≈  P(S[0] | BAD) * P(S[1] | BAD) * ... * P(S[N-1] | BAD)
where P(S[0] | BAD) is the probability that the first element in a random permutation generated by BAD is in fact S[0]. Without the independence assumption, we would have terms like P(S[1] | BAD + S[0]), which means "the probability that the second element is in fact S[1] given that BAD is used to generate the permutation and that the first element is S[0]". Naive Bayes allows us to remove the italicized assumption, which makes the calculation tractable (but not completely accurate).

To be fair, we should make the same simplifying assumption for P(S | GOOD). Luckily, this is easy; in the GOOD algorithm, each element has a 1 / N chance of moving to each position, so P(S | GOOD) = 1 / NN regardless of what S we are given.

Now, let’s see how we can implement the Naive Bayes classifier. Let Pk[i][j] be the probability that number i ends up at position j after k steps of the BAD algorithm. We are interested in the probabilities where k = N (i.e., after N steps have been performed). We can compute Pk from Pk-1 in O(N2) time by simulating all possible swaps. P0 is easy; nothing has moved, so we just have the identity matrix. See the pseudo-code below. Pk[i][j] is the prev[i][j] variable which will contain the probability of number i ends up at position j after k steps generated by the BAD algorithm.

  prev[i][i] = 1.0 for all i, otherwise 0.0  // An identity matrix.
  pmove = 1.0 / N  // Probability of a number being swapped.
  pstay = 1.0 - pmove  // Probability of a number not being swapped.

  for k in 0 .. N-1:
      for i in 0 .. N-1:
          next[i][k] = 0
          for j in 0 .. N-1:
              next[i][k] += prev[i][j] * pmove  // (1)
              if j != k:
                  next[i][j] = prev[i][j] * pstay +
                               prev[i][k] * pmove  // (2)
      Copy next to prev

Note for (1): P[i][k] for the next step is equal to: (P[i][j] in the previous step) * (move probability to k). Note for (2): P[i][j] for the next step is equal to: (P[i][j] in the previous step) * (staying probability at j) + (P[i][k] in the previous step) * (move probability to j).

The above algorithm runs in O(N3). It may take several seconds (or minutes if implemented in a slow scripting language) to finish. However, we can run this offline (before we download the input), and store the resulting probability matrix in a file. Note that the above algorithm can also be optimized to O(N2) if needed. See Gennady Korotkevich's solution for GCJ 2014 Round 1A for an implementation.

Next, we compute the approximation probability of P(S | BAD) as described previously.

  bad_prob = 1.0
  for i in 0 .. N:
    bad_prob = bad_prob * prev[S[i]][i]
Finally, to produce the output, we compare it with P(S | GOOD) and see which one is greater.

  good_prob = 1.0 / N^N
  if good_prob > bad_prob:                             
    S is produced by the GOOD algorithm
  else:
    S is produced by the BAD algorithm
Note that the pseudo code above is dealing with very small probability that may cause underflow in the actual implementation. One way to avoid this problem is to use sum of the logarithm of the probabilities instead of the product of the probabilities.

Empirically, the Naive Bayes classifier has a success rate of about 96.2%, which translates into solving at least 109 cases correctly out of 120 in 99.8% of all inputs.

Finally, in this editorial we described two methods to solve this problem. There are likely other solutions that perform even better and we invite the reader to try come up with such solutions.

Round 1B 2014 - Code Jam 2014

The Repeater (10pts, 13pts)

Let us first examine the winning condition for Omar. For this we will consider the following example:

3
aabcaa
abbcaa
abccaa
We represent the given strings by borrowing an idea from Run-length Encoding. Using the idea from this encoding the string “aabcda” can be represented as “2:a, 1:b, 1:c, 1:d, 1:a”. The representation shows an ordered list of pairs of frequency and the corresponding character, let’s call it the frequency string. Similarly, we can figure out the frequency strings for all the original strings. The frequency strings for the strings in the example are listed below:

aabcaa  ->  2:a, 1:b, 1:c, 2:a
abbcaa  ->  1:a, 2:b, 1:c, 2:a
abccaa  ->  1:a, 1:b, 2:c, 2:a
Note that in the problem, an action is defined as either (i) repeating a character, or (ii) deleting a repeated character. In the frequency string, our allowed actions can be re-stated as follows. The first action is similar to increasing the frequency of a character, while the second action is similar to decreasing the frequency of a character. However note we can not decrease the frequency of a character below 1. Given these strings, we notice that Omar will win when the ordered list of characters (excluding the frequencies) are identical. Otherwise, Fegla will win as the allowed actions can only increase the frequency of a character in the encoded frequency string or decrease the frequency (but it is never allowed to go below 1). So if the ordered list of characters of any two strings do not match it will not be possible for Omar to win.
So we know how to figure out if Omar can win or not, we are now interested in the minimum number of actions to win the game. Notice that the actions performed on the ith character in the frequency strings can be applied independently from characters in other positions in the frequency strings. Thus, we can process a set of characters at position i before processing the characters in the next position. For the example above, we can solve the leading “a” first, then “b”, then “c”, and finally the trailing “a”. From hereon, we will describe solving for one character (at the ith position) in the frequency strings. All other characters can be solved in the same way.

Brute force algorithm:
In the brute force solution, we will try all possible frequencies for a single character, that is for the character “a”, we will try all possible frequencies of “a” which are “1:a”, then “2:a”, then “3:a”, etc. We call each frequency we try the target frequency. In this problem, as the length of the string is only upto a 100 therefore we can try all possible frequencies from 1 through 100. For each target frequency, we can compute the sum of total actions (which will be the absolute difference between the frequency of that character and the target frequency) and maintain the target frequency that gives us the minimum sum of total actions.

The time complexity for the brute force algorithm is O(L*N*X), where L is the maximum length of any string, N is the number of strings, and X is the number of characters in the frequency string.

This solution should be sufficient to solve the provided small and large data set.

Efficient algorithm:
In the brute force solution, we tried all possible frequencies for a single character. But we don’t need to do so, we can just try only one target frequency!

We would like to point out that intuitively, one might think that the mean would be the target frequency. Let’s go through an example. Suppose we are given the frequencies {1, 1, 100}. The mean is 34. The sum of absolute differences from the provided frequencies to the target frequency is 33 + 33 + 66 = 132. But what if the target frequency was 30 instead of 34? Then the sum of absolute differences is 128, which is less than 132! In fact, if we tried all possible values for the target frequency (from 1 through 100), we would find that the minimum is at 1 with a sum of 99. In fact, it is the median that minimizes sum of absolute differences (1 is the median of {1, 1, 100}).

In our example above, the frequencies for leading “a” are {2, 1, 1}, for “b” are {1, 2, 1}, for “c” are {1, 1, 2}, and for trailing “a” are {2, 2, 2}. Therefore for the leading “a”, we would use the median which is 1 as the target frequency. Similarly for “b” and “c” we use 1, while for the trailing “a” the target frequency is 2.

You might be wondering why the median is the right target frequency, we refer to an excellent explanation by Michal Forišek to the question “Why does the median minimize the sum of absolute deviations?” which we quote here:

Imagine the given values as trees along a road. The quantity you are trying to minimize is the sum of distances between you and each of the trees.

Suppose that you are standing at the median, and you know the current value of the sum. How will it change if you move in either direction? Note that regardless of which direction you choose, in each moment you will be moving away from at least half of the trees, and towards at most half of the trees. Therefore, the sum of distances can never decrease -- and that means the sum of distances at the beginning had to be optimal.

In the explanation above, minimizing the sum of absolute deviations is equivalent to minimizing the total actions required to convert any ith character in the frequency string to a target frequency in our problem.

Round 1B 2014 - Code Jam 2014

New Lottery Game (8pts, 24pts)

In this problem, we are given three positive integers A, B and K. We are asked to calculate the number of pairs of integers (a, b) such that:

0 ≤ a < A
0 ≤ b < B
(a AND b) < K (denote AND means bitwise-AND of a and b).
We will refer to the set of valid pairs as S(A, B, K), and the number of valid pairs as f(A, B, K).
Solving the small dataset
For the small dataset, we can simply enumerate all pairs (a, b) that meet the first two constraints and count how many match the third constraint. Most programming languages have the built-in bitwise-AND operation, usually as the character “&”. This procedure will have a time complexity of O(AB), which is sufficient for the small dataset. Following is a sample implementation in Python 3:

def f(A, B, K):
  return len([(a, b) for a in range(A) for b in range(B) if (a & b) < K])
Solving the large dataset
We present two approaches for solving the large dataset in this problem. The first one is a standard Dynamic Programming solution (DP on digits/bits) that can be applied to different problems which are of a similar flavor. The second approach divides the search space into different sets, and computes the size of each set recursively.

First approach
Before delving into the actual solution, let us start thinking about a simpler problem: given an integer M, write a recursive function to count all non-negative integers m where 0 ≤ m < M. The answer is trivially M, but bear with us as our intent for this simpler problem is to build an intuition for the Dynamic Programming (DP) function which will be similar to the DP function we will write for the actual problem.

The core idea is based on counting the number of ways to generate the bits for the number m, such that they are always less than or equal to M but then we only count those that are strictly less than m. We can start by generating the bits of m from the most significant bit to the least significant bit, and for each bit position, generating only the feasible values. Note that the feasible values for the i-th bit can sometimes be both 0 and 1, or only 0.

Let’s say M is 29 (which is 111012 in binary). The most significant bit is the bit at the fourth position and the least significant bit is at the zeroth position as shown here:

11101 (M = 29)
^^^^^
|||||
43210 (bit positions 4, 3, 2, 1, 0)
We will refer to the i-th bit as mi. Let’s say we have generated value 1 for m4, value 0 for m3, and we want to generate the feasible values for the bit at position i = 2. We can represent our current state as: 10cyy, where 1 and 0 are the values we have chosen, ‘c’ is the current (i-th) bit we want to generate and ‘y’ denotes the bits we will try to generate in the future. We call the bits to the left of position i as prefix. The prefix for the partially generated mi = 10cyy is 10 (where i = 2). Now we explain the rules which help us decide the feasible values to use for the i-th bit c:

Rule 1: we can always use value 0 for bit at position i.
We can use value 1 for bit at position i if either:
Rule 2a: The prefix of the bits before position i in m (which is already generated) is less than the prefix of the bits before position i in M, or
Rule 2b: The i-th bit of M is 1.
Using the above rules, we can ensure that the prefix generated for m before position i will never be greater than the prefix of M before position i. Let’s explore two scenarios which touch the above rules:
Let’s say our current state is m = 10cyy as before. We want to determine candidate values for ‘c’. In this case, the prefix is 10, and M’s prefix is 11 therefore both values 0 and 1 are feasible. Value 0 is always feasible (Rule 1) while from Rule 2a, value 1 is feasible.
If our current state is m = 11cyy, then both values 0 and 1 are feasible since value 0 is always feasible (Rule 1) while from Rule 2b, value 1 is feasible.
If our current state is m = 111cy, then only value 0 is feasible (Rule 1) but value 1 is not feasible since neither Rule 2a nor Rule 2b can be satisfied.
Note that as soon as the current prefix of m before position i is less than the prefix of M before position i, we can use both values 0 and 1 for the rest of the bit positions from i down to 0. Remember that we generate the bits from the most significant bit to the lowest (i.e., decreasing i).

We now proceed with describing the implementation. We define a recursive function count(i, lessM, M), where i is the i-th bit being generated and lessM is a boolean which denotes whether the prefix of m before position i is less than the prefix of M before position i.

As noted earlier, we start generating the number from the most significant bit (the leftmost bit) to the least significant bit (the rightmost bit). Therefore, the base case is when i is -1 which implies we have successfully constructed a whole number m. For the base case we return 1 if the generated number m is less than M otherwise we return 0 (since we only want to count such m that is strictly less than M).

We can do Dynamic Programming by caching (memoizing) on the parameters i and lessM and the result. Here is a sample implementation in Python 3 (note that lru_cache provides the required memoization):

from functools import lru_cache

def getBit(num, i):
  return (num >> i) & 1  # Returns the i-th bit value of num.

@lru_cache(maxsize = None)
def count(i, lessM, M):
  if i == -1:  # The base case.
    return lessM  # only count if it is strictly less than M.

  maxM = lessM or getBit(M, i) == 1

  res = count(i - 1, maxM, M)  # Value 0 is always feasible. See (1) below.

  if maxM: # Value 1 is feasible if maxM is true. See (2) below.
    res += count(i - 1, lessM, M)  # See (3) below.

  return res

# Prints how many non-negative numbers that are less than 123456789
print(count(31, False, 123456789))
Notes:
(1): To compute the boolean value of lessM for the next bit of m in the recurrence, we look at the value of the current lessM. If the current lessM is already true, then lessM for the next bit in the recurrence will also be true. Another case when lessM for the next bit is true is when the i-th bit of M is equal to 1. Since we pick value 0 for the current (i-th) bit in m and it is less than the i-th bit of M (which is 1), it means that lessM is true for the next bit. maxM captures what we described just now, therefore the next value for lessM for the next bit in the recursion is set to maxM.
(2): Value 1 is feasible if lessM is true (which means we are free to use both values 0 and 1) or the i-th bit of M is 1 (which means we are still generating feasible partial number m that is less than or equal to M).
(3): The value for lessM in the next bit can only be true if lessM is previously true. If the current lessM is false, then we know that the i-th bit of M is 1. Since we picked value 1 for the current bit, the next value for lessM will not change (since 1 is not less than 1).

Now, with the above intuition for generating non-negative numbers that are less than M, we can generalize it to count all possible pairs (a,b) that are less than A and B respectively and where the bitwise ANDing of the pair (a,b) is less than K. We enumerate all possible values for current bit in a and b (i.e. the 4 possible values (0, 0), (0, 1), (1, 0), (1, 1)) and add new constraints to ensure that the bitwise ANDing of the pair (a,b) < K.

The code for the original problem is of a similar style as in the simpler problem. The code is presented below. The purpose of the variable lessA is equivalent to lessM, similar for lessB and lessK. We try to generate all feasible values for a and b and keep k in check (see the following notes).

@lru_cache(maxsize = None)
def countPairs(i, lessA, lessB, lessK, A, B, K):
  if i == -1:  # The base case.
    return lessA and lessB and lessK  # Count those that are strictly less.

  maxA = lessA or getBit(A, i) == 1
  maxB = lessB or getBit(B, i) == 1
  maxK = lessK or getBit(K, i) == 1

  # Use value 0 for a, b, and k which is always possible. See (1).
  count = countPairs(i - 1, maxA, maxB, maxK, A, B, K)

  if maxA:  # Use value 1 for a, and 0 for b and k. See (2).
    count += countPairs(i - 1, lessA, maxB, maxK, A, B, K)

  if maxB:  # Use value 1 for b, and 0 for a and k. See (3)
    count += countPairs(i - 1, maxA, lessB, maxK, A, B, K)

  if maxA and maxB and maxK:  # Use value 1 for a, b, and k. See (4)
    count += countPairs(i - 1, lessA, lessB, lessK, A, B, K)

  return count
Notes:
(1): If we choose 0 for a and 0 for b, the value for k should be 0 since 0 & 0 = 0
(2): If we choose 1 for a and 0 for b, the value for k should be 0 since 0 & 1 = 0
(3): If we choose 0 for a and 1 for b, the value for k should be 0 since 1 & 0 = 0
(4): If we choose 1 for a and 1 for b, the value for k should be 1 since 1 & 1 = 1

To avoid overflows, you should take care to use 64-bit integers. Also, this solution pattern is a standard way to solve these kinds of problems and can be generalized to any number system (and not just base 2 as was the case in our problem).

The complexity of this solution is based on the size of the DP table which here is 31 * 2 * 2 * 2.

Second approach
We can group the pairs (a, b) in S(A, B, K) based on whether a and b are odd or even, i.e. have the least significant bit set. Each such pair will be accounted for in one of the four sets below (written using set-builder notation), so summing up the sizes of the four sets will give us the value f(A, B, K).

{(a/2, b/2) | (a, b) ∈ S(A, B, K) && a even && b even}
= S(ceil(A/2), ceil(B/2), ceil(K/2))
{(a/2, (b-1)/2) | (a, b) ∈ S(A, B, K) && a even && b odd}
= S(ceil(A/2), floor(B/2), ceil(K/2))
{((a-1)/2, b/2) | (a, b) ∈ S(A, B, K) && a odd && b even}
= S(floor(A/2), ceil(B/2), ceil(K/2))
{((a-1)/2, (b-1)/2) | (a, b) ∈ S(A, B, K) && a odd && b odd}
= S(floor(A/2), floor(B/2), floor(K/2))
Note that in the 4 sets, the values for ‘k’ are forced. If ‘a’ or ‘b’ is even, ‘k’ will also be even while if both ‘a’ and ‘b’ are odd, then ‘k’ is also odd (because of the bitwise ANDing).

Let us provide more intuition on the 4 sets. We show it with an example where we list even numbers. If A is odd, e.g. 7 then the possible ‘a’ values that are even are 0, 2, 4, 6. If A is even, e.g. 8 then the possible ‘a’ values that are even are also 0, 2, 4, 6. The numbers in their binary representation are 000, 010, 100, 110. The 4 sets is akin to fixing (or eliminating) the least significant bit (i.e. by shifting right by 1, i.e. a>>1) which results in the set: 00,01,10,11 i.e. values 0, 1, 2, 3. The new value for A here is 4 (i.e. first integer greater than 0, 1, 2, 3). We generalize and say the following: If we are considering even values for ‘a’, then the new A is given by (A+1)>>1 (which is also ceil(A/2)). By going through a similar exercise for when ‘a’ is odd and A is odd or even (which we leave to the reader), we find the new A is given by A>>1 (which is also floor(A/2)).

The 4 sets give us a simple recursive procedure to compute f(A, B, K), as the sizes of the sets on the right hand side of the equations are simply calls to f. The appropriate base cases here are f(1, 1, K) = 1 and f(A, B, K) = 0 if any of A, B, K are equal to zero. To turn this recursive procedure into an efficient algorithm, we can memoize computed values. This means that after computing some f(A, B, K), we cache the result so future computations with the same values will take constant time.

Sample implementation in Python 3:

@lru_cache(maxsize = None)
def f(A, B, K):
    if A == 0 or B == 0 or K == 0:
        return 0
    if A == B == 1:
        return 1
    return f((A+1)>>1, (B+1)>>1, (K+1)>>1) + \
           f((A+1)>>1, B>>1, (K+1)>>1) + \
           f(A>>1, (B+1)>>1, (K+1)>>1) + \
           f(A>>1, B>>1, K>>1)
Complexity Analysis
It can be shown by induction that a recursive call f(A’, B’, K’) of recursive depth n has A’ equal to either floor(A / 2n) or 1+floor(A / 2n), and that B’ and K’ satisfy similar relations. From this, we can see that at a given recursive depth there are at most 8 different calls to f. Due to our chosen base cases, we can also see that the maximum recursion depth is O(log(max(A, B))). We conclude that the algorithm is O(log(max(A, B))).

Round 1B 2014 - Code Jam 2014

The Bored Traveling Salesman (15pts, 30pts)

We can model this problem as a graph problem where the cities are nodes in the graph and the bidirectional flight tickets between cities are the bidirectional edges connecting the nodes. Each node has a distinct zip code number. The given constraints on the tickets and how it should be used can be modelled as a modified depth-first-search-like traversal where: when we visit a node, we are not required to visit all of its neighbors but in the end all nodes must be visited. When a node is visited, its zip code is printed (i.e., pre-order traversal). When all nodes have been visited, the printed (concatenated) zip codes must form the smallest number possible.

Brute Force Solution:
The small input has at most 8 nodes. It is small enough that we can try all possible paths according to the rules (including all possible starting nodes), and pick the one with the smallest number (of concatenated zip codes). The brute force solution can have a time complexity of O(N! * N), where N! comes from having to potentially list all possible permutation of cities and for each permutation we can check its validity in O(N). As the large input can have up to 50 cities, this algorithm will be too slow. Therefore we propose an alternative solution to handle the large input.

Greedy Solution:
We can rank nodes based on its zip codes (i.e., the smallest node is the node with the smallest zip code). Since all of the zip codes are the same length, we can think of each zip code as a single digit number and to form the smallest concatenated digits we can just greedily concatenate the digits in increasing order. This means that the node with the smallest zip code should be the first node to be visited (i.e., the source node for the traversal). Thus, to minimize the final concatenated number, we should always visit the next smallest feasible node (we will discuss node feasibility later). Note that it is always possible to complete the traversal from the smallest node (or any node) since the input graph is connected.

Before we provide the pseudocode of the greedy algorithm, let’s define some variables. We provide examples that make use of these variables in subsequent paragraphs:

DEAD: The set of nodes we’ve already visited and left (which we may never visit again).
ACTIVE: The stack of nodes along our current path (originating from the source node).
HEAD: The node at the top of the ACTIVE stack, which is the node we are currently on.
At each step, we may either:

Visit some not-yet-visited neighbor of HEAD, which adds the newly visited node to the top of the ACTIVE stack and make it as the new HEAD. This action is analogous to flying to a new city for the first time. Note that when we visit a new city, we should concatenate its zip code to our final answer.
Leave HEAD, which pops HEAD from the ACTIVE stack and moves it to the DEAD set. This action is analogous to taking the return flight from HEAD using the return ticket used to visit HEAD. Note that we do not concatenate the city’s zip code to the final answer when leaving the city.
With that, we are ready to present the pseudocode for our greedy algorithm:

  root = the node with smallest zip code
  DEAD = new Set()
  ACTIVE = new Stack()
  ACTIVE.push(root)
  answer = “”
  concatenate zipcode[root] to answer
  while ACTIVE is not empty:
    HEAD = ACTIVE.peek()
    next = next_smallest_feasible_node_to_visit()
    if next is EMPTY or no flight from HEAD to next:
      # leave the HEAD node
      insert HEAD to the DEAD set
      ACTIVE.pop()
    else:
      # visit the next node
      ACTIVE.push(next)
      concatenate zipcode[next] to answer
  print answer
Now, the hard part is: how do we compute the next_smallest_feasible_node_to_visit()? We first demonstrate it with an example.

Suppose we have a graph and we start traversing from the source node S (which has the smallest zip code). Suppose the next nodes with smallest zip codes are nodes A, B, C in that order and A is connected to S, B is connected to A, and C is connected to B. Then the greedy algorithm will go from S -> A -> B -> C. The figure below shows the current state of the traversal and the rest of the graph. Nodes S, A, B, C are in the ACTIVE stack where C is the HEAD:

How do we pick the next smallest feasible node to visit from C? Could we arbitrarily pick a node that is connected to the ACTIVE stack then decide to visit that node next? Actually not. We investigate the following three scenarios to clarify why we cannot arbitrarily pick a node connected to the ACTIVE stack.

Scenario 1: Z’s zip code is smaller than both X and Y
In this case, we cannot visit Z as the next node since it requires us to travel back from C -> B -> A -> S, then visit Z. This will put nodes C, B, A in the DEAD set. This becomes a problem for node X since it has not yet been visited and now there is no way to reach node X from any ACTIVE nodes (note that as we cannot visit the DEAD nodes therefore the DEAD nodes A, B, C disconnect X from S, Z and Y). Thus, node Z is not feasible as the next node to be visited next (after C).
Scenario 2: X’s zip code is smaller than Y’s, and Y’s is smaller than Z’s
In this case, we can directly visit X from the currently active node C. Then, we can visit the next smallest node Y by traveling back from X -> C -> B -> Y. Finally, we visit the next smallest node Z directly from Y. After that, we can go back all the way to S via Z -> Y -> B -> A -> S to complete the traversal.
Scenario 3: Y’s zip code is smaller than X’s, and X’s is smaller than Z’s
In this case, we can visit the next smallest node Y by travelling back from C -> B -> Y while putting node C in the DEAD set. The next smallest node X can still be reached by traveling back from Y -> B -> A -> X and putting nodes B and Y in the DEAD set. Finally, the next smallest node Z can be visited by travelling back from X -> A -> S -> Z.
From the scenarios above, we know that Z is not feasible as the next node but both X and Y are feasible and we can visit the one with the smaller zip code first.

In general, we need to be able to figure out if a node is feasible as the next node. To do so, we can instead ask when is a node not feasible as the next node. It is not a feasible node if by visiting that node, some nodes that are not-yet-visited become unreachable from the nodes in the ACTIVE stack (i.e., they can never be visited later because they are disconnected from the ACTIVE set due to some nodes that will be placed in the DEAD set when taking the return flight as in Scenario 1). Observe that nodes that are still reachable from nodes in the ACTIVE stack can still be visited later.

To check whether the not-yet-visited nodes are still reachable from the ACTIVE nodes, we can do a connectivity check (via breadth-first-search or depth-first-search) from the source node to all the not-yet-visited nodes, avoiding the DEAD nodes. Note that since all ACTIVE nodes are all connected to the source node, doing the connectivity check from the source node is equivalent to doing the connectivity check from all ACTIVE nodes.

Now, we are ready to devise an iterative algorithm to find the feasible nodes and pick the smallest node. Referring to our example above, in this iterative algorithm we will generate X, Y and Z in that order. The algorithm will terminate when we reach Z (please refer to Scenario 1 for the reason). Note that when this algorithm returns, it should not alter the ACTIVE stack and the DEAD set. Therefore, when running this algorithm we can either make a local copy of the ACTIVE stack and the DEAD set, or restore the changes we made (if we run this algorithm in place). The iterative algorithm is as follows:

Check the neighbors of the current HEAD and record the next smallest node to visit.
Try to abandon the current HEAD node and take the return flight to the previous node in the ACTIVE stack. If it is not possible to abandon this HEAD (i.e., it makes some not-yet-visited nodes unreachable), then we stop and return the smallest node we recorded. Otherwise, we take the return ticket from HEAD (abandon the current HEAD) and keep on looping by going to step 1.
The following pseudocode shows one way to implement the above algorithm (with in place modification of the ACTIVE stack and DEAD set, and restoration of the changes before returning):

  def next_smallest_feasible_node_to_visit():
    temp = new Stack()
    best = EMPTY
    while ACTIVE is not empty:
      HEAD = ACTIVE.top()
      # Check the neighbors of HEAD and record the 
      # next smallest node as best.
      for each neighbor i of HEAD that is not-yet-visited:
        if best == EMPTY or zipcode[i] < zipcode[best]:
          best = i

      # Abandon HEAD and go back up in the ACTIVE stack.
      insert HEAD to the DEAD set
      temp.push(HEAD)
      ACTIVE.pop()

      if there exists a not-yet-visited node that \
        is not reachable from the source node:
        break

    # Restore the ACTIVE nodes and the DEAD set.
    while temp is not empty:
      HEAD = temp.top()
      remove HEAD from the DEAD set
      temp.pop()
      ACTIVE.push(HEAD)

    return best
How fast is this greedy algorithm? The greedy algorithm calls the search for the next smallest feasible node routine N times. Each search runs through all the nodes in the ACTIVE stack with at most O(N) nodes. For each node in the ACTIVE stack, we perform one connectivity check that takes O(N) time, and we loop through all of the node’s O(N) children. Therefore overall, it is O(N3), which fits easily into the time limits.

Round 1C 2014 - Code Jam 2014

Part Elf (8pts, 12pts)

In the first generation (i.e., 40 generations ago) there were only either pure Elf (1/1 Elf) or Human (0/1 Elf). If we enumerate all possible children for the next 3 generations we have:

1st gen:  0/1                                     1/1
2nd gen:  0/2                 1/2                 2/2
3rd gen:  0/4       1/4       2/4       3/4       4/4
4th gen:  0/8  1/8  2/8  3/8  4/8  5/8  6/8  7/8  8/8
It is apparent that at any generation, the denominator is always a power of two. Thus, the answer for the impossible case is easy to check: first reduce the given fraction to its lowest terms and check whether the denominator is a power of two. A fraction can be reduced to its lowest terms by dividing both the numerator and denominator by their greatest common divisor.

Now, the remaining question is for a P/Q Elf, what is the minimum number of generations ago that there could have been a 1/1 Elf?

For a small input, where Q is at most 1000, we can generate the first 10 generations (with 210 possible Elf combinations) to cover all possible small input. When we generate the child from the two parent we record their relationship. Then to answer the question, we simply do a breadth-first search (shortest path in unweighted graph) in the relationship graph from the P/Q Elf and stop whenever we encounter 1/1 Elf and report the length (shortest path length). This algorithm runs in O(2N * 22N) to generate the relationship graph of size 1001 x 1001 which is still feasible for a small input but not for the large input.

For the large input, another insight is needed. Let’s do some examples to get the intuition. Suppose Vida is an 3/8 Elf, what are the possible parents? Let’s enumerate them:

(0/8 + 6/8) / 2 = 3/8
(1/8 + 5/8) / 2 = 3/8
(2/8 + 4/8) / 2 = 3/8
(3/8 + 3/8) / 2 = 3/8
The possible parents of 3/8 are: 0/8, 1/8, 2/8, 3/8, 4/8, 5/8, 6/8 which form a “consecutive” (for the lack of a better word) fraction from 0/8 to 6/8.

Formally speaking, a P/Q Elf could have a Z/Q parent where Z is ranged from max(0, P - (Q-P)) to min(Q, P * 2). Notice that the denominator Q does not change.

With this intuition, it is obvious that to get to the pure 1/1 Elf as fast as possible, we want to greedily generate a parent with numerator as large as possible. In other words, from an P/Q Elf we would like to generate Z/Q parent where Z is maximized. We continue the process for Z/Q, picking the parent with the greatest numerator and so on until we get to a 1/1 Elf. This greedy algorithm runs in O(log(Q)). Below is a sample implementation in Python 3:

from fractions import gcd

def is_power_of_two(x):
  return x & (x - 1) == 0

def min_generation(P, Q):
  g = gcd(P, Q)
  P = P // g
  Q = Q // g

  if not(is_power_of_two(Q)):
    return "impossible"

  gen = 0
  while P < Q:
    P = P * 2
    gen += 1
  return gen

for tc in range(int(input())):
  print("Case #%d: %s" % (tc+1, \
    min_generation(*map(int, input().split('/')))))

Round 1C 2014 - Code Jam 2014

Reordering Train Cars (10pts, 25pts)

This is a straightforward problem, but the main issue to get it correct is to enumerate all invalid cases. The main idea of the solution is to join some strings to create sets of disjoint groups, such that every group contains a set of strings that must be joined together. Then calculate the number of ways to create every group, and the final result will be the factorial of number of disjoint groups multiplied by number of ways to create every group.

One of the main issues (that lead to huge number of incorrect attempts) was to validate that the given set of strings is invalid. So let us start the analysis by specifying valid cases that should be handled before creating the groups.

Another good hint that will help a lot in validation process is to collapse all given strings by removing all duplicate consecutive letters, for example, if given a string "aabbccdddeaa", after collapsing this string it should be "abcdea". So from now, we will assume that all given strings are always collapsed, even after joining two strings "ab" and "bc" this will be collapsed automatically to be "abc" not "abbc".

Validation Process
First, check that for every string there is no letter repeated in two different places in the string. For example, ["aba", "abca", "adeab"] are all invalid strings as 'a' is repeated, while ["ab", "abc", "adbe"] are valid strings.

We start by precomputing some information that will help to enumerate the rest of invalid cases. We need to store the position of every character c in the alphabets, i.e. all letters from 'a' to 'z'. Character c will be either (i) the first character, (ii) the last character, or (iii) a middle character of any given non-single-character string. Also, we need to count the number of single-character strings for every character c. For example, if the given list of collapsed strings is ["abc", "cdef", "a", "a", "gh"], then the precomputed arrays are:

begin = {'a': 0, 'c': 1, 'g': 4}   -- (1)
end = {'c': 0, 'f': 1, 'h': 4}     -- (2)
middle = {'b': 0, 'd': 1, 'e': 1}  -- (3)
singleChars = {'a': 2}             -- (4)
Notes:
begin means that for any character c, it is the first character of string number begin[c].
end means that for any character c, it is the last character of string number end[c].
middle means that for any character c, it is in the middle of string number middle[c]. Notice that, "cdef" (string number 1) has two middle characters, while "gh" (string number 4) does not have middle characters.
singleChars means that for any character c, There are singleChars[c] single-character strings composed of character c. For example, there are 2 single-character strings of character 'a'.
While precomputing the previous arrays, you have identify a valid set of strings by sustaining the following conditions. A character is valid only if it satisfies one of the following conditions:

It appears in the middle of one string, and nowhere else.
It appears at most once in the beginning of one string, at most once at the end of one string, and any number of times as a single-character string.
For example, all the following cases are invalid:

["abc", "ade"]: character 'a' conflicts with second condition.
["bca", "dea"]: character 'a' conflicts with second condition.
["abc", "dbf"]: character 'b' conflicts with second condition.
["abc", "ead"]: character 'a' conflicts with first condition.
["abc", "gcf"]: character 'c' conflicts with first condition.
["a", "a", "bac"]: character 'a' conflicts with first condition.
By considering the previous conditions, we will be able to identify valid cases in any given set of strings.

Groups creation
Let us define first what it is a group, the group is a set of strings that should be joined to form a valid combined string. So in this section we focus on creating a set of disjoint groups to count the number of ways. For example, ["ab", "bc"] is a group as "ab" and "bc" should be joined together to form "abc", which is a valid string. Also, assume given ["ab", "bc", "de", "xy", "yz"], this set of strings can be divided into 3 disjoint groups, which are ["ab", "bc"], ["de"], and ["xy", "yz"].

First, we need to handle the special case of single-character strings. Every single-character string can be considered initially as a group, with the number of ways to create it being equal to the factorial of the number of such strings, for example, ["a", "a", "a"] can be grouped in 3! (which is 6 different ways). And this group will be represented by only one character which is "a" and not "aaa" (as we described in the beginning, we will collapse these strings too).

An important point to notice while creating groups is that the first string in the group should start with a character that is not used as the last character in any other string. For example, the following 2 strings ["abc", "cde"] can form a valid group that should be represented by string "abcde", we notice that we cannot start forming the group with string "cde" as it starts with character 'c' which is the last character of string "abc" (we can make this check in constant time by using the end array). Also, notice that this group can be created in only 1 way.

We now proceed with counting the number of disjoint groups. We loop over all non-single-character strings that start with a character not used as the last character of any other string and use this as the first string in the group, then join it to the string that starts with its last character (get the index of this string using begin array) and so on till you reach the last string in that group (where you cannot find any other string to join it to). For example:

["cde", "mno", "abc", "opq", "xyz"]
From these strings we can form 3 disjoint groups as follows:
  1) "mno" + "opq" = "mnopq"
  2) "abc" + "cde" = "abcde"
  3) "xyz"
After counting the number of disjoint groups there is one more step remaining which is to join any single-character string to an disjoint group, if possible. As stated previously, we assumed initially that every single-character string is considered as a disjoint group, so we may assume that total number of disjoint groups equals to number of non-single-character groups and number of single-character groups. But there are some cases that we need to join a single-character strings to a one of the non-single-character strings. We can elaborate that with the following two examples:

["ab", "b", "b"]: you must join the single-character string "b" to the end of string "ab" to form the group "ab" in 2 ways, i.e. 2 ways for the 2 single-character strings and only 1 way to join the single-character string to "ab".
["a", "a", "bc"]: in this case we consider "a" as an disjoint group and will not be joined to any other groups. So there are 4 ways, i.e. 2 ways for the single-character string, and 2 ways to order "a" and "bc" either "abc" or "bca".

Round 1C 2014 - Code Jam 2014

Enclosure (15pts, 30pts)

The goal is to place as few stones as possible to enclose at least K intersection points in an N * M grid. Intuitively, it is wasteful to create more than one enclosure since we can always combine the stones into one larger enclosure that cover same or more intersection points. Thus, we can restrict our search to solutions with one enclosure only.

The figure below shows an enclosure for an N = 5 by M = 5 grid that covers K = 19 intersection points with the minimum number of 11 stones.

-***-   # the first row
*XXX*   # intermediate row
*XXX*   # intermediate row
*XX*-   # intermediate row
-**--   # the last row
We use '-' to represent empty intersection, 'X' to represent an enclosed intersection point, and '*' to represent a stone placed in the intersection point (which is also enclosed). Observe that:

The stones in the first row and the last row are filled in consecutive position in the row.
Each intermediate row (i.e., the row except the first and the last row) has exactly two stones (i.e., the left stone and the right stone).
Each intermediate row always contains two stones. If there exists an intermediate row that contains only one stone, then it may form two enclosures that touches their boundaries at that row. Since we are not interested in searching solutions with more than one enclosure, we can restrict each intermediate row to always contains exactly two stones.

What about having more than two stones in an intermediate row? This is wasteful and we can avoid that too. To see this, let’s enumerate all possible ways to move the left stone boundary for the next row and at the same time it explains why each intermediate row always contains two stone in the final configuration. The following figure shows all three possible ways to move the left boundary stone:

previous row:  --*XXX     --*XXX      --*XXX
    next row:  -*XXXX     --*XXX      ---*XX
              (expand)  (unchanged)  (shrink)
Notice that we can only expand the boundary by one otherwise it will create a gap and it will no longer form a closed enclosure. Could we place stones in between to close the gap? Such as (shown in bold):

previous row:  --*XXX      -**XXX
    next row:  **XXXX      *XXXXX
                          (push up)
Yes, we can, but it is wasteful because we can always “push up” the placed stone to get one more enclosed intersection point as shown in the right figure. Moreover, we can do another push up on the previous row to the previous-previous row and so on until it is pushed up to the top row (each push up gains one more enclosed intersection point). Thus, in the end, after all the push ups, each intermediate row will contain exactly two stones.

To make the search simple, we do not want any push up to happen. In the search, we fix the number of stones at the top row, and generate the next row and we do not want the next row to alter the previous row ("push up" alters the previous row). This explains why we do not want to expand the left boundary more than one position (i.e., it is simpler just to change the number of stones in the top row and do a separate search).

For the shrink case, observe that we only need to shrink at most one position because shrinking more than one is wasteful since we will need more stones to fill the gaps (to maintain enclosure). See the following examples:

previous row:   --*XXX     --**XX
    next row:   ---**X     ----*X
               (case 1)   (case 2)
In the shrinking case 1, the bolded stone '*' (the rightmost stone) is not needed since it is already enclosed. The shrinking case 2 is wasteful since the bolded stone '*' (the middle stone) can be pushed down to enclose one more intersection point (which will still be wasteful because it will then became case 1).

Looking at all the possible ways to move the left boundary, we can conclude that it is sufficient to place exactly two stones in each intermediate row.

For the last row, we close the enclosure by connecting the left and right stone boundary of the previous row by placing the stones consecutively. For example:

previous row:  --*XXXXX*--
    last row:  ---*****---
Note that we only close the enclosure if we are sure that the intersection points enclosed by all the previous rows and the last row is at least K.

In summary, our search algorithm goes as follows:

First place a number of stones consecutively from left to right on the top row
Place two stones for the following (intermediate) rows by expanding / shrinking / unchanged the left and right boundary with respect to the previous row
Finally if we have enclosed enough intersection points, close the enclosure by placing stones consecutively at the last row.
We will discuss two common ways to solve this problem. The first is a dynamic programming (DP) solution and the second is a greedy solution.

Dynamic Programming (DP)
Since we brute-force the first row, we only need to perform DP for the intermediate rows and the last row. For each row, we need to know:

The remaining rows left.
The left stone boundary position.
The right stone boundary position.
The remaining intersection points to enclose.
To minimize the left / right stone boundary position, we transpose the grid (if necessary) so that we ended up with the grid with smaller columns than its rows, without affecting optimality. With this transformation, now the left / right boundary position is at most sqrt(N * M). This leads to O(N * sqrt(N * M) * sqrt(N * M) * N * M) solution. While the amortized cost can be less, it still seems large and has potential to run more than the time limit. Can we do better?

It turns out that the exact position of the left and right stone boundary does not really matter as long as the distance between them is not larger than the column size. This also applies for the stones at the top row. What matters is the number of stones placed consecutively in the top row. Where the stones are exactly placed does not matter as long as the number of stones is at most the column size.

With this intuition, we can make the DP state smaller. Instead of maintaining the left and right stone boundary position, we can just maintain the distance between the two stones instead. The three possibilities of moving the stone boundaries in the next row (expand, shrink, unchanged) now translate to five possibilities of adding the stone distance, by -2, -1, 0, 1, or 2 as shown in the following examples.

prev row:  -*XXX*-  -*XXX*-  -*XXX*-  -*XXX*-  -*XXX*-
next row:  --*X*--  --*XX*-  -*XXX*-  -*XXXX*  *XXXXX*
            (-2)     (-1)      (0)      (1)      (2)
Note that for (-1) and (1) there is another possibility for the next row, but both have the same distance between the left and right stone boundaries.

By switching the left and right boundary position to the distance between the left and right stone, we can reduce the DP states to three states (the remaining rows left, stone distance of the previous row, the remaining intersection to enclose). This reduces the complexity to O(N * sqrt(N * M) * N * M). The amortized cost is less than 32 million operations per test case which is fast enough to answer 100 test cases. Below is a sample implementation in Python 3:

from functools import lru_cache
import sys

@lru_cache(maxsize = None)  # Memoization.
def rec(rem_rows, prev_dist, rem_points, M):
  if rem_points <= 0:  # If the remaining area is non positive,
    return 0  # then no stone is needed.

  ret = 1000000  # Infinity.
  if rem_rows <= 0:  # No more row but rem_points is still > 0.
    return ret  # Return infinity.

  if M == 1:  # Special case where each row only has one stone.
    return rem_points  #  rem_rows >= rem_points is guaranteed.

  min_dist = max(prev_dist - 2, 1)
  max_dist = min(prev_dist + 2, M)
  for next_dist in range(min_dist, max_dist + 1):
    if next_dist >= rem_points:
      # Close the enclosure for the last row.
      ret = min(ret, next_dist)
    elif next_dist > 1:
      # Cover this row using 2 stones.
      next_rem_points = rem_points - next_dist
      ret = min(ret, \
        2 + rec(rem_rows - 1, next_dist, next_rem_points, M))

  return ret

def min_stones(N, M, K):
  if N < M:  # If the row size is smaller than the column size
    (N, M) = (M, N)  # Transpose the grid

  res = 1000000
  # Try all possible number of stones for the top row.
  for stones in range(1, min(K, M) + 1):
    # The stones needed to cover the top row + the next rows.
    stones = stones + rec(N - 1, stones, K - stones, M)
    res = min(res, stones)

  return res

sys.setrecursionlimit(5000)
for tc in range(int(input())):
  print("Case #%d: %d" % (tc+1, \
    min_stones(*map(int, input().split()))))
Greedy
Intuitively, the two stones in each intermediate row can be greedily placed as far as possible from each other to maximize the area enclosed without adding any additional stone. Consider the following example:

---------      ---------      ----*----
---------      ---------      ---*X*---
--*****--      --*****--      --*XXX*--
--*XXX*--      -*XXXXX*-      -*XXXXX*-
--*XXX*--  ->  *XXXXXXX*  ->  *XXXXXXX*
--*XXX*--      -*XXXXX*-      -*XXXXX*-
--*****--      --*****--      --*XXX*--
---------      ---------      ---*X*---
---------      ---------      ----*----
   (a)            (b)            (c)
The enclosure in Figure (a) can be improved by moving the two stones (in each of the three intermediate rows) as far as possible from each other as shown in Figure (b). Similar reasoning can be made for each column that contain only two stones: move the top and bottom stones as far from each other as possible in that column. The enclosure in Figure (b) can be improved in the same way, resulting the enclosure as shown in Figure (c).

Knowing that the optimal shape resembles a diamond, the greedy approach is to try to construct a diamond-shaped enclosure with area at least K. However, this is not always feasible if the grid is not large enough. In such case, the diamond-shape may be “truncated” at the top / left / right / bottom sides as shown below for the best enclosure for N = 6, M = 7, K = 27

--***--
-*XXX*-
*XXXXX*
-*XXXX*
--*XX*-
---**--
Another useful observation is that the empty intersections at the corners always forms a right triangle. This allows us to generate all possible truncated (and perfect) diamond by placing empty triangles at the corners. Notice that the sizes (the length of its side) of the empty triangles at the corners may be different by at most one size.

Fortunately, the large input is small enough that we can brute-force for all possible truncated (and perfect) diamond shapes. First we try all possible grid size, and for each possible grid size, we try to put empty triangles at the corners and compute the enclosure size and the stones needed. We record and return the minimum stones needed to construct the shape with area at least K. Below is a sample implementation in Python 3:

def empty_triangle(size):
  return size * (size + 1) / 2

def min_stones(N, M, K):
  if N > M:
    (N, M) = (M, N)

  best = K
  for R in range(2, N + 1):
    for C in range(R, M + 1):
      if R * C >= K:
        for i in range(2 * R):
          cover = R * C
          cover -= empty_triangle(i // 4)
          cover -= empty_triangle((i + 1) // 4)
          cover -= empty_triangle((i + 2) // 4)
          cover -= empty_triangle((i + 3) // 4)
          if cover < K:
            break
          stones = 2 * (R + C) - 4 - i
          best = min(best, stones)

  return best
The complexity of the above solution is O(N2 * M). However, if you are well versed in Mathematics, you can further improve the search to O(log(K)) by doing a binary search on the number of stones needed to form the truncated diamond shape and compute the number of enclosed intersection points in O(1) to make the binary search decision.

Round 2 2014 - Code Jam 2014

Data Packing (5pts, 8pts)

Given a list of file sizes, Adam wants to pack these files in the minimum number of compact discs where each disc can fit upto 2 files and a single file cannot be divided into multiple discs. All compact discs have the same capacity X MB.

There are several greedy approaches that can solve this problem. We describe one and show how it can be implemented efficiently. We first sort all file sizes. Then we linearly scan the sorted list going from smallest to biggest. For each file, we try to “pair” it with the largest possible file that can both fit in X. If no such largest file is found, we put the file by itself. We finally report the number of discs used.

Let’s analyze the running time for this solution. The sorting take O(N log N), and the linear scan combined with searching for largest possible file take O(N2). Therefore the running time for this algorithm is O(N2) which is sufficient to solve the large data set.

We can further optimize this solution by avoiding the O(N2) computation. To do so, we use a common idea used in programming competitions. First, as before, we sort the file sizes. We then keep track of two pointers (or indices) A and B: A initially points to the smallest index (i.e., index 0), and B initially points to the largest index (i.e., index N-1). The idea being that at any instance, the two pointers A and B point to two candidate files that can be paired and placed in one disc. If their combined size fits in X, then we place it in one disc and then “advance” both pointers simultaneously, i.e. we increase A by 1, and decrease B by 1. If the combined file size does not fit in X, then we repeat the process of decreasing B by 1 -- which means the large disc which was previously at index B will need to be put in a disc by itself -- until the combined file size at A and B fits in X. During this iteration, we should handle the case when A and B point to the same index i.e. ensure we do not double count that file. After a matching file has been found at B, we simultaneously advance A and B as before. We should repeat this process only when B is greater than or equal to A. As for the running time, advancing the largest and smallest pointers takes O(N) time, therefore the running time is dominated by sorting which is O(N log N).

As an aside, we would like to point out that for the first solution sorting isn’t actually necessary. We can instead pick any file, then find the largest file that fits with it in a disc. If no such largest file exists, we can put the file by itself. Intuitively, the reason why it works is because we greedily try to “pair” each file with the best possible largest file. This algorithm still has a O(N2) running time.

Round 2 2014 - Code Jam 2014

Up and Down (7pts, 11pts)

Given a sequence of distinct numbers, we are asked to rearrange it by using a series of swaps of adjacent elements resulting in an up and down sequence. We want to do it in such a way that the total number of swaps is minimized.

We describe a greedy solution. We use the example sequence 6,5,1,4,2,3 to help explain the solution.

Repeat N times:
Pick the smallest value in the sequence. In the example sequence, it is 1.
Determine if the smallest value is closer to the left end of the sequence or the right end of the sequence. In the example, the left end is where 6 is and the right end is where 3 is. The smallest value, 1 is closer to the left end (2 swaps away) than the right end (3 swaps away).
Move the smallest value towards the closest end by swapping with adjacent elements, and record the number of swaps. In the example, we move 1 towards 6, first swapping 1 and 5, then swapping 1 and 6 resulting in the sequence: 1,6,5,4,2,3. We performed 2 swaps.
Remove the smallest number from the sequence. In the example, we remove 1 resulting in the sequence 6,5,4,2,3.
Repeat the process on the resulting sequence. In our example, we repeat the process on sequence 6,5,4,2,3. We identify the smallest element, which is 2, then move it towards the closer end which is the right end with 1 swap, then remove 2 resulting in sequence 6,5,4,3. We then repeat the process on this sequence. First pick smallest value which is 3. Since 3 is already on the right most end, we perform 0 swaps to move it to the closer end, then we remove 3. We repeat a similar process for values 4, 5, then finally 6.
Finally, report the total number of swaps. In our example, we performed 2 + 1 = 3 swaps.
That’s it! You might be wondering why the greedy algorithm works. We provide an intuition here. At each step in our algorithm, we pick the smallest number. In the final up and down sequence, this smallest number will need to go to one of the ends at some point in time. We move this smallest number to the closer end to minimize the number of swaps. After this smallest number reaches the closer end, its position is final, i.e. its position in the final up and down sequence is set, therefore it will never need to be moved/swapped further. Thus, in the next step we can remove that smallest number from consideration and work on a completely new sequence that is one size smaller. We repeat the process for this new sequence, i.e. move the smallest element to the closer end, then remove that number, until we reach the empty sequence. Since we perform the minimum number of swaps in each step on each sub-problem (sequence which is 1 size smaller), we minimize the number of swaps for the whole problem.

Round 2 2014 - Code Jam 2014

Don't Break The Nile (10pts, 20pts)

We are given a grid (representing a river) of size W x H filled with disjoint rectangles (representing buildings). Each cell in the grid that is not covered by a rectangle can sustain a flow of 1 unit of water through it and the water can flow to edge-adjacent cells. Cells that are covered by a rectangle cannot sustain any flow. All cells at the bottom (south side) have an implicit incoming flow of 1 unit. You are to find the maximum unit of water that can flow to the cells at the top (north side).

A Greedy Approach
If we consider a "stream" of water being a one unit connected path from the bottom to the top, then the problem can be considered as finding the maximum number of disjoint streams that can fit within the grid. If we had to find one valid stream at a time, then it would make the most sense to try to keep the stream as close to a boundary of the board as possible, because that leaves the most spaces available for future streams.

Taking this idea into an algorithm, we can start at the bottom-left-most available square, and use the left-hand-rule described here. We essentially walk forward with the goal of reaching the top, keeping as close to the left wall as possible. We show this idea in the example below. The figure shows the buildings in black, and the other colored (i.e., red, green, yellow, blue, and magenta) portions are the river. To help visualize the left-hand-rule walk, we can imagine an infinite wall on the left and right side of the river.

The first stream starts from the bottom-left-most available square which is on the 3rd cell from the left (colored red). The path to the top keeps as close to the left wall as possible as shown by the red colored cells. The second stream is the green colored cells. It goes upwards, to the left and back (since it is a dead-end) and continues to the right, keeping as close to the left wall as possible. The third stream is the yellow colored cells. And finally, the fourth stream is the magenta colored cells which fails to reach the top.

The time complexity is O(WH), which is fast enough for the small input but not for the large input. There are tricks for scaling this up to the large input, e.g. we can use coordinate compression (see the editorial for SafeJourney here) but it can be tricky to implement correctly. Therefore, in the following sections, we describe alternative approaches.

Maximum Flow
The problem statement hints that it is a maximum flow problem where the cells are the vertices in a graph. (If you are not familiar with maximum flow, you can learn about it here). Each vertex has capacity = 1 if the cell is not covered by a rectangle, otherwise the capacity of the vertex is zero. The edges between adjacent vertices can have infinite capacity. We can connect all cells at the bottom to a new vertex called the source with infinite edge capacity. Similarly, we can connect all cells at the top to a new vertex called the sink. Then we find the maximum flow from the source to the sink. With this graph modeling it is feasible to solve the small input where the number of edges is at most 4 x W x H = 4 x 100 x 500 = 200,000 and the maximum flow, f, is at most W = 100 (i.e., the total number of cells at the bottom that have implicit incoming flow of 1). If we use the Ford-Fulkerson method which has O( E f ) runtime complexity, each small test case only requires at most 20 million operations. However, this is still not enough to solve the large (in fact, this is slower than the greedy solution).

Minimum Cut
To solve for the large input, we need to look at the problem from a different point of view. It is known that the maximum flow problem is the dual of the minimum cut problem. That is, the maximum flow from the source to the sink is equal to the minimum capacity (or vertex capacity in our case) that needs to be removed (cut) from the graph so that no flow can pass from the source to the sink. It turns out that for this problem, it is easier to find the minimum cut than the maximum flow. That is, we need to determine the minimum number of cells (vertices) to be removed so that the source becomes disconnected from the sink.

     
Let’s look at the example shown above. The left figure shows a river without any buildings. The source is connected to all cells at the bottom and the sink is connected to all cells at the top. The maximum flow from the source to the sink is 5. Looking at the structure of the river, It is obvious that to make a valid cut (via removing cells) that disconnects the source and the sink, the cut must form a path from the left side of the river to the right side of the river. The right figure shows the minimum cut solution where we remove the cells (highlighted with yellow color) to cut the river from left to right so that the source is now disconnected from the sink (i.e., no water can flow from the source to the sink anymore). Observe that the minimum cut (number of cells removed) is equal to the maximum flow.

Now, let’s look at examples where we actually have buildings. Since the cells covered by a building behave the same as removed cells (i.e. they do not allow water to flow through them), we can take advantage of the buildings to minimize the number of cells we need to remove to make a valid cut. See an example below where we have one building.

     
The left figure shows a river with an alien building in the middle. To disconnect the source from the sink, we only need to remove two more cells: one cell that connects the left side of the river to the left side of the building (highlighted with red color), and another cell that connects the right side of the building to the right side of the river (highlighted with yellow color). Thus, the minimum cut (and consequently the maximum flow) of this graph is 2.

Minimum cut as shortest path
With the above insights, we can rephrase our problem as follows: find the shortest path from the left side of the river to the right side of the river where the vertices are the buildings and the edge-cost between two buildings is the shortest distance between them. Note that the shortest distance between two buildings is the number of cells we need to remove to cut the region between the two buildings. If you are not familiar with shortest path algorithms, you can read up this tutorial. To see how the shortest path algorithm works to solve our problem, we show it for the example in the figure below.

Remember that the black cells are buildings, and the other colored cells are water. The red cells show the shortest paths from the left side of the river to the three buildings while the yellow cells show the shortest paths from the right side of the river to the buildings (the top yellow edge connects the left side to the right side). The green cells show the distance from one building to the other, e.g. the distance between building A and B is 4.

To find a valid cut, we need to form a path from the left side of the river to the right side by going through the colored cells (other than blue and black cells). In the example above, the minimum cut is formed by the shortest path from the left side of the river to A (with distance 2) then from A to B (with distance 4) then from B to the right side of the river (with distance 5). Thus the minimum number of cells (minimum cut) that needs to be removed to disconnect the source and the sink is 11 cells, which is equal to the maximum flow.

To compute the distance between two buildings, it is enough to just look at their gap in the horizontal direction or vertical direction, and take the maximum of the two. The horizontal gap would be essentially the smallest distance between the two buildings if you were to project the two buildings on the horizontal (x) axis (likewise for the vertical gap). For example, building A and building B has 2 unit of vertical gap and 4 unit of horizontal gap. The distance from A to B is the one with the greater gap (i.e., 4 for the horizontal gap). Note that the vertical gap between building A and C is 0 as they overlap in their projection on the vertical (y) axis.

When computing the gap, we can imagine the left side and right side are buildings too (with infinite height). For those, we compute the horizontal gap only and ignore the vertical gap.

One may wonder about the case where there is another building between any two buildings. It means the actual distance between the two buildings can be shorter (by taking advantage of the building in between). This occurs when we look at the distance between building A and C. The actual gap between and A and C is 9 but due to the presence of building B, the distance is reduced to 8 (i.e. the sum of distance between A and B, and distance between B and C). Since we run a standard shortest path algorithm, it should take advantage of the buildings in between (i.e., the algorithm will not pick the edge (with higher distance) between A and C and instead go for the shorter distance through another building).

Round 2 2014 - Code Jam 2014

Trie Sharding (9pts, 30pts)

We are given M distinct set of strings S. We want to assign these strings into N servers so that each server has at least one string. Each server then builds a trie from the strings it is assigned to. You are to find string assignments where the total number of nodes in the trie in all the servers is the maximum possible, also need to report the total number of different assignments.

To solve this problem, we first build a trie for the strings in S, then we compute the answers from that trie. To help with our explanation, we will use a different set of strings than the one provided in the problem statement. We use: AA, AB, AAA, B, with N = 3 servers. The corresponding prefixes for our example are “”, “A”, “AA”, “AAA”, “AB”, and “B”. The following figure shows the trie for these strings. Please refer to this tutorial if you are not yet familiar with tries.

The nodes with double circles signify that its prefix is in the set of strings S. For each node p, the prefix of the node is defined as the string formed by concatenating the characters from the root to that node. The value K_p where p is the prefix of the node is explained in the following paragraph.

Before describing K_p, let’s discuss T_p. For a prefix p, let T_p be the number of strings that contain prefix p. In our example, the corresponding values for T_p are: T_=4, T_A=3, T_AA=2, T_AAA=1, T_AB=1 and T_B=1. Let us imagine an optimal assignment where the number of nodes is the maximum possible, we will describe how we compute an optimal assignment in a subsequent paragraph. In the optimal assignment, let us assume that a particular prefix p will appear the maximum number of times allowed (we will discuss this fact later). But the number of times the prefix can appear in an optimal assignment is capped by the number of servers N. In our example, the empty string can potentially appear a maximum of only 3 times instead of 4 (remember T_=4), the reason being that there are only 3 servers it can appear in! To account for this fact where T_p is capped by N, let us define another number K_p. For each prefix p let K_p be min(N, T_p). Therefore, K_p is the maximum number of times a prefix p can appear in any assignment. Going back to our problem, we note that maximizing the number of nodes is essentially equivalent to finding an assignment where each prefix p appears K_p times (if possible), and also to figure out the number of different assignments that have the maximum number of nodes.

Determine maximum number of nodes:
To generate an assignment with the maximum total number of nodes across all servers, we will use a greedy method. First, we sort the provided strings lexicographically. In our example, the provided strings are: AA, AB, AAA, B. Sorting it lexicographically will result in the sequence AA, AAA, AB, B. Let’s examine this sorted sequence. From our definition above, prefix p appears as a prefix for T_p strings. We claim that, after sorting the strings the prefix p will be in T_p consecutive strings in sorted order. Referring to our example, if we were to consider the prefix A, it appears in 3 strings which appear consecutively in the sorted sequence. Similarly AA appears as a prefix two times, and trivially the empty string appears four times.

We now describe a greedy strategy to assign the strings to servers. We will assign the sorted string #i to server #i mod N. In our example with three servers, we would have the following assignment: {AA,B}, {AAA}, and {AB}, i.e. the first and fourth strings get assigned to the first server, while the second and third strings get assigned to the second and third servers respectively. If instead we had two servers, we would have the assignment: {AA,AB}, {AAA,B}. We claim that this greedy assignment will give us the optimal assignment maximizing the number of nodes, resulting in each prefix p appearing K_p times on different server. The reason why each prefix p appears K_p times is because of an earlier argument where we pointed out that each prefix p appears in T_p consecutive strings. When assigning strings sequentially to the N servers, this will result in prefix p appearing in min(N, T_p) nodes, which is K_p. Therefore to compute the maximum number of nodes, it is sufficient to find the sum of K_p for all prefix p.

Count number of assignments with maximum number of nodes:
Before proceeding with the explanation, we would like to briefly talk about combinations. We can use Pascal’s triangle to precompute combinations. Each addition and multiplication operations are done modulo 1,000,000,007 (the modulo value is what our problem requires). For the purposes of this editorial, let’s say we precompute and save combinations in Choose(n, k) which chooses ‘k’ elements from ‘n’ elements. By definition, Choose(n, k) where n < k is defined to be zero.

Let us assume we already built the trie for strings S and for each node (with prefix p) in the trie, we already computed K_p (the maximum number of servers that this node with prefix p can be assigned to). We can traverse the trie using post-order traversal and compute the number of ways W_p to distribute the current node’s subtree among K_p servers, we will describe how we compute W_p in subsequent paragraphs. Note that the subtree of node p contains all the strings with prefix p. Since the root (node with prefix p = “”) is the prefix for all strings, we will have the final answer when we finish processing the root node.

Let’s examine the trivial case when we visit the leaf nodes during our traversal. A leaf node represents a whole string. Since we can only put one string in exactly one server, thus K_p = 1 for all leaf nodes p. Also, there is exactly one way to put a single node on a single server, thus W_p = 1 for all leaf nodes p.

Now, let’s examine the case of non-leaf (internal) nodes. After visiting all the children nodes c of node p, we can then compute W_p by using the following information:

K_c: the number of servers assigned for each child node c.
W_c: the number of assignments that yield the maximum number of nodes for each child node c.
K_p: the number of servers assigned to this node p. Note that whenever a node is assigned to some server, all of its parent nodes also have to be assigned to the same server. Therefore, it is not possible to put any node on more servers than its parent node will be on, therefore K_p >= K_c.
Whether prefix p is a whole string (i.e. it is one of the strings in set S).
To compute W_p for internal nodes p, we first need to compute:

C1: the number of ways to choose which children go to which of the K_p servers
C2: the number of ways to arrange the subtrees of each child c on its K_c servers
C3: the number of ways to put the node's string (if prefix p is one of the strings in the set S, e.g. the node labeled K_AA in the figure above) to K_p servers
W_p is the product of C1, C2 and C3. C2 is the product of W_c of all the children c of node p because each child can assign its nodes to servers independently.

We can combine the computation for C1 and C3. If the prefix p is one of the strings in S (as in the node labeled K_AA in the figure above), then we can add an artificial leaf child node to node p. After adding an artificial leaf child node, computing C1 also covers computing C3. Note that the computation for C1 does not depend on the children's W_c. To compute C1, we only need the values of K_c. That is, to compute the number of ways to choose which children goes to which of the K_p servers, we only need to know the number of servers each child c is assigned to (i.e. the value K_c). To combine C3 into C1 computation, we can add a artificial child node c with K_c = 1.

We now proceed with the computation for C1. C1 can be computed via dynamic programming. We can store the values of K_c for all the child nodes in a list (including the case for C3, i.e. add K_c=1 for an artificial child) and we want to assign these child nodes into K_p servers such that for each child, no two children is assigned to the same server (since we want to maximize the number of servers used) and all K_p servers are used (i.e. none of the servers are empty). To better illustrate the computation, we explain it in terms of counting colored marbles.

Counting colored marbles:
At this point, we can think of the list of K_c as a collection of different colored marbles. Each entry in the list gives us the number of marbles of a particular color. The question is -- how many ways can we distribute these marbles into K bins (i.e. K_p servers) in such a way that each bin contains at least one marble and no bin contains two marbles of the same color? Notice that a bin translates to a server in our problem. Also, note that none of the servers can be empty therefore there should be at least one marble per bin.

The "at least one marble in each bin" condition is tricky, so we start by ignoring it and instead compute the number of distributions that have at most one marble of each color in each bin, but may have empty bins. This will lead to overcounting. Let us first focus on marbles of one color. If we have x marbles of some color, then they must go into x different bins (out of K bins). There are Choose(K, x) combinations to do such distributions. To generalize it to all colors, we multiply Choose(K, x) for each element x in the list. Let’s call this quantity OC_K to denote over counting for K. The quantity OC_K is over counting because when distributing the colored marbles to K servers, some of the servers might be empty. We in fact need to compute OC_i for all ‘i’ between 1 and K, you'll see why we need OC_i in a minute. Computing OC_i is similar to computing OC_K, but instead of K, we use i i.e. we multiply Choose(i, x) for each element x in the list.

We explain this idea of computing OC_i with a few examples. If the list contains {1, 2, 2, 3}, and the number of bins is 4, then OC_4 is Choose(4, 1) * Choose(4, 2) * Choose(4, 2) * Choose(4, 3). Note that if any of the values in the list is greater than number of bins i, then OC_i will be zero, e.g. if we have the same list but the number of bins is 2, then OC_2 is Choose(2, 1) * Choose(2, 2) * Choose(2, 2) * Choose(2, 3) but Choose(2, 3) is zero! Therefore OC_2 is also zero. Intuitively, the reason why it is zero is the following: We are counting the number of ways we can place 3 marbles in 2 bins such that each bin can have at most one marble, but that is impossible! Therefore it is zero.

So now, let’s focus on adding back the "no empty bins" condition. Now that we have OC_i for all values of ‘i’, we have the number of distributions into 'i' bins with some bins being potentially empty. How many distributions are there with no empty bins? Let's call this value Count_i. Note that Count_K is actually the answer we want. First off, we point out that Count_1 = OC_1. Why? The reason is because OC_1 is counting the number of ways we can place all the colored marbles into one server. This means there cannot be any empty servers because there is only one server to begin with! Hence Count_1 = OC_1.

To recap, we know OC_i and we also know Count_1 and we would like to compute Count_K. To do so, let’s look at the problem of computing Count_i. To do so, we will use OC_i. For a second, let us imagine how we would compute OC_i if we knew Count_i. It would be:

OC_i = sum(Count_a * Choose(i, i-a) for a in [1, i])
Let us explain this formulation. Remember, OC_i is counting for distributions when some of the bins out of the ‘i’ bins are empty. Let us say there are ‘a’ non-empty bins therefore there are ‘i-a’ empty bins. There are Choose(i, i-a) ways to choose ‘i-a’ empty bins from ‘i’ bins. We will now fill the ‘a’ non-empty bins with marbles, which is actually given by Count_a!
We can switch the LHS and RHS of the above formulation, and rewrite the resulting LHS as follows:

Count_i * Choose(i,0) + sum(Count_a * Choose(i, i-a) for a in [1, i-1]) = OC_i
Note that Choose(i, 0) is 1. Therefore, after rearranging the above formulation, we get:

Count_i = OC_i - sum(Count_a * Choose(i, i-a) for a in [1, i-1])
This way, we can compute Count_i iteratively: first compute Count_2, then Count_3, and so on until we finally compute Count_K. Count_K is the number of ways to distribute the colored list of marbles into K bins in such a way that each bin contains at least one marble and no bin contains two marbles of the same color.

Please refer to rng..58’s implementation which implements the counting idea discussed above.

Round 3 2014 - Code Jam 2014

Magical, Marvelous Tour (5pts, 8pts)

There is an array of devices, each containing a known number of transistors, and among these there is 1 golden transistor both players want. Player 1 (Arnar) specifies a section of the array (this will divide the array into 3 sections [0,i), [i,j], (j,N-1]), then player 2 (Solveig) will pick one of the sections to take, leaving the other two for player 1. The problem asks us to find Arnar's chances of getting the special transistor given that both players act optimally and that every transistor has the same probability of being the golden one.

First, we recognize that we can compute the summation of any section of the array in O(1) time after precomputing a sum array. A sum array can be constructed using the following recurrence from 0 to N:

Then, for any partition of the device array, we can efficiently find the number of transistors inside each section using:

In order to determine the best boundaries that Arnar can put on the array, we can iterate through all the possible boundaries and store the best one. In code, this translates to using 2 for loops that cover all combinations of i and j with i <= j. For each, we can obtain the number of transistors in each resulting section (using the sum array as discussed above). Then, since Solveig will always take the section with the most transistors, Arnar’s score for a particular choice of boundaries is the minimum of any combination of 2 sections (right + left, middle + left, middle + right).

This solution runs in O(N^2) because we consider all combinations of i and j. This is good enough to pass the small input; however, it will timeout when running the large input. To pass the large input, we should come up with an O(N log N) algorithm. We need a couple more key observations for the O(N log N) algorithm:

First, notice that we can reformulate this problem as finding which 3-way partitioning minimizes the largest value of any partition, since Solveig will definitely choose the section with the most transistors after Arnar has decided on the 3-way partition.

Second, as a follow-up to the observation above, it only makes sense for Arnar to choose boundaries in which the values of the 3 sections are as close as possible with each other (otherwise Solveig will gain the upper hand by choosing the section that has significantly more transistors than the other 2, leaving less for Arnar).

The last insight that will allow us to come up with an O(N log N) algorithm is that the cumulative sum array is by nature sorted therefore it is monotonically increasing, which hints at the idea that we can utilize binary search. The algorithm idea is to iterate through the possible values for i (left boundary), and then to binary search for the optimal j (right boundary) given that left boundary. Specifically, we want the value of j that minimizes the difference between the transistor counts in the [i,j] section and the (j,N-1] section. We now describe the binary search process. We will start with 2 pointers (the possible range that our right boundary could be). The pivot will be in the middle of the 2 pointers (this is our guess at where the right boundary will be). If the right side of the pivot is greater than the left side of the pivot then we know our right boundary will be somewhere on the right and vice versa, so we can cut our possible range by half (and do so repeatedly until there is only one possibility).

Let’s go through an example for clarification. This example is taken from sample input 2 :

INPUT: 10 17 1 7 1  generates  [2 5 1 4 7 3 6 2 5 1]
Now let’s say our left pointer is between index 0 and 1 and we are trying to find the best right boundary using binary search. Our first guess should be for the right boundary to be between the left boundary and the end of the array (which puts it between values 3 and 6). The table below will illustrate the progression of our binary search. The first column contains a visualization of our array and the boundaries we are checking. The next 3 columns contain the values of each partition that is made by the boundaries. The last 2 columns tells us how many transistors each player will receive with the given partition. Each row is an iteration of our binary search.

Notice that the right partition is always moving in such a way to maximize the value of the partition (middle and right) that is lacking (i.e. in the second row in the figure above we moved to the left because the middle partition value is greater than the right partition value in the first row).

Since we have a way to find the optimal partitioning with a given left boundary, our answer is then the best answer out of those after we iterate through all possible left boundaries. Binary search takes O(log N) and we do that for N left boundaries which gives us a total of O(N log N) runtime.

This solution is good enough to pass all of our tests; however, it is possible to do even better. For example, we can try to find a tight lower bound on Solveig's score by answering questions of the form: "Can Arnar produce a 3-way partitioning that limits Solveig to a score of at most Z?". Clearly there is some value K such that the answer to this question is "no" for Z < K and "yes" for Z >= K, which means that K can be found by binary search.

In order to answer such a question, we note that given Z, Arnar can't do better than to choose the left and middle sections greedily: he selects i as large as possible while putting no more than Z transistors in the left section, and then j as large as possible while putting no more than Z transistors in the middle section. Then, if the resulting right section also contains no more than Z transistors, the answer to the question is "yes", otherwise "no". Finally, we see that these values for i and j can be found efficiently by using binary search in the sum array, so we can answer our question for a specific Z in O(log N) time.

This strategy takes O((log N)^2) for the outer binary search (we ask O(log N) questions that each take O(log N) time to answer), but of course it also requires use of the sum array which takes O(N) time to compute. Therefore, the overall complexity is O(N). Perhaps surprisingly, it is even possible to solve this problem in O((log r)(log N)^2)! See if you can figure out how to use some complicated math to avoid computing all O(N) elements of the sum array (hint: you need to exploit the particular formula given for the number of transistors in each device).

Round 3 2014 - Code Jam 2014

Last Hit (10pts, 14pts)

Diana and the tower take turns in shooting N monsters and Diana goes first. Diana can shoot any monster or skip the turn, while the tower always shoots the monster that is the closest to the tower. Each monster i starts with a certain hit points Hi and it decreases by P when shot by Diana and decreases by Q when shot by the tower. If the hit points goes below 1, the monster dies and cannot be shot further. Diana is awarded Gi gold if her shot kills the i-th monster, but none if the tower’s shot kills it. What is the maximum amount of gold Diana can obtain?

The key observation here is that, for Diana, shooting a monster other than the one currently targeted by the tower is exactly equivalent to not shooting any monster in this turn and instead getting an “extra” shot that can be used at a later turn. Later, Diana may use some or all of her accumulated extra shots consecutively. So instead of making the decision “which monster do I shoot?” before each tower shot, Diana only needs to decide whether to use one of her extra shots (on the closest living monster) or let the tower take a shot.

This reduces the problem to a dynamic programming (DP) solution with the state being: the current monster i to target, the remaining hits points for the current monster and the number extra shots that Diana has. Note that since there is a lower limit on P and Q, the maximum number of extra shots for Diana is 1000. In the DP solution, there are three transitions:

The current monster is dead and we move on to the next monster,
Diana skips a shot and gains one extra shot (letting the tower shoot once),
Diana shoots the monster once using her extra shots, possibly killing the current monster and getting its gold.
Below is the pseudocode for the top-down dynamic programming with some clarifying comments:

# We are at monster i which has rem_hp HP left and Diana has
# extra_shots shots saved up, how much gold can she get?
function rec(i, rem_hp, extra_shots)
  # Base case: all monsters have been killed.
  if (rem_hp <= 0 && i + 1 == N) return 0

  # Monster i is dead, move on to the next one.
  if (rem_hp <= 0) return rec(i + 1, H[i + 1], extra_shots)

  # Memoization.
  if is_set(memo[i][rem_hp][extra_shots])
    return memo[i][rem_hp][extra_shots]

  # The tower shoots next. Diana saves up another shot.
  ret = rec(i, rem_hp - Q, extra_shots + 1)

  # Diana shoots next, using one of the saved up shots.
  # If the shot kills the current monster, she gets its gold.
  if (extra_shots > 0)
    gold = (rem_hp <= P) ? G[i] : 0
    ret = max(ret, gold + rec(i, rem_hp - P, extra_shots - 1))

  return memo[i][rem_hp][extra_shots] = ret

# Since Diana plays first, she has one extra shot initially.
print rec(0, H[0], 1)

Round 3 2014 - Code Jam 2014

Crime House (12pts, 22pts)

You are given a log of criminals entering and leaving a house from the front door. The log is for one day, and at the start of the day there might be criminals in the house. This house may have other entrances and exits besides the front door, and the criminals might wear masks when entering/leaving the house. The task is to figure out if, given the log, it is possible that there is only one door (i.e. the front door only), and if so to figure out the minimum number of people that could be in the house at the end of the day.

Building intuition:
Before proceeding with the solution, let us develop some intuition for the problem. Let us say we are actually given a log where none of the criminals wore a mask, i.e. none of the entries are ‘E 0’ or ‘L 0’. In that case, the solution would be to simulate the log from start to end, i.e. simulate criminals entering and leaving the house via one door only, and to see if the simulation is valid. When is a simulation valid? Let us instead discuss when a simulation would be invalid. During the simulation, if we encounter a ‘E X’ command (note X > 0), and ‘X’ was already in the house then it is invalid (impossible to enter twice without leaving). Similarly if we encounter a ‘L X’ command, and ‘X’ isn’t already in the house then it is invalid. Note that if it was the first time encountering the ‘L X’ command then we can say that ‘X’ was already in the house at the start of the day so it would be valid.

The reason why the problem we are given is complicated is because of the presence of masked criminals. We are going to tackle this problem using a greedy strategy. We will first assume that there are ‘S’ number of criminals at the start of the day. Then we will do a simulation like we described before (i.e. simulation from start to end), except whenever we encounter a masked criminal, we will try to greedily assign a criminal number to the masked criminal. We will describe this greedy assignment later, but first let us describe a few terms we use in the editorial.

Terms:
Let us first describe some of the terms we use in this editorial. We call the log an event queue, which is a sequence of ‘E 0’, ‘L 0’, ‘E X’, or ‘L X’ (where X > 0) events. During our simulation, we will go through the queue from start to end. During the simulation let us say we are at index i in the event queue, then we define the current event queue to be the subsequence of the event queue starting from index i to the end of the queue. During the simulation, we will also maintain a set of criminal numbers that are inside the house: we call the set INSIDE. Finally, the last term to define is the next known event for a particular criminal (non-zero) number. To help understand this term, we will use an example. Let us say our current event queue (along with the location indices) is:

0: ‘E 0’
1: ‘E 0’
2: ‘E 5’
3: ‘L 1’
4: ‘E 1’
5: ‘L 1’
6: ‘E 1’
7: ‘L 5’
8: ‘E 2’
There are three unmasked criminal numbers: 1, 2 and 5. Note we do not consider the masked criminals. Criminal ‘1’ appears 4 times in the current queue at indices 3, 4, 5, and 6. The next known event for criminal ‘1’ is defined to be ‘L 1’ at index 3 (which is the earliest among the 4 events). Similarly criminal ‘2’ appears 1 time at index 8, therefore the next known event for criminal ‘2’ is ‘E 2’ at index 8. Similarly criminal ‘5’ appears 2 times, at indices 2 and 7. The next known event for criminal ‘5’ is defined to be ‘E 5’ at index 2.

Greedy strategy:
Now, let us proceed with the explanation for the greedy solution. First, let us assume we start with ‘S’ criminals already in the house. We will simulate the presence of these ‘S’ criminals in the house by adding ‘S’ ‘E 0’ events at the start of the event queue. To make things simpler during the simulation, we will pretend that all the criminals leave at the end of the day. To do so, we can add ‘T’ ‘L 0’ events to the end of the event queue such that the number of ‘E’ events and ‘L’ events are equal. We will then proceed to simulate this resulting event queue to check for validity and also in the process, if possible, assign numbers to the masked criminals. If the simulation is a valid simulation then for this event queue, our answer is ‘T’ i.e. the number of criminals remaining in the house at the end of the day. We will discuss how we find ‘S’ (and correspondingly ‘T’) later.

We now describe the simulation. As mentioned earlier, we simulate from the start to the end. If we are able to successfully go through all the events then this event queue is a valid event queue. At any instant we have a current event queue (which is defined above). Note that at the start of the simulation, the set INSIDE is an empty set. We will now discuss how to address the four cases for the first event in the current queue, i.e. the cases ‘E X’, ‘L X’, ‘E 0’ and ‘L 0’ (note that X > 0).

Case ‘E X’:
In this case, person ‘X’ is entering the house. It will only be valid if ‘X’ is not already inside the house, which we can check by seeing if the INSIDE set does not already contain ‘X’. If true, then this is a valid event, else it is an invalid event hence an invalid simulation therefore we end the simulation. Also, if ‘E X’ is valid, we need to update the INSIDE set by adding ‘X’ to it.

Case ‘L X’:
In this case, person ‘X’ is leaving the house. It will only be valid if ‘X’ is already inside the house, which we can check by seeing if the INSIDE set contains ‘X’. If true, then this is a valid event, else it is an invalid event hence an invalid simulation therefore we end the simulation. Also, if ‘L X’ is valid, we need to update the INSIDE set by removing ‘X’ from it.

Case ‘E 0’:
In this case, a masked criminal is entering. Our objective is to assign a number to this masked criminal. If we are able to assign a number to the masked criminal, the criminal will then enter the house which we do so by adding the assigned criminal number to the INSIDE set. Now the question is how do we assign numbers to this masked criminal. There are two cases to consider:

Case ‘E 0’ (a):
First, we consider the set of known criminals (i.e. ones that aren’t masked) in the current queue who are also not inside the house (i.e. not in the INSIDE set). Out of these criminals, we consider the criminals for which the next known event is a leave event (i.e. ‘L X’ for criminal ‘X’). These are the people who need to leave the house at some point, but it isn’t known when they leave the house. All of these people must enter the house in ‘E 0’ events that happen before they leave. If such a person exists, we make a greedy choice and choose the person who is leaving soonest, to leave ourselves with maximum time to get the remaining people out. Why do we make this greedy choice? Consider two such people, 1 and 2. The sequence, then, looks like this:

‘E 0’ ... ‘L 1’ ... ‘L 2’
Note that the ‘...’ denotes other events in the event queue which we do not list out to avoid complexity. In order for this event queue to be valid, there must be another ‘E 0’ to match the ‘L 1’ and ‘L 2’ events (note that 1 and 2 are not in the INSIDE set). There are two possibilities then:

‘E 0’ ... ‘E 0’ ... ‘L 1’ ... ‘L 2’
and
‘E 0’ ... ‘L 1’ ... ‘E 0’ ... ‘L 2’
In both situations, choosing the entering masked criminal to have been criminal number ‘1’ works. But if we had instead chosen the entering masked criminal to have been criminal number ‘2’, then the second possibility listed above would be invalid (the second ‘L 1’ cannot be matched with the ‘E 0’ that comes after it). Therefore we are better off if we choose ‘1’. This logic generalizes to the case where there are more than two people.

Case ‘E 0’ (b):
If no such person in Case ‘E 0’ (a) exists, we simply create a new person who doesn't show up anywhere unmasked (i.e. assign a number that is big e.g. a million; make sure to assign unique numbers to each new person). Why create a new person? It is because the alternative is to use someone who is outside the building, but whose next known event is an enter event. For example, let's suppose the person doing that is person 5. Then we have a sequence like this:

‘E 0’ ... ‘E 5’
For person 5 to be entering at the start, we need the sequence to look like this:

‘E 0’ ... ‘L 0’ ... ‘E 5’
In that situation, having person 1,000,000 enter is just as good as having person 5 enter. But in the situation where there is no ‘L 0’ between the ‘E 0’ and the ‘E 5’, person 1,000,000 could leave the house later (if there is a ‘L 0’ afterwards), but person 5 is stuck entering the house, then entering it again causing the simulation to be invalid. Therefore we better off to create a new person with a new unique number in such cases.

Case ‘L 0’:
In this case, a masked criminal is leaving the house. As in the case for case ‘E 0’, the goal is to assign a number to the masked criminal. After assigning the number ‘X’ to the criminal (if that is possible), we must update the INSIDE set by removing ‘X’ from the set. Note that if the INSIDE set is empty when we encounter the ‘L 0’ event then it is an invalid simulation. For cases when there are criminals in the INSIDE set, there are three cases to consider:

Case ‘L 0’ (a):
Consider all the criminals who are inside, and whose next known event is that they are entering the house. By the same logic as in Case ‘E 0’ (a), we choose the one who is entering the soonest. If we are able to do such an assignment, we need not consider any of the other ‘L 0’ cases.

Case ‘L 0’ (b):
If no such person in the previous case exists, then we consider all criminals who are inside the house. If there is one criminal who has no next known event (either ‘E X’ or ‘L X’) with the house, have that person leave; they have to leave at some point in no particular order, and it's better to have them leave than someone whose next known event is that they are leaving. If we were able to choose such a criminal, then we need not consider the last case.

Case ‘L 0’ (c):
If no such person in the previous two cases exists, we're now stuck with criminals who are inside and whose next known event is that they're leaving. In such a case, we make a greedy choice and take the one who leaves as late as possible, so we have the maximum number of chances for our selected person to enter the house again (with an ‘E 0’ event). Why do we make this greedy choice? Consider two such people we need to choose from, 1 and 2. The sequence, then, looks like this:

‘L 0’ ... ‘L 1’ ... ‘L 2’
In order for this to be valid, there must be another ‘E 0’. There are two possibilities:

‘L 0’ ... ‘E 0’ ... ‘L 1’ ... ‘L 2’
and

‘L 0’ ... ‘L 1’ ... ‘E 0’ ... ‘L 2’
In both situations, assigning the leaving masked person (‘L 0’) to be person 2 works, but choosing 1 does not work for the second possibility. So we are better off to choose person 2 for ‘L 0’. This logic generalizes to the case where there are more than two people.

And so that concludes the four cases to consider for the first event in the current queue. As discussed earlier, if the first event is deemed invalid when considering the four cases, then we have an invalid simulation. But if we are able to successfully assign a criminal number, then we advance to the next event in the event queue and get a new current queue, and continue on with our simulation. If we reach the end, then it is a valid event queue.

Picking ‘S’ criminals initially in the house:
Earlier on, we had assumed there were ‘S’ criminals initially in the house. But the question is what value do we consider for ‘S’? We can try all values from 0 to N (the size of the queue). Doing so one by one might still be slow for the large input. However, we can use binary search to make the search faster. Note that having more people (larger ‘S’) in the house gives us more flexibility to make the simulation work i.e. bigger values of ‘S’ gives us more ‘E 0’ at the start and more ‘L 0’ at the end to work with. Also if the simulation was valid for ‘S’ = i, it would be valid for ‘S’ = i + 1 too because we are adding a ‘E 0’ at the start and correspondingly a ‘L 0’ at the end (which can match each other in our already valid simulation for ‘S’ = i). This means that for the smaller values of ‘S’, we might get invalid simulations (i.e. false values), but after a certain ‘S’ value we will start getting valid simulations (i.e. true values). This indicates that the boolean function (i.e. that function that indicates whether the simulation is valid or not) is a monotonic boolean function (for input ‘S’ = 0 to N). When a function is monotonic, we can use binary search to make the search faster.

Finally, as specified in the problem, if we are unable to find any valid simulation for any of the values of ‘S’ then we report “CRIME TIME” as the answer.

Round 3 2014 - Code Jam 2014

Willow (15pts, 24pts)

The problem can be paraphrased as follows: we are given a tree with N vertices where each vertex i has Ci coins. There are two players taking turns in the game. First, each player picks a starting vertex (could be the same vertex), then player 1 makes the first move. In a move, the player picks a new neighboring vertex adjacent to the last picked vertex, forming a simple path. The path of one player is allowed to intersect with at most one vertex (i.e., no edges overlap) with the path of the other player. The game ends when neither player can make a move. The score for the player is the sum of all coins in the vertices picked by the player subtracted by the sum of all coins in the vertices picked by the other player. Our task is to find the maximum score player 1 can get.

There are N possible starting vertices for player 1. After player 1 picks a starting vertex, player 2 also has N possible starting vertices. Both players will pick a starting vertex that maximizes their score. The high level solution for finding the maximum score for player 1 is shown in the pseudocode below:

  p1_max_score = -INFINITE
  for p1_start_vertex in 1 .. N:
    min_score = INFINITE
    for p2_start_vertex in 1 .. N:
      p1_score = minimax(p1_start_vertex, p2_start_vertex)
      min_score = min(min_score, p1_score)
    p1_max_score = max(p1_max_score, min_score)

  print p1_max_score
Player 1 tries to pick the starting vertex that maximizes the p1_max_score while player 2 tries to pick the starting vertex that minimizes player 1’s score. The function minimax is the main algorithm to maximize the score for the first player, given the starting vertex of each player.

In the next two sections, we will present two minimax algorithms. The first minimax algorithm is based on a depth-first search simulation which runs in O(N^2), thus giving the overall complexity of O(N^4). The other minimax algorithm is based on dynamic programming which can be precomputed in O(N^2) and gives answers in O(1), thus giving the overall complexity of O(N^2 + N^2) = O(N^2).

Minimax using simulation O(N^2)
Given the starting vertex for each player, we can do a depth-first search (DFS) simulation for the minimax algorithm. The DFS state is the last vertices picked by each player and the player currently making the turn. The current player that makes the turn first grabs the coins at the current vertex and then tries to pick a neighboring vertex to visit that maximizes the total coins. Before visiting the neighboring vertex, the (bi-directional) edge connected to the new vertex is removed and later restored when the DFS backtracks. Finally, if the current player cannot make a move, then the player gives the turn to the other player to continue to make a move. The pseudocode below shows the sketch of the algorithm.

function rec(i, j, turn)  # See note 1
  if visited[i][j][turn] return 0  # See note 2
  visited[i][j][turn] = true

  ci = C[i]  # See note 3
  C[i] = 0  # Remove the coins at vertex i

  ret = -INF
  for each neighbor ni of i
    remove edge[i][ni]  # See note 4
    ret = max(ret, -rec(j, ni, 1 - turn))
    restore edge[i][ni]

  if ret == -INF
    ret = -rec(j, i, 1 - turn)  # See note 5

  C[i] = ci  # Restore the coins at vertex i

  return ret + ci
Notes:

The first parameter i of the rec function is the last vertex picked by the current player that is currently making the turn. The second parameter j is the last vertex picked by the other player. The third parameter turn is the player ID that is making the turn (0 is the first player and 1 is the second player)
The state (i, j, turn) is visited exactly once. The next visit to this vertex returns 0 which signifies a terminal condition.
We temporarily store the coins at the vertex of the current player. Then we remove coins from the vertex i (so that further moves in the DFS cannot reuse the coins again), and later restore the coins before returning.
Removing the connecting edge before visiting (recursing) the next vertex ensures that the edge will not be used any further by any of the players later in the DFS. Notice that the edge is restored later when the recursion backtracks. The same is true for the coins.
If the current player cannot move, it must give the turn to the other player.
Given the starting vertex of both players, the DFS simulation above runs in O(N^2) to compute the maximum score for the first player. This comes from the number of distinct states in the parameters (N^2) and the fact that we never process the same state twice. The inner loop which loops through the neighbors of i can be amortized to O(1) since on average each vertex has 1 outgoing edge.

Unfortunately, for different starting vertices, we cannot reuse the computation (i.e., we need to clear the visited states and start from scratch). Therefore the overall runtime complexity is O(N^4) which is only good for the small input N = 80.

Minimax with Dynamic Programming O(1)
The idea behind the O(1) runtime complexity is to be able to reuse the computation when computing for different starting vertices for each player. The DFS state described in the previous section is not independent from another DFS state because it has to keep track of the coins that have already been taken and also the edges that cannot be used anymore. Different DFS states have different sets of coins and edges that are available. If we can design a state where it does not need to care about which coins or edges are available, then we can make each state independent and we can memoize (cache) the state results and reuse it to compute other states. This way, each state can be computed in O(1).

To design a state that is independent of each other, we need to make an important observation: after a player makes a move, the edge connecting the last vertex to the new vertex is removed. If the new vertex is now unreachable from the other player’s last vertex then the two vertices are disconnected and each vertex is in its own tree, then the solution becomes trivial: each player simply takes the best path that remains open (the best path is the path that gives the maximum total coins) in its own tree. We therefore only need to perform minimax across states where the last vertices for both players are still in the same tree.

In our new minimax algorithm, the state space can be entirely described by the last edge traversed by each player (which is only O(N^2) pair of edges). There are two choices for the current player to move (i.e., to pick the new neighboring vertex):

Pick the vertex towards the other player’s last vertex. Note that the input graph is a tree, therefore, there is exactly one unique path connecting the last vertices of both player. The player picks the next vertex in this path that brings them closer together. This move will transition (recurse) to a new state where the other player is now the one taking the turn. The new state is independent from the previous state since we do not need to care about which coins or edges have been removed (i.e., the current player cannot go back to the previous vertex and the other player cannot reach the vertices behind the current player anyway). The vertex that leads to the other player’s last vertex can be found in O(1) by precomputing it beforehand. The precomputation uses dynamic programming where the states are the last edge used by the current player, and the last vertex of the other player. The transition for the current player is to move closer to the last vertex of the other player which can be done in O(1) since each vertex has 1 edge on average (please refer to the sample implementation below for the details of how the next_node_to is pre-calculated). Now, let’s see an example for the first choice we have just described: 
Player 1 is the current player that is making the move and is at vertex i1 and the last edge used is the connecting edge to vertex p1. Player 2 is at vertex i2 and the last edge used is the connecting edge to vertex p2. The current player 1 makes a move by picking the vertex n1 that leads to i2 (there is exactly one path that leads to i2). After player 1 makes a move, we can recurse to a new independent state where the current player is player 2 at i2 with the same last edge and the other player is player 1 at n1 with the last edge being the connecting edge to i1.

Pick the next best vertex that does not lead to the other player’s last vertex. We can pick the vertex in O(1) if we precompute the list of next best vertex beforehand. This move will disconnect the current player’s newly picked vertex from the other player’s last vertex and their maximum total coins can now be processed independently in O(1) as explained in the example below. The other player can then independently pick the best path that avoids the edges used by the current player in O(1). We demonstrate it with the following example: 
Just like before, player 1 is the current player that is making the move and is at vertex i1 with the last edge connecting to vertex p1. The other player is player 2 at vertex i2 with the last edge connecting to vertex p2. The current player 1 picks the new vertex n1 which is the vertex leading to the best path that is disconnected with the other player’s last vertex i2. At this point, both players become independent. Player 2 is free to pick the best path that avoids the two edges used by player 1 (marked as red edges). Player 2 can try moving in the direction to i1 and then “branch-off” at any time to another direction to find its best path. The special case is when it has reached i1, it cannot branch-off in the direction of p1 nor n1 since those edges are already used. Player 2 can only branch-off to vertex f2 from vertex i1. This means that it is sufficient to keep the best 3 outgoing edges for each vertex because when we branch-off, we only consider the next best path to branch-off and the 3rd best path is used only in the special case. We can pre-compute the maximum total coins gained via branching-off in between i2 to i1 (both inclusive) using dynamic programming in O(N^2) (please refer to the branch_off_between method and its comments in the sample implementation below). Thus, to answer the maximum coins that player 2 can get in this case can be answered in O(1).
The two cases above can be answered in O(1), therefore each state can be computed in O(1) and the result can be cached and reused to compute other states (i.e., it does not need to be re-set for different starting vertices). Since there are O(N^2) possible last edges for each player, there are only O(N^2) distinct states. Therefore, given any two starting vertices for each player, the new minimax algorithm can answer the maximum score for player 1 in O(1).

Below is the sample implementation in C++11.

#include <algorithm>
#include <cstdio>
#include <cstring>
#include <vector>

using namespace std;

#define MAXN 4001
#define MAXE (MAXN * 3)

vector<int> con[MAXN];
int T, N, C[MAXN], id;
int edge_id[MAXN][MAXN];
int next_node_to[MAXN][MAXN];
int best_coins[MAXE];
int best_nodes[MAXE][3];
int memo_rec[MAXE][MAXE];
int memo_branch_off[MAXE][MAXE];

// Pre-calculate the next_node_to, best_coins, best_nodes.
void precalc(int i, int pi, int from, int first_node) {
  next_node_to[from][i] = first_node;
  int &best = best_coins[edge_id[i][pi]];
  best = 0;
  vector<pair<int, int> > arr;
  for (int ni : con[i]) if (ni != pi) {
    precalc(ni, i, from, first_node);
    int coins = best_coins[edge_id[ni][i]];
    arr.push_back(make_pair(coins, ni));
    best = max(best, coins);
  }
  sort(arr.rbegin(), arr.rend());
  for (int j = 0; j < arr.size() && j < 3; j++)
    best_nodes[edge_id[i][pi]][j] = arr[j].second;
  best += C[i];
}

// Returns the best next vertex when coming from
// edge (pi -> i), excluding vertex v1 and v2.
int next_best_except(int i, int pi, int v1, int v2 = -1) {
  int ei = edge_id[i][pi];
  int j = 0, *arr = best_nodes[ei];
  if (arr[j] == v1 || arr[j] == v2) j++;
  if (arr[j] == v1 || arr[j] == v2) j++;
  return arr[j];
}

// Maximum coins for sub-tree i with parent pi.
int max_coins(int i, int pi) {
  return (i < 0) ? 0 : best_coins[edge_id[i][pi]];
}

// Maximum coins for branching off at any vertex in [i, j].
int branch_off_between(int i, int pi, int j, int pj) {
  int ei = edge_id[i][pi];
  int ej = edge_id[j][pj];
  int &ret = memo_branch_off[ei][ej];
  if (ret != -1) return ret;

  if (i == j) {
    int ni = next_best_except(i, pi, pj);
    int nj = next_best_except(i, pi, pj, ni);
    // The other player takes the third best vertex nj since
    // the best two are already taken by the current player.
    return ret = max_coins(nj, j);
  }

  int nj = next_node_to[j][i];
  int njb = next_best_except(j, pj, nj);
  int branch_off_now = max_coins(njb, j);
  int branch_off_later = ((nj == i) ? 0 : C[nj])
    + branch_off_between(i, pi, nj, j);

  return ret = max(branch_off_now, branch_off_later);
}

// Minimax for the current player with last edge (pi -> i)
// and the other player with last edge (pj -> j).
int rec(int i, int pi, int j, int pj) {
  int ei = edge_id[i][pi];
  int ej = edge_id[j][pj];
  int &ret = memo_rec[ei][ej];
  if (ret != -1) return ret;

  if (i == j) {
    // The current player pick the next best path.
    int ni = next_best_except(i, pi, pj);
    // The other player pick the next next best path.
    int nj = next_best_except(i, pi, pj, ni);
    return ret = max_coins(ni, i) - max_coins(nj, j);
  }

  // The first option for the current player:
  // The current player pick the vertex ni
  // that leads to other player last vertex.
  int ni = next_node_to[i][j];
  int option1 = ((ni == j) ? 0 : C[ni]) - rec(j, pj, ni, i);

  // The second option for the current player:
  // The current player go to the best path other than ni.
  ni = next_best_except(i, pi, ni);
  int p1coins = max_coins(ni, i);
  // The other player branch off at any point
  // between vertex i and j (inclusive).
  int p2coins = branch_off_between(i, pi, j, pj);
  int option2 = p1coins - p2coins;

  // Pick the best outcome for the current player.
  return ret = max(option1, option2);
}

int main() {
  scanf("%d", &T);
  for (int TC = 1; TC <= T; TC++) {
    scanf("%d", &N);
    for (int i = 0; i < N; i++) {
      scanf("%d", &C[i]);
      con[i].clear();
    }
    id = 0;
    memset(edge_id, -1, sizeof(edge_id));
    for (int i = 0, j; i < N - 1; i++) {
      scanf("%d", &j); j--;
      con[i].push_back(j);
      con[j].push_back(i);
      edge_id[i][j] = id++;
      edge_id[j][i] = id++;
    }
    for (int i = 0; i < N; i++) {
      edge_id[i][N] = id++;
    }

    // These memoizations are reset per test case.
    memset(best_coins, -1, sizeof(best_coins));
    memset(best_nodes, -1, sizeof(best_nodes));
    memset(next_node_to, -1, sizeof(next_node_to));
    memset(memo_rec, -1, sizeof(memo_rec));
    memset(memo_branch_off, -1, sizeof(memo_branch_off));

    // Pre-calculation.
    for (int i = 0; i < N; i++) {
      precalc(i, N, i, N);
      for (int j : con[i]) precalc(j, i, i, j);
    }

    int max_diff = -1000000000;
    for (int i = 0; i < N; i++) {
      int min_diff = 1000000000;
      for (int j = 0; j < N; j++) {
        int cost = C[i] - (i == j ? 0 : C[j]);
        min_diff = min(min_diff, cost + rec(i, N, j, N));
      }
      max_diff = max(max_diff, min_diff);
    }
    printf("Case #%d: %d\n", TC, max_diff);
  }
}

World Finals 2014 - Code Jam 2014

Checkerboard Matrix (4pts, 9pts)

Video of Igor Naverniouk’s explanation.

Let’s start by making some observations about a checkerboard matrix:

Since a matrix is of dimension 2*N by 2*N, each row and each column have the same number of 0s and 1s.
Corners of a checkerboard matrix always have two 0s and two 1s, and if we take any submatrix with at least 2 rows and 2 columns (not necessarily square) there will be an even number of 0s and 1s in the corners.
Here is an example of a checkerboard matrix and two sample submatrices (shown with red borders), there are four 1s and no 0s in the corners of one of them and two 1s, two 0s in the other’s. It is easy to see that it is impossible to find any submatrix with at least 2 rows and 2 columns where the above property is false.

Now, let’s analyze what happens when we swap 2 rows or 2 columns. Such an operation doesn’t change the number of 0s and 1s in a row or column, but more importantly corners of any submatrix will still have even number of 0s and 1s. Indeed, by swapping rows or columns we transform any submatrix with corners located on both of the swapped rows or columns and it will not change the numbers in the corners. This means that any matrix for which the above properties doesn’t hold cannot be transformed into a checkerboard matrix.

Let’s take a matrix for which the above properties hold and consider a pair of rows (the same arguments hold for columns). We can show that all rows that start with 1 will actually be equal to each other and so will be all rows that start with 0 (let’s call these sets of rows A and B). Additionally, corresponding (same column) numbers of a type A and type B row will be different. Thus A rows will be the inverse of B rows. Let’s prove this by considering two rows of the same type. Let’s choose a submatrix with 2 corners fixed in the first column and the others in some other column. By definition of row types, we have 2 corners in the first column equal to each other, but then by the property of a checkerboard matrix we cannot have two different numbers in the other corners as we would get an odd number of 0s or 1s. Therefore, all numbers of same type rows (i.e. A or B) are the same. Analogously we can show that A rows and B rows are the inverse of each other. Since in a checkerboard matrix each row and column has the same number of 0s and 1s, the number of rows of each type is the same. The same arguments hold for columns.

Here is an example of a matrix obtained by permutation of rows and columns of a checkerboard matrix, which demonstrates the facts we just proved:

In a checkerboard matrix, type A and type B rows (and columns) alternate. We note that transformations of rows and columns are independent and we can first find the optimal number of row swaps, then the optimal number of column swaps, and then sum these numbers. Let us denote a type A row (or column) as ‘A’, and a type B row (or column) as ‘B’. Therefore the problem generalizes to the following problem: Given a sequence of 'A' and 'B' characters where the number of ‘A’s is the same as the number of ‘B’s, swap pairs of elements to make the ‘A’s and ‘B’s alternate. We have 2 choices on which letter goes first, either ‘A’ or ‘B’. We can try both. Given the desired order is fixed we can take a pair of ‘A’ and ‘B’ that are in the wrong place and swap them and record the number of swaps.

To conclude, here is the algorithm for the whole problem:

Start by assigning the first row to type A
Go over all rows and compare them element by element with the first row
if rows equal each other, they are of the same type
if they are the inverse of each other, they are of type B
otherwise the given matrix cannot be transformed into a checkerboard matrix and we can output “IMPOSSIBLE”
Check that the sets A and B have the same number of rows, otherwise output “IMPOSSIBLE”
The minimum number of row (similarly for column) swaps is equal to min(nswaps_A, nswaps_B) where nswaps_X is the number of wrong positions for type X if we assume that the first row is of type X. The number of wrong positions for type X is equal to the number of rows of type X that reside at even positions. Remember that type A and B must alternate.
Repeat 1-4 for columns.
Output the sum of the answers for rows and columns.
Sample implementation in Python 3:

def count_swaps(pos):
  nswaps = 0
  for i in pos:
    if i % 2 == 0:
      nswaps = nswaps + 1
  return nswaps

def inverse(A):
  return [chr(ord('0') + ord('1') - ord(c)) for c in A]

# min_swaps returns minimum swaps required to 
# form alternating rows. If B is an invalid matrix, 
# returns -1 to denote an impossible case.
def min_swaps(M, N):
  # Step 1.
  typeA = M[0]
  typeB = inverse(typeA)

  pos_A = []
  pos_B = []
  for i in range(2 * N):
    # Step 2 a.
    if M[i] == typeA:
      pos_A.append(i)
    # Step 2 b.
    elif M[i] == typeB:
      pos_B.append(i)
    # Step 2 c.
    else:
      return -1
  # Step 3.
  if len(pos_A) != len(pos_B):
    return -1

  # Step 4.
  return min(count_swaps(pos_A), count_swaps(pos_B))

def solve(M, N):
  # Step 1-4.
  row_swaps = min_swaps(M, N)
  if row_swaps == -1:
    return "IMPOSSIBLE"

  Mt = [list(i) for i in zip(*M)]  # Transpose matrix M

  # Step 5.
  col_swaps = min_swaps(Mt, N)
  if col_swaps == -1:
    return "IMPOSSIBLE"

  return row_swaps + col_swaps

for tc in range(int(input())):
  N = int(input())
  M = []
  for i in range(2 * N):
    M.append(list(input()))
  print("Case #%d: %s" % (tc + 1, solve(M, N)))

World Finals 2014 - Code Jam 2014

Power Swapper (4pts, 12pts)

Video of Bartholomew Furrow’s explanation.

Observe that a swap operation of size 2^x can be performed independently from a swap operation of size 2^y. Thus, we can simplify the sorting process by performing the swaps from the smallest size to the largest size and assume that when performing a swap of size 2^z, the adjacent numbers inside it are fully sorted (by the previous, smaller sized swaps). Fully sorted here means the adjacent numbers are increasing consecutively (i.e., each number is bigger than the previous by 1).

Let’s look at the possible swappings for the smallest 2^k size element where k = 0 (i.e., swapping two sets of size 1). There are 2^N valid sets of size 1. However, we are only interested in swapping two valid sets such that all 2^(k+1) sized valid sets are fully sorted. This is important because when swapping larger sizes, we assume the smaller sized valid sets are all already fully sorted. Let’s look at some examples:

Permutation: 2 4 1 3. The only way is to swap 2 with 3, yielding 3 4 1 2 where all 2^1 sized valid sets are fully sorted.
Permutation: 1 4 3 2. There are two ways to perform a swap so that all 2^1 sized valid sets are fully sorted. The first is by swapping 1 with 3 and the other is by swapping 4 with 2.
Permutation: 1 4 3 2 6 5. In this case, there are 3 valid sets of size 2^1. It needs at least two swaps to make all 2^1 sized valid sets fully sorted. Thus, it is impossible to sort this permutation.
Learning from the examples above, it is sufficient to only consider swapping at most two valid sets of size 2^k for k = 0. We can generalize this for larger k, assuming all valid sets of smaller size are already fully sorted.

To count the number of ways to sort, we can use recursion (backtracking) to simulate all possible swaps. The recursion state contains the underlying array, the value k, and the number of swaps performed so far. The recursion starts from the smallest sized swaps (k = 0) with the original input array, then it decides which valid sets to swap (if any) and recurses to the larger (k + 1) swaps, and so on until it reaches the largest sized swaps (k = N). The depth of the recursion is at most N and there are at most two branches per recursion state (since there are at most two possible swaps for each size) and each state needs O(2^N) to gather at most two valid candidate sets to be swapped. Thus, the algorithm runs in O(2^(N*2)). When the recursion reaches depth N, it has the number of swaps that have been done so far. Since the ordering of the swaps matters, the number of ways is equal to the factorial of the number of swaps. We propagate the number of ways back up to the root to get the final value. Below is a sample implementation in Python 3:

import math

def swap(arr, i, j, sz):  # Swap two sets.
  for k in range(sz):
    t = arr[i + k]
    arr[i + k] = arr[j + k]
    arr[j + k] = t

def count(arr, N, k, nswapped):
  if k == N:
    return math.factorial(nswapped)

  i = 0
  idx = []  # Candidates’ index for swappings.
  sz = 2**k
  while i < 2**N:
    if arr[i] + sz != arr[i + sz]:
      idx.append(i)
    i = i + sz * 2

  ret = 0
  if len(idx) == 0:  # No swap needed.
    ret = count(arr, N, k + 1, nswapped)

  elif len(idx) == 1:  # Only one choice to swap.
    swap(arr, idx[0], idx[0] + sz, sz)
    if arr[idx[0]] + sz == arr[idx[0] + sz]:
      ret = count(arr, N, k + 1, nswapped + 1)
    swap(arr, idx[0], idx[0] + sz, sz)

  elif len(idx) == 2:  # At most 2 choices.
    for i in [idx[0], idx[0] + sz]:
      for j in [idx[1], idx[1] + sz]:
        swap(arr, i, j, sz)
        if arr[idx[0]] + sz == arr[idx[0] + sz]:
          if arr[idx[1]] + sz == arr[idx[1] + sz]:
            ret = ret + count(arr, N, k + 1, nswapped + 1)
        swap(arr, i, j, sz)

  return ret

for tc in range(int(input())):
  N = int(input())
  arr = list(map(int, input().split()))
  print("Case #%d: %d" % (tc+1, count(arr, N, 0, 0)))

World Finals 2014 - Code Jam 2014

Symmetric Trees (7pts, 18pts)

Video of Igor Naverniouk’s explanation.

Given a vertex-colored tree, we want to know whether it is possible to draw the tree in a 2D plane with a line of symmetry (as defined in the problem statement).

An important question to ask is: how many vertices in the tree are located on the symmetry line? We can categorize the answer into two cases. The first is where there is no vertex on the symmetry line, and the second is where there are one or more vertices on the symmetry line. We will discuss each category in the following sections.

No vertex on the symmetry line
This is the easier case. Since the tree has at least one vertex, there must be a vertex A on the left of the symmetry line connected to another vertex A’ on the right of the symmetry line. The rest of the vertices must belong to the subtree of A or to the subtree of A’. The only edge that crosses the line of symmetry is the edge connecting vertex A and A’. There cannot be any other edges that cross the line of symmetry (otherwise there would be a cycle). The last requirement is that subtree A and subtree A’ are isomorphic. We will elaborate later on how to check whether two subtrees are isomorphic. The following figure illustrates this case:

One or more vertices on the symmetry line
In this case, we can pick a vertex A and put it on the symmetry line. Next, we look at the children of A. Each child represents a subtree. For each child (or subtree) of A we find another (different) child of A that is isomorphic to it. If found, we can put one child on the left side of the symmetry line and the other on the right side of the symmetry line. The root nodes of the subtrees that cannot be paired must be put on the symmetry line. Since there are only two available positions on the line (one is above A and the other is below A), the number of children that cannot be paired must be at most two. These (at most) two children are put on the symmetry line using the same technique described above, except that they can only have one unpaired child (since they have only one available position on the symmetry line). See the following illustration:

We pair up all children of A that are isomorphic to each other (e.g., B and B’, C and C’ and so on) and put one of them on the left and the other on the right of the symmetry line. There can only be at most two remaining unpaired children (subtree X and subtree Y). We can recursively put X and Y on the symmetry line just above and below vertex A respectively. The subtree of X can only have at most one unpaired child since there is only one available position to put the unpaired child on the symmetry line (i.e., above X). Similarly with the subtree of Y.

How to check whether two subtrees are isomorphic?
The above placing strategies require a quick way of checking whether two subtrees are isomorphic. One way to do this is by encoding each subtree into a unique string representation and then checking whether the strings are equal. One way to encode a subtree into a string is by using a nested-parentheses encoding: starting from the root node of the subtree, construct a string that begins with “(“ followed by the color of the node, then a comma, followed by a sorted, comma-separated list of the encodings of its children (which are computed recursively in the same way), and finally followed by a “)”. For example if node X is the parent of node Y and node Y is the parent of node Z, then the encoding of the subtree X is “(X,(Y,(Z)))”. If node X is the parent of node Z and node Y, then the encoding of the subtree X is “(X,(Y),(Z))”. Note that the children encodings are sorted because we want two subtrees with identical children to produce the same string.

Here is a sample implementation in Python 3:

import sys

def encode_subtree(a, parent):
  children = []
  for b in con[a]:
    if b != parent:
      if con[a][b] == -1:
        con[a][b] = encode_subtree(b, a)
      children.append(con[a][b])

  m = '(' + colors[a]
  for c in sorted(children):
    m += ',' + c
  return m + ')'

def rec_symmetric(a, parent):
  first_pair = {}
  for b in con[a]:
    if b != parent:
      if con[a][b] in first_pair:
        del first_pair[con[a][b]]
      else:
        first_pair[con[a][b]] = b

  keys = list(first_pair.values())
  if len(keys) == 0: return True

  ok = rec_symmetric(keys[0], a)
  if len(keys) == 1 or not ok: return ok

  # Non-root is only allowed one unpaired branch.
  if len(keys) > 2 or parent != -1: return False

  return rec_symmetric(keys[1], a)

def symmetric():
  # No vertex in the middle line.
  for a in range(N):
    for b in con[a]:
      if con[a][b] == con[b][a]:
        return True

  # Pick a vertex in the middle line.
  for a in range(N):
    if rec_symmetric(a, -1):
      return True

  return False

sys.setrecursionlimit(100000)
for tc in range(int(input())):
  colors = []
  con = []

  N = int(input())
  for i in range(N):
    colors.append(input())
    con.append({})

  for i in range(N - 1):
    e = input().split()
    a = int(e[0]) - 1
    b = int(e[1]) - 1
    con[a][b] = -1
    con[b][a] = -1

  for a in range(N):
    encode_subtree(a, -1)

  if symmetric():
    print("Case #%d: SYMMETRIC" % (tc + 1))
  else:
    print("Case #%d: NOT SYMMETRIC" % (tc + 1))

World Finals 2014 - Code Jam 2014

Paradox Sort (4pts, 28pts)

Video of Igor Naverniouk’s explanation.

We can view the set of preferences as a directed graph. We have a node in the graph for each candy, and if we prefer candy X over candy Y, then there is a directed edge from the node for X to the node for Y.

We first describe a procedure to determine if we can find any valid permutation, then we describe a process to find the lexicographically smallest permutation.

Is a permutation possible?
To answer this question, we will use a depth-first search in the graph described above. We use A, the target candy, as the root of the search. A valid permutation exists if and only if we can visit all other candies from the root. Otherwise, there is no valid permutation. Why is that?

If all the candies are reached in the DFS, then we have a directed tree rooted at A containing all the nodes. Then we can generate a valid permutation by printing the nodes using a post-order traversal of the tree (i.e. each node is placed in post-order in the permutation order).

In that permutation, A is the last candy that Vlad will be offered, and Vlad will keep it. To see why this is true, consider the tree implied by the depth-first search. Let X be the candy Vlad is holding before receiving candy A. X must have been preferred to every candy between X and A in the post-order traversal. If X was not a child of A, then one of those candies must be X's parent in the tree, which Vlad would prefer over X. So X must be a child of candy A in the depth-first search, so he will prefer to keep A when offered it.

We demonstrate this with an example for the following test case:

4 1
-YNY
N-YN
YN-N
NYY-
There are 4 candies, and the target candy is candy 1. The candy preferences are shown by directed edges in the figure below. The solid lines represent the tree the DFS found, while the dashed lines represent the edges that were not taken during the DFS.

In the post-order traversal, we get the permutation (ordering) 3, 2, 4, 1, which is a valid permutation that leads to our target candy 1 being chosen in the end.

Now we show that if there is a candy that we do not visit in the DFS, then there is no valid permutation. Let X be a candy that is not reached. Let Y be the candy that Vlad keeps after receiving candy X. Y is either X, or another candy which Vlad prefers to X, in which case there is a directed edge in the graph from Y to X. Y is not A, because there cannot be an edge from A to X, or X would have been visited in the depth-first traversal. Consider the list of unique candies that Vlad chooses to keep, in order of time. Assume that Vlad will end up with candy A. The list of kept candies must then end in A, and contain Y earlier in the list. In the sublist of candies between Y and A, Vlad prefers each candy to the one before it. But this means there must be a directed path in the graph from A to X, which is impossible because X was not visited in the DFS. So Vlad cannot end up with candy A.

Thus by using the depth-first traversal procedure, we can answer if the given input can form a valid permutation.

In the following sections, we describe the procedure to generate the lexicographically-smallest permutation.

Checking for valid partial solutions
We will use a greedy strategy to find the lexicographically-smallest permutation. To help us with the greedy strategy, we discuss the case of a partial solution. We want to answer if we can generate a valid permutation from this partial list. In the partial list, the first few numbers (candies) of the lexicographically-smallest permutation have been selected, and one of those candies is the current “best” (preferred) candy B (which might be equal to A). We modify our graph by removing all the numbers (except B) that are already selected in the permutation.

The goal is to answer if we can end up with A from the current partial solution. Trivially, if A is part of the set of removed numbers then it is impossible to generate a valid permutation (we can never end up with A from this state). But if A is not part of the removed set then there are two cases to consider: (i) B is equal to A, or (ii) B is not equal to A. If it is case (i), then A must be preferred over all remaining candies for us to end up with A, otherwise we can report that we cannot generate a valid permutation from this partial solution. Now, let us deal with the more complicated case (ii).

First note that we cannot just use the idea proposed in the section “Is a permutation possible?”, i.e. pick A as the root, then do post-order depth-first traversal on the remaining nodes. The depth-first search might contain a path as follows: A->..->B->Y->Z (see figure below). Then we can't use the post-order traversal, because B is already being held by Vlad, so Y and Z can't occur before it.

Instead, we follow the following procedure:

Do the same depth-first search, but when visiting B, do not follow any edges out of B.
Then, for each node X which was not visited by the DFS, and for which there is an edge from B to X, add X as a child of B.
If this succeeds in adding every node to the tree, then it is possible to complete the permutation, otherwise it is not.

To construct the remainder of a permutation from this tree, we first append to the partial solution all the children of B that were added in step 2. Vlad prefers B to each of these. Then we remove B and its children from the tree, and append a post-order traversal of the remaining tree. This results in A winning, for the same reasons as for the algorithm in the previous section for determining feasibility of the whole permutation.

Also similarly to the previous section, we can show that if there is a node X which is not added to the tree by the above procedure, then Vlad cannot be left with A, or else there would be a path from A to X that would have allowed X to be added to the tree.
Finding the lexicographically-smallest permutation
We described above a method to determine if we can generate a valid permutation from a given partial solution. Using this idea, we give an algorithm that provides us with the lexicographically-smallest permutation.

We start with an empty prefix of the permutation, then iteratively add the lexicographically smallest candy that leads to a valid prefix. It reports a partial solution as impossible if no candy can be added to the partial solution leading to a valid solution. This algorithm is outlined in the pseudocode below:

Let P = []  // The partial solution

If !IsValidPartialSolution(P)
  return IMPOSSIBLE

Repeat N times
  For i = 1 to N
    // Test if appending i to P gives a valid partial solution
    If P.contains(i) is false
      If IsValidPartialSolution(P + [i]) is true
        P = P + [i]
        Break

Return P
Sample implementation in Python 3:

def dfs(i, removed, visited):
  visited.add(i)
  for j in range(len(prefer[i])):
    if prefer[i][j] == 'Y':
      if j not in visited and j not in removed:
        dfs(j, removed, visited)

def valid(partial):
  B = partial[0]
  for i in partial:
    if prefer[i][B] == 'Y':
      B = i

  removed = set(partial)
  removed.remove(B)

  # Trivial case.
  if A in removed: return False

  # Case (i)
  if A == B:
    for i in range(N):
      if i != A and i not in removed:
        if prefer[A][i] != 'Y':
          return False
    return True

  # Case (ii)
  visited = set([B])
  dfs(A, removed, visited)
  for i in range(len(prefer[B])):
    if prefer[B][i] == 'Y':
      visited.add(i)
  return len(visited.union(removed)) == N

def solve():
  visited = set()
  dfs(A, set(), visited)
  if len(visited) != N: return "IMPOSSIBLE"

  partial = []
  for i in range(N):
    for j in range(N):
      if j not in partial and valid(partial + [j]):
        partial = partial + [j]
        break
  return ' '.join(map(str, partial))

for tc in range(int(input())):
  [N, A] = map(int, input().split())
  prefer = []
  for i in range(N):
    prefer.append(input())
  print("Case #%d: %s" % (tc + 1, solve()))

World Finals 2014 - Code Jam 2014

Allergy Testing (15pts, 35pts)

Video of Bartholomew Furrow’s explanation.
We describe two solution strategies. First a dynamic programming solution, then a more direct combinatorial solution.

Dynamic programming on the number of foods
At a glance, this sounds like a standard dynamic programming problem where we compute the minimum number of days needed to identify the bad food among the N foods. We can split the N foods into two groups, perform an experiment on one of the groups (i.e., eat all the foods in the first group) and wait for the outcome. If Kelly gets an allergic reaction, then the bad food is in the first group, otherwise it’s in the second group. The number of days needed then depends on the number of days it takes to solve each of the smaller sets.

We can try all possible split sizes for the first group, which gives an O(N^2) solution. We can improve this to O(N) by noticing that the optimal split size for N is always at least as large as the optimal split size for N-1. So we only need to consider split sizes starting at the previous split size, and in total only O(N) split positions are considered. Looking at the constraints for the small input, where N can be as large as 10^15, it is obvious that this is not the way to go. However, this approach may give some insight on how to improve the solution (as we shall see in the next section).

You may wonder why splitting the N foods equally and performing experiments on one of the groups does not give an optimal answer. Consider an example where A = 1, B = 100, and N = 4. In this case, the cost of having an allergic reaction is very high. It is better to perform an experiment one food at a time because as soon as Kelly eats the bad food, she will know about it the next day and thus the whole process only takes at most 3 days. But if she splits into two groups of size 2 and performs an experiment on one of the groups, she risks getting an allergic reaction and still being unable to identify the bad food and will have to wait 100 days for the next experiment. Therefore splitting the N foods equally may not lead to an optimal solution.

Dynamic programming on the number of days
After playing around with some small cases where A and B are at most 100, you will discover that the minimum number of days needed is only in the range of thousands, even for large N. This gives a hint that using the number of days as the DP state could be much more efficient. We can use DP to compute max_foods(D), the maximum number of foods for which Kelly can identify the bad food using at most D days. If we can compute max_foods(D) quickly, we can compute the final answer by finding the smallest X where max_foods(X) >= N.

To compute max_foods(D), we are going to construct a full binary tree that represents a strategy that handles the largest possible number of foods in at most D time. Each node in the tree corresponds to a set of foods. Kelly starts at the root node, and performs a series of tests. Each test narrows down the set of foods, and she moves to the node corresponding to that set. To do this, she eats all the foods in the right child of the current node. If she does not get a reaction, she moves to the left child. If she does get a reaction, she moves to the right child. Leaf nodes of the tree correspond to single foods. When Kelly reaches a leaf node, she has figured out the food to which she is allergic, and can stop.

Define the "height" of a node to be the number of days remaining at that node. The root node has height D, and each other node must have height at least 0. If a node has height X, then its left child has height X-A, since it takes A days until Kelly knows she has not had a reaction. Its right child has height X-A if it is a leaf, and X-B otherwise. This is because Kelly only has to wait B days if she needs to perform another test when the reaction has worn off.

Define the "value" of a node to be the size of the set of foods to which that node corresponds.

Let’s look at an example of a node with height = D and value = max_foods(D) = max_foods(D-A) + max_foods(D-B):

The height of the left child is D - A days, and it has value max_foods(D - A). The height of the right child is D - B days, and it has value max_foods(D - B). This means that if Kelly has a budget of D days and max_foods(D - A) + max_foods(D - B) foods to be processed, she can divide the foods into two groups: the first group is of size max_foods(D - A) foods and the second group is of size max_foods(D - B) foods.

If the tree corresponds to a strategy for handling max_foods(D) foods, then each subtree must contain as many nodes as possible given the time constraints, otherwise we could produce a better strategy by changing that subtree.

So if more than one food can be handled at a node A, then its value must be max_foods(height(A)).

The situation for leaf nodes, where only one food can be handled, is more complex. If the node is the left child of its parent, then its height must be less than A, otherwise we could handle at least two foods there instead. If the node is the right child of its parent, then its height must only be less than B. This is because if another food is added, then Kelly must wait B days after the previous test instead of A days.

Let’s see an example where we want to compute max_foods(8) where A = 2 and B = 3. We can construct the following full binary tree:

The root node has height = 8 days. Using the rules described above, we can continue traversing the children until we reach the leaf nodes. Going up from the leaf nodes all the way to the root, we can compute the value of the intermediate nodes by summing the value of their two children. From the figure above, we can figure out that max_foods(8) = 9, and similarly max_foods(6) = 5, max_foods(5) = 4, etc.

Nodes with the same height always have the same value (except for the case of leaves which are right children.)

Thus, we can use memoization to do the computation in O(days needed). Here is a sample implementation in Python 3:

from functools import lru_cache

@lru_cache(maxsize = None)  # Memoization.
def max_foods(D, A, B):
  if D < A: return 1  # Leaf node.
  return max_foods(D - A, A, B) + max_foods(D - B, A, B)

# We can also use binary search here.
def min_days(N, A, B):
  days = 0
  while max_foods(days, A, B) < N:
    days = days + 1
  return days

for tc in range(int(input())):
  print("Case #%d: %d" % (tc+1, \
    min_days(*map(int, input().split()))))
The above solution assumes that the answer (i.e., the number of days) is limited to a few thousand days, which is true for the small input cases where A and B is at most 100. For the large input cases, A and B can be up to 10^12 which makes the above solution infeasible.

Dynamic programming on the number of edges of each type
Observe that for large A and B, the set of days D where max_foods(D) is larger than max_foods(D-1) is sparse. Each edge reduces the remaining time by either A or B. Call these A-edges and B-edges. Since the height of a node depends only on the number of A-edges and B-edges on its path to the root, we can use a more “compact” DP state to compute max_foods(D) by using the number of A-edges and B-edges instead of using the height as shown in the sample implementation in Python 3 below:

INF = 1e16

def max_foods(D, A, B):
  ans = 0
  mem = [0] * 60  # zero-array of 60 elements.
  mem[0] = 1
  for i in range(int(D / A + 1)):  # A-edges.
    for j in range(int(D / B + 1)):  # B-edges.
      H = i * A + j * B  # Height of this node.

      # Skip this node if it uses more than D days.
      if H > D: break

      # Aggregate the child’s value.
      if j > 0 and H + A <= D: mem[j] += mem[j-1]

      # if we are too close to D to add a B-edge,
      # the right child is a leaf,
      # so add this node to the answer.
      if H + B + A > D: ans += mem[j]

      # Avoid overflows.
      ans = min(ans, INF)
      mem[j] = min(mem[j], INF)

  return ans
Notice that the above code reuses the DP array as it iterates through the A-edges, and the number of B-edges is limited to < 60, so the memory footprint is very small. However, this approach does not work when the number of A-edges is very large. This number can be as large as 50 * B / A, so whether this solution works depends closely on the ratio B / A. In the next two sections we provide two solutions to address this issue.

Solution 1: Use a closed-form solution for #B-edges <= 2.
Observe that as the ratio B / A gets large, the number of B-edges in the solution ends up being very small and the number of A-edges ends up being very large (which makes the dynamic programming solution above run very slow). In fact, if B / A > N, then the solution will involve no B-edges at all. This corresponds to a strategy of trying the foods one at a time until you find the correct one. It is possible to derive closed forms to compute max_foods(D) if there are at most 0, 1 and 2 B-edges in the solution, corresponding to tree heights <B+A, <2B+A and <3B+A respectively. For a ratio B / A greater than 200,000, the maximum tree height is less than 3B+A and so we can get the solution from one of these closed forms. If the ratio is less than 200,000, we can fall back to our dynamic programming solution.

Below is the sample implementation in Python 3 of the closed form solutions for computing max_foods(D) where there are at most 0 (linear), 1 (quadratic), or 2 (cubic) B-edges:

def linear(D, A, B):
  return int(D / A + 1)

def quadratic(D, A, B):
  R = int((D - B) / A)
  if INF / R < R: return INF
  return int(linear(D, A, B) + (R * (R + 1)) / 2)

def cubic(D, A, B):
  ans = quadratic(D, A, B)
  a = int((D - 2 * B) / A)
  for i in range(a):
    R = a - i
    if INF / R < R: return INF
    ans += int((R * (R + 1)) / 2)
    if ans > INF: return INF
  return ans
To put it all together, we can determine which closed form solution to use, or use the compact dynamic programming as the fallback and then use binary search to find the minimum days needed. Here is the rest of the code:

def binary_search(N, A, B, low, high, func):
  while high - low > 1:
    D = int((high + low) / 2)
    if func(D, A, B) >= N:
      high = D
    else:
      low = D
  return high

def min_days(N, A, B):
  if quadratic(B + A, A, B) >= N:
    return binary_search(N, A, B, -1, B + A, linear)
  if cubic(2 * B + A, A, B) >= N:
    return binary_search(N, A, B, B + A, 2 * B + A, quadratic)
  if cubic(3 * B + A, A, B) + 1 >= N:
    return binary_search(N, A, B, 2 * B + A, 3 * B + A, cubic)
  return binary_search(N, A, B, 3 * B + A, 51 * B, max_foods)

for tc in range(int(input())):
  print("Case #%d: %d" % (tc+1, \
    min_days(*map(int, input().split()))))
Solution 2: Speed up the dynamic programming solution through fast matrix exponentiation
There is another way to handle very large number of A-edges without using closed form formulae. Observe that in the dynamic programming solution code, the mem vector at stage i+1 is a linear sum of itself at stage i, so we can represent the transition as a matrix-vector multiplication, implementing the answer as an accumulator parameter in the vector. We can therefore accelerate the process using fast matrix exponentiation.

There is a slight complication in that the matrix is not the same for all i. If p_j is the smallest index j for which H + A > D, then the matrix describing the state transition remains constant as long as p_j doesn't change. However, we can see from the structure that p_j is monotonically decreasing with increasing i and it can therefore only take 50 different values at most. We can therefore break up the computation into at most 50 ranges of continuous p_j and perform our matrix exponentiation to get the answer to each of them.

The complexity of this method could be as high as log^6 N, from factors log N for the binary search, log N ranges of p_j, matrix multiplications of log^3 N and log N for the fast exponentiation. However, not all of these parameters can be large simultaneously and it's likely that the actual time bound is somewhat tighter than this coarse estimate.

Combinatorial solution
We can compute max_foods(T) more directly, and then do a binary search to find the minimum T such that max_foods(T) >= N.

For each leaf, we write down a string of 'x' and 'y' characters that indicates the path taken from the root to the leaf. 'x' indicates a left branch, and 'y' indicates a right branch. For paths that end with a right branch, and have height < B-A, remove the final 'y'.

Each leaf's string now consists of some unique arrangement of K 'x' characters and L 'y' characters, where T-B < KA + LB <= T. Also, each possible string of 'x' and 'y' characters satisfying the above constraint corresponds to a leaf in the tree. We can determine the number of leaves by counting the number of these strings. There are (K+L) choose L strings that contain 'x' K times and 'y' L times. So, we have:

where T-B < KA + LB <= T

We can easily compute the minimum and maximum K such that T-B< KA+LB <= T for a given L. So we can rewrite the sum above as:

Which equals:

Since the number of strings grows exponentially with L, the maximum L is O(log N), so we can compute this sum efficiently.

Below is a sample implementation in Python 3. Note that Python has a built in big integer, thus we don’t need to worry so much about overflow.

def nCk(n, k):
  if k > n: return 0
  res = 1
  for i in range(1, min(n - k, k) + 1):
    res = res * (n - i + 1) // i
  return res

def max_foods(D, A, B):
  cnt = 0
  for L in range(min(51, D // B + 1)):
    K_min = (D - L * B - B) // A + 1
    K_max = (D - L * B) // A
    cnt += nCk(K_max + L + 1, L + 1) - nCk(K_min + L, L + 1)
  return cnt

def min_days(N, A, B):
  lo = 0
  hi = int(1e15)
  while lo < hi:
    D = (lo + hi) // 2
    if max_foods(D, A, B) >= N:
      hi = D
    else:
      lo = D + 1
  return lo

for tc in range(int(input())):
  print("Case #%d: %d" % (tc+1, \
    min_days(*map(int, input().split()))))

World Finals 2014 - Code Jam 2014

ARAM (22pts, 42pts)

Video of Bartholomew Furrow’s explanation.
We are going to do a binary search to find the answer. At each iteration of the binary search, we have some estimate of the answer Q, and we need to decide if that estimate is too high or too low. We do this by trying to find a strategy that can achieve a fraction Q of wins in the long run.

We need to describe what a valid strategy is. In a straightforward dynamic programming formulation of the problem, the current state would be the champion we have been assigned, the amount of money we have, and the number of games that remain; the decision we would need to make at each state is whether to reroll.

Obviously we cannot compute the decision for each of these states individually; there are O(10^100 * N * R * G) of them! But since the number of games that will be played is immense, we can assume that the number of games remaining is unimportant, and just optimize assuming there are "infinite" games remaining. It can be proved that the amount of error this will induce in the answer is far less than 10^-10, the required precision. Also note that if we reroll when assigned a certain champion for a particular amount of money, then it also makes sense to reroll when assigned a champion with a lower winning probability, with the same amount of money. So to compute a strategy, we we only need to decide for each amount of money, which hero we would reroll that has the highest winning probability.

But how do we evaluate how good a strategy is, when we're going to play infinite games? When we are low on money, we need to be more conservative about when we reroll. If we reroll too aggressively when we have very little money left and our amount of money drops below 1 dollar, we will not be able to reroll for a while and our expected number of wins will drop. Intuitively what we need to determine is, if our money gets low, to say M dollars, how much will we fall behind our expected number of won games before our amount of money improves to M+1/G?

We define the "surplus" for a set of games as the number of games won in the set minus Q multiplied by the number of games played in the set. We choose our strategy at M money to maximize the expected surplus of the set of games that occur until we reach M+1/G money. (If M is equal to R, then our amount of money will not increase, so we use the set of games that lasts until we have M money again.) Let this value be A[M].

If M<1 then we cannot reroll, so

since the set will consist of exactly one game, and we get a random champion.

For 1 <= M < R, suppose that our strategy requires that we will reroll the worst K_M heroes. Sort the values Pi. Then:

Similarly, for M=R:

We can compute the optimal values for A by optimizing for K_M separately for each M from 0 to R. Note that A[M]>=-1 for all M, since the strategy of never rerolling has a surplus no worse than -1, so the optimal strategy must be at least as good as that.

If A[R]>=0, we decide that Q is too high. If A[R]<0, we decide that Q is too low. We now show that this works.

We start with R money. We will return to R money multiple times, with expected surplus A[R] each time, until finally we reach 10^100 games and stop. Consider instead the scenario where once we reach 10^100 games, we keep playing until we reach R money again. The expected surplus in this scenario is T*A[R], where T>=1 is the expected number of times we return to R money. Now, let X be the expected surplus of the games after the 10^100th. From the definition of A, X=A[M] + A[M+1/G] + ... + A[R-1/G], where M is the amount of money we have after game 10^100.

Assume that A[M]<0 for M<R (if not, we can play as if R was equal to the lowest M such that A[M] >= 0 and the proof will work). Then -RG <= X < 0.

Now the surplus for the first 10^100 games is Y=T*A[R] - X. Therefore T*A[R] < Y <= T*A[R] + RG.

RG is insignificant over 10^100 games. So the fraction of expected wins for this strategy is Q + T*A[R] * 10^-100. So we assert that if A[R] >= 0, then we can achieve a strategy which wins a fraction of at least Q, and otherwise we cannot. This will allow our binary search to converge to a sufficiently accurate answer.

Below is a sample implementation in Python 3. To avoid fractional dollars, we introduce a smaller unit (e.g., a coin) C for the money where G coins is equal to one dollar.

# Can you win at least X fraction of the time?
def CanWin(X):
  A = []
  last_G_values = 0

  # C < G, not enough coins for a reroll.
  for C in range(0, G):
    A.append(avg_win_prob_top[N] - X)
    last_G_values += A[C]

  # C >= G, enough coins for a reroll.
  for C in range(G, R * G + 1):
    A.append(-1e100)
    for K in range(1, N + 1):
      p = (N - K) / N  # Probability of rerolling.
      p_reroll = p / (1 - p) * last_G_values
      p_not_reroll = avg_win_prob_top[K] - X
      A[C] = max(A[C], p_reroll + p_not_reroll)

    if A[C] >= 0: return True
    last_G_values += A[C] - A[C - G]

  return False

for tc in range(int(input())):
  [N, R, G] = map(int, input().split())
  win_prob = map(float, input().split())
  win_prob = sorted(win_prob, reverse=True)

  avg_win_prob_top = [0]
  for topK in range(1, N + 1):
    avg_win_prob_top.append(sum(win_prob[0:topK]) / topK)

  lo = 0
  hi = 1
  for i in range(60):
    mid = (lo + hi) / 2
    if CanWin(mid):
      lo = mid
    else:
      hi = mid

  print("Case #%d: %.15f" % (tc + 1, lo))
An alternative solution is to evaluate strategies by modelling the game as a Markov chain and computing its stationary distribution, then iteratively improving the strategy. It is quite difficult to do this with sufficient accuracy.

Qualification Round 2015 - Code Jam 2015

Standing Ovation (7pts, 10pts)

What kind of shyness level friends should we invite?
Since we can invite any friend with any shyness level, the problem seems really complicated. However, if you think about it you will realize that inviting friends with shyness level 0 is optimal and makes this problem simpler. Friends with shyness level 0 are always better than those with other shyness levels because they always stand up and clap, therefore helping all other audience members who have shyness level greater than 0. This kind of choice is greedy but makes sense as for any scenario: if you can invite friends with shyness level greater than 0 and solve the problem, then you can always replace them with friends whose shyness level is 0 and still solve the problem.

How many friends should we invite?
For audience members with shyness level k, to meet their needs, we must have at least k people who already stood up before them. These include both the audience members whose shyness level is less than k and the friends we invited (with shyness level 0). Assuming that there are t audience members who stood up already, the number of friends we need to invite is max(k - t, 0). This provides us with an algorithm. Let’s say we are given an array of audience members for which the kth entry contains the total audience members of shyness level k. Now, for each shyness level, we compute the number of friends we need to invite to get the audience members of that level to stand up and clap, and we record the maximum value out of all such values. This max value is the minimum number of friends we need to invite to let every audience member stand up and clap for the prima donna.

Below is a sample implementation in Python:

for tc in range(input()):
  smax, string = raw_input().split()
  t = 0
  min_invite = 0
  for k in range(int(smax) + 1):
    min_invite = max(min_invite, k - t)
    t += int(string[k])
  print "Case #%d: %d" % (tc + 1, min_invite)
Because the input of audience members is already given in increasing order, we only need to walk through this array once and compute the result. Since min_invite is initially 0, max(k - t, 0) is redundant (i.e., k - t is sufficient). The overall time complexity is O(Smax).

Qualification Round 2015 - Code Jam 2015

Infinite House of Pancakes (9pts, 12pts)

As the head server, you have two choices for each minute:

eat: you do nothing and let every diner with a non-empty plate eat one pancake from his or her plate.
move: you ask for the diners' attention for a special minute, choose a diner with a non-empty plate, and move some number of pancakes from that diner's plate to another diner's (empty or non-empty) plate.
Imagine that you, as the head server, have planned a strategy to move the pancakes. Let's call it strategy A. In this strategy, you pick a successive pair of minutes t and t+1 such that at minute t you choose eat and at minute t+1 you choose move. As a natural philosopher, you start to question your decision, "What if I swap my plan at minute t with my plan at minute t+1, i.e. move pancakes at minute t (instead of t+1) and eat at minute t+1 (instead of t)?". Out of curiosity, you swap your plan at minute t and t+1 and calculate the breakfast end time. Surprisingly, breakfast does not take longer. After that, you start to find several other successive pairs of minutes with the same property and do other swaps. You find that breakfast never takes longer than if you strictly follow strategy A. Surprisingly, on certain swaps, breakfast even ends earlier than strategy A!

Of course you become more curious! You start to observe what happened when you did those swaps. If the plan at minute t is eat and the plan at minute t+1 is move, you notice that all pancakes that were eaten at time t before you did the swap were also eaten at time t+1 after swapping. Hence, the number of pancakes that were eaten after time t+1 does not decrease, which implies that breakfast will not take longer. After you swap the plan at t with t+1, you notice that several plates that were empty at time t might become non-empty at time t+1 after swapping (and hence, the number of pancakes that were eaten after time t+1 might increase by one).

"Whoa! That means if I swap <eat, move> pairs to <move, eat>, the amount of pancakes that were eaten at time t before the swap is either less than or equal to the amount of pancakes that were eaten at time t+1 after the swap. Hence, such swaps are always good (i.e. it will not make the breakfast time longer but might make the breakfast time shorter)!"
Excited with your finding, you grab a whiteboard and start to swap every successive pairs of <eat, move> to be <move, eat>, until none of the <eat, move> pairs are left. You look at your revised strategy.

"Move, move, move, eat, eat, eat, eat, eat" you read out loudly. "Of course! The only strategy without a <eat, move> pair is the strategy in which we do all the moves at the beginning, and always eat after that. If we look at this carefully, swapping pairs of elements that are in the wrong order (in this case eat then move) until none are left will actually sort the strategy. This is basically doing bubble sort on strategy A!"
Knowing that you are going to get a bonus from your boss for ending breakfast early, you quickly present this solution to your boss. Your boss is not convinced. He replies without showing much interest, "Well, I understand that moving pancakes at the beginning will always lead to an optimal solution. But what is more important is that for every move, do you know how you are going to move the pancakes?" You quickly answer, "At least I know that we can always move pancakes to empty plates! There are infinitely many of them, and we do not have any reason to let diners with non-empty plates to eat more than is needed. We cannot even wait for them to finish their existing pancakes!"

Your boss stops reading emails and starts to pay attention to your explanation. "How will you pick the plates to move pancakes from?" he asks. "Perhaps from the customer with the most pancakes on their plate? They need the most help to finish their pancakes," you reply.

"You mean we can end breakfast earlier if we keep moving several pancakes from the plates with most pancakes to empty plates, and stop moving after several minutes?" he enthusiastically says. "Congratulations! You get a big bonus and extra time off!"
You are extremely happy. You gather all the kitchen staff and explain the new strategy... of course after bragging a bit on social media! A new intern staff raises her hand and asks, "When should we stop moving pancakes? Also, how many pancakes should we move every time?"

The whole kitchen goes silent. Everyone is looking at you expecting answers. Breakfast will start in half an hour. "Quick, think of something useful," you tell yourself. Your mind feels like it is exploding!

"Erm, honestly I don't know. Let's try to simulate breakfast to get some insights. Please bring me two plates of pancakes with 15 and 17 pancakes respectively. Ah, don't forget to bring me several empty plates too," you order one of your staff.

You start to simulate your new strategy multiple times with different amount of pancakes to be moved and stop moving at different times. Suddenly, your new intern says to you, "Look! You take 10 pancakes from the plate 1 and put it on an empty plate 3. Later, you picked 3 pancakes from plate 3 and moved it to another empty plate 4. Instead of doing that, why didn't you move 7 pancakes from first plate to an empty plate 3, and then move 3 pancakes from first plate to another empty plate 4? That way, you can always move pancakes from plates that are initially non-empty."

"I see," you say, "That means we can take half the pancakes from the plate with largest amount of pancakes, then move it to an empty plate, then pick another plate with the largest amount of pancakes and do the same thing multiple times..."
She replies quickly, "Your strategy is bad. Imagine that you had a plate of 9 pancakes. If we use your strategy, the best we can do is to split the plate into plates with 4 and 5 pancakes, and let the diners eat them. It costs us 6 minutes to end breakfast. But if you split the plate into three plates with 3 pancakes each (using 2 moves), we can finish breakfast in only 5 minutes!"
"I have a suggestion," she continues, "If you expect the customer to finish their pancakes x minutes after you stop moving pancakes, any strategy that satisfies this will be equivalent to repeatedly moving at most x pancakes from every initially non-empty plate to an empty plate until the number of pancakes on the initially non-empty plate becomes no more than x."

"And if you always move exactly x pancakes, for a plate with Pi pancakes you need only M(Pi)=ceil(Pi/x)-1 moves until the number of pancakes in the plate is no more than x. You cannot do less than that! Overall, we are going to move sum of M(Pi) times, where Pi is the number of pancakes on plate i. That is the minimum amount of moves that we can do for the given x! We can try all possible values of x to find the optimal x that has earliest breakfast end time!" Everyone gives her a standing ovation.

"Let's summarize our strategy. First of all, we fix a number x to be the number of minutes that we expect breakfast to end in after we stop moving pancakes. After that, we pick a plate with more than x pancakes, take x pancakes from that plate and move the pancakes to an empty plate. We keep doing that until all plates has at most x pancakes, then we let the customers eat their pancakes and breakfast ends the earliest for that x value! If we move sum of M(Pi) times, in total, the breakfast ends exactly after x + sum of M(Pi) minutes," you say. "And we can try all possible values of x, since the amount of pancakes cannot be more than 1000. The complexity of the algorithm is O(D*M), where D is the number of diners and M is the maximum number of pancakes. This is fast enough to solve our problem. We can use a spreadsheet to..."
"Certainly, but I have prepared for this," says your intern. She opens her laptop and types some really short functions, of course in C++. "Why? Because C++ is cool!"

// Get the minimum possible breakfast end time, given 
// P[i] is the number of pancakes of diner i initially.
int f(const vector<int>& P) {
  const int max_pancakes = *max_element(P.begin(), P.end());
  int ret = max_pancakes;
  for (int x = 1; x < max_pancakes; ++x) {
    int total_moves = 0;
    for (const int Pi : P) {
      // (Pi - 1) / x is equivalent to M(Pi), 
      // which is ceil(Pi / x) - 1
      total_moves += (Pi - 1) / x;
    }
    ret = min(ret, total_moves + x);
  }
  return ret;
}
"Whooa! Run it, run it!" you exclaim.

"Hold on, hold on. Although the algorithm above is fast enough to solve our problem, I have an even faster algorithm. Notice that the list of ceil(a/1), ceil(a/2), ... only changes values at most 2*sqrt(a) times. For example, if a=10, the list is: 10, 5, 3, 3, 2, 2, 2, 2, 2, 1, 1, .... That list only changes value 4 times which is less than 2 * sqrt(10)! Therefore, we can precompute when the list changes value for every diner in only O(D*sqrt(M)). We can keep track these value changes in a table. For example, if Pi=10, we can have a table Ti: 10, -5, -2, 0, -1, 0, 0, 0, 0, -1, 0, .... Notice that the prefix sum of this table is actually: 10, 5, 3, 3, 2, 2, 2, .... More importantly, this table is sparse, i.e. it has only O(sqrt(M)) non-zeroes. If we do vector addition on all Ti, we can get a table where every entry at index x of the prefix sum contains sum of ceil(Pi/x). Then, we can calculate sum of ceil(Pi/x)-1 in the code above by subtracting the xth index of the prefix sum with the number of diners. Hence, only another O(M) pass is needed to calculate candidate answers, which gives us O(D*sqrt(M) + M) running time. A much faster solution!"

"One more thing, we cannot do binary search or ternary search, at least in trivial ways, on a function that maps x to its minimum breakfast end time as the function can have multiple minimas. e.g. for 2 diners with 9 pancakes each, the function forms the following mappings: (1,17), (2,10), (3,7), (4,8), (5,7), (6,8), (7,9), (8,10), (9,9)."
"I don't need that! The previous algorithm is fast enough. Please run it," you say impatiently.

"Sigh. I won't tell you how to code the faster solution then. The answer is..." Everyone gasps while the program blinks. "... 42."

Qualification Round 2015 - Code Jam 2015

Dijkstra (11pts, 17pts)

We can solve this problem with the help of the following two insights. First, the product of the whole string must equal -1 (i ⨉ j ⨉ k). Second, we only need a small number of copies of the input string to check for a solution. One way to do this is by finding the shortest prefix of the whole string that reduces to i and the shortest subsequent prefix that reduces to j. If we find such two substrings, there is a solution because the rest of the string reduces to k thanks to the associative property. The rest of this editorial explains the two insights and provides a sample implementation.

No solution if the whole string cannot be reduced to -1
Suppose the string S is the whole concatenated string formed by repeating the given input string X times. If we can break the string S into three non-empty substrings A, B, C where A + B + C = S such that A reduces to i, B reduces to j, and C reduces to k, then the string S reduces to string "ijk", which then reduces to -1. Therefore, if the string S cannot be reduced to -1, then there is no solution. Reducing the string S can be done by simply multiplying all the characters in S into one value.

If the string S can be reduced to -1, it doesn't mean that there is a solution for S. There are many strings (e.g., "ii", "jj", etc.) that reduces to -1 but do not form a concatenation of three substrings that reduce to i, j, and k, respectively.

The first two substrings must reduce to i and j, respectively
From now on, we only consider whole strings that reduce to -1. To determine whether a string S can be broken into three non-empty substrings A, B, C where each reduces to i, j, k respectively, we only need to find the first two substrings. The last substring is guaranteed to reduce to k since the whole string reduces to -1.

(There are other alternatives. For example, we can find the shortest prefix and the shortest suffix that reduce to i and k, respectively. If the prefix does not overlap with the suffix, then we can reduce the rest to j. Care must be taken as the multiplication operator is not commutative.
Exercise: Can the prefix-suffix pair actually overlap while the whole string can still be reduced to ‘ijk’? Answer: No.)

To find the first substring, we start from the first character of the string S and start multiplying it with the next characters and so on until we get the value i. Afterward, we repeat the procedure from the current position until we get the value j. The rest of the string is guaranteed to be non-empty and reduce to k. The complexity of finding the first and the second substrings is O(L * X), since we need to scan the whole string S of length L * X.

With these insights, we can solve the small input in O(L * X). Below is a sample implementation in Python:

M = [[ 0,  0,  0,  0,  0 ],
     [ 0,  1,  2,  3,  4 ],
     [ 0,  2, -1,  4, -3 ],
     [ 0,  3, -4, -1,  2 ],
     [ 0,  4,  3, -2, -1 ]]

def mul(a, b):
  sign = 1 if a * b > 0 else -1
  return sign * M[abs(a)][abs(b)]

def multiply_all(S, L, X):
  value = 1
  for i in range(X):
    for j in range(L):
      value = mul(value, S[j])
  return value

def construct_first_two(S, L, X):
  i_value = 1
  j_value = 1
  for i in range(X):
    for j in range(L):
      if i_value != 2:
        i_value = mul(i_value, S[j])
      elif j_value != 3:
        j_value = mul(j_value, S[j])
  return i_value == 2 and j_value == 3

for tc in range(input()):
  L, X = map(int, raw_input().split())
  # maps 'i' => 2, 'j' => 3, 'k' => 4
  S = [(ord(v) - ord('i') + 2) for v in raw_input()]
  ok1 = multiply_all(S, L, X) == -1
  ok2 = construct_first_two(S, L, X)
  print "Case #%d: %s" % (tc + 1,
    "YES" if ok1 and ok2 else "NO")
The multiplication matrix M is defined as a 5 x 5 matrix where the first column and the first row are not used (for value 0). The second row and the second column is for an identity value 1. The third, fourth and fifth (rows and columns) represent i, j, and k, respectively, identical to the quaternion multiplication matrix.

Optimizations for the large input
The maximum whole string length is 10^16 which is too large for an implementation of the algorithm above to finish within the time limit when it is executed in a today's computer. We need to optimize both of these functions: multiply_all() and construct_first_two().

Optimizing the multiply_all() method
Observe that the whole string is formed by repeating the input string (of length L) X times, giving the complexity of O(L * X). Since the multiplication operator is associative, we can reduce the input string into a single value before multiplying this value to itself X - 1 times. Thus, we can rewrite the multiply_all() method as follows:

def multiply_all(S, L, X):
  value = 1
  for i in range(L):
    value = mul(value, S[i])
  return power(value, X) # computes value^X
To quickly multiply the value with itself X - 1 times (that is, computing value^X), we can use the exponentiation by squaring technique which runs in O(log(X)):

def power(a, n):
  if n == 1: return a
  if n % 2 == 0: return power(mul(a, a), n // 2)
  return mul(a, power(mul(a, a), (n - 1) // 2))
With this optimizations, the multiply_all() complexity is now O(L + log(X)). Since L is at most 10000 and X is at most 10^12, the number of multiplication operations is at most 10040.

We can improve it further to O(L). Observe that the multiplication values (to itself) will always repeat to the original value after 4 consecutive multiplications, thus we only need to do at most X mod 4 multiplications to compute value^X:

def power(a, n):
  value = 1
  for i in range(n % 4):
    value = mul(value, a)
  return value
Optimizing the call to construct_first_two()
To find the prefix, we start with an identity value 1 and multiply it with the value of the first position of the input string, and so on until we get a value i. Supposing X is sufficiently large, if we reach the end of the input string and haven't obtained the value i, we repeat this procedure for the next copy of the input string. At this point, the current value may be 1, j, k, -1, -i, -j, or -k. If the current value is 1, we may as well stop here and declare no solution because continuing the multiplication will result in the same value 1 again. However, if the current value is not 1, we can continue the procedure.

We know from the previous section that multiplying a value to itself will repeat to its original value every 4 consecutive multiplication. Thus, if we don't encounter the value i after executing the procedure for 4 copies of the input string, it is impossible to construct the desired prefix. If we do encounter value i, then we can proceed to find the second substring where the same insight applies.

Thus, we can safely limit the call to construct_first_two() from X repeats:

  ok2 = construct_first_two(S, L, X)
to min(8, X) repeats:

  ok2 = construct_first_two(S, L, min(8, X))
This reduces the complexity of the construct_first_two() method from O(L * X) to O(L).

Therefore, the complexity of the overall algorithm is now O(L) per test case.

Qualification Round 2015 - Code Jam 2015

Ominous Omino (8pts, 26pts)

Introduction
There are various aspects to solving this problem. First off, there is one key observation about X-ominoes where X>=7 which we will cover shortly. Beyond that, we describe a brute force solution where we generate all possible X-ominoes, simulate Richard picking one of the X-ominoes, then simulate Gabriel placing that particular X-omino in all possible locations on the grid to determine if a winning configuration can be found. We also present an alternative solution that involves an exhaustive case-by-case analysis of input to determine the winner.

Trivial case where X>=7
There is a key observation for X-ominoes where X>=7. We point out that in such cases, it is impossible for Gabriel to win. How is that?

###  ####
#.#  #.##
##.  ###.
When X>=7, Richard can always choose a X-omino which has a hole in the middle as shown in the figure above (a 7-omino in the left, and a 10-omino in the right). This means that it is impossible for Gabriel to win using that X-omino at least once, as it is impossible to fill the hole in the middle with another X-omino.

Therefore in cases where X>=7, Richard will always win.

Available cells is a multiple of X
Another insight is the following: if R*C is not a multiple of X, then it is impossible for Gabriel to win. Why? The X-omino occupies X cells exactly, so if the total number of cells is not a multiple of X, it is impossible to cover all the cells using only X-ominoes. Therefore, in such cases, Richard will always win.

In fact, we can extend this principle. Let's imagine that we have already placed one X-omino in the RxC grid, and it results in the RxC grid being separated into two (or more) edge-connected regions. In the figure below, we use a 4-omino in a 2x6.

.##...
##....
Since the sizes of the connected component (connected components can be found by using flood fill) of the blank areas represented as '.' are 1 and 7, and 1 is too small to fit while 7 is not a multiple of X, then it is not possible for Gabriel to win that particular configuration. Therefore we can say that if we ever arrive in a grid configuration where any of the remaining connected components size is not a multiple of X then we can say that Richard will win that particular configuration. If a connected blank area of size M is a multiple of X, it can be guaranteed that there is a way to place M/X X-ominoes to fill in the blank area. It can be proven by using exhaustive case-by-case analysis which is described in the next section.

Now, armed with the above knowledge, we proceed with describing a brute force solution. First, we describe a way to generate all possible X-ominoes. Then we describe the general brute force strategy to test if Richard can pick a X-omino that will guarantee him a win, and if he is unable to do so then Gabriel will win.

Generating all possible X-ominoes
To generate all possible X-ominoes, we describe a recursive strategy. Let's start with a 1-omino. Well, for a 1-omino there is trivially only 1 possibility which is the following:

#
Similarly, we can generate the configurations for a 2-omino as follows:

#   ##
#
How can we do this? We can build the X-ominoes recursively. We start with an empty board (let's say of size 20x20) then place a '#' in middle. Then we can recursively add a '#' adjacent to any of the existing '#' we have already placed, and stop when we have placed X '#' to create a X-omino. But you might have noticed that this recursive process will likely create a lot of duplicate X-omino configurations, e.g. in the 2-omino case, after placing the '#' in the middle, there are 4 possible placements of an adjacent '#' (labeled as 1-4 in the figure below).

.1.
4#2
.3.
This means that this recursive procedure will generate four 2-ominoes! But we know that there are only two 2-ominoes, as "4#" and "#2" are equivalent, and likewise for "1#" and "3#". Therefore, we can come up with a way to remove duplicate X-omino configurations if needed, which we leave as an exercise.

We can precompute all the X-ominoes for 1<=X<=6. Also note that when generating X-ominoes this way, we will generate all X-ominoes under reflection and rotations which we show with an example below:

#.  .#  .##  ##.
##  ##  ##.  .##
.#  #.
For our solution, it is fine to generate all such X-ominoes. Note that it is also feasible to generate all possible X-ominoes for X<=6 by hand. We cover this in the "Alternative Solution" section below.

Brute force strategy
Now, after we have generated all possible X-ominoes, we proceed with describing the brute force strategy. As mentioned earlier, if X>=7, we report that Richard will win, and also if R*C is not a multiple of X, we report that Richard will win.

In the brute force strategy, we simulate Richard picking each of the X-ominoes one-by-one, then simulate if it is possible for Gabriel to win with that particular X-omino. If we can find any X-omino that Richard can pick which results in Gabriel being unable to win, then we report that Richard can win. But if Gabriel wins for all X-ominoes that Richard picks, then we report Gabriel as the winner.

Simulating Richard picking a X-omino is straightforward. Richard can pick each of the X-omino one by one. The trickier part is to check if Gabriel can win, we now describe a strategy to perform this check.

We take the RxC grid and we have the X-omino that Richard has required that Gabriel use at least once. We can brute force the placement of this X-omino in the RxC grid (NOTE: We have to try the CxR grid too, we elaborate on the reasons to take the CxR grid in a subsequent paragraph). If it is impossible to place the X-omino (e.g. if the width of the X-omino is bigger than C) in either the RxC grid or the CxR grid then we trivially say that Richard wins for that particular X-omino. If it is possible to place the X-omino in a particular location, we still need to check whether it is possible for Gabriel to win. Let’s see some examples. Suppose that the X-omino Richard chose is the following.

.##
##.
and we are given a 2x4 grid. In that case, we can place the X-omino in the following two configurations.

.##.
##..
or

..##
.##.
We notice that in both configurations, the X-omino has divided the grid into two connected components of '.' of size 1 and 3. Well, we had mentioned earlier in the section "Available cells is a multiple of X" that such configurations imply Gabriel will always lose.

In fact, after placing a X-omino if it results in connected components of size M where all such M is a multiple of X, then we can say that Gabriel wins with that X-omino placed at that particular location.

As another example, if we use the following 4-omino:

##
##
and we are given a 2x6 grid, and if we placed the 4-omino as follows:

.##...
.##...
then it is not possible for Gabriel to win (there are two connected regions of size 2 and 6, and 2 and 6 are not multiples of 4). But if we placed the 4-omino as follows:

##....
##....
or

..##..
..##..
Then in both configurations we say that Gabriel wins (in the first case, there is one connected component of size 8, and 8 is multiple of 4; in the second case, there are two connected components of size 4 each, and 4 is a multiple of 4).

As mentioned in an earlier paragraph, we try the CxR grid too. The reason is because we have to check for a 90 degree rotation of the X-omino that Richard selected. Remember, Gabriel can rotate or reflect the X-omino when initially placing it on the grid. Instead of rotating the X-omino, we can instead just rotate the grid! Therefore it suffices to check if we can fill a RxC grid, or a CxR grid with that X-omino. Note that we don't need to consider all possible reflections and rotations of the X-omino because because in grid space, things are symmetric therefore it suffices to check for both the RxC and CxR grid.

Therefore to generalize it, after placing the selected X-omino at a particular place, we can check the sizes of the edge-connected '.' components, and if all such components have size M is a multiple of X, then it means that we can always fill the M space with M/X X-ominoes. In such a case, we say that that particular grid configuration is one for which Gabriel can win. An explanation of this winning condition for our brute force solution is based off the conclusion that we arrive at from the "Alternative solution" presented below.

Alternative solution
The alternative solution involves careful analysis of various cases. Let S = min(R, C) and L = max(R, C) so that S<=L. Suppose that the grid dimensions are SxL (if the dimensions are LxS, the grid can be rotated without affecting the win conditions).

Richard can force a win if any of the following conditions hold; otherwise Gabriel will win.

X does not divide S*L,
X=3 and S=1,
X=4 and S<=2,
X =5 and either (i) S<=2 or (ii) (S, L) = (3, 5),
X=6 and S<=3,
X>=7.
We have already explained (1) and (6) in the previous paragraphs. For (2), (3), (4i), (4ii) and (5), Richard can choose the following pieces and always guarantee a win:

(2)

#.
##
It is impossible for Gabriel to fit the above piece in a grid with S=1. Similar explanation follows for the following two cases.

(3)

###
.#.
For the above piece, when the grid has S=2 notice that it will divide the '.' cells into two connected regions and that it is impossible for these regions to have a size which is a multiple of 4. It is impossible for Gabriel to fit the above piece in a grid with S=1.

(4i)

#..
##.
.##
Similar to the above cases, it is impossible for Gabriel to fit the above piece in a grid with S<=2.

(4ii) For this case, Richard can use the same piece as used in (4i). By trying all possibilities, one can see that it is impossible for Gabriel to win with X=5 and a 3x5 grid.

#....  .#...  ..#..
##...  .##..  ..##.
.##..  ..##.  ...##
(5)

.#..
####
.#..
A similar explanation follows for the above piece as the explanation for (3).

For all combinations of X, S and L not satisfied by the above condition, Gabriel will win. We provide here an explanation for the X=6 case, and leave the other cases as an exercise. Let's consider the 4x6 grid, which is the smallest grid for which S>3 and S*L is a multiple of X. We show below Gabriel's strategy for filling the 4x6 grid, and then generalize for cases where the grid is bigger than 4x6.

First off, we point out that for a 6-omino, there are only <a href="http://en.wikipedia.org/wiki/Hexomino" >35 choices. In fact, listed below are the number of choices for each X-omino:

1-omino, 1 choice,
2-omino, 1 choice,
3-omino, 2 choices,
4-omino, 5 choices,
5-omino, 12 choices.
Since the 6-omino only has 35 choices, we list out below a case-by-case analysis of the 35 choices that Richard can make, and Gabriel's strategy for filling up the rest of the cells to guarantee a win for himself. '#' denotes the original piece that Richard chooses while 'a', 'b', and 'c' are 6-ominoes that Gabriel chooses.

######   a#####   a#####   a#####   ####cc   ##abbb   ##abbb
aaaaaa   aaaaa#   aaaa#c   abb#cc   aab##c   ##abbb   #aabbb
bbbbbb   bbbbbb   abbbbc   aabbcc   aabbcc   #aaccc   ##accc
cccccc   cccccc   bbcccc   aabbcc   aabbbc   #aaccc   #aaccc

##abbb   #aabbb   ###bbb   #aaaaa   #aaaaa   aaa#bb   aa#bbb
#aabbb   ##abbb   #aabbb   ###cca   ####ca   a####b   a####b
#aaccc   ##accc   #aaccc   #ccccb   #ccccc   aacc#b   aaac#b
##accc   #aaccc   #aaccc   #bbbbb   bbbbbb   ccccbb   cccccb

a#bbbb   aa#bbb   aa#bbb   a###bb   ###bbb   ###bbb   ###bbb
a####b   a####b   a####b   aaa##b   #a##bb   aa###b   a###bb
aacc#b   aaa#bb   aa#ccb   acc#bb   aaaccb   aacccb   aaaccb
aacccc   cccccc   accccb   accccb   aacccc   aacccb   aacccc

###aaa   aaa#bb   aaa###   aa#bbb   ###bbb   ###bbb   aaaaaa
###aaa   a###bb   aaab##   a###bb   aa#bbb   aa##bb   ###bbb
cccbbb   aac##b   bbbb#c   aaa##b   aa##cc   aaa#cb   #c#bbb
cccbbb   cccccb   bccccc   cccccc   aacccc   accccc   #ccccc

aaaaab   aabbbb   ##bbbb   aaaaaa   aaaaab   aaaabb   ##aaaa
#a#cbb   a#b#bc   a###bb   #bbbbc   a#bbbb   aa#cbb   c##aab
###cbb   a###cc   aaa#cc   ##bbcc   ###ccb   ###cbb   cc##bb
#ccccb   aa#ccc   aacccc   ###ccc   ##cccc   ##cccc   cccbbb

Whew, that was a lot of cases! Clearly, Gabriel will always win with a 4x6 grid and any 6-omino that Richard chooses.

We now explore the case when the grid is bigger for X=6. Let's pick the 6x8 grid. To win in such cases, Gabriel can use the following strategy. Gabriel can use the top-left corner of the board to place the piece that Richard chooses so that he can complete a 4x6 portion similar to the manner described in the 35 cases above. Then Gabriel can label the remaining cells using a Hamiltonian path (with cells as vertices, and adjacent cells as neighbors). 'Z' denotes the 4x6 portion on the top-left corner of the grid, and the rest is filled with a sequence of 'a'-'x' which denotes the Hamiltonian path. In this case, Gabriel can start the path from the top-right corner and 'snake' back and forth. In general, depending on the parity of the number of rows of the 'Z' region (let's call the number of these rows T), Gabriel can start from the top-right (when T is even), or from the cell adjacent to the top-right corner of the Z-region (when T is odd) then 'snake' back and forth to fill the rest of the cells.

ZZZZZZba
ZZZZZZcd
ZZZZZZfe
ZZZZZZgh
ponmlkji
qrstuvwx
Observe that the cells 'a'-'x' forms a chain. We can simply chop off this chain into sizes of 6 each (i.e. a 6-omino!). This way, Gabriel can win for X=6 and grids that are bigger than 4x6.

Similar to above, we can list out cases for the other X-ominoes, which we leave as an exercise.

Here is a sample implementation in Python:

def richard_wins(X, R, C):
  S = min(R, C)
  L = max(R, C)
  if (S * L) % X != 0: return True
  if X == 3 and S == 1: return True
  if X == 4 and S <= 2: return True
  if X == 5 and (S <= 2 or (S, L) == (3, 5)): return True
  if X == 6 and S <= 3: return True
  if X >= 7: return True
  return False

for tc in range(input()):
  X, R, C = map(int, raw_input().split())
  print "Case #%d: %s" % (tc + 1,
    "RICHARD" if richard_wins(X, R, C) else "GABRIEL")

Round 1A 2015 - Code Jam 2015

Mushroom Monster (7pts, 8pts)

Each method can be solved independently because they answer different questions. With the first method, Kaylin can eat any number of mushroom pieces at any time. Since the only way mushrooms can leave the plate is by being eaten, whenever we see a decrease in the number of mushrooms (in an interval) it must be because they were eaten by Kaylin. The minimum number of mushrooms Kaylin could have eaten using this method is the sum of the observable decreases for each interval. Since we only care about how many mushrooms Kaylin ate, we do not need to calculate how many mushrooms Bartholomew added.

With the second method, Kaylin always eats mushrooms at a constant rate whenever there are mushrooms on her plate. For each interval, we can observe Kaylin’s eat-rate (i.e., the decrease of the number of mushrooms for that time interval). Since we want to find the minimum number of mushrooms Kaylin could have eaten, we should find Kaylin’s minimum eat-rate. Since the eat-rate must be constant for each interval from the beginning until the end, only the highest observable eat-rate makes sense. It may appear that Kaylin eats fewer mushrooms than her eat-rate in some intervals, either because her plate becomes empty during the interval and she stops eating, or because Bartholomew added more mushrooms during the interval.

The number of mushrooms Kaylin could have eaten using the second method is the sum of min(M[i], max_rate) for all intervals i, where M[i] is the number of mushrooms at the beginning of interval i and max_rate is the highest observable eat-rate. That is, if at the beginning of the interval the number of mushrooms is larger than the maximum eat-rate, Kaylin can only eat max_rate mushrooms, otherwise Kaylin can only eat M[i] mushrooms and the plate becomes empty until the end of that interval. Note that we don’t care about the number of mushrooms at the end of an interval. Since we want to minimize the eat-rate, we should assume that Bartholomew puts in mushrooms instantaneously at the end of the interval to maximize Kaylin’s idle time.

Below is a sample implementation in Python:

def first_method(M, N):
  min_eat = 0
  for i in range(1, N):
    min_eat += max(0, M[i - 1] - M[i])
  return min_eat

def second_method(M, N):
  max_rate = 0
  for i in range(1, N):
    max_rate = max(max_rate, M[i - 1] - M[i])

  min_eat = 0
  # exclude the last mushroom
  for i in range(0, N - 1):
    min_eat += min(M[i], max_rate)
  return min_eat

for tc in range(int(input())):
  N = int(input())
  M = map(int, raw_input().split())
  print "Case #%d: %d %d" % (tc + 1,
    first_method(M, N), second_method(M, N))

Round 1A 2015 - Code Jam 2015

Haircut (11pts, 22pts)

We describe a few algorithms that solve the problem, of increasing efficiency.
Direct simulation
Iterate over each minute, from the shop opening at time T = 0 until the last customer is served. At time T assign new customers to all available barbers (a barber is available if T is multiple of his M). Finally, report the barber who serves you.

public int naiveGetBarberNumber(int N) {
  int customer = 1;
  for (int T = 0; ; T++) {
    for (int barber = 0; barber < B; barber++) {
      if (T % M[barber] == 0) {
        if (customer == N) return barber;
        customer++;
      }
    }
  }
}
This algorithm has time complexity O(N * max(M) * B), so it will be very slow, even for a small input, since N can be as large as 1,000,000,000.

Exploit periodicity
Consider the case where there are two barbers B1 and B2, who take 2 and 3 minutes respectively to cut a customer's hair.

Time	Events
T = 0  	Both barbers are ready to serve customers.
B1 serves customer #1 and B2 serves customer #2
T = 2	B1 serves customer #3
T = 3	B2 serves customer #4
T = 4	B1 serves customer #5
T = 6	Both barbers are ready to serve customers.
B1 serves customer #6 and B2 serves customer #7
At T = 6, both barbers have become available, just as they were at T = 0. So we will see the same pattern of availability for the next 6 minutes as we did for the first 6 minutes — at T = 2 + 6, B1 will serve customer #(3+5); at T = 3 + 6, B2 will serve customer #(4+5), and so on until T = 6 + 6, at which point the process starts repeating itself again.

What's so special about 6? It's the least common multiple (LCM) of M1 = 2 and M2 = 3. At time T = LCM(M1, M2) = 6 each barber is available, because T % M = 0 for every barber. We can compute the LCM of all Ms as follows: LCM(M1, M2, M3, ...) = LCM(M1, LCM(M2, M3, M4, …)) and the least common multiple of two numbers is LCM(A, B) = A * B / GCD(A, B).

We can exploit the fact that the whole process is periodic and only simulate for a small number of customers. For example, say M1 = 2, M2 = 3, and you are N = 14th in line. We already know that we have a period of LCM(2, 3) = 6. During one phase B1 serves LCM(2, 3) / M1 = 3 customers and B2 serves LCM(2, 3) / M2 = 2 customers, i.e. in total 5 customers per phase are served in the shop. Since N = 14, you will be served in the 3rd phase. When the third phase starts you are going to be 4th in line, because a total of 10 customers have been served in the previous two phases. Finally, to figure out your barber's number, we can naively simulate your phase, similar to what we did in the first solution.

Since we are only simulating a single phase, we only really need to simulate at most LCM(M1, M2, M3, …) minutes. So our improved algorithm has time complexity O(B * LCM(M1, M2, M3, …)). Note that the LCM of all Ms is not going to exceed max(M)^B, i.e. LCM of all Ms is less than 25^5 for the small input.

public int slowGetBarberNumber(int N) {
  int period = M[0];
  for (int i = 1; i < B; i++)
    period = period / gcd(period, M[i]) * M[i];
  int customers_per_phase = 0;
  for (int i = 0; i < B; i++)
    customers_per_phase += period / M[i];
  int N_in_my_phase = N % customers_per_phase;
  return naiveGetBarberNumber(N_in_my_phase != 0
    ? N_in_my_phase : customers_per_phase);
}
For the large input, B and Ms can be as high as 10,000, so the LCM of them can get very large, and this approach will not work.

Binary Search
For a given time T, it is easy to compute the number of customers who have been assigned to a barber up to and including at time T. The number of customers who have been assigned to barber i is T/M_i + 1 (rounded down), so we can just sum these values for all the barbers.

public int countServedCustomers(long T) {
  if (T < 0) return 0;
  int served_customers = 0;
  for (int barber = 0; barber < B; barber++)
    served_customers += T / M[barber] + 1;
  return served_customers;
}
This means we can use a binary search to figure out the time T when you are going to be served. After that, all you are left to do is figure out which of the available barbers at time T is going to serve you.

Keep in mind that at time T multiple barbers may become available, so you have to account for the customers that are ahead of you in line and are going to be served at the same time. Since you know that you will be served at time T, the number of potential customers ahead of you that are going to be served at time T is less than the number of available barbers. We can then simulate that, given that we know the number of customers that are going to be served up to and including time T-1. More precisely, the number of customers to be seated in the barber chair at time T is equal to countServedCustomer(T) - countServedCustomers(T-1).

What should the bounds for the binary search be?
For the upper bound, imagine a worst-case scenario: every customer ahead of you is served by the slowest and the only available barber. Meaning you are guaranteed to be served after max(M)*N minutes. For the lower bound, in the best case you are going to be served at the time the shop opens.

The implementation below assumes that you always will have been served at T = high or earlier, while at T = low you have not been served yet. So initially we want low to be -1, not 0.

The final complexity of this solution is O(B * log(N * max(M))).

public int fastGetBarberNumber(int N) {
  long low = -1, high = 10000L*N;
  while (low + 1 < high) {
    long mid = (low + high) / 2;
    if (countServedCustomers(mid) < N)
      low = mid;
    else
      high = mid;
  }
  long T = high;
  int customers_served_before = 
    countServedCustomers(T - 1);
  int customers_to_be_served = 
    N - customers_served_before;
  for (int barber = 0; barber < B; barber++) 
    // Is the barber available at time T?
    if (T % M[barber] == 0) {
      customers_to_be_served--;
      if (customers_to_be_served == 0)
        return barber;
    }
}

Round 1A 2015 - Code Jam 2015

Logging (18pts, 34pts)

If there is only one tree in the forest, then obviously there is no need to cut anything down, and the answer for that tree is zero.

Otherwise, let's assume we have cut down some trees so that a given point P is on the boundary. If we follow the boundary clockwise from P, we will reach another boundary point Q. Imagine we are standing at P and looking along the boundary line towards Q. There can be no trees still standing to the left of this line, otherwise this would not be the boundary.

This suggests an algorithm for determining the minimum number of trees to cut down — for each point P, try each other point as a candidate for the next point around the boundary, Q; then check how many points lie on the left of the line PQ. The size of the smallest of those sets of points is the best answer:

For each point P
  Let M = N-1
  For each point Q ≠ P
    Let Temp = 0
    For each point R ≠ P, Q
      If R is to the left of the line PQ
        Temp = Temp + 1
    If Temp < M
      M = Temp
  Output M
Cutting down all the trees to the left of PQ for some Q is sufficient to produce a valid solution, because no matter where the remaining trees are, P will be on the boundary. We can also be sure that this method will find the minimal solution, since whatever the minimal solution is, it will have some point Q which is the next along the boundary, and we have cut down the minimal number of trees to make that so.

Figure 1
Consider the case in figure 1. Point #4 is already on the boundary, with Q equal to point #2, and we can see that there are no points to the left of the line PQ that we need to cut down, so the answer for point #4 is 0.

Figure 2
However, if P is point #5, then P is not already on the boundary. When we choose Q to be point #3, as in Figure 2, we find that there are two points to the left of the line PQ: point #2 and point #4. Cutting down those two trees will put P on the boundary.

However, this is not the minimal solution — when we try point #2 for Q, we will find a better solution, since only point #4 will need to be cut down.

This algorithm takes time O(N^3). But we can do better than this.

Figure 3
For each new point P, build an array S containing all the other points. Sort S by the angle of the line from P to that point. Now, we can try each possible point Q ≠ P by iterating through S, which gives an ordering of the points which moves counter-clockwise around P.

The advantage of this method is that for any choice of Q, all the points which lie to the left of the line PQ will occur after Q in the list S. So we can represent our current choice of Q and the set of points which lie to the left of PQ together as a "window" of points, implemented as two pointers into S — a "tail" pointer which points to Q, and a "head" pointer which points to the last point after Q which lies to the left of PQ. We call this point R in the diagrams.

In figure 3, we can see the state of this window for P = #1, Q = #2, R = #6. The tail of the window (in red) is at point #2, and the head of the window (in blue) is at point #6. The points #3, #4, #5, #6 are the ones which are to the left of PQ, so this choice of Q gives us a candidate solution which requires 4 trees to be cut down.

Figure 4
To update the position of the window for the next choice of Q, we need to do two things — move the tail ahead one point to the new choice of Q, and scan forward from the current position of the head pointer to find the new choice for R. In Figure 4, we can see that the tail has moved to point #3, and the head has moved to point #8. We now have a candidate solution which requires cutting down 5 trees (#4, #5, #6, #7, #8).

Since the tail of the window iterates through each point once, and the head of the window iterates through each point at most twice, this part of the algorithm takes O(N) time for each choice of P.

Sorting the points takes O(N log N) time for each choice of P, so in total this solution takes O(N^2 log N) time.

There are a few things to be careful of when implementing this algorithm:

The head of the window will reach the end of S before the tail does. The head pointer then needs to wrap around to the start of S again, to include all the points to the left of PQ. For example, when Q = #6, the head of the window is #11. Then when Q advances to #7, the head of the window needs to wrap around to #2, which is at the start of the list, because point #2 is to the left of the line PQ.
There may be no points to the left of PQ at all; for example, if P=#4, and Q=#2. A simple way to deal with this case is to have the head of the window equal to the tail. The calculation of the number of trees to cut down should naturally give zero. Then, when we advance the tail of the window by one point, also advance the head if it was at the same point.
There may be more than one point at the same angle from P. This case is handled automatically. The points that are at the same angle will occur consecutively in S. When we choose the last such point as Q, we will correctly calculate the number of trees that need to be cut down. For the other points at the same angle, we will mistakenly think that extra trees need to be cut down, but we will still find the minimum, so we needn't write extra code to handle this case.
When updating the position of the head of the window, we need to stop when the next point will be at an angle π or more greater than the angle of Q. Floating point numbers are imprecise, so we need to use an epsilon when doing comparisons in these calculations. The largest difference in angle between two points is approximately 1.25 * 10-13 radians. 64-bit floating point values have easily enough precision to represent angles with this difference, so using these with an epsilon of 10-13 will work.
Burunduk1, who won round 1A and had the fastest time for this problem, used an implementation of this algorithm.

Round 1B 2015 - Code Jam 2015

Counter Culture (11pts, 14pts)

For the small dataset, it suffices to use a breadth-first search to find the minimum number of moves required to generate all numbers from 1 to 106. You will never want to construct a number larger than 106 and later flip it to get a smaller number, so you only need to consider 106 states.

For the large dataset, the following solution works. The key ideas are:

You should first build the numbers 10, 100, 1000, ... until you get a power of 10 with the same number of digits as N, and then build N.
You should make at most one flip while the number is at a given number of digits.
Our algorithm works in two parts.

Part 1: Get to the right number of digits as fast as possible.
To get from a 1 followed by X 0s to a 1 followed by X+1 0s: first count until the right half of the number is filled with 9s. (When the length is odd, make the left half shorter than the right half.) Then flip, then count until the number is all 9s. Then add 1. (To get from 1 to 10, obviously we don't need to flip.)

Part 2: Either count directly to the answer, or do some counting, flip, and then do some more counting, whichever is faster.
Once we're at the right number of digits, we use a similar algorithm to produce N: count until the right half looks like the left half of N in reverse, then flip, then count up to the target. For example, to get from 100000 to 123456:

count to 100321
flip to get 123001
count to 123456.
If the left half is just a 1 followed by 0s, we can skip the first two steps and just count to N.

When the right half of N is all zeroes the above method doesn't work, because after we flip, the right half ends with 1. Instead, we make the right half look like the left half of N-1, flip, then count up to N. For example, to get from 100000 to 300000:

count to 100992
flip to get 299001
count to 300000.
However, as before, if the left half of N-1 is a 1 followed by 0s, we can skip the first two steps. For example, to get from 100000 to 101000, it's best to count up directly.

Why do we need to do at most one flip? The goal of the flip is to allow us to reach the left half of the number as quickly as possible. Raising the right half to the desired value is better done by directly counting the number up. In other words, there is no added value in doing multiple flips.

Sample implementation in C++:

#include <cstdio>
#include <cstdlib>

#include <string>
#include <algorithm>

using namespace std;

long long p10[10];  // p10[i] == 10^i

bool is_1_followed_by_0s(string S) {
  reverse(S.begin(), S.end());
  return atoi(S.c_str()) == 1;
}

int solve(long long N) {
  if (N < 10) return N;  // Trivial case.

  char X[20]; sprintf(X, "%lld", N);
  string S = X;
  int M = S.length();  // Number of digits of N.

  // Starts from 1.
  int ans = 1;

  // Part 1: from 1, get to the M digits as fast as possible.
  for (int d = 1; d < M; d++) {
    // For digits = 7, it starts from 7 digits:    1000000
    ans += p10[(d + 1) / 2] - 1; // Count up 9999: 1009999
    if (d > 1) ans++;            // Flip once:     9999001
    ans += p10[d / 2] - 1;       // Count up 999: 10000000
  }

  // Part 2:

  // Split N into two halves. For example N = "1234567"
  string L = S.substr(0, M / 2);  // L = "123"
  string R = S.substr(M / 2);     // R = "4567"

  // Handles the case where the right half is all zeroes.
  if (atoi(R.c_str()) == 0) return solve(N - 1) + 1;

  // Special case: Count directly to the answer.
  if (is_1_followed_by_0s(L))
    return ans + atoi(R.c_str());

  // Count until the right half looks like the left half of N
  // in reverse. In this case, count from 1000000 to 1000321.
  reverse(L.begin(), L.end());
  ans += atoi(L.c_str());

  // Flip 1000321 to 1230001.
  ans++;

  // Count up 4566 to the target from 1230001 to 1234567.
  ans += atoi(R.c_str()) - 1;
  return ans;
}

int main() {
  p10[0] = 1;
  for (int i = 1; i < 10; i++)
    p10[i] = p10[i - 1] * 10;

  long long T, N;
  scanf("%lld", &T);
  for (int TC = 1; TC <= T; TC++) {
    scanf("%lld", &N);
    printf("Case #%d: %d\n", TC, solve(N));
  }
}

Round 1B 2015 - Code Jam 2015

Noisy Neighbors (12pts, 15pts)

Small number of tenants, N <= ceil(R * C / 2)
For N <= ceil(R * C / 2), we can put the tenants in a checkerboard pattern inside the R x C apartment complex and get the minimum possible unhappiness score of zero. See the following examples:

 .X.X     .X.X.     X.X.X
 X.X.     X.X.X     .X.X.
 .X.X     .X.X.     X.X.X
 X.X.     X.X.X     .X.X.
                    X.X.X

4 x 4     4 x 5     5 x 5
The left figure shows a 4 x 4 complex where we can place up to 8 tenants to apartments marked by ‘X’ and get zero unhappiness score (an empty apartment is marked by ‘.’). Observe that no two occupied apartments share a wall. Similarly for the middle figure, we can place up to 10 tenants and get zero unhappiness score.

With either R or C even (the left and middle figures), exactly half of the apartments can be rented. However, when both R and C are odd, two checkerboard patterns are possible. We should pick the checkerboard pattern with larger size. So for the right figure, we pick the checkerboard that has 13 ‘X’ marks, instead of the other checkerboard with only 12 ‘X’ marks.

Large number of tenants, N > ceil(R * C / 2)
For N > ceil(R * C / 2), some level of unhappiness will be present, since we must place at least one pair of tenants next to each other. Instead of starting with an empty building and adding tenants, it is easier to solve this case by starting with a building completely filled with tenants, and then removing K = R*C - N of them. We now need to determine which K tenants to remove in order to decrease the unhappiness score by as much as possible.

Let’s first solve the smaller cases where R = 1 or C = 1. For those cases, we can always remove up to K tenants in such a way that each removal decreases the unhappiness score by 2, which is the maximum possible, and thus is optimal. To show this, look at the two possible examples below where C is even and C is odd:

  .X.X..     .X.X.X.
even C=6     odd C=7
For even C, K is at most C / 2 - 1 since N > ceil(R * C / 2). Thus for even C, it is always possible to remove tenants using the pattern shown on the left figure (marked by ‘X’) to guarantee that each removal decreases the unhappiness score by 2 (i.e., the maximum possible decrease).

For odd C, K is at most (C - 1) / 2. In this case, it is always possible to remove tenants using the pattern shown on the right figure (marked by ‘X’) to guarantee that each removal decreases the unhappiness score by 2 (i.e., the maximum possible decrease).

To get some idea for solving a general case where R and C are at least 2, let’s observe the four buildings below with the checkerboard pattern. The cells marked by a number represent the apartments where the tenants may be removed. The numbers represent the amount of unhappiness each tenant is contributing to the building, or equivalently, the decrease in unhappiness that will occur if those tenants were removed:

 .3.2     .3.3.     2.3.2     .3.3.
 3.4.     3.4.3     .4.4.     3.4.3
 .4.3     .4.4.     3.4.3     .4.4.
 2.3.     2.3.2     .4.4.     3.4.3
                    2.3.2     .3.3.

4 x 4     4 x 5     5 x 5     5 x 5
We can devise an optimal K-tenant-removal strategy (i.e., remove K tenants such that the total unhappiness score is minimal) as follows:

For K <= (R - 2) * (C - 2) / 2, we can always remove K tenants from the inner building (at the positions marked by ‘4’ in the figures) where each tenant removal decreases the unhappiness score by 4, which is the maximum decrease we can get, and thus it is optimal.
For K > (R - 2) * (C - 2) / 2, after removing all the tenants at positions marked by ‘4’, we start removing more tenants at the sides of the building marked by ‘3’ (each tenant removal at these positions decreases the unhappiness score by 3). If all tenants at the sides of the building have been removed but we still need to remove more tenants (i.e., the total number of removed tenants has not reached K yet), remove more tenants at the corners of the building marked by ‘2’ (each tenant removal at these positions decreases the unhappiness score by 2). It is guaranteed that K tenants can be removed by now, since K is at most R * C / 2.
Note that for odd R and odd C, there are two possible checkerboard patterns. Both of them must be considered, and we pick the one that yields the minimum unhappiness score.

Why does this strategy work?
Each removal can reduce unhappiness by at most 4. Below is a 5 x 5 building where the number in each apartment cell is the amount of unhappiness score reduction if the tenant at that cell location is removed:

23332
34443
34443
34443
23332
We can place at most ceil((R-2) * (C-2) / 2) 4s, so we cannot do better than to place this many 4s, and for the remainder place 3s. So we can put an upper bound U on the amount of unhappiness reduction we can achieve — U <= 4 * K for K <= ceil((R-2) * (C-2) / 2), and U <= 3 * K + ceil((R-2) * (C-2) / 2) otherwise.

Consider the removal strategy using this checkerboard pattern (let's call this pattern1):

2.3.2
.4.4.
3.4.3
.4.4.
2.3.2
This achieves U exactly when K <= floor(R * C / 2) - 3 for R and C odd, and K <= floor(R * C / 2) - 2 when one of R and C are even, so this strategy is optimal for these cases.

For the remaining cases, this strategy achieves either U-1 and U-2, because we need to place some 2s. Since this strategy uses the maximal number of 4s, and the maximal number of 3s possible when using the maximal number of 4s, the only possible way to improve upon it would be to use 1 fewer 4 in order to use more 3s.

It is indeed possible to do this to improve the case K = floor(R * C / 2) - 1 from 4 total unhappiness to 3 total unhappiness, using the other checkerboard pattern (let’s call this pattern2):

.3.3.
3.4.3
.4.4.
3.4.3
.3.3.
That is, if we were to use the checkerboard pattern1, removing K = 11 tenants reduces the unhappiness score by 5*4 + 4*3 + 2*2 = 36. On the other hand, pattern2 reduces the unhappiness score by 4*4 + 7*3 = 37, by using 1 fewer 4s and more 3s.

On the other hand, pattern1 is optimal is when K = 5, for example. In this case, pattern1 reduces the unhappiness score more than pattern2 since pattern1 has 5 of 4s while pattern2 only has 4 of 4s and would need to remove another 1 of 3s.

Sample implementation in C++:

#include <cassert>
#include <cstdio>
#include <algorithm>

using namespace std;

int T, R, C, N;

int remove_tenants(int &K, int max_remove, int remove_cost) {
  int removed = min(K, max_remove);
  K -= removed;
  return removed * remove_cost;
}

int get_score(int all, int corners, int inners) {
  int sides = all - corners - inners;
  int K = R * C - N;
  int unhappiness = R * C * 2 - R - C;
  unhappiness -= remove_tenants(K,  inners, 4);
  unhappiness -= remove_tenants(K,   sides, 3);
  unhappiness -= remove_tenants(K, corners, 2);
  assert(K == 0);
  return unhappiness;
}

int min_unhappines() {
  // Guaranteed zero unhappiness.
  if (N <= (R * C + 1) / 2) return 0;

  if (R == 1) {
    int K = R * C - N;
    int unhappiness = C - 1;
    int remove_cost = 2;
    return unhappiness - K * remove_cost;
  }

  if (R % 2 == 1 && C % 2 == 1) {
    // 2.3.2
    // .4.4.
    // 3.4.3
    // .4.4.
    // 2.3.2
    int pattern1 = get_score(
      (R * C + 1) / 2,  // Max #tenants that can be removed.
      4,  // #tenants at the corners of the building.
      ((R-2) * (C-2) + 1) / 2  // #tenants at inner building.
    );

    // .3.3.
    // 3.4.3
    // .4.4.
    // 3.4.3
    // .3.3.
    int pattern2 = get_score(
      (R * C) / 2,  // Max #tenants that can be removed.
      0,  // #tenants at the corners of the building.
      ((R-2) * (C-2)) / 2  // #tenants at inner building.
    );

    return min(pattern1, pattern2);
  }

  // .3.2      2.3.      2.3.2
  // 3.4.  or  .4.3  or  .4.4.
  // .4.3      3.4.      3.4.3
  // 2.3.      .3.2      .3.3.
  return get_score(
    (R * C + 1) / 2,  // Max #tenants that can be removed.
    2,  // #tenants at the corners of the building.
    ((R-2) * (C-2) + 1) / 2  // #tenants at inner building.
  );
}

int main() {
  scanf("%d", &T);
  for (int TC = 1; TC <= T; TC++) {
    scanf("%d %d %d", &R, &C, &N);
    if (R > C) swap(R, C);
    printf("Case #%d: %d\n", TC, min_unhappines());
  }
}

Round 1B 2015 - Code Jam 2015

Hiking Deer (13pts, 16pts, 19pts)

Some programming contest problems have their origins in everyday life. This year, the Qualification Round's "Standing Ovation" was written during a concert, and Round 1A's "Haircut" was written while waiting in a long line for a sandwich at Google's Go Cafe. You may not be surprised to learn that this problem was inspired by a circular hike on a day with an unusually large number of hikers on the trail!

Let H be the total number of hikers. One important observation is that the answer can't be larger than H, since no matter what the hikers' initial positions and speeds are, Herbert can just go around so fast that no hiker gets a chance to move very far, so he encounters each of them once.

Another key observation is that allowing Herbert to wait or vary his speed makes no difference to the answer. If we know the time he arrives at the end, we can put a lower bound on the answer based on the number of hikers he must have overtaken and the number of times hikers must have overtaken him. This lower bound is exactly the answer we get by having Herbert move at a constant speed, so no strategy of waiting or changing speed can improve on it.

So now, we will find a finish time that minimizes the number of encounters.

Consider the case of a single hiker. As the trail is circular and the hiker walks endlessly, the hiker repeatedly reaches Herbert’s starting point. Call the time at which Herbert finishes his hike X, and the times when the hiker reaches Herbert’s starting point T1, T2, T3, etc.

If X <= T1, then there is one encounter with the hiker as Herbert passes.
If T1 < X < T2, then there are no encounters with the hiker.
If T2 <= X < T3, then the hiker passes Herbert once.
If T3 <= X < T4, then the hiker passes Herbert twice.
etc.
So as we increase Herbert's finish time, there are multiple "events" where the number of encounters changes. It decreases by 1 for the first event, then it increases by 1 for each event after that.

Now when we have multiple hikers, there is a series of events for each of them that can increase or decrease the total number of encounters.

There will never be a reason for Herbert to take so long that we see more than H events for a single hiker, because then the number of encounters with that single hiker will be at least H. So a solution that will work for the small datasets is to make a sorted list of the first H events for all of the hikers, and then find the optimal number of encounters in this way:

Initialize the number of encounters to H.
For each event, in order of time, subtract 1 from the number of encounters if it is the first event for the hiker, otherwise add 1.
The answer is the minimum number of encounters for any time.

For the large dataset, we need a more efficient solution. Note that there are only H events that subtract 1 from the number of encounters, and the rest add 1. So once we have processed 2H events, we will never find a time with fewer than H encounters, so we can stop searching.

We still need to avoid storing H^2 events. To do this, we can maintain a priority queue of events. We initialize the queue with one event for each hiker -- the event for that hiker's T1. Whenever we process an event, we replace that event on the queue with the next event for that hiker. In this way we only need O(H) memory, and O(H log H) time.

One subtlety to note is that if multiple hikers reach Herbert’s starting point at the same time, we must process the events that add an encounter (those for a hiker's T2, T3, etc.) before the events that subtract an encounter (those for a hiker's T1) since the number of encounters only really decreases once Herbert's finish time is larger than T1.

52 people successfully solved the large input for this problem during the contest. See, for example, Belonogov's solution, which you can download from the scoreboard.

Round 1C 2015 - Code Jam 2015

Brattleship (11pts, 22pts)

Once we first make a hit, it will take at least W-1 more moves to win, since we have to hit the remainder of the ship.

If there's still more than one possible position for the ship, then the little brother will get at least one more opportunity to answer "miss." To limit the number of additional misses to one, we make moves adjacent to the hits we already have. If we get a "miss," then we will have a row of "hits" with a miss at the end, so we will know the exact location of the ship.

Now, the little brother may as well try to maximize the number of misses we make until we first get a hit. He can't control what cells we name, so all he can do is answer "miss" until we make a guess which must unavoidably be a hit. So we need to find a pattern of guesses that uses as few cells as possible, and such that each possible ship position is covered by one of them. To do this, we use a pattern that chooses every Wth cell of each row.

The total number of guesses is then R * floor(C/W) for the pattern, plus W-1 to hit the remainder of the ship, plus 1 more guess if there is more than one possibility for the position of the ship, which occurs if C is not an exact multiple of W. Below is a sample code in C that implements this solution:

#include <stdio.h>

int main() {
  int T, TC, R, C, W, score;

  scanf("%d", &T);
  for (TC = 1; TC <= T; TC++) {
    scanf("%d %d %d", &R, &C, &W);

    // The R * floor(C/W) for the guess pattern.
    score = R * (C / W);

    // Plus W-1 to hit the remainder of the ship.
    score += W - 1;

    // Plus 1 more guess if there is more than one
    // possibility for the position of the ship,
    // which occurs if C is not an exact multiple of W.
    if (C % W) score++;

    printf("Case #%d: %d\n", TC, score);
  }
}
tos.lunar wrote a solution for this question using Haskell, which you can download from the scoreboard.

A recursive search of the game tree, simulating all choices for our moves and the little brother's moves, will also work for the small input.

Round 1C 2015 - Code Jam 2015

Typewriter Monkey (11pts, 22pts)

This problem naturally has two parts — computing the maximum number of bananas needed, and computing the expected number of bananas needed.

Take, for example, the target string X = ABACABA. To find a string of length S containing the maximum number of copies of X, we start by putting X at the start of the string. Then to fit as many more copies as possible, we want to overlap each copy of X as much as possible with the previous copy. For this example, we could overlap with the final "A" and add "BACABA", but it is even better to overlap with "ABA" and add just "CABA" to get a second copy. To find the maximum amount of overlap, we can just try every possible amount and check which ones work, since L is small. It can also be computed in linear time using the initialization phase of the Knuth-Morris-Pratt algorithm. If the maximum amount of overlap is O, then we can fit 1+(S-L)/(L-O) copies of the string.

To find the expected number of copies, we start by computing the probability P of the word occurring at a fixed place. This is equal to the product of the probabilities for each letter of the word being correct. The probability for a single letter being correct is the fraction of keys which are that letter.

By linearity of expectation, the expected number of copies is then just P multiplied by the number of places the string can occur, which is S-L+1. This is a convenient fact to use, because we don't need to take into account that the string occurring in one position and the string occurring in an overlapping position are not independent events.

Sample implementation in Python:

# Find the maximum amount of overlap. We can just try
# every possible amount and check which ones work.
def max_overlap(t):
  for i in range(1, len(t)):
    if t[i:] == t[0:len(t)-i]:
      return len(t) - i
  return 0

# Returns the probability of the target word
# occurring at a fixed place.
def probability(target, keyboard):
  P = 1.0
  # Compute the product of the probabilities 
  # for each letter of the word being correct.
  for i in range(len(target)):
    # The probability for a single letter being correct
    # is the fraction of keys which are that letter.
    C = keyboard.count(target[i])
    P = P * C / len(keyboard);
  return P

for tc in range(input()):
  K, L, S = map(int, raw_input().split(' '))
  keyboard = raw_input()
  target = raw_input()
  res = 0
  P = probability(target, keyboard)
  if P > 0:
    O = max_overlap(target)
    max_copies = 1.0 + (S-L) / (L-O)
    min_copies = P * (S-L+1)
    res = max_copies - min_copies
  print("Case #%d: %f" % (tc + 1, res))

Klockan wrote a solution in C++ that also uses this approach, which you can download from the scoreboard.

We could also use a dynamic programming algorithm for both parts of the problem, using O(LS) states, where the state is the number of characters typed and the largest number of characters of the word that are currently matched, and the value at each state is the probability of that state and the maximum number of copies of the word that could have been produced while reaching it. For each of the states where the entire word has just been matched, we add the probability of reaching that state to the expected number of copies of the word, and update the maximum number of copies that are possible. linguo wrote a solution of this type in Python.

Round 1C 2015 - Code Jam 2015

Less Money, More Problems (11pts, 23pts)

We will incrementally build a set S of denominations that solves the problem using the minimal number of additional denominations, by restricting ourselves to adding denominations from smallest to largest.

As we add denominations to S, we also maintain an integer N, which is the largest value such that we can produce each value up to and including N. In fact, after all of our choices, S will be able to produce exactly the set of values from 0 to N, and no others.

When we add a new denomination X to S, the new set of values we could produce include each of the values we could produce with the existing set S plus between 0 and C of the new denomination X. If X is at most N+1, then this new set of values will be the set of all values from 0 to N+X*C, so we can update N to N+X*C.

So, we initialize S to the empty set, and N to 0.

Then while N is less than V, we do the following:

Identify the smallest value we cannot produce: N+1.
If there is still a pre-existing denomination which we haven't used, let the minimum such denomination be X. If X is less than or equal to N+1, we add it to S, and update N to N+X*C.
Otherwise, we have no way yet to produce N+1 using the denominations we have, so we must add to S a new denomination X between 1 and N+1. This will increase N to N+X*C. We use X=N+1. No other choice for X could lead to a better solution, since for X=N+1, the set of values the new S will be able to produce is a superset of the values S would be able to produce with any other choice.
Finally, when we have a set S which can produce all values up to V, we output the number of new denominations we had to add.

In the above algorithm, the first option — using a pre-existing denomination — can only occur D times. When the second option is chosen, N increases to (C+1)N+C. Since we stop when N reaches V, this will occur O(log V) times. So the overall time complexity is O(D+log(V)).

Sample implementation in Java:

import java.util.*;

public class C {
  public static void main(String[] args) {
    Scanner scan = new Scanner(System.in);
    int T = scan.nextInt();
    for (int TC = 1; TC <= T; TC++) {
      int C = scan.nextInt();
      int D = scan.nextInt();
      int V = scan.nextInt();
      Queue<Integer> Q = new ArrayDeque<>();
      for (int i = 0; i < D; i++) {
        Q.add(scan.nextInt());
      }

      long N = 0;
      int add = 0;
      while (N < V) {
        // X = The smallest value we cannot produce.
        long X = N + 1;
        if (!Q.isEmpty() && Q.peek() <= X) {
          // Use pre-existing denomination we haven't used.
          X = Q.poll();
        } else {
          // No way to produce N+1, add a new denomination.
          add++;
        }
        N += X * C;
      }
      System.out.printf("Case #%d: %d\n", TC, add);
    }
  }
}
Vitaliy's solution in C, which you can download from the scoreboard, is another good example of this approach.

Round 2 2015 - Code Jam 2015

Pegman (5pts, 10pts)

If Pegman walks off the map, there must have been some arrow that points towards an edge of the grid, with no other arrows in between. So we must take every such arrow and point it towards another arrow. We simply count how many of these arrows there are, and if any of them can't be pointed towards another arrow — which is when there are no other arrows in the same row or column — then the answer is IMPOSSIBLE.

Round 2 2015 - Code Jam 2015

Kiddie Pool (7pts, 18pts)

While the pool is being filled, each source of water is turned on for some fraction of the time. This is equivalent to having each source turned on for the whole time at a fixed rate which is between zero and Ri. So the problem is to find what rate each source should be set to in order to maximize the total flow and get the correct temperature. Then the fill time is the capacity of the pool divided by the total flow rate.

Find the temperature of the water we get when setting each source's rate to the maximum. If the temperature is exactly the desired temperature, then we have the solution.

If the temperature is too hot, then we need to lower the temperature by lowering the flow rate of some of the sources. To lower the total flow by as little as possible, we decrease flow starting with the hottest source, moving to the cooler sources if necessary, until the average temperature of the flowing water equals the desired temperature. If we never reach the desired temperature, then no solution was possible.

If the temperature is too low when all the sources are set to their maximum rate, then the solution is similar — we raise the temperature by reducing the flow rate of sources, starting from the coldest source.

Round 2 2015 - Code Jam 2015

Bilingual (6pts, 24pts)

Construct a graph with a node for each sentence and word. For each sentence S, add edges in the graph between the node for S and each of the nodes for the words in S.

Now consider a path in the graph from sentence 0 to sentence 1. The nodes on the path will alternate between sentences and words. Since the language of the sentences in the path must at some point change from English to French, one of the words along the path must belong to both languages. We need to find a minimal set of words such that every path from sentence 0 to sentence 1 in the graph goes through one of the words in the set. This is equivalent to finding a minimal vertex cut in the graph, where the cut vertices can only include the nodes for words.

We can solve this by transforming the problem into an edge cut problem in a directed graph. For each word w, create two nodes Aw and Bw. Add a directed edge from Aw to Bw with capacity 1, and for each sentence S which contains w, add directed edges from S to Aw and from Bw to S, both with infinite capacity.

Now we find the size of the minimum cut using a maximum flow algorithm, and the max-flow min-cut theorem. Each edge in the cut will cut the edge from Aw to Bw for some word w, since those are the only finite-capacity edges. These words form a minimal set of bilingual words which solves the problem.

Round 2 2015 - Code Jam 2015

Drum Decorator (11pts, 19pts)

Some important initial insights are:
A cell can only take three values: 1, 2, or 3. 4s would have to keep propagating until they hit an edge cell, and edge cells only border three other cells.
If a 3 appears, it must be in a band of 2 complete rows. To see this, place a 3 along the border of the top of the drum, and note that the 3 adjacent cells must all contain 3s. This must continue all the way around the top, which also fills up the row below. Suppose that some other finite arrangement of 3s was valid — then there must be a highest 3 in the pattern, above which no other 3 can be placed, but then that 3 can only be part of a set of 2 rows of 3s, by the same argument above.
If a 2 appears, it must be either in a 2x2 square of 2s, or in a (possibly winding) line of 2s that goes all the way around the drum and reconnects with itself.
1s can only appear in adjacent "dominoes" of two.
Besides the bands of 3s, there are four patterns using 1s and 2s:

I. one-thick bands of 2s:

2...
II. alternating upright dominoes of 1s and 2x2 squares of 2s:

122...
122...
III. a line of 2s winding around horizontal dominoes of 1s:

222112...
112222...
IV. a line of 2s winding around vertical dominoes of 1s:

2212...
1212...
1222...
None of these patterns can border each other, but they can be separated by bands of 3s. The number of columns in a drum can rule out some patterns -- II, III, and IV require multiples of 3, 6, and 4, respectively.
Now we can use dynamic programming to assign patterns to the drum. Our state is how many rows we have filled, and whether the previous pattern was a pair of rows of 3s.

Each of the five patterns repeat with some period P, which is either 1, 3, 4 or 6, so there are P ways to place that pattern on the drum.

Each overall drum assignment also has a period, which is the least common multiplier of the periods of all of the patterns on it.

An extra complication is that drum assignments that are rotations of each other should only be counted once. So our dynamic programming state should also include the period of the current drum assignment. When counting the final number of solutions, we divide the number of solutions in each state by the state's period. For example, if we have 18 ways to produce a solution of period 3, this only corresponds to 6 solutions.

Another way to handle equivalent patterns is to use Burnside's lemma. We leave the details as an exercise to the reader!

Round 3 2015 - Code Jam 2015

Fairland (3pts, 9pts)

We need to find a salary X such that all salaries at the company are between X and X+D, and the number of remaining employees is maximized.

If employee i is kept, then X must be in the interval [Si-D, Si]. Not only that, but X must be in the interval [Sj-D, Sj] for each employee j who is the manager of employee i, or the manager of the manager of employee i, and so forth. So for each employee, we want to find the intersection of all those intervals. This is easily done using a preorder traversal of the tree, computing the intersection of all the intervals in the path from the root node to the current node. (If the intersection is empty, we can't keep that employee.)

Now we want to find the maximum number of intervals that overlap at any point. Let the interval we get for employee i be [Ai, Bi]. Create an array of pairs of integers, with two pairs for each employee: one containing (Ai, +1) and the other containing (Bi, -1). Sort this array by the first numbers of the pairs, breaking ties using the second number, by putting entries with +1 before those with -1. Then we start a counter at zero, and iterate through the array in sorted order. Add the second number of each pair to the counter. The maximum value of the counter is the answer to the problem.

Round 3 2015 - Code Jam 2015

Smoothing Window (6pts, 7pts)

One approach towards a solution is to construct the temperature sequence that begins with K-1 zeroes, followed by a single pass through all the N-K+1 smoothing window sums to fill in the rest of the sequence. For example, running this construction for the sum array “0 12 0 12 0” with K = 3 (from the third sample case) produces the sequence “0 0 0 12 -12 12 0”, where the difference between the maximum and minimum temperatures is 24. To reach a sequence that minimizes this difference, we tweak some values in the sequence as explained below.

Let’s define GROUP(i) where (0 ≤ i < K) as the group containing the (i % K)-th temperatures in the sequence. The sequence “0 0 0 12 -12 12 0” above is grouped as follows:

“0 12 0” from the first, fourth, and seventh temperatures
“0 -12” from the second and fifth temperatures
“0 12” from the third and sixth temperatures
Changing the i-th temperature by Z degrees means that we need to compensate for this addition with other temperatures in the sequence. One way is by adding Z to the other temperatures in the same group, and subtracting Z from all the temperatures in another group. For example, to increase the first temperature reading by 3 degrees, we also increase the fourth and seventh temperature readings by 3, and reduce the temperatures in the second (or third) group by 3. The resulting sequence is “3 -3 0 15 -15 12 3”. Notice that its sum array stays the same: “0 12 0 12 0”.

Now let’s define:

lo(i) as the minimum element in GROUP(i)
hi(i) as the maximum element in GROUP(i)
interval(i) as a range [lo(i), hi(i)]
SHIFT(i, y) as an operation that adds y to each member of GROUP(i).
We can remodel the original problem as follows:

You are given K intervals, where the i-th interval spans from lo(i) to hi(i). You can perform any number of interval adjustments by choosing i, j, and y (0 ≤ i, j < K), shifting the i-th interval by y (i.e., SHIFT(i, y)), and shifting the j-th interval by -y (i.e., SHIFT(i, -y)). Find a sequence of adjustments (where order doesn’t matter) that minimizes the covering range of all intervals, where the covering range of the intervals is max{hi(i)} - min{lo(j)} for all 0 ≤ i, j < K. The minimum covering range is equivalent to the smallest possible difference between the minimum and maximum temperatures in the original problem.
Another important insight is that the shifting can be done independently and can be aggregated. That is, we can accumulate all positive shifts and do it in bulk, similarly with the negative shifts. Therefore, we can “normalize” all intervals by shifting them such that the lower bound of all intervals becomes 0. Denote Q to be the total sum of shifts, which is the sum of all lo(i) where 0 ≤ i < K. For example, if we have two intervals [-10, -8] and [333, 777], after normalization they become [0, 2] and [0, 444] with Q = -10 + 333 = 323. Finally, we have to shift back the normalized intervals by Q degrees, but with the flexibility to distribute the shifts such that the covering range of all intervals is minimized.

Notice that if we increment (or decrement) all the intervals by one, the relative positioning of each interval to one another won't change and thus it does not change the covering range of the intervals. Using this insight, we can reduce Q to Q % K by distributing the excess shifts Q evenly among the intervals without affecting the final answer. When Q is negative we can keep adding K to Q until it is non-negative.

Suppose we have three normalized intervals [0, 4], [0, 9], and [0, 7], and the total sum of shifts Q is 40. We can distribute back 39 shifts evenly to all three intervals where each interval is shifted by 13, ending with [13, 17], [13, 22], and [13, 20]. Then, we can shift all of them down back to [0, 4], [0, 9], [0, 7] without changing their relative positioning and and we are left with Q = 1.

Finally, we must distribute back the remaining shifts to the intervals and minimize the covering range of all intervals.

Suppose that L is the size of the largest interval. If Q = 0, then there is no excess shift to distribute, and the minimum covering range is L. Otherwise, we can assign L - (hi(i) - lo(i)) excess shifts to interval i without changing the answer. Using the previous example, the current minimum difference is L = 9. Without changing the minimum covering range, we can assign 1 excess shift to interval [0, 4], and get [1, 5].

If there are still some excess shifts remaining, it means we cannot distribute any more excess shifts without increasing the minimum covering range. Increasing the minimum covering range by 1 will make room for distributing K more excess shifts, which is enough for the remaining Q (since Q = sum of shifts % K, and thus it is less than K).

The complexity of this solution is O(N).

Round 3 2015 - Code Jam 2015

Runaway Quail (8pts, 15pts)

The optimal strategy will consist of running either to the left or right until we catch a particular quail, changing direction, then running until we catch a particular quail on the other side, changing direction again, etc., until we have caught every quail. Changing direction at a time other than when catching a quail would be wasteful.

Consider a partial solution where we have caught some of the quail on each side, and returned to the origin. For each side, let Q be the fastest quail we have not caught. (To simplify the analysis, if there is more than one quail on the same side with the same speed, we ignore all but the farthest one.) Each other quail R on that side must fall into one of these categories:

R is faster than Q. In this case, R has already been caught.
R is slower than Q, and is farther away. R cannot have been caught in this case, because we would have had to run past Q to get to R.
R is slower than Q, and is not farther away. In this case, we might have already caught R, or we might not. But it doesn't affect our solution — if we have not yet caught R, we will run past R when we go to catch Q anyway.
So we can solve this problem with dynamic programming, where the states only need to include the identity of the fastest uncaught quail in each direction. We will keep the fastest time at which we can achieve each state we generate.

For each state, starting from the initial state where we have caught nothing, we try running in either direction until we catch the fastest uncaught quail in that direction, and then run back to the origin. We also try running to catch each quail farther than the fastest uncaught quail in each direction, and then running back. For each of these options, we compute the new fastest uncaught quail in the direction we ran, which will tell us what the new state is when we return to the origin. If we have achieved the new state with a faster time than we had for it before, we update the time for that state.

Whenever we generate the state where every quail has been caught, we check the time at which we caught the last quail. The fastest of these times is the answer.

Round 3 2015 - Code Jam 2015

Log Set (6pts, 19pts)

Let x be the largest element of S'. Then x is the sum of all of the positive elements of S, and none of its negative elements.

Let y be the second-largest element of S'. Either this is the sum of all but the smallest of the non-negative elements, or it is the sum of all of the non-negative elements and the smallest negative element. So if d = x-y, then the smallest magnitude of any element in S is d — either d ∈ S or -d ∈ S.

These two cases are basically identical. Consider the process of building every subset of S, and finding the sum of each of those subsets. To build a subset, we consider each element of S, and either include it in the subset or not. If we include it, the sum increases by the value of the element. If we don't include it, the sum increases by zero. So in the case where d is an element of S, we will be choosing between adding 0 and adding d to the sum. If -d is an element of S, we will be choosing between adding -d and adding 0. These situations are equivalent, except that every element of S' is offset by a constant depending on whether d or -d is in S.

We can now scan through S' and match elements into pairs that are d apart, and keep only the smallest of each pair. Then we can recursively apply this procedure to get the magnitudes of all the other elements of S.

When we are done, we have the magnitudes of the elements of S. Next we need to figure out which elements are positive and which are negative. First, make them all negative. The largest element of S' will be zero in this case. Then we need to choose which elements to change from negative to positive in order to make the largest element of S' equal to x. This is a Subset Sum problem, with required sum x, and elements equal to the magnitudes. This is easily solvable with dynamic programming — the number of unique sums, and hence the number of states in the dynamic program, is guaranteed to be small because the number of unique values in S' is small.

Round 3 2015 - Code Jam 2015

River Flow (10pts, 17pts)

Assume the farmers are not cheating. The river flow data is periodic, with period 2D.

Let xi be di - di-1 when i>1, and x1 = d1 - d2D. xi represents the difference in flow between day i and the previous day in the 2D-day cycle.

Each farmer's actions change xi in the following way. Let the first time a farmer changes between diverting water and allowing it to flow, or vice-versa, be T, and let the number of days for which that farmer performs the same action be P. At times T, T+P, T+2P, etc., the farmer will alternate between adding 1 to xi and subtracting 1 from xi. (Whether it adds 1 or subtracts 1 first depends on whether the farmer is diverting water or allowing it to flow at the start of the 2D-day cycle). For example, a farmer that starts diverting water at time 3, and changes every 8 days, contributes -1 to x3, +1 to x11, -1 to x19, +1 to x27, etc.

We can assume that if two farmers share the same T and P, they either both start the cycle diverting water or both start the cycle allowing it to flow. If they were performing different actions, we could remove the two farmers and their tributaries, and get the same data.

Now consider the quantity F(T,P) = xT - xT+P + xT+2P - xT+3P + XT+4P - ... for some T and P. Any farmers with those values of T and P will each contribute 2D/P or -2D/P to F(T,P), depending on which action they perform first. Any farmers with a different value of T or P will contribute zero to F(T,P). So the number of farmers for T and P is |F(T,P)| * P / 2D, and the sign of F(T,P) tells us their initial action.

We can try every valid value of T and P to find out how many farmers of each type there are. Once we know this information, we can check that the original data matches it for some number of tributaries. If it does not, we output "CHEATERS!", otherwise we output the number of farmers.

World Finals 2015 - Code Jam 2015

Campinatorics (6pts, 21pts)

Let N be the size of the grid, and K be the number of 3-family tents. We can decompose the problem of counting the number of arrangements for given values of N and K by noting that we can first choose a set of K rows and K columns to contain the 3-family tents, and the remaining set of N-K rows and N-K columns will contain the 2-family and 1-family tents. (See the diagram above.) The number of arrangements is then the product of the following:

The number of ways to choose the sets of rows and columns. There are C(N,K)^2 ways of choosing the K rows and columns, where C(N,K) is a binomial coefficient.
The number of ways to assign the 3-family tents to (row, column) pairs. We can do this assignment by taking a permutation of the columns, and assigning the ith column in the permutation to the ith row in the set of K rows. There are K! such permutations.
The number of ways to assign the 2-family tents to (row, column) pairs. Similarly to the 3-family tents, there are (N-K)! ways to do this.
The number of ways to assign the 1-family tents to (row, column) pairs. Not all of the (N-K)! permutations of columns can be used, since we have the additional requirement that we can't use a (row, column) pair in which we've already placed a 2-family tent. To deal with this, we reformulate what we need to do here: for each row X in the set of N-K rows, we need to choose a unique row Y from the same set, find the column C such that there is a 2-family tent at (Y,C), and place a 1-family tent at (X,C). The space at row X, column C is guaranteed not to already have a tent. So we need a permutation of the N-K rows, with the additional requirement that no row is unchanged by the permutation – that is, for each row X, we choose a row different to X as the corresponding row Y in the permutation. Such a permutation is called a derangement. The number of derangements of size N-K is written !(N-K).
Now, the product of all these terms is:

C(N,K)^2 × K! × (N-K)! × !(N-K) = N!^2 / (K! × (N-K)!) × !(N-K).

We get the answer to the problem by summing this value (mod 10^9 + 7) for all values of K from X to N.

We can do that efficiently by precomputing K!, 1/K!, and !K mod 10^9 + 7 for all values of K up to N. Factorials can be computed with the obvious recurrence. 1/K! mod 10^9+7 can be computed from K! with the extended Euclidean algorithm or using Fermat's little theorem. !K can be computed with the recurrence:

!1 = 0, !2 = 1, !X=(X-1)(!(X-1)+!(X-2)).

World Finals 2015 - Code Jam 2015

Costly Binary Search (8pts, 19pts)

Let us call the length of the input sequence N. There are N+1 possible "slots" in which the new element can end up — we number them from 0 to N, where 0 is the slot before the first element of a, and N is the slot after the last element of a.

At any point in our search, we will have narrowed down the possible location of the correct slot to an interval of possible slots. We then want to find out which slot in that interval is the correct one; we call this "solving" the interval.

For a given cost C and position X, we want to compute the largest possible interval of slots, beginning at X, which we can solve in cost C or less. Call the rightmost slot of that interval f(C,X). We will compute f using dynamic programming.

Clearly if C=0, then f(C,X)=X and the interval is empty. (Since each comparison costs at least 1, we cannot make any comparisons.)

Otherwise, assume we have an interval [X,Y], where X < Y, that we can solve at a cost no greater than C. The solution must involve an initial comparison at an index i, where i is in [X,Y-1], which costs ai. Then we will have narrowed down the correct slot to either the interval of slots to the left of index i, which is [X,i]; or the interval of slots to the right of index i, which is [i+1,Y].

Each of these intervals must be solvable with a cost no greater than C-ai, i.e., f(C-ai, X) ≥ i, and f(C-ai, i+1) ≥ Y.

We want to find the largest Y for which the above statements hold. To limit the number of possible choices for i and Y, we always use Y=f(C-ai, i+1), since by definition this is the largest possible Y for a given choice of X and i. Also, instead of trying every value of i in [X, f(C-ai, X)], we iterate through each possible value of ai from 1 to 9, and try the largest index i in [X, f(C-ai, X)] with that value. (Smaller indices i with the same value of ai could not produce a larger Y). If there are no usable values of i (because C is too small, or X=N) then f(C,X)=X.

We compute f(C,X) in this way for increasing values of C, until we find the minimal C such that f(C,0)=N. This C is the answer.

The maximum value for the resulting C occurs when every array value is 9, in which case we can't do better than a standard binary search, which takes O(log N) comparisons of cost 9 each. That means computing f(C,X) over a domain of size 9 log N × N. Since the computation at each position takes constant time (we try 9 things), the overall algorithm takes O(N log N) time, with a somewhat large constant because of the two factors of 9 mentioned previously.

World Finals 2015 - Code Jam 2015

Pretty Good Proportion (5pts, 22pts)

A brute force O(N2) strategy will work for the Small dataset: for each position, compute the cumulative number of ones seen so far, and then check every possible pair of start and end points in the sequence. But N can be up to half a million in the Large dataset!

We will read the string from left to right and keep track of the number of ones Oi seen after reading the first i digits. We will examine each i between 0 and N, inclusive, so that we consider both the empty prefix and the full string. We will maintain an initially empty set of points, and for each i, we will add pi = (i, F × i - Oi) to the set. Notice that the fraction of ones in the substring starting at position i+1 and ending at position j is (Oj - Oi) / (j - i), and the difference from F is F - (Oj - Oi) / (j - i) = (F(j - i) - Oj + Oi) / (j - i), which is the slope between pi and pj. Finding the pi and pj that minimize the absolute value of this slope is equivalent to solving the problem.

It is hard to see (but easy to prove) that the slope with minimal absolute value occurs between two points that are consecutive in sorted order by y coordinate: if there is a point r that has a y coordinate between the y coordinates of two other points p and q, the slope between p and q (call it s) is the weighted average of the slope between p and r and the slope between q and r; either those two slopes are equal to s, or one of them must be less than s. A more visual way to see this is to draw the segment p-q and place r on it. That makes all three slopes equal. Moving r horizontally clearly makes the slope of one of the segments pr and qr increase and the slope of the other one decrease.

This insight saves us from checking all pairs of points, and reduces the time complexity of the problem to O(N log N), which is imposed by the single sorting operation.

World Finals 2015 - Code Jam 2015

Taking Over The World (7pts, 29pts)

Let G be the graph, s be the entrance node, and t be the node with the secret weapon.

For the small input, we start by finding the shortest distance through the graph from s to t using, e.g., Dijkstra’s algorithm. Call this distance L. If K=0, we cannot obstruct anything, so the answer is simply L.

Now assume K > 0. It is always advantageous to obstruct s, so let’s begin by doing this: reduce K by one, and increase L by 1. Next we want to find if we can increase L one more by obstructing other nodes. This means that every shortest path from s to t must have another obstructed node in it (other than s). We can turn this into a standard graph problem in the following way -- take the subgraph of G which contains all the edges and nodes in a shortest path from s to t. Make each edge directed, pointing in the direction of the path it belongs to.

Now, if there is a set of up to K nodes we can obstruct that covers all the paths, for example a and d in the diagram above, then removing those nodes cuts the graph, separating s and t. So we need to solve the minimum vertex cut problem on this graph and check if the cut has size at most K. To do this, we convert it to an edge cut problem by changing the graph so that each vertex except s and t is split into two nodes, with an edge of capacity 1 between them. The original edges are converted to infinite-capacity edges, as in the diagram below:

A vertex cut of K vertices in the earlier graph will correspond to an edge cut of capacity K in this new graph. In the example above, cutting the edges between the pairs of nodes for a and d cuts the graph. Now we have a minimum edge cut problem, and we can apply the max flow / min cut theorem — the size of the minimum edge cut is the size of the maximum flow — and use one of the standard algorithms for finding the maximum flow. If there exists a flow that is larger than K, then the answer is L, otherwise the answer is L+1.

Now for the large input. We will check, for successive values of X starting at 1, whether it is possible to obstruct up to K nodes in order to stop the guards traveling from node s to node t in time ≤ X. We call this proposition P(X). The first value of X where P(X) is false is the answer.

To determine the truth of P(X) for a specific X, we construct a new graph G', which has a node for each triplet (v, x, b) where v is a vertex in G, x is an integer between 0 and X, and b is true or false. Node (v, x, b) represents that the guards can reach node v in time x if b is false, or reach node v and surpass the obstruction in time x, if b is true. We add the following edges:

(v, x, false) to (v, x, true), for each v and x. (This means a free pass through an obstruction; removing one of these is equivalent to not adding an obstruction at v, if you remove the one for the appropriate x.)
(v, x, false) to (v, x+1, true), for each v and x < X-1. (a non-free pass through an obstruction) (v, x, false) to (v, x+1, false), for each v and x < X; (Stay put for 1 time unit.)
(v, x, true) to (w, x+1, false), for each x < X and each edge vw in G. (Transverse an edge in 1 time unit).
Let σ be the node (s, 0, false), and τ be the node (t, X, false). Assuming no nodes are obstructed, we can see that the following properties hold for G':

There is a path from σ to (v, x, false) if and only if the guards can arrive at node v at a time ≤x.
There is a path from σ to (v, x, true) if and only if the guards can leave node v at a time ≤x.
There is a path from σ to τ if and only if it is possible for the guards to travel from s to t in G in time ≤ X.
If some nodes in G are obstructed, the properties above still hold if we delete all the edges in set 1 of the four sets above, where the corresponding v is one of the obstructed nodes. So P(X) is equivalent to a graph cut problem -- determining whether we can disconnect σ and τ by cutting the set 1 edges for up to K nodes of G.

In G, after we choose the nodes to obstruct, for each node v, all of the optimal paths from s to t that pass through v do so at the same time, so the ability to obstruct v was only "useful" at a given time time. So if our ability to obstruct a node were changed to an ability to obstruct a node at just one particular time, the result would be no different. Similarly, in G', after removing a set of edges, for each v, there will only be at most one x for which it was "useful" to cut the edge from (v, x, false) to (v, x, true).

We can prove this more formally: let A be a set of edges from set 1 that have been removed to disconnect σ and τ, and say that it contains the edges from (v, x0, false) to (v, x0, true) and from (v, x1, false) to (v, x1, true) for some v and x0 < x1. If both edges are necessary, then there must be paths from σ to (v, x0, false) and (v, x1, false), and paths from (v, x0, true) and (v, x1, true) to τ. But now there is a path from σ to τ via (v, t0, false), (v, t1-1, false), and (v, t1, true), so σ and τ are not disconnected at all! So for any set of edges A which disconnect σ and τ, there is a subset of A which disconnects σ and τ and has at most one edge cut for any node in G.

This means that P(X) is equivalent to being able to disconnect σ and τ by cutting up to K edges from set 1. As with the small input, this is equivalent to a max flow problem, with unit capacity on the set 1 edges and infinite capacity on the others.

We can solve all these flow problems quite quickly, since we do not need to recompute the maximum flow from scratch each time we increase X — we just add one more "layer" of nodes and edges for the new X value, and update the flow.

World Finals 2015 - Code Jam 2015

Merlin QA (8pts, 30pts)

Every spell in the spell-testing procedure can be interpreted as an M-dimensional vector, and the current state of Edythe’s inventory is also an M-dimensional vector (initially an all-zero vector). Casting a spell is equivalent to adding the vector describing the spell to the vector describing Edythe’s inventory, and then changing all the negative values in Edythe’s inventory to zero (this corresponds to retrieving the exact necessary amount from Merlin’s storehouse). Edythe is aiming to maximize the sum of all the coordinates at the end.

Note that even if we allowed Edythe to zero a coordinate of the vector describing her inventory at any time, it would never be advantageous to zero it when it is positive, and it would never be disadvantageous to zero it when it is negative - so the final answer will not change if we give Edythe full freedom over when and which coordinates of her inventory get zeroed. We will solve this less constrained problem from now on.

Notice that it never makes sense to zero a given coordinate more than once, since the last zeroing will make the previous ones irrelevant anyway. So from now on, we will solve an equivalent problem — given a list of spells, and the opportunity to zero each coordinate of the accumulated sum exactly once, at any point of the summation, maximize the final sum of all the coordinates. Thus, a solution candidate can be equivalently described as a permutation of the N spells, plus the moments in time at which we zero each coordinate.

Let’s fix the order in which coordinates get zeroed (we will later iterate over all M! possible orderings), and look at one particular spell. We will try to figure out the best time to cast it. If we cast it before any coordinate is zeroed, then the result will be the same as if we had not cast it at all — every coordinate will eventually be reset to zero after casting the spell anyway. Now assume we cast it when k coordinates have already been zeroed, and the others have not. Compared to the situation when we don’t cast it at all, the end result is that the values of the spell on the k already-zeroed coordinates will be added to the final values of these coordinates, and so the sum of these values will be added to Edythe’s final result. So, for every spell, we check each of the M+1 possibilities for k, and choose the one which will give the largest contribution to the final result.

So a solution can work as follows: take every possible ordering P of the M dimensions. For each ordering P, iterate over all the spells, and for each spell find which of the M+1 values for k will cause the spell to contribute the most to the final result. The sum of all these contributions for all the spells is the best possible score for this ordering P; the maximum score over all orderings is the answer. The runtime is O((M+1)! N), which will easily run in time for N = 100 and M = 8.

World Finals 2015 - Code Jam 2015

Crane Truck (8pts, 37pts)

In this problem, we are given a version of a small programming language, and we are required to track the number of operations executed as a program runs. The memory is a circular array of 240 positions, and all values are considered modulo 256 (for convenience, we shift values one position to make them go [0,255]). The program can run for a really long time, so our approach is to simulate it efficiently by bundling together many of those operations. The actual code is long and complicated — this is the last problem in a finals round, after all — so we just present an overview of the main idea.

The program can be factorized into 5 parts: A(B)C(D)E. Some of these may be empty; in particular, in the Small dataset, D and E are always empty. Each sequence of simple instructions can be translated, via simulation, into a complex instruction of the form:

Let i be the current position of the truck.
Add some xj to position i+j for j in [-2000, 2000]. (2000 is the maximum length of each part.)
Move the current position to i+k, where k is also in [-2000, 2000].
Running A
After executing A, only values within 2000 of the starting position can be modified. Let us call the area within 2000 of the starting position the "red" area.

Running (B)
After running B several times, we can either finish or move out of the red area. After several more runs outside, the memory outside the red area will start to look periodic with period |k| and we are going to keep moving the current position according to B's k in the 3rd step until we come back to the red area on the other side, completing a full circle. So, we bundle together all the instructions required to complete the circle and we represent the non-red area as a repetition of a period with length at most 4001 (the range of j's in step 2). We can keep doing this in this way until the loop finishes (which is guaranteed). Notice that after at most 4001 complete circles around the memory, we are going to repeat the initial positions inside the red area. And once we've done that, we are going to start repeating the values there, so we can't do more than 4001 × 256 loops without it being an infinite loop (which is forbidden by input constraints).

Running C
After that, we can simulate C, which can modify the range [-4000, 4000] because the previous loop finished within 2000 of the starting point. Let us call that the "green" area.

Running (D)
When running D, something similar to running B happens, but the memory outside the green area is now the sum of two periodic things with period of size at most 4001, so it can have a period of up to 40012. Luckily, this is still small enough to simulate, and other than that, we can proceed with the same idea as we did for B.

Running E
For E, we proceed again in the same fashion. In this case, the non-periodic area's size can increase to a couple of millions due to the quadratic size of the last period, but that is still well under the memory limits.

Collapse right panel

Qualification Round 2016 - Code Jam 2016

Counting Sheep (7pts, 8pts)

The most natural approach to this problem is to simulate the process: keep track of which digits Bleatrix has seen so far, and then keep generating and checking the numbers she names until she has seen all of the digits 0-9. But are there other cases like N = 0 that go on forever or take an unacceptably long time?

One way to answer this question for the purposes of the problem to just check all possible cases between 0 and 106 before downloading the Large dataset. With a well-written program on a reasonably fast machine, this should take only a few seconds.

More generally, it can be proven that for any N > 0, the sheep doesn't have to keep naming numbers for very long:

Regarding the digit 0: The tenth number that Bleatrix names is 10 times N, and is therefore guaranteed to end in 0.
Regarding digits 1-9: Consider the smallest power of 10 greater than N; call it P. Once the process reaches a number at least as large as P, then the leftmost digit will take on every possible value from 1-9 as the number increases up to (or past) 9P. No digit can be skipped, because that would require the step between successive numbers (which equals N) to be larger than P... but we know that N is less than P because of how we chose P.
Since, by definition of P, 10N ≥ P, we will reach a number larger than P after naming at most 10 numbers, and we will reach a number greater than 9P after naming at most 90 numbers. So, after checking for the special case of N = 0, we can run our simulation without fear of running forever, running for long enough to run out of time, or overflowing even a 32-bit integer. Within the limits of the Small and Large datasets, the worst cases turn out to be 125 followed by any number of zeroes. In those cases, Bleatrix will name 72 numbers before falling asleep.

Qualification Round 2016 - Code Jam 2016

Revenge of the Pancakes (10pts, 10pts)

In the small case we have a stack of at most 10 pancakes. The total number of possible different states we can get by performing any number of flips is thus 210 = 1024. As this is a small number of states, we can solve the small test case by performing a breadth first search over this state space. For N pancakes, this can be implemented to complete within O(N 2N) time.

The large case requires a much more efficient solution. Imagine a large stack of pancakes, starting with a few happy side up, then a few blank side up, a few more happy side up, and so on. Intuitively it seems like a good idea to avoid flipping between two pancakes facing the same direction and instead try and make larger groups of pancakes facing the same direction. Define the "grouped height" of a prefix of the stack of pancakes (possibly the entire stack) to be the number of groups of contiguous pancakes facing the same direction. The goal state has a grouped height of 1. We can consider how various flips affect the grouped height of a stack of pancakes:

If we flip a group of pancakes that has an even grouped height, then the stack's grouped height will be unchanged.
If we flip a group of pancakes that has an odd grouped height, then the stack's grouped height will
increase by 1 if we flip between two pancakes facing the same direction.
decrease by 1 if we flip between two pancakes facing opposite directions.
be unchanged if we flip the entire stack.
Since the above statements categorize all possible flips, a flip can reduce the grouped height of a stack by at most 1. As the goal state has a grouped height of 1, a stack with grouped height of H requires at least H-1 flips to reach the goal state. Considering simple cases, for example those in the sample input, we can see that H-1 flips is not necessarily sufficient. A stack consisting only of blank side up pancakes has H = 1 but we need to flip the entire stack to make the pancakes happy side up. Given a stack where the bottom pancake is blank side up, we need to flip the entire stack at least once as the bottom pancake is only flipped when we flip the entire stack. Flipping the entire stack does not affect the grouped height of a stack so this gives us a stricter lower bound of either H-1 flips if the bottom pancake is happy side up or H flips if the bottom pancake is blank side up.

For a stack with H > 1, we can always flip the topmost group of pancakes facing the same direction to decrease the grouped height by 1. Thus the stricter lower bound is a sufficient number of flips as the following greedy strategy takes exactly that many moves: repeatedly flip the topmost group of pancakes facing in the same direction (reducing the grouped height by 1 each flip) until the grouped height is 1 and then flip the entire stack once if needed.

The problem only asks for the minimum number of flips and not which flips to make. As argued above, this is 1 less than the grouped height if the bottom pancake is happy side up and exactly the grouped height if the bottom pancake is blank side up. A sample implementation in Python is provided below, using the fact that the grouped height is one more than the number of times the string changes from + to - or vice versa.

def minimumFlips(pancakes):
  groupedHeight = 1 + pancakes.count('-+') + pancakes.count('+-')
  if pancakes.endswith('-'):
    return groupedHeight
  else:
    return groupedHeight - 1

Qualification Round 2016 - Code Jam 2016

Coin Jam (10pts, 20pts)

One approach here is to enumerate strings of length N and see if they are jamcoins. To iterate over potential jamcoins, we can iterate over the base-2 interpretations (odd numbers between 2N-1+1 and 2N-1, inclusive) and use a recursive method to convert these to different bases.

For the Small dataset, it is sufficient to use trial division to find factors of potential jamcoins. Concretely, we can look for a nontrivial divisor of an integer k by testing divisibility of every integer in increasing order from 2 to the square root of k, inclusive and stopping if we find one. Searching past the square root of the number is not needed as if k has some nontrivial divisor d, then k / d is also a non-trivial divisor and the smaller of d and k / d is at most the square root of k. A sample implementation of this in C++ looks like this:

long long convertBinaryToBase(int x, int base) {
  // Some languages have built-ins which make this easy.
  // For example, in Python, we can avoid recursion and
  // just return int(bin(x)[2:], base)
  if (x == 0)
    return 0;
  return base * convertBinaryToBase(x / 2, base) + (x % 2);
}

long long findFactor(long long k) {
  for (long long d = 2; d * d <= k; d++)
    if (k % d == 0)
      return d;
  return 0;
}

void printCoins(int N, int X) {
  for (long long i = (1 << N-1) + 1; X > 0; i += 2) {
    vector<long long> factors;
    for (int base = 2; base <= 10; base++) {
      long long x = convertBinaryToBase(i, base);
      long long factor = findFactor(x);
      if (!factor)
        break;
      factors.push_back(factor);
    }
    if (factors.size() < 9)
      continue;

    cout << convertBinaryToBase(i, 10);
    for (long long factor : factors)
      cout << " " << factor;
    cout << endl;
    X -= 1;
  }
}
Solving the large test case may present unique challenges, depending on the programming language used. We're given that N = 32, which means that in base 10 we'll have 32-digit numbers, which we can't store in a 64-bit integer. These numbers are also large enough that running the trial division algorithm on a single prime would take a huge amount of time, potentially longer than the duration of this contest! While we can solve these issues by stopping the trial division early (say, after checking up to 1000) and using arbitrary precision integers, there's actually a much nicer approach!

We can make some observations about jamcoins if we look at the output to the Small dataset from our program above. Considering the first 50 jamcoins of length 16, we find that 18 of them have divisors "3 2 3 2 7 2 3 2 3" and 11 of them have divisors "3 2 5 2 7 2 3 2 11". What's the pattern in these numbers? The numbers 5, 7, 11 from the second list provide a useful hint: they're one more than their respective bases. Giving this some thought, we can notice that the second list of divisors is formed by taking the smallest prime factor of b+1 for each base b. This suggests that b+1 is always a divisor for each of these 11 jamcoins, and we can easily verify that this is true. Understanding the other common divisor list is left as an exercise for the reader.

Note that b+1 in base b is 11b. A simple test exists to determine divisibility of 11 in base-10: a number is divisible by 11 if the sum of its digits in odd positions and the sum of its digits in even positions differ by a multiple of 11. This divisibility rule for 11 can be extended to a simple rule: a string of 0s and 1s which starts and ends with a 1 and has the same number of 1s at odd and even indices is divisible by b+1 when interpreted as a base-b number. Such a string is therefore a jamcoin, but not every jamcoin necessarily matches this condition. A stricter condition that may be easier to notice is that a string of 0s and 1s which starts and ends with a 1 is a jamcoin if all 1s are paired, i.e. it matches the regular expression 11(0|11)*11. For example, in any base b, 11011b = 1001b × 11b.

The last example here also suggests a more general rule. Consider any string p of 0s and 1s with at least two characters, which starts and ends with a 1. Any string which is p repeated multiple times with any number of 0s between repetitions is a jamcoin. In the previous example we have p = 11 and this general rule can be expressed as the regular expression (1[01]*1)(0*\1)+. For example, in any base b, 1110100011101011101b = 11101b × 100000001000001b.

We can use any of these rules to mine jamcoins easily. The Python 2 code below mines jamcoins with exactly 5 pairs of 11s, which finds enough for both the Small and Large datasets.

def printCoins(N, X):
  # N digits, 10 1s, N-10 0s
  for i in range(N-10):
    for j in range(N-10-i):
      for k in range(N-10-i-j):
        l = N-10-i-j-k
        assert l >= 0
        template = "11{}11{}11{}11{}11"
        output = template.format("0"*i, "0"*j, "0"*k, "0"*l)
        factors = "3 2 5 2 7 2 3 2 11"
        print output, factors
        X -= 1
        if X == 0:
          return
  # If we get here, we didn't mine enough jamcoins!
  assert False

Qualification Round 2016 - Code Jam 2016

Fractiles (10pts, 25pts)

This problem is more about analyzing an existing algorithm than writing a new one. Once you understand how more complex artwork depends on the original sequence, you can solve the problem with a short piece of code.

The first thing to notice is that if the original sequence is all Ls, the artwork will be all Ls, no matter what the value of C is. If we choose some set of tiles that all turn out to be Ls for some original sequence other than all Ls, then our solution is invalid, because we won't be able to tell whether the artwork was based on that original sequence or on an original sequence of all Ls. This means we have to come up with a set of positions to check out such that for any original sequence besides all Ls, we will see at least one G.

Small dataset
In the Small dataset, since we can check as many tiles as the length of the original sequence, we may be tempted to try to reconstruct it in full. And while this is possible (we'll get there in a moment), there is an easier alternative. The simplest solution, as it turns out, is to always output the integers 1 through K. It can be easily proved that it works with the following two-case analysis. Let us call the original sequence O, and let Ai be the artwork of complexity i for a fixed O.

1. Suppose that O starts with an L. Let us prove that each Ai starts with O. This is trivially true for A1 = O. Now, if Ai starts with O, it also starts with an L, and since the transformation maps that first L into a copy of O, Ai+1 starts with O. By induction, each Ai starts with O. Then, by checking positions 1 through K, we are checking a copy of the original sequence O, so if there are any Gs in O, we will see a G.

2. Suppose instead that O starts with a G. Let us prove that each Ai starts with a G. This is trivially true for A1 = O. Now, if Ai starts with a G, then Ai+1 also starts with a G, since the transformation maps that G at the start of Ai to K Gs at the start of Ai+1. By induction, each Ai starts with G. Then, since we are checking position 1, we will see a G.

Since we will see at least one G for any original sequence that is not all Ls, and only Ls for the original sequence that is all Ls, we have answered the question successfully. Notice that this also proves that there is no impossible case in the Small dataset.

The proofs above hint at another possible solution for the Small dataset that gets enough information from the tiles to know the entire O. We will explain it not only because it is interesting, but also because it is a stepping stone towards a solution for the Large dataset.

We have seen that position 1 of any Ai is always equal to position 1 of O. Is there any position in Ai that is always equal to position 2 of O? It turns out that there is, and the same is true for any position of O.

Consider position 2 of O as an example. It is position 2 in A1 = O. When A2 is produced from A1, the tile at position 2 of A1 determines which tiles will appear at positions K + 1 through K + K of A2. In particular, the second of those tiles, the tile at position K + 2 of A2, is the same as the tile at position 2 of A1. Then, it follows that position K + 2 of A2 generates positions K*(K + 2 - 1) + 1 through K*(K + 2) of A3, and the second of those tiles, at position K*(K + 2 - 1) + 2 of A3, is also a copy of position 2 of O. You can follow this further to discover which position of AC is equal to position 2, or you can write a program to do it for you. Similarly, for each position of O there is exactly one "fixed point" position in AC that is always equal in value, and you can get those with a program by generalizing the procedure described for position 2. If you check out all of those positions, you obtain a different result for every possible O, which makes the solution valid.

Large dataset
The reasoning that we just used to find fixed points will help us solve the Large. Each position in Ai generates K positions in Ai+1. So, indirectly, each position in Ai also generates K2 positions in Ai+2, K3 positions in Ai+3, and so on. Let us say that a position in Ai+d is a descendant of a position p in Ai if it was generated from a position in Ai+d-1 generated from a position in Ai+d-2 ... generated from position p in Ai. Notice that a G in any given position of any Ai implies a G in all descendant positions. However, if there is an L in position p of Ai, a descendant position (p - 1)*K+d (with 1 ≤ d ≤ K) of Ai+1 will be equal to position d of O. So, position (p - 1)*K+d of Ai+1 is an L if and only if both position p of Ai and position d of O are Ls. If we take this further, we arrive at a key insight: any position of any Ai is an L if and only if a particular set of positions in O are Ls.

We can find those positions by thinking about the orders in which the descendants at each level were produced. For instance, for K=3, position 8 of A3 is descendant number 2 of position 3 of A2, which in turn is descendant number 3 of position 1 of A1. That means that position 8 of A3 is L if and only if positions 2, 3 and 1 of O are all Ls. So, just by looking at position 8 of A3, we know whether the original sequence had a G in at least one of those three positions.

Generalizing this, if we start at position p1 of A1 = O, and take its p2-th descendant in A2, and then take its p3-th descendant in A3, and so on, until taking the pC-th descendant in AC, we have a single position that tells us whether the original sequence has a G in positions p1, p2, ..., pC. And, conversely, for any position in AC, we can find a corresponding sequence of C positions that lead to it. So, each position we check on AC can cover up to C positions of O, and will cover exactly C positions if we make the right choice. Since we need to cover all K positions of the original sequence, that means the impossible cases are exactly those where S*C < K — that is, where getting C positions out of every one of our S tile choices is still not enough. For the rest, we can assign a list of positions [1, 2, ..., C] to tile choice 1, [C+1, C+2, 2C] to tile choice 2, and so on until we get to K. If the last tile choice has a list shorter than C, we can fill it up with copies of any integer between 1 and K. Now all we need to do is match each of these lists to a position in AC, which we can do by following the descendant path (descendants of position p are always positions (p - 1)*K+1 through (p - 1)*K+K). This simple Python code represents this idea:

def Solve(k, c, s):
  if c*s > k:
    return []  # returns an empty list for impossible cases
  tiles = []
  # the list for the last tile choice is filled with copies of k
  # i is the first value of the list of the current tile choice
  for i in xrange(1, k + 1, c):
    p = 1
    # j is the step in the current list [i, i+1, ..., i+C-1]
    for j in xrange(c):
      # the min fills the last tile choice's list with copies of k
      p = (p - 1) * k + min(i + j, k)
    tiles.append(p)
  return tiles

Round 1A 2016 - Code Jam 2016

The Last Word (9pts, 11pts)

The Last Word: Analysis
Small dataset
In the Small dataset, there will be at most 15 letters in S. At each step of the game, we are given a letter to add to either the front or the back of the current word. The number of possible words after adding the i-th letter (during step i) is at most twice the number of possible words after step i-1, since we have two choices of where to add the new letter. This means that the number of possible last words that can be made from all of the letters is at most 215. We can generate each of these possible last words and find which one comes last alphabetically.

Here is one way of doing this in Python:

def alphabetically_last_word(S):
  possible_words = set([''])
  for c in S:
    possible_words = set([c + r for r in possible_words] + [r + c for r in possible_words])
  return max(possible_words)
Large dataset
The approach in the previous paragraph is too slow for the Large dataset, and so some additional observations are required. During step i, we are adding a single new letter Si to the front or the back of our current word Xi-1, yielding a new word Xi = Xi-1Si or Xi = SiXi-1. To have the end result be as alphabetically late as possible, it is always better to have Xi be as alphabetically late as possible as well. We can show this formally: during every step of the process that produces the alphabetically latest answer, after i letters have been chosen, our string Xi should be the alphabetically latest substring that we can produce from the first i letters of S under the given rules.

Assume we are at the i-th step, and we can either add the new letter Si at the beginning or the end of Xi-1. Let Yi be the alphabetically earlier of SiXi-1 and Xi-1Si, and Zi be the alphabetically later of the two. Suppose that Yi were the optimal choice at this step. Then we could write our last word as AYiB for some A and B. We could instead choose Zi and insert the letters during future steps in the same way to yield AZiB as the last word. No matter what the values of A and B are, it is always true that AZiB comes no alphabetically earlier than AYiB, because Zi comes no alphabetically earlier than Yi. This means that any word using Yi can be turned into a word that is at least as late in alphabetical order by substituting Zi at this step instead. It follows that choosing Zi is always correct.

This means that our Xi must be the alphabetically latest of Xi-1Si and SiXi-1. Therefore, when we add Si to Xi-1, we only need to check whether putting Si in the front or putting Si in the back would produce the alphabetically latest string.

Here is some simple Python code that implements the optimized procedure:

def alphabetically_last_word(S):
  result = ''
  for c in S:
    result = max(c + result, result + c)
  return result
Note that the solutions for the Small and Large datasets are very similar. The only difference is that the solution to the Large recognizes which one of the possible words that can be formed at each step will necessarily be part of the optimal last word. Instead of keeping an amount of information that may grow exponentially with the number of steps in the game, the code for the Large keeps track of a single string at each step, allowing it to run much faster and use less memory. The presented solution for the Small dataset requires exponential time and memory, whereas the presented solution for the Large dataset requires only polynomial time and memory.

Another way to think about this is that the max operation commutes with the set-building step inside the code for the Small, allowing us to keep the maximum at each step rather than computing the maximum at the end. This observation shows a path for extending a solution that solves the Small dataset into the one we explained that can also solve the Large dataset. (Check out "A possible stepping stone..." in this essay.)

Round 1A 2016 - Code Jam 2016

Rank and File (14pts, 21pts)

Rank and File: Analysis
Small dataset
A natural first approach is to reconstruct the grid by brute force: try all ways of assigning the lines to the rows and columns until the lines all overlap perfectly, with no conflicts. But as we get into later rounds of Code Jam, pure brute force becomes less likely to work for every Small dataset! In this case, with N = 10, we can have up to 19 lines that we must turn into a 10 x 10 array. Trying all 20! = about 2.4 * 1018 ways of assigning those 19 lines (plus the one empty line that we need to figure out) to the 20 rows and columns would just take too long, and Sergeant Argus isn't going to give us that much time.

Here's one approach that takes advantage of the unusual properties of the grid. Let's continue with our 19-line case. We'll assume (without loss of generality) that the grid has a column missing. In that case, our grid has 10 rows. We can try all 19 choose 10 = 19! / (10! * 9!) = 92378 ways of choosing ten of our lines to call the "rows". But what order should they go in? Well, we know that the values in the first column, like the values in any other column, must be in strictly increasing order. If any two of the "rows" that we've chosen begin with the same number, then we must not have chosen the right set, and we can move on. Otherwise, we know exactly how to order them, so we already have a complete potential grid! Then we can just check the 10 columns to see if 9 of them match the 9 lines we didn't use in this set. If they do, then the remaining line is the answer.

Large dataset
With up to 99 lines, the approach above won't work: 99 choose 50 is roughly 5 * 1028.

It is possible to write a solution that reconstructs such a large grid. But it turns out that we don't need to frame the problem that way at all! We haven't fully taken advantage of the fact that we're only asked for the missing row or column, not for the entire grid.

In a complete set of lists of rows and columns of a grid, every cell of the grid appears twice: once in the list for the row it is in, and once in the list for the column it is in. There may be multiple copies of the same number in the grid, but we can still say that every number appears an even number of times in total within that complete set of lists.

But what about our input set of lists, which is missing the list for one row or column? All of the numbers in that missing list are different, and each of them appears one time fewer in the input than it would in the complete set of lists. This means that all of those numbers appear an odd number of times in our input. Moreover, they are the only numbers that appear an odd number of times in our input!

Therefore, we don't need to build a grid at all. All we need to do is look through all the numbers in all the lists and see which N of them appear an odd number of times; those are the numbers in our missing row/column. We can put them in the order they must go in (strictly increasing), and then we have our answer. And we haven't even done a single push-up!

Round 1A 2016 - Code Jam 2016

BFFs (16pts, 29pts)

BFFs: Analysis
Small dataset
The Small dataset has a pretty low limit of 10 kids. That allows us to try every possible arrangement and check which ones would make valid circles (circles in which every kid is next to their BFF). For every possible subset of kids and every possible circular ordering of them, check that for every kid at least one of their neighbors is a BFF, and if that is the case, update a running global maximum if it is greater than the size of the circle we just checked.

There are a number of small but powerful optimizations to this approach. Notice that the permutations of a subset of kids S always appear as a prefix of the permutations of any other subset of kids that includes S. Moreover, the only difference in BFF checking is what we do for the first and last kid. So, we can just check all the permutations of N kids and consider whether each prefix forms a valid circle, and that accounts for all permutations of all subsets while reducing the number of total checks substantially. Note that some subsets are checked multiple times, but that is unimportant, as long as the procedure runs in the allotted time.

There are other possible optimizations, but this is more than enough. The following Python code implements the approach outlined above:

import itertools
# The F parameter is the list of BFF identifiers, but 0-based (subtracting 1 from the input).
def cc(F):
  n = len(F)
  r = 0
  # Iterate over all possible orderings of the n kids.
  for O in itertools.permutations(xrange(n)):
    first = O[0]
    second = O[1]
    for i in xrange(1, n):  # Iterate over the permutation, skipping the first.
      # Check if i can be the last one by checking it and the first.
      prev = O[i - 1]
      cur = O[i]
      if ((F[cur] == first or F[cur] == prev) and
          (F[first] == cur or F[first] == second)):
        r = max(r, i + 1)
      # Check if i can be in the middle, and stop if it can't.
      if F[cur] != prev and (i == n - 1 or F[cur] != O[i + 1]):
        break
  return r
Large dataset
Of course, a simple brute force approach, even with many additional optimizations, will not be fast enough for the Large dataset. Let's examine the input more closely. It is actually a function BFF that maps each kid to another kid. We can represent this function with a graph where the nodes are kids and the edges go from each kid to that kid's BFF. As you can see from the linked article, this type of graph has a particular property: each connected component is made up of a directed cycle and branches of nodes with the edges directed towards the cycle. To visualize it better, if we compressed the cycle into a single node, we would obtain a tree with all the edges pointing towards the root. Here's an important fact that will come up a lot: each connected component contains exactly one cycle.

Now that we have examined the input a bit, let's examine the output, or, better yet, let's examine the form of a valid circle. It must contain at least one kid k1. It must also contain k1's BFF, k2, who must be sitting next to k1. And k2's BFF k3 (who might or might not be k1), and so on. The BFF of the BFF of the BFF ... of k1 must be in the circle. In terms of the graph we mentioned above, we are starting on the node representing the first kid k1, and moving through the edges. Therefore, we will eventually end up cycling through the cycle in k1's connected component. (That cycle may or may not include k1.) This a second important property: for any connected component containing at least one kid who is in the circle, all of the kids in that component's cycle must be in the circle..

Consider a connected component with a cycle of more than 2 kids. If we put that cycle in the circle, there is no room for anyone else, as the cycle already forces the two neighbors of each kid. So, one possibility is that the final circle consists entirely of a single cycle from the graph.

Since nobody is their own BFF, there are no cycles with just 1 kid. If we consider a connected component with a cycle of exactly 2 kids l and r, the situation is different. We can sit l and r together, and we already know they are both happy, and we have room on l's left and r's right (or vice versa, but it is equivalent) to seat more kids. We could choose anybody, even from another component. However, we want the maximum number of kids, so we might as well choose an l1 whose BFF is l to sit next to l (if there exists such an l1). l1 is already happy, so can choose an l2 whose BFF is l1 and sit it next to l1. And we can continue this process. We can build a chain of kids to l's left following the edges of the graph in reverse, and similarly, we can build a chain of kids on r's right. When we are done, having added zero or more kids to each side, we have a line of kids from the same component that are all happy, so we can continue to add kids from other components right next to them!

To summarize the previous paragraph, we can build a chain of kids from each component with a cycle of length 2 (we already showed that cycles longer than that do not allow chains to be added). Since we want to construct the largest possible chain, we take the longest chain from each component with a cycle of length 2, and put them all together. That is, we sum their lengths as a possible final result to be compared with the largest cycle from the first case.

The following image illustrates the previous paragraph. On the left, a graph with three separate connected components is displayed. Red nodes are nodes in the cycles of each component. From the component with a cycle of length 4, we can build a circle (which we've shown on the right side of the image) but not add anything else. However, we can build a chain from each component with a cycle of length 2. Marked in green and orange are optimal choices for a chain on each side, and on the right you can see the circle of kids with the arrows indicating their BFFs at their side. You can see here how the cycles of length 2 allow us to add more kids, even including different connected components of the graph in the same circle, whereas longer cycles don't leave room for anyone else.

It is easy to find the connected components and their cycles using DFS or a number of other ways that we leave up to the reader to find and choose. There are also a number of ways to find the longest chains on each side of each cycle of length 2, that we also leave up to the reader. Notice that the process on each side is really similar to finding the height of a tree.

Round 1B 2016 - Code Jam 2016

Getting the Digits (11pts, 12pts)

Getting the Digits: Analysis
Small dataset
Since the string can be no longer than 20 letters in the Small dataset, the phone number can be no longer than six digits (since the digits with the shortest spelled-out length, ONE and TWO, have three letters, and 7 * 3 > 20). A key observation is that this means there are no more than a million possible answers; in fact, there are far fewer than that, since only phone numbers with digits in nondecreasing order are valid.

So, you can look at all possible phone numbers from 0 to 9, 00 to 99 (note that 00 is different from 0), and so on, up to 000000 through 999999, and save the numbers that have their digits in nondecreasing order. (In fact, for that last range, you only need to go up to 222222.) Then, turn each of those into letters. There are multiple possible orderings for those letters — 1 could be given as EON, ENO, or NEO, among others — but you can just take the alphabetically ordered version. 0 becomes EORZ, 1 becomes ENO, ..., 01 becomes EENOORZ, and so on.

Then you can make a dictionary (hash table) with these strings as keys and the phone numbers as values. (It turns out that each of those keys maps to only one phone number — more on that later.) To solve a test case, take the string your friend gave you, sort it alphabetically, and then look that up in the dictionary. (Your friend could have given you the same string in any order, so you can permute it however you want without changing the identity of the underlying phone number.)

There's no need to generate this dictionary every time you solve a test case; you can do it just once, before your program starts processing test cases. You can even produce the entire dictionary before you download an input file and store it in your source code or in a separate file that your source code reads, as long as this does not cause your source code to exceed the standard limit of 100kB.

Large dataset
The dictionary for all 1000-digit phone numbers would be too huge to generate and store beforehand, let alone produce on the fly.

One tempting approach is to greedily remove spelled-out digits: for instance, keep taking the letter set NINE (one E, one I, and two Ns) away until you no longer can, then keep taking the letter set EIGHT away, and so on. But this won't necessarily work — what if there are no 9s in the actual phone number, and the first NINE you try to take away is really the N from a SEVEN, the I from a SIX, and the NE from a ONE? This will eventually leave you with a mess of letters from which no digit's letters can be removed!

It turns out that you can make this method work... as long as you pick the right order in which to remove the digits! For instance, FOUR is the only digit out of the ten that contains a U. So, if you see three Us in the string, there must be three instances of FOUR. You can safely remove three Fs, three Os, three Us, and three Rs, and record that the phone number has three 4s. Once all the FOURs are gone, FIVE is the only remaining digit with an F, so you can remove as many FIVEs as there are Fs, and so on. If you had tried to do this in the other order, starting by removing as many FIVEs as there were Fs, it might not have worked, depending on how many FOURs, SIXes, SEVENs, etc. were in the number!

Here is one safe order in which to remove digits, based on their letters that are unique at the time of removal: ZERO, SIX, EIGHT, TWO, FOUR, FIVE, SEVEN, THREE, NINE, ONE. Note, for example, that even though all of the letters in ONE appear in other digits besides ONE, by the time we get to removing ONEs, there are only ONEs left.

The existence of an ordering like this also explains why we didn't have to worry about two different phone numbers having the same alphabetized string in our dictionary for the Small solution. Two different phone numbers cannot produce the same string because no subset of digit words is a linear combination of other digit words. This would not necessarily hold for arbitrary sets of words, though. For example, in a language in which the digit words are AB, AC, BD, and CD, given the string ABCD, it is impossible to know whether it was formed from one AB and one CD, or from one AC and one BD.

Round 1B 2016 - Code Jam 2016

Close Match (10pts, 23pts)

Close Match: Analysis
Small dataset
A brute force algorithm will work just fine for the Small. For each case, we can try all possible ways to fill in the ?s and see which way produces the smallest absolute difference. It is important to break any ties first by the first score, or, if necessary, by the second score. Even in the worst case for brute force, ??? ???, there are only 106 = one million different possibilities to check.

Large dataset
Many Code Jam problems look complex and challenging, but have a surprisingly simple solution. But sometimes we throw in just the opposite sort of problem! In this case, it's easy to come up with a tantalizing greedy approach that will sometimes fail. We'll present an approach that gets greedy only when it's safe to do so.

We will investigate various different ways to fill in the ?s, and we will constantly keep track of the best set of scores (using tiebreaker rules) seen so far. We will look at the pair of characters in the first position of each string, then the pair of characters in the second position of each string, and so on.

When we look at a pair of characters, we must decide how to fill in any ?s. We may be able to make those two digits of the final scores be the same, either by filling in ?s appropriately, or because the two characters are already the same digit and we have no choice. If we're lucky, and we never encounter a position where the two strings have different digits, we can make every pair of digits in the final scores the same! This is as good as it gets; our difference will be 0.

For example, for the case ?1? 2??, we can make all the digits equal to get 210 210. We choose 0s for any pair of ?s to satisfy the tiebreaker rules.

However, we won't have this option in every test case. If the strings have different digits at some position, then there's no way the score difference can be 0. Moreover, we may even need to introduce our own difference before that first existing point of difference! For instance, in the case ??0 ?99, we don't want to just make the digits equal until we get to the first point of difference, because that would leave us with 090 099, whereas we can do better with 100 099.

So, if we have two different digits at any position in the strings, then either we can make everything equal up to that first existing point of difference, or we can introduce a first point of difference earlier. That is, we will start from the left, and for some number of positions (possibly zero), we will make the digits in the final scores equal. Then we will encounter (or make) our first point of difference.

Once we've reached our first point of difference, we know something crucial: which of the two final scores is larger. Because the scores have been equal up until the first point of difference, the score with the larger digit in this position is guaranteed to be larger, no matter how we fill in any ?s beyond this point.

Moreover, we know exactly how to fill in those ?s to minimize the difference between the two scores! We have no reason to make the larger number any larger, so all of its ?s should become 0s. And we want to make the smaller number as large as possible, so all of its ?s should become 9s.

So, as we step through our strings from left to right, for every position of the strings, we'll do the following:

If we encounter two identical digits, move on.
If we encounter two different digits, fill in all remaining ?s, compare those scores with the best we've seen so far, and stop. We're done!
If we encounter a digit and a ?:
If the digit is not 9: try changing the ? to 1 more than that digit. This would introduce a first point of difference, so we know how we would fill in all remaining ?s. Compare those scores with the best we've seen so far. Then...
If the digit is not 0: try changing the ? to 1 less than that digit. This would introduce a first point of difference, so we know how we would fill in all remaining ?s. Compare those scores with the best we've seen so far. Then...
Change the ? to that digit and move on. (Why didn't we try changing the ? to all possible digits? We can get away with doing this, but it's not necessary. If we have control over the digits at the first point of difference, there's no reason to make them differ by more than 1; this would just make the overall difference larger, regardless of what we do with the rest of the strings.)
If we encounter two ?s:
Try changing the first ? to a 0 and the second ? to a 1. Figure out the scores and compare. Then...
Try changing the first ? to a 1 and the second ? to a 0. Figure out the scores and compare. Then...
Change both ?s to 0s and move on. (Again, there is no reason to make the ?s differ by more than 1.)
We must make sure our implementation also checks the case in which there is no point of difference. Once we're done, the best scores will be our answer.

This approach has some room for improvement, but it solves the Large very quickly. It makes one full pass through the strings, and at each position, it might make up to 2 additional partial passes, so the worst case running time is O(N2), where N is the length of C (and J). We leave an O(N) approach as an exercise!

Round 1B 2016 - Code Jam 2016

Technobabble (14pts, 30pts)

Technobabble: Analysis
Small dataset
Each topic on the list could be "faked" or "un-faked." One natural brute force solution is to enumerate all possible orderings of the topics, and pick the one that has the most faked topics.

It's easy to check whether a topic could have been faked: simply check whether the first word of the topic appears as a first word earlier in the list, and the second word of the topic appears as a second word earlier in the list. Note that marking a topic as faked does not change whether or not topics later in the list can be marked as faked, so using this strategy, it is optimal to always count a topic as faked if possible.

However, there are N! possible orderings, which is too large to enumerate even for the small dataset where N ≤ 16 (16! is on the order of 21 trillion). We need a better approach. Rather than trying to maximize the number of faked topics, let's think about the reverse problem: trying to minimize the number of un-faked topics.

The key observation is that any possible set of un-faked topics must contain every first word at least once and every second word at least once — otherwise, there would be a faked topic that contained a word that was not available for the faker to use. Conversely, any set containing every first word at least once and every second word at least once could be a possible set of un-faked topics — simply put all the topics from the un- faked set at the top of the list. So, the question we need to answer is this: What is the smallest set of topics that contains every first word at least once and every second word at least once?

A brute-force approach, which works for the Small dataset, is to enumerate all subsets of topics and pick the subset with the fewest topics that covers every first word at least once and every second word at least once. Since there are 2N subsets, this solution runs in exponential time, which is fine for the Small dataset (216 = 65,536).

Large dataset
For the large dataset, the exponential time solution will not work. It turns out that there is a polynomial time solution to the problem. We will illustrate the solution using graph theory.

Let each word be a vertex in a bipartite graph in which each topic is an edge connecting two vertices. The sample input below corresponds to the following graph (we'll explain the colors of the edges in a moment).

HYDROCARBON COMBUSTION
BIOMASS COMBUSTION
QUAIL COMBUSTION
QUAIL BEHAVIOR
QUAIL CONTAMINATION
GROUNDWATER CONTAMINATION
GROUNDWATER HYDROLOGY
Bipartite graph in which each node is a word and each edge is a topic.

The problem we're trying to solve on this graph is the minimum edge cover; that is, finding the smallest set of edges such that each vertex is connected to at least one edge. This corresponds to the smallest set of topics containing every first word at least once and every second word at least once.

The minimum edge cover problem is related to finding a maximum matching of a graph (the largest set of edges without any common vertices): the two will always have the same number of connected components. If it's not immediately obvious why this is the case, convince yourself by drawing some graphs on paper and trying to come up with a counter-example. We can additionally observe that every vertex left out of a maximum matching must be connected to a vertex in the maximum matching; otherwise, we could have added that pair to the maximum matching. With these facts, we can use a two-step algorithm to compute a minimum edge cover on our bipartite graph:

Find a maximum cardinality bipartite matching of the graph, which can be done in polynomial time using an approach such as the Ford-Fulkerson algorithm or the Hopcroft–Karp algorithm. One such matching is shown above in red.
Iterate over the remaining edges, and greedily add edges that connect to an unused vertex. The edges added by this step are shown above in blue.
In fact, all we really need to know is the size of a minimum edge cover: the number of edges in a maximum matching plus the number of vertices not included in a maximum matching. The solution to the problem is then simply the total number of edges (topics) minus the size of a minimum edge cover.

Round 1C 2016 - Code Jam 2016

Senate Evacuation (8pts, 10pts)

Test set 1
With at most three parties and at most nine senators, various brute force approaches will work. One exhaustive strategy is to generate all possible different evacuation orders, treating senators from the same party as interchangeable, and then try all possible different ways of chopping those into groups of one or two senators.

Another, simpler strategy is to keep randomly choosing and trying one of the nine possible evacuations (A, B, C, AA, AB, AC, BB, BC, CC) as long as the chosen senator(s) exist and the evacuation will not cause a new absolute majority. You may worry that this strategy could get stuck, but the outcome of any legal evacuation will just be another possible test case for the problem, and the statement guarantees that every test case has a solution! With more parties and senators, though, this strategy might bog down in the details of checking the legality of evacuations, so we should come up with a more efficient approach.

Test set 2
Intuitively, it is safest to remove one senator at a time, and to always draw from whichever party has the most remaining senators (or any such largest party, if there is a tie). But this strategy won't always work! For example, if we have two senators from party A and two from party B, and no others, which is a valid test case, then removing one senator from either party will give the other party an absolute majority.

However, this strategy is always safe whenever there are more than two parties present. Suppose that party 1 is currently the largest, or tied for the largest, of at least three parties, and that we remove a single senator from party 1. Clearly, making party 1 smaller cannot give it an absolute majority that it didn't have before. But could some other party acquire an absolute majority as a result? Suppose that the removal of a senator from party 1 were to cause party 2, which currently has X senators, to have an absolute majority. But since party 1 was the largest, or tied for the largest, before a senator was removed, party 1 must still have at least X-1 senators. Moreover, since at least one more party is present, there is at least 1 other senator who is not from party 1 or 2. So there are a total of at least X remaining senators who are not from party 2, which means the X senators of party 2 are not enough to give it an absolute majority, so we have a contradiction.

If we start with three or more parties and keep evacuating a single senator from the largest party in this way, then at some point, we must reach a step in which we go from three parties to two parties. These two remaining parties must have only one senator each. Since we just removed the one remaining senator from the third party, it must have been a largest party, so the other two can be no larger. So we can remove this last pair of senators in a single evacuation as a final step.

What if we start with two parties? Since the problem statement guarantees that no party begins with a majority, these parties must have equal numbers of senators. So, we can evacuate them in pairs, one from each party, until the evacuation is complete.

This approach takes more steps than are needed — most of those single evacuations can be paired up — but it gets the job done.

Round 1C 2016 - Code Jam 2016

Slides! (13pts, 21pts)

Slides!: Analysis!
Small dataset
The Small dataset has bounds that suggest we can construct all possible sets of slides, but this turns out to be overly optimistic. We represent a set of slides as a directed graph G, with each node representing a building, and a directed edge from node i to node j representing a slide leading from building i to building j. The most straightforward construction tries either including or not including each of the B * (B-1) possible slides, for a total of 2B * (B-1) possible sets of slides. Unfortunately, even for B = 6, this is too many: there are approximately 230 sets to check, or about a billion.

However, one observation allows us to dramatically cut down the number of sets we have to examine. Notice that there can never be a cycle as part of any valid path from building 1 to building B. If there were a cycle, then we could generate new, valid paths by traversing that cycle arbitrarily many times before continuing to our destination, meaning that the number of valid paths would be infinite. (The G that we use can still contain a cycle that is not on any valid path; however, removing that cycle would not affect the number of valid paths, and thus we only need to consider graphs G with no cycles at all.)

This means that any valid path from building 1 to building B cannot visit the same building twice, so each path can have length at most B. As a result, running a depth-first search on G starting from node 1 will take O(B) time for each path found. If we find more than M paths, then we can terminate our search immediately, since this set of slides cannot be valid. This means that our worst-case running time to test any given set of slides is O(M*B). We can also calculate a smaller upper bound on the number of sets we have to examine: for each pair of slides i and j, exactly one of three possibilities must be true:

There is a slide from i to j.
There is a slide from j to i.
There is no slide from i to j and no slide from j to i.
Since there are B * (B-1) / 2 different pairs of slides, this gives us an upper bound of 3B * (B-1) / 2 possible sets of slides. For B = 6, this number is around fourteen million, which is a manageable number of sets to check.

Another helpful observation that makes the small even more tractable is that since our graph has no cycles, it is a directed acyclic graph, and so it has a topological sorting. So, for any correct solution, we could renumber the buildings (other than 1 and B) such that every slide's end building has a larger number than its start building. Since this is true, we only need to consider slides that go from lower to higher building numbers.

Large dataset
The Large dataset requires a more efficient approach. A natural first question to ask is: what is the maximum number of paths from building 1 to building B that we can possibly construct? One straightforward construction that seems to yield a large number of paths is to construct a slide from building i to building j for every pair of positive integers i, j with 1 ≤ i < j ≤ B. To compute the number of paths for this set of slides, notice that every path from building 1 to building B corresponds uniquely to a set of distinct integers from the set {2, ..., B-1} representing the buildings visited along that path. For example, if B = 5, then the set {2, 4} would correspond to the path 1 -> 2 -> 4 -> 5, and the empty set would correspond to the path 1 -> 5. Since there are B-2 integers strictly between 1 and B, each of which can be either absent or present in a set, there are 2B-2 unique sets that can be constructed, and thus 2B-2 possible paths from 1 to B.

But is this the largest possible number of paths we can construct? It turns out that it is. We can show this by the pigeonhole principle. We assume that there exists some set of slides that yields some number M > 2B-2 paths, and derive a contradiction. Each path corresponds to some set of distinct integers {2, ..., B-1} representing the buildings visited along that path, and since there are 2B-2 distinct such sets, it follows that two paths of slides must visit the exact same buildings. This means there is some pair of buildings i and j such that i is visited before j on one of these paths, but is visited after j on the other path. (If there were no such pair, then these two paths would be exactly the same, since no building can ever be visited twice.) Since we can reach building i from building j and vice versa, it follows that there is a cycle between the two buildings. This contradicts what we showed earlier, meaning that we cannot construct a set of slides with exactly M paths for M > 2B-2.

Now we show how to extend the ideas above to handle the case where M < 2B-2. We start by constructing all possible slides from building i to building j for every pair of positive integers i, j with 2 ≤ i < j ≤ B. Notice that there are exactly 2B-1-i ways to get from building i to building B. Each path from building i to building B maps uniquely to a subset of distinct integers from the set {i + 1, ..., B-1}. This set contains B-1-i integers, so there are 2B-1-i possible subsets that we can choose. If we build a slide from building 1 to building i for i strictly between 1 and B, then this increases the number of ways to get from 1 to B by exactly 2B-1-i, since there are that many ways to get from building i to building B. This suggests a method for generating a network with exactly M slides. We start by writing M in binary. If the i-th digit of M (counting from the right, starting from 1) is a 1, then we add a path between building 1 and building B-i. This will add 2i-1 new paths to our slide network. If we repeat this process for each value of i, then we will end up with a network with a number of paths from 1 to B exactly equal to M. This process will work if M has at most B-2 digits, meaning M ≤ 2B-2-1. Since M = 2B-2 is the largest value we are able to construct, this gives us a method for constructing all values of M between 1 and 2B-2. We have previously shown a construction for M = 2B-2, which is equivalent to the solution for M = 2B-2 - 1 with an additional path from building 1 to building B.

This means that a sequence of slides is therefore possible to construct if, and only if, M ≤ 2B-2, and the above construction works for any such M. The sequence itself is computed by the construction above.

To illustrate the above method, here are valid answers for all cases with B = 5, and M = 1 through 8:

M = 1 M = 2 M = 3 M = 4 M = 5 M = 6 M = 7 M = 8

00010 00100 00110 01000 01010 01100 01110 01111
00111 00111 00111 00111 00111 00111 00111 00111
00011 00011 00011 00011 00011 00011 00011 00011
00001 00001 00001 00001 00001 00001 00001 00001
00000 00000 00000 00000 00000 00000 00000 00000

Observe that the solutions only differ in their first lines. The first lines of the solutions for M = 1 through 7 are 1, 2, ..., 7 in binary plus an extra 0 at the end. The first line of the solution for M = 8 is 7 in binary, plus an extra 1 at the end: the direct connection from building 1 to building 5 that brings the total to 8. For M ≥ 9, the answer is IMPOSSIBLE.

Round 1C 2016 - Code Jam 2016

Fashion Police (14pts, 34pts)

Fashion Police: Analysis
Small dataset
Let us denote the number of possible outfits by W; W = J * P * S. A viable brute force approach is to produce all 2W subsets of the set of possible outfits, check them for violations, and take one of the largest sets with no violation. We can check a set of outfits without worrying about what order they are worn in; if some outfits in that set create a fashion violation, it will eventually happen no matter what order those outfits (and any other outfits) are worn in. Another helpful observation is that if S ≤ K, then the answer is trivial: simply return every possible outfit.

This approach works for most of the Small cases, but not all of them. For the cases 3 3 3 1 and 3 3 3 2, there are 227 possible sets of outfits, and that is over 100 million. One approach is to just solve these "problem cases" before downloading a dataset. You can notice a pattern in the other answers (more on this later) and infer that the maximum numbers of outfits you can wear are 9 and 18, respectively. It is more tractable to find a set of outfits of a particular size than to find the set of outfits of maximum size. If you are pretty sure that that size is 18, for instance, you can check all sets of size 18 until you find a working one, and maybe even check all sets of size 19 to confirm that none of them work. This is much faster than checking every size between 1 and 19, especially since 27 choose 18 is much smaller than 27 choose 13 and 27 choose 14, for example.

In fact, there are only 100 possible test cases that fall within the limits of the Small dataset, so you can compute the answer to any possible input before even downloading the dataset!

Large dataset
Again, if S ≤ K, we can return every possible outfit. Otherwise, a key insight is that that the pigeonhole principle tells us that a maximum solution can have no more than min(J × P × K, P × S × K, J × S × K) different outfits. Since J ≤ P ≤ S, this puts an upper bound of J × P × K on our solution. You can also infer this from the output of a brute force solution.

One pitfall to watch out for in this problem is that there exist sets of outfits that are maximal — that is, you cannot add any new outfit to such a set without going to jail — but are not the largest possible. That is, they are maximal but not maximum.

For instance, for an input of J = 1, P = 3, S = 3, K = 2, the following is a maximal set of outfits that is not maximum (the maximum size is 6): 1 1 1, 1 1 2, 1 2 2, 1 2 1, 1 3 3.

So, if we just randomly choose outfits without violating the law, we are in danger of being trapped in a locally maximal set.

Fortunately, there are several greedy approaches to achieve a set of size J × P × K outfits without angering the Fashion Police. As argued above, we know that J × P × K is the maximum possible size, so if we can find a set of that size, we are done!

We cannot use a jacket-pants combination more than K times, so if we want to produce J × P × K outfits we are forced to use each combination exactly K times. To simplify the math, we use jacket, pants and shirt numbers between 0 and the appropriate total minus 1. We just need to remember to add 1 to every identifier when we print it to the output.

Let us fix a combination with jacket number j and pants number p. Our proposal is assign to it shirts (j + p) % S, (j + p + 1) % S, ..., (j + p + K - 1) % S, where % stands for the modulo operation. Since S > K, these are all different, and by construction, the combination of jacket and pants is used exactly K times, as desired.

What about jacket-shirt and pants-shirt combinations? Let us fix a jacket number j and a shirt number s. If the outfit (j, p, s) appears in the constructed set, then s = (j + p + d) % S for some d between 0 and K - 1, inclusive. Then, by modular arithmetic, and noticing that j % S = j, p % S = p, and s % S = s, it turns out that p = (s - j - d) % S. Then, each choice of d uniquely determines p, so there cannot be more ps to go with a given combination of j and s than choices of d, and there are K choices of d, which means the combination of j and s does not exceed the maximum.

Since this is valid for any j and s, and a symmetrical proof is valid for each pants-shirt combination, we have proven that the proposed set of outfits does not break any of the rules of the Fashion Police.

Another way of thinking about it
The problem is equivalent to trying to select as many cells as possible in a 3-D J by P by S grid such that no line of three cells in the x, y, or z direction includes more than K selected cells. Each outfit corresponds to one such cell.

Here is an illustration of this approach for two cases. The left-right axis corresponds to shirts, the up-down axis corresponds to pants, and the layers (imagine them stacked up) correspond to jackets. Each * represents a selected outfit, and each . represents an unused outfit.

J = 2, P = 3, S = 4, K = 1:

*... .*..
.*.. ..*.
..*. ...*

Outfits: 1 1 1, 1 2 2, 1 3 3, 2 1 2, 2 2 3, 2 3 4.

J = 2, P = 3, S = 4, K = 2:

**.. .**.
.**. ..**
..** *..*

Outfits: 1 1 1, 1 1 2, 1 2 2, 1 2 3, 1 3 3, 1 3 1, 2 1 2, 2 1 3, 2 2 3, 2 2 4, 2 3 4, 2 3 1.

Observe that:

In both cases, the second layer (jacket 2) is the first layer (jacket 1) shifted one unit to the right (and wrapping around).
We can get the answer to the second case from the answer to the first case just by adding a * to the right of every existing * (and wrapping around).
By construction, the first layer does not have more than K *s in any row or column. This is also true of each additional layer, since they are all rotations of the first layer. Moreover, no line of cells parallel to the jacket axis (that is, across layers) can possibly have more than K *s; considering how the layers are constructed, that would directly imply more than K *s in one row of the first layer.
This construction works for any set of J, P, and S values. It takes advantage of the J ≤ P ≤ S condition in the problem statement. We put that condition in to save you the trouble of adding a bunch of case-based logic to try to figure out which dimension was largest; the fashion-conscious GCJ staff is well aware that some closets have more pants than shirts!

Round 2 2016 - Code Jam 2016

Rather Perplexing Showdown (4pts, 14pts)

Rather Perplexing Showdown: Analysis
There are multiple ways to attack this problem. We will present two methods for building the correct tournament tree, and a method for finding the arrangement of that tree that produces the alphabetically earliest lineup. It is possible to combine the tree building and tree optimization methods into a single algorithm, but we present the analysis this way for ease of explanation.

Building the tree: starting from the beginning
Let's start from the beginning of the tournament and create each new round. At any point, you have some number of Rs, Ps, and Ss remaining, and you can only create RP, RS, and PS matches, because anything else would result in a tie. Call the number of RP matches you will create x — that is, you will make x of the Rs match up with x of the Ps. Then all other Rs must face Ss, so you will create R-x RS matches. There will be P-x leftover Ps and S-(R-x) leftover Ss, and these numbers must be equal to avoid creating tied matches, so P-x = S-R+x and x = (R+P-S)/2. If this x causes an impossible situation (e.g, there must be more RP matches than there are Rs or Ps), then the answer is IMPOSSIBLE. Otherwise, match the players accordingly, note the winners (all RPs become Ps, all RSs become Rs, and all PSs become Ss), and then you have a smaller instance of the same problem. This strategy tells you whether the tournament will end, and how to make all your matchups; with that information and some careful bookkeeping along the way, you can generate the entire tree.

Building the tree: starting from the end
Let's start from the end of a tournament instead. Suppose that the winning player is a P. What do we know about the match that produced that winner? One of the participants must have been that P, and the other must have been the opponent that the P defeated, namely, an R. That R must have defeated an S, and so on. That is, for any node in the tournament tree, including the bottom (winning) node, we can easily regenerate the entire part of the tree that led to it!

This also implies that for a given N, there is only one possible (R, P, S) triplet that will produce a successful tournament ending in R, and likewise for P and S. Almost all triplets are doomed to fail! There are only three valid ones for any N, and each of them must produce a different winner.

So, we can try all three possible winners (R, P, and S) for every value of N from 1 to 12, and store the resulting tournament trees and their numbers of Rs, Ps, and Ss. Then, for each test case, either the given N, R, P, and S values match one of the stored trees, or we know the case is IMPOSSIBLE.

Finding the alphabetically earliest lineup
Having the tournament tree is not enough, because a tree can generate many possible lineups. For any internal (non-leaf) node in the tree, you can swap the two branches; this does not change the tree, but it does change the initial lineup! For instance, the lineups PSRS, PSSR, RSPS, RSSP, SPRS, SPSR, SRPS, and SRSP all represent the same tree. There are 2N-1 internal nodes in the tree, and we can't try all 2 to the (2N-1) ways of flipping or not flipping each of them. Fortunately, we don't have to.

Consider any pair of players who face off in the first round; let's say they're using moves X and Y, where X is alphabetically earlier than Y. These two players will contribute to two consecutive characters in the lineup; either XY or YX, depending on whether we flip their node. Flipping other nodes in the tree may move this pair of characters around in the final lineup, but it cannot reverse or separate them. So we have nothing to lose by choosing XY; this decision is totally independent of whatever we do with other nodes later. More generally, for any node, we should put the "alphabetically earlier" branch before the "alphabetically later" branch. Moreover, we should optimize shallower nodes in the tree before optimizing deeper nodes, so that we can be sure that we're only making decisions about branches that are already themselves alphabetically optimized.

So we can start with any lineup corresponding to our tree (ideally, whatever came out of our algorithm earlier), and first examine the lineup in 2N-1 chunks of length 2 and swap the letters in each chunk whenever that would make the chunk alphabetically earlier. Then we can examine the lineup in 2N-2 chunks of length 4, and swap the subchunks of length 2 in each chunk whever that would make the chunk alphabetically earlier. And so on, until we've examined and possibly swapped the 2 chunks of length 2N-1; that final lineup will be our alphabetically earliest answer.

Round 2 2016 - Code Jam 2016

Red Tape Committee (5pts, 17pts)

Red Tape Committee: Analysis
This problem poses two challenges: figuring out which sets of members to consider as possible committees, and calculating the tie probability for each committee. In the Small dataset, brute force will suffice for both of these subproblems. However, in the Large dataset, we will need more efficient methods for both of them.

Who should we choose?
One might think that in order to create ties, we should choose from among the most "moderate" department members — that is, the ones with Yes probabilities closest to 0.5. In fact, the opposite is true! The best way to create a tie is to choose department members from one or both extremes. That is, we should choose the M (possibly zero) of the department members with the lowest Yes probabilities, and the K - M of the department members with the highest Yes probabilities. This makes sense intuitively; to take an extreme case, a committee of two members with Yes probabilities of 0.00 and 1.00 will always tie, whereas a committee of two members with Yes probabilities of 0.50 and 0.50 will tie only half the time. Experimentation bears this idea out. But how can we prove it?

Without loss of generality, let's sort the members in increasing order of Yes probability. Suppose that we have chosen a committee of these members that maximizes the tie probability. If there are multiple such committees, suppose that we have chosen the one that also minimizes the sum of the members' indexes in that sorted list.

We'll show that this set consists of the M (possibly zero) leftmost members and the K - M rightmost members, as described above. Suppose that there exist the following: a member X, who is in our set, and members Y and Z, who are not in our set, and that they are in the left to right order Y, X, Z. Fix all the other members and consider the tie probability as a function of member X's Yes probability. This is a linear function. If it has slope 0, then we can get an equally good set with a smaller sum of member indices by replacing X with Y. If it has slope > 0, we can get a better set by replacing X with Z. If it has slope < 0, we can get a better set by replacing X with Y. So X must not exist!

Therefore, we can try all values of M and consider only those committees. This linear search adds a multiplier of O(K) to the running time of the calculation of tie probabilities. The one-time sort also adds a single O(N log N) term.

How likely is a tie?
For a large committee, we cannot explicitly consider all 2N possible voting outcomes. Many of these outcomes are very similar, and we would do a lot of redundant work. This is an ideal situation for dynamic programming.

Let's build a table in which the columns represent the committee members, the rows represent the total number of Yes votes so far, and the numbers in the cells measure the probability of being in the situation represented by that row and column. We start with a 1.00 in the upper left cell, which represents the situation before anyone has voted; there is a 100% chance that there will be no "Yes" votes at this stage. Let's consider a committee with Yes probabilities of 0.10, 0.20, 0.50, and 1.00. We will label the columns in that order (although the order does not matter).

- init 0.10 0.20 0.50 1.00
0 1.00 ---- ---- ---- ----
1 ---- ---- ---- ---- ----
2 ---- ---- ---- ---- ----
3 ---- ---- ---- ---- ----
4 ---- ---- ---- ---- ----
When the first member votes, either the vote will be "Yes" with 10% probability (and we will have one Yes vote), or "No" with 90% probability (and we will have zero Yes votes). So the 1.00 value gets split up among two cells in the next column: the "0 Yes votes after 1 member has voted" and "1 Yes vote after 1 member has voted" cells.

- init 0.10 0.20 0.50 1.00
0 1.00 0.90 ---- ---- ----
1 ---- 0.10 ---- ---- ----
2 ---- ---- ---- ---- ----
3 ---- ---- ---- ---- ----
4 ---- ---- ---- ---- ----
Let's look at the "0 Yes votes after 1 member has voted" cell, which represents 90% of all possible situations after the first member has voted. That probability will feed into two of the cells in the next column: the one just to the right, and the one just below that. Since the second member has an 80% probability of voting No, 80% of that 90% possibility space branches off to the "0 Yes votes after 2 members have voted" cell. The other 20% of that 90% branches off to the "1 Yes vote after 2 members have voted" cell.

- init 0.10 0.20 0.50 1.00
0 1.00 0.90 0.72 ---- ----
1 ---- 0.10 0.18 ---- ----
2 ---- ---- ---- ---- ----
3 ---- ---- ---- ---- ----
4 ---- ---- ---- ---- ----
And now for the "1 Yes vote after 1 member has voted" cell, which represents 10% of all possible situations after the first member has voted. Again, that probability will feed into the right and down-and-right neighboring cells in the next column. Note that we add 0.08 to the existing value of 0.18 in the "1 Yes vote after 2 members have voted" cell; there are multiple ways of getting to that cell. The power of dynamic programming is that it merges separate possibilities like this and lets us consider them together going forward; this prevents an exponential increase in the number of possibilities.

- init 0.10 0.20 0.50 1.00
0 1.00 0.90 0.72 ---- ----
1 ---- 0.10 0.26 ---- ----
2 ---- ---- 0.02 ---- ----
3 ---- ---- ---- ---- ----
4 ---- ---- ---- ---- ----
Continuing in this way, we can fill in the whole table. Note that every column sums to 1, as expected.

- init 0.10 0.20 0.50 1.00
0 1.00 0.90 0.72 0.36 0.00
1 ---- 0.10 0.26 0.49 0.36
2 ---- ---- 0.02 0.14 0.49
3 ---- ---- ---- 0.01 0.14
4 ---- ---- ---- ---- 0.01
The tie probability is the value in the "2 Yes votes after 4 members have voted" cell: 0.49. We could have optimized this further by not considering any rows below the number of Yes votes needed for a tie. In practice, in problems like this, one should store the logarithms of probabilities instead of the actual values, which can become small enough for floating-point precision errors to matter.

The number of calculations in this method is proportional to the dimensions of the table, each of which is proportional to K, so the running time of this part is O(K2). Combining that with the O(K) method of selecting committees, the overall running time of our approach is O(K3) + O(N log N). Since K cannot exceed N, and N cannot exceed 200 for the Large, this is fast enough, albeit not optimal. (For example, we could do a ternary search on the value of M mentioned above, instead of a linear search.)

Round 2 2016 - Code Jam 2016

The Gardener of Seville (6pts, 23pts)

The Gardener of Seville: Analysis
Small dataset
For the Small dataset, there are at most 16 cells in the courtyard that we must assign hedge directions to, a total of at most 65536 different hedge mazes. Note that because every cell must have a diagonal hedge, any maze creates a bijection between courtiers; it is impossible for the paths used by two different pairs of lovers to intersect, so we don't need to worry about that. We can use brute force to generate and check all possible hedge mazes, as long as we can efficiently work out which outer cells are paired up through the hedge maze. This can be done in various ways, including:

Consider hedges to be mirrors, and imagine shining a beam of light into the maze from an outer cell, perpendicular to the edge it's on. At each cell the light will reflect off the cell's mirror at a 90 degree angle and continue to the next cell in its new direction. It repeatedly bounces off mirrors until it exits the maze at the cell it's paired with.
Imagine drawing both hedges in each cell, splitting each cell into four quadrants. We can represent the maze as a graph in which the nodes are these quadrants, and there are edges between adjacent quadrants that are not blocked by part of a hedge. We can find which courtiers are paired by starting at the node corresponding to that courtier's starting edge of the maze, and traversing the graph until we reach another edge. Equivalently, we can find the connected components of the graph.
Large dataset
The Large dataset has test cases with up to 100 cells, which is too large for our brute force approach. A more fruitful approach is to take the given pairs and install hedges so that those pairs are connected. The easiest cases would be connecting two cells which are adjacent (either along an edge or around a corner). For example, to connect the two outer cells adjacent to the upper left corner, a single / hedge is sufficient. Does it ever make sense to connect them any other way? The illustration for test case 3 from the sample input connects the two outer cells adjacent to the upper right corner via a longer winding path, but it would also work to make this connection direct and leave the center of the garden unreachable. The direct connection covers just two triangular quadrants (as defined in the Small dataset section) in the corner, and it is easy to see that any other possible path between the two cells also covers these two quadrants. Thus there is no reason not to use this direct path if we need to connect the cells at a corner.

How about outer cells next to each other along an edge? These can be easily connected using two hedges, which covers four triangular quadrants. All possible connections will necessarily cover the two quadrants at the edge of the board, but we can construct paths which do not cover the other two quadrants. However, any path that does not use those two quadrants is guaranteed to block them off and make them inaccessible from other edges. Thus, any path between the two cells will either cover the four quadrants forming the simplest path, or render some of them unusable; there is no reason to use anything more complex than the simplest path.

If every pair to connect has a similarly optimal path which we can easily determine, then we can solve the problem by installing hedges so each pair is connected via its respective optimal path, and if any of the paths intersect then there is no solution. Consider, however, a pair between outer cells on the left and right sides of the garden. Depending on the other connections we need to make, we may be able to freely choose between (for example) having the path go through the top half of the garden and the bottom half of the garden. As such, there isn't a clear single optimal path for connecting this pair. However, we can consider uppermost and lowermost paths, which leave the most space for paths below and above them, respectively. For an uppermost path, for example, we want to take the least space possible to connect the pair, and all the pairs above it. turns out there is a optimal way to connect such pairs.

For the rest of the analysis, we will assume that there is a solution to the problem. If there is a solution, then our strategy will provide a way to find it. If there is no solution, our strategy may not be correct but we can easily detect that it fails by checking the hedge maze as we did in the Small solution.

Define a 'group of pairs' as a non-zero number of pairs where all the outer cells used form a fully contiguous section around the perimeter (but not the full perimeter). For a pair connecting the left and right sides of the garden, we can consider the group of pairs above the path connecting this pair, and the group of pairs above and including this pair. Every group of pairs has an optimal set of triangular quadrants to join all pairs in that section. Similar to before, optimality here means that we can install hedges to connect each pair in the group without covering quadrants outside the optimal set (this property is sufficiency), and if all pairs in the group are connected, no paths from other pairs can ever cover quadrants inside the set (this property is necessity).

We already know the optimal sets for the groups with a single pair of adjacent outer cells. If we have two groups of pairs, which together would form a larger group of pairs, the optimal set will be the union of the optimal sets for the two smaller groups. We can prove that this meets both the sufficiency and necessity properties (as stated before, this requires the assumption that there is a solution).

Consider again the case of a pair connecting the left and right sides of the garden. If we have the optimal set for the group of pairs above this pair, then we can try and extend this to the optimal set for the group of pairs above and including this pair. It makes sense to try and make the path for this pair as high up as possible, staying as close as possible to the paths above it. It can be proven that including this path makes a new optimal set. In general, this works for any non-adjacent pair. If we have the optimal set of quadrants for the group of pairs on one side of a pair, we can extend it by adding a path that stays as close to those quadrants as possible. This means we can inductively find optimal sets of quadrants until we cover all the pairs (note that we did not define all pairs as a valid group of pairs, as the definition of optimality doesn't work for that case).

These ideas give us the following algorithm:

Start with no hedges in the garden
Iterate over pairs, in increasing order of distance (along the perimeter) between the two cells. Ties can be broken arbitrarily.
Let the two outer cells be A and B, such that A→B clockwise around the edge is shorter than counterclockwise. Due to the chosen iteration order, we've already built paths for all points on the left side of the A→B path we're going to construct.
Walk through maze starting at A (the mirror analogy is useful here). We want to stay to the left as much as possible, so if we get to a cell without a hedge installed we pick one so that we turn left. Once we exit the maze, check if we actually made it to B. (If there is no solution we might end up somewhere else.)
Fill in remaining cells arbitrarily
A sample implementation of this in Python is provided below. We encode directions with integers, which allows us to rotate direction and calculation movement easily using bitwise operations and array lookups.

def position(v, R, C):
    # Map from outer cell number to a direction facing into the maze
    # and the position of the outer cell
    # 0->downwards, 1->leftwards, 2->upwards, 3->rightwards
    if v <= C: return 0, v-1, -1
    v -= C
    if v <= R: return 1, C, v-1
    v -= R
    if v <= C: return 2, C-v, R
    v -= C
    return 3, -1, R-v

def move(x, y, direction):
    return x + [0,-1,0,1][direction], y + [1,0,-1,0][direction]

def solve(R, C, permutation):
    board = [[None] * C for _ in range(R)]
    size = 2*(R+C)
    permutation = zip(permutation[::2], permutation[1::2])
    permutation.sort(key=lambda(a,b): min((b-a)%size, (a-b)%size))
    for start, end in permutation:
        if (start-end) % size > R+C:
            start, end = end, start
        direction, x, y = position(start, R, C)
        x, y = move(x, y, direction)
        while 0<=x<C and 0<=y<R:
            if board[y][x] is None:
                board[y][x] = "/\\"[direction & 1]
            direction ^= {"/": 1, "\\": 3}[board[y][x]]
            x, y = move(x, y, direction)
        if (x, y) != position(end, R, C)[1:]:
            return "IMPOSSIBLE"
    return "\n".join("".join(c or "/" for c in row) for row in board)

if __name__ == "__main__":
    for t in range(1, input() + 1):
        R, C = map(int, raw_input().split())
        permutation = map(int, raw_input().split())
        print "Case #%d:" % t
        print solve(R, C, permutation)

Round 2 2016 - Code Jam 2016

Freeform Factory (6pts, 25pts)

As the first step in solving this problem, we will move it from worker/factory terms to graph terms: we are given a bipartite graph with N vertices in each part, and need to add the smallest amount of edges to this graph to guarantee that every maximal matching (one where it is impossible to add more edges) is a perfect matching (one which covers all vertices).

In order to determine that, we need to understand which bipartite graphs have the property that every maximal matching is perfect. One can tackle this part by drawing a few graphs on paper and trying to add edges one by one to find a maximal matching.

After a few experiments, we can form a hypothesis: every maximal matching is perfect if and only if each connected component of the bipartite graph is a complete bipartite graph with same number of vertices in each part. The "if" part is somewhat clear, but the "only if" part looks surprising at first and definitely needs a formal proof, which you can find at the end of this editorial. Here's an example of such graph:

Now we can return to solving our problem. As the first step, we need to find the number of vertices in each half for each connected components of the graph. Let us put those numbers into a list of pairs, one pair per connected component: (p1, q1), (p2, q2), ... The above hypothesis tells us that we need to split this list into groups such that in each group the sum r of all ps equals the sum of all qs, those groups corresponding to the connected components of the graph after adding more edges. The number of added edges is the total number of edges in the resulting graph minus the number edges we have initially, and the number of edges in the resulting graph is equal to the sum of squares of rs. Thus we need to minimize the sum of squares of rs.

Since N is quite small — up to 25 — there are quite a few approaches that work, almost all revolving around dynamic programming/memoization.

Here is one possibility: for each subset Y of the (multi-)set X of pairs we have, and each number t between 0 and N, we will check if it is possible to group all components in Y into groups in such a way that there are several balanced groups with total size t, and possibly one unbalanced group with all remaining components. In case it is possible, we will find the smallest possible sum of squares of sizes of the balanced groups, denoted as dpY,t. Looking at dpX,N will give us the answer to the problem.

At first sight, it seems that we are considering 250 subsets here as we might have up to 50 components in the original graph (if we have no edges at all), but we can note that equal components are interchangeable, so a graph with no edges simply has 25 components of one type and 25 components of the other type, and thus has the total of 26*26=676 different subsets of components. The maximum amount of different subsets of components for N=25 is 43008, formed by the following initial components: 6×(0,1), 5×(1,0), 3×(1,1), 1×(1,2), 1×(1,3), 1×(1,4), 1×(2,1), 1×(2,2), 1×(3,1), 1×(3,2), 1×(4,1).

The most straightforward way to compute the dpY,t values is to use the so-called forward dynamic programming: after finding dpY,t, we will iterate over all ways to add a new element to the unbalanced group in Y, updating t in case the unbalanced group becomes balanced.

All that is left is to prove the hypothesis. We will prove by contradiction: assume that the hypothesis is false — in other words, that there exists a bipartite graph where every maximal matching is perfect, but one of its connected components is not a complete bipartite graph with same number of vertices in each part.

Consider such counterexample G with the smallest number of vertices (note that looking at the smallest counterexample is essentially the same as proving by induction). First of all, G is connected — otherwise any of its connected components would form a smaller counterexample. It is also clear that both its parts have the same number of vertices, as otherwise no perfect matching exists at all, while at least one maximal matching always exists, so we would have a contradiction. Since G is a counterexample, it is missing at least one edge. Let us say that the missing edge connects vertices a and b.

Consider any edge (a, c) that exists from a (there is one since G is connected). Consider the graph G' obtained by removing a, c and all their incident edges from G. Every maximal matching in this smaller graph is perfect, since it can be extended to a maximal matching in G by adding (a, c). And since G' has fewer vertices than G, it is not a counterexample to our hypothesis, and thus each connected component of G' is a complete bipartite graph with same number of vertices in each part.

Let us look at the connected component H of G' containing b. There are three cases, each leading to a contradiction:

There is at least one edge (d, c) in G from H to c. Since all connected components of G' are complete, we can easily build a matching M' in G' that covers all vertices except d and b. By adding the edge (d, c) to this matching we get a matching M in G. Matching M is maximal: its only two uncovered vertices are a and b, and there is no edge between them. M is not perfect, and thus we get a contradiction with the definition of G.
There is no edge from H to c, but there exists an edge (a, e) from a to H. Consider any vertex f from H in a different part from e. Since H and all other connected components of G' are complete, we can now build a matching M' in G' that covers all vertices except e and f. By adding the edge (a, e) to this matching we get a matching M in G. Matching M is maximal: its only two uncovered vertices are f and c, and there is no edge between them since there is no edge between the entire H and c. M is not perfect, and thus we get a contradiction with the definition of G.
Finally, if H is not connected to a and c, then G is disconnected, which is also a contradiction.
The first two contradiction cases are depicted below:

There is also a more beautiful argument leading to a contradiction which does not even require the counterexample G to be the smallest, but it can be a bit harder to come up with. Since G is connected, there is a simple path P between a and b. Since the graph is bipartite, P has odd length and covers some subset of the vertices, the same number from each part. We construct a maximal (therefore perfect) matching as follows: we take every odd edge along the path P, and then complete it with arbitrary edges. Now, we can change our choice of edges and take the even edges of P instead of the odd ones. As a result, our matching has one less edge, and vertices a and b are the only non-covered ones. Since a and b are not connected by an edge, we are left with a maximal matching which is not perfect, in other words a contradiction.

Round 3 2016 - Code Jam 2016

Teaching Assistant (5pts, 10pts)

Teaching Assistant: Analysis
Let's define N as the number of days in the test. Notice that since a student is not allowed to do nothing on a test day, the student must request at least N/2 problem sets. Further, there is no reason to request more than that, since the extras cannot be submitted.

We'll begin with a dynamic programming solution that is sufficient for the Small dataset, and then discuss a mathematical formula that solves the Large dataset.

A dynamic programming approach
Because we can only submit the problem we requested most recently, we can view the problems that we hold as a stack, where requesting corresponds to pushing and submitting the most recently requested problem corresponds to popping.

Consider what happens on day 1. Clearly, we must request a problem set, and we now have to pick a day to submit it. Now, between the request and the submission, we can push to and pop from the stack, but after we are finished, the stack must contain the problem we requested on day 1, and nothing else. That means we need to have an even number of days between our request on day 1 and the corresponding submission.

We can think of the period of time between our request and our submission as a subproblem, and the time after the submission as another subproblem. Each of those subproblems must leave the stack the way they found it, so we can use the same set of rules that we used above for every subproblem.

Now, we'll express how to solve the problem in terms of recursion. At each stage, we'll be looking at some range of days, from day i to day j. So the subproblem (i,j) is to find the best possible score we can get over the days from i to j. We'll also say we can't touch the existing entries on the stack (corresponding to any days before i), and must leave the stack as we found it. We can then solve the subproblem as we did the original problem: request a problem set on day i, then pick where in the (i,j) interval to submit it such that there's an even number of days between the request and the submission.

At this point, each of our choices divides the subproblem into two additional, possibly-empty subproblems. For a subproblem of length 6, for example, here are the ways to place the request and submission:

_ _ _ _ _ _
↓
R S _ _ _ _
R _ _ S _ _
R _ _ _ _ S
Given the pairs of request and submission days, how do we decide which type of problem set to request? If we request the type that doesn't match the assistant's mood on request day, the best score we can get is 5 points. But we always get at least that much by requesting the type that matches the assistant's mood, so we should always do that. Then, the score we get when we submit just depends on whether the assistant's mood on submit day matches the type of problem we requested: we get 10 points total if they match, and 5 if they don't.

The empty subproblems are the base cases of our recursion. For larger problems, we find, for each possible submission point, the score we get from requesting and submitting that problem set (10 if the assistant's mood on request day matches its mood on submit day, 5 otherwise), plus the optimal scores of the subproblems. We can use this to recursively find the solution to the original problem; memoizing the solutions for the subproblems yields a DP solution. Because the memoization table is size O(N2) and it takes O(N) iterations to fill each cell, this algorithm is O(N3), which is sufficient to solve the Small problem.

A formulaic approach
It's also possible to solve the problem with a simple mathematical formula based on the input.

Count the number of even- and odd-numbered days on which the assistant is in the mood for Coding and Jamming. That gives us four numbers, which we'll call CE, CO, JE, and JO (for instance, CE is the number of even-numbered days on which the assistant is in the mood for Coding). We will show that the maximum possible score is:

S = 10 * (min(CE, CO) + min(JE, JO)) + 5 * abs(CE - CO).

We'll first show that this is an upper bound (we can't get a score higher than S), then show that it is tight (we can get a score equal to S).

First, note that any request/submit pair for the same problem (which we'll just call a pair) must have one even and one odd day; that is, for any problem set, it was either requested on an odd day and submitted on an even day, or vice versa. That must be true because we need an even number of days between the request and the submission. Now, since we need an even Coding day and an odd Coding day to make a pair where both days are Coding, the amount of those pairs that we can make is equal to the minimum of CE and CO. We can apply the same logic to Jamming. Then, we can pair up the leftovers; the amount of leftover pairs is the amount by which CE and CO differ (you can show that JE and JO differ by the same amount). No matter how we pair up the leftovers, there is no way to form more matched pairs.

To show that this bound is tight, we will construct a solution that achieves that upper bound. We make a list of the days in order. Then, whenever we see two adjacent days with the same mood, mark the first one as a request and the second as a submission, then remove those two days from our list. Observe that we can continue making pairs on this list, and they'll be valid on the real list (it's impossible for a new pair to be halfway inside and halfway outside any pair that we've removed, since we only remove pairs when there's nothing inside them). Continue this process until we can't anymore. At this point, since there aren't any adjacent days with the same mood, the mood must alternate between days. But that means that all even-numbered days have the same mood, and all odd-numbered days have the other mood. So we can pair all of those up, completing the proof.

A greedy approach
If you're curious, it's also possible to use a greedy stack-based algorithm to solve the Large problem. Our algorithm proceeds from left to right, and at each point decides whether to request or submit. We maintain a stack of problem sets that we hold, where a request is a push and a submit is a pop, then select our action each day as follows:

If we have no unsubmitted problem sets, request.
If we've made N/2 requests thus far, where N is the number of days in the test, submit.
If the top of the stack contains a problem set that matches the assistant's mood, submit.
Otherwise, request.
It's fairly difficult to see why this is valid on its own, but we can show that this also obeys the formula that we give above.

Both algorithms (the stack algorithm and the adjacent pair algorithm) find an adjacent pair with the same mood some number of times; we'll call these hits. Pairs with a different mood are misses. It remains to prove that the stack algorithm gets the optimal number of hits.

In most cases, both algorithms hit and miss in the same places. Intuitively, steps 3 and 4 of the stack algorithm correspond to finding and removing adjacent pairs with the same mood. Then, steps 1 and 2 clean up the remaining unmatched pairs.

There are some corner cases, however, where the algorithms don't precisely match. Take the input CJCJJC, for example: the stack algorithm produces RRRSSS, while the adjacent-pairs algorithm results in RRSRSS.

All of these corner cases occur when there's an unmatched pair that the stack algorithm doesn't match, failing to see that there's a matched pair later on at the same level. In other words, they all look like the following. (You can swap the Js and Cs, but we'll use the case that we show below without loss of generality.)

... J ... C ... J ... J ...
Then, the stack algorithm requests for the first two and submits for the last two, while the adjacent pairs algorithm matches the second pair, then the first pair. Notice that these have the same result either way: there's always one unmatched pair and one matched pair. Hence, the algorithms both produce the optimal score, even though they may attain it in different ways.

Round 3 2016 - Code Jam 2016

Forest University (25pts)

Forest University: Analysis
An unusual problem
Small-only problems are relatively rare in Code Jam. Sometimes, as with 2012's Proper Shuffle from Round 1A 2014, this is because we have a randomized solution in mind and want contestants to be able to try again if necessary, even if the probability of a contestant's optimal solution actually failing due to chance is very small. The Forest University problem is Small-only for both of these reasons. We could have added another Small dataset small enough to solve precisely via dynamic programming, but we didn't think it would add useful resolution to help us pick our 26 advancers.

Clearly, with up to 100 courses, there are far too many course sequences to enumerate. The problem has an unusually relaxed tolerance for less precise answers, so it is a natural candidate for a simulation-based approach: we can generate course schedules and check whether they contain the cool substrings. Unusually for a simulation problem, though, the challenge here is to figure out how to do the simulation correctly even once! How do we sample the set of all course sequences uniformly at random?

A weighted-sampling method
Some pen and paper analysis of a small forest can reveal a simple rule that is sufficient to guide our simulations. Let's consider a case with five courses A, B, C, D, and E; A and E are basic, A is the prerequisite of both B and D, and B is the prerequisite of C.

Then there are fifteen possible orders in which to take the courses: ABCDE, ABCED, ABDCE, ABDEC, ABECD, ABEDC, ADBCE, ADBEC, ADEBC, AEBCD, AEBDC, AEDBC, EABCD, EABDC, EADBC. Notice that four-fifths of these begin with A and one-fifth begin with E. A is a root node with a total of 4 descendants (including itself); E is a root node with a total of 1 descendant (including itself). This is a promising pattern, and it is not coincidental! It suggests a method for building up a course string while sampling uniformly: keep choosing one of the available courses, with probability proportional to the size of that course's subtree (itself plus all its descendants).

To apply this method to the example above: we start with an empty course string, and at first, we can only choose either A or E. A has 4 descendants including itself, and E has 1, so we choose A with probability 4/5 and E with probability 1/5. Suppose that we choose A. Now we have three choices for the next course: B, D, and E. B has 2 descendants including itself; D has 1; E has 1. So, we choose B with probability 2/4, D with probability 1/4, and E with probability 1/4. Suppose that we choose D. Now we have two choices: B and E; we choose these with probability 2/3 and 1/3, respectively. We continue in this way until we've built a full sequence.

Why does this work? Speaking more generally: let's add a single root node to be the parent of all nodes with no prerequisites. We can now recursively compute, for each subtree, a schedule of just the classes in that subtree. Suppose that the subtree has root node V and size S. First, we compute a schedule for each subtree whose root is one of the children of V. Then, we put V first in the new schedule we are creating, and we choose an assignment (uniformly at random) of the remaining S-1 positions to each of the subtrees, such that each subtree is assigned as many positions as it has nodes. Then, we copy the ordering for each subtree into that subtree's positions. This results in a uniformly randomly chosen schedule.

Since we are picking a uniformly random assignment for each subtree, then interleaving them together uniformly randomly, the fraction of possible orders in which a given top-level course (i.e. the root of a certain subtree) appears first can be computed using a multinomial coefficient. For example, if we have to interleave A elements from one subtree and B elements from another subtree, the total number of ways to do so is (A+B)! / (A! × B!). The number of these ways that start with an element from subtree A is (A+B-1)! / ((A-1)! × B!), and the number of these ways that start with an element from subtree B is (A+B-1)! / (A! × (B-1)!). The ratio between these is (A! × (B-1)!) / ((A-1)! × B!), and this reduces to A / B. This explains why it suffices to sample proportionately to the size of each subtree.

A surprisingly elegant method
Generating a random sequence is equivalent to assigning distinct numbers from 1 to N to nodes in our forest, in such a way that the number of a node is smaller than the number of all its descendants. Let's start with any of the N! possible assignments, chosen uniformly. Now let's go through nodes from top to bottom, and if a node is not the smallest in its subtree, we swap its number with the smallest. This generates sequences uniformly, since each sequence can be obtained from P different permutations, where P is the product of sizes of subtrees of all nodes. The probability that a given node X is the smallest (once it's available to be chosen) is proportional to the size of the tree it is rooting, because any of those nodes that get the smallest number will "give" it to X.

But will we get a precise enough answer?
If we check K uniformly generated sequences, and the true probability to find a given substring is p, then the fraction of those K sequences that contain this substring follows a binomial distribution with parameters K, p, which we can approximate with a normal distribution with mean p and standard deviation sqrt(p × (1 - p) / K). (This Wikipedia article has some guidelines on when this approximation is valid.) If we run 10000 iterations, this is at most 0.5 / 100=5e-3, so the required precision of 3e-2 is 6 standard deviations. So, the probability of having one particular answer incorrect is roughly 1 in 500 million, which means that the probability of having at least one of the 500 required answers incorrect is at most 1 in a million.

The error bounds are generous enough that, with a fast enough implementation, the problem is solvable in slower languages such as Python.

Round 3 2016 - Code Jam 2016

Rebel Against The Empire (8pts, 17pts)

The intended solution is roughly as follows:
Treat it at a graph problem. We have a graph with N nodes, and each edge is available in an interval of time.
For each node, order the edges by, say, availability start time. As long as we have at least one edge available, we can jump back and forth. And, obviously we can arrive on an asteroid only when there's at least one edge open. So, there are "gaps" such that you need to leave the asteroid between the gap, because there will be no edges in between. So, we can split each node into multiple nodes across these gaps (note that the number of edges doesn't grow, so the size of the whole graph is still O(N^2)).
Now we run a Dijkstra's on the new graph. When we arrive in a node, we can only take the outgoing edges which haven't been closed yet (and the time at which we take them is the max of the current time and the time in which they open).
Done.

Round 3 2016 - Code Jam 2016

Go++ (7pts, 28pts)

go++: Analysis
To make the analysis a tiny bit easier to follow, we'll typeset strings (B, G, and output from the Go++ programs) in monospace font, instructions from the first program in normal font, and instructions from the second program with an underline.

First, let's try to figure out when the problem is impossible to solve. It's clearly impossible when B is in G; that case is even in the sample input. What about otherwise?

Surprisingly, if B is not in G, the problem is always solvable.

To prove this, I'll construct two Go++ programs that, when run as described in the problem statement, can produce any L-length string except B. That sounds harder than making one that just produces the strings in G, but sometimes, making the problem more difficult is a great way to gain the insight you need to solve the original problem. For this problem in particular, the set G is a red herring!

The first program
Given an input string B, construct the first program as follows:

Start with the inverse of every character of B (0 ↦ 1, 1 ↦ 0).
Add a ? instruction after every 0 or 1 instruction.
For instance, if B = 111, which could appear on the Small input, the first program is 0?0?0?. If B = 010, which could only appear on the Large input, the first program is 1?0?1?.
Why does this work? Notice that by default, this program generates the opposite of B. In order to generate a different string, we need to override one or more of the ? instructions; that is, the programs need to be interleaved such that right before the ? from the first program, some other instruction from the second program changes what it prints. For instance, given the first program 1?0?1?, we can print 011 instead of 101 using the second program 01, if it's interleaved as follows: 10?01?1?. In order to print B, we need to override all three ? instructions.

So how do we prevent that from happening? We write a second program that cannot override all three ? instructions. Our solution differs for the Small and Large inputs.

The second program (Small)
For the Small input, B is always a string of 1s. That means the first program is always 0?, repeated L times. For instance, if B = 111, the first program is 0?0?0?.

Our second program needs to be able to override any two of those ? instructions; that way, we can print any output with up to two 1s. However, we can't override all three ? instructions, since that means our program might print B. So we need a second program that contains L-1 1s, but not L 1s. The solution to this is fairly simple: our second program simply consists of 1, repeated L-1 times. For instance, if B = 111, the second program is 11.

The second program (Large)
To solve the problem for arbitrary B, we need a second program that, when taken as a string, has every (L-1)-length string as a subsequence (so we can print any string except B), but doesn't have B as a subsequence. (Here, we use "subsequence" to mean any set of elements, ordered as they appear in the original sequence; they need not be adjacent.) We'll discuss how to produce such a program shortly.

Now, with our first program and this hypothetical second program, we can produce any L-length string except B. That's because the second program has every possible subsequence of length L-1, so we can override up to L-1 of the ? instructions of the first program. However, we cannot override all L of them to produce B, because the second program doesn't have B as a subsequence.

There are many ways to generate the second program. This one won't produce the shortest program, but it is perhaps the easiest to explain:

Start with a copy of B, excluding the last character.
Replace each character with a two-character sequence as follows: 0 ↦ 10, 1 ↦ 01.
For instance, if B = 010, the second program is 1001. The first 0 becomes 10, the 1 becomes 01, and we ignore the last character of B.
We outlined the two requirements above for the second program to be valid:

The second program must have every (L-1)-length string as a subsequence.
The second program must not contain B as a subsequence.
Now, we will show that the steps above give us a second program that satisfies these requirements.
To get any (L-1)-length subsequence, split the second program into pairs of adjacent instructions. Each pair contains a 0 and a 1, since that's how we built the program. So we just pick one character from each pair, and we have any (L-1)-length string!

Now, let's try to get B as a subsequence of our second program. Say B = 010 and the second program is 1001. In order to get 010 as a subsequence, we start from finding the first 0 in 1001, which occurs at the second character. Next we find the first 1 after that, which is the fourth character. The pattern continues: because of how we've built the second program, we always go through two characters whenever we try to find B as a subsequence. And because the second program only has 2L-2 characters, we can't find all of B as a subsequence. Therefore, our second program is valid.

World Finals 2016 - Code Jam 2016

Family Hotel (10pts, 20pts)

Let P(N, K) denote the probability that room K is occupied when the NO VACANCY sign goes up. This is essentially what the problem asks for, except that it is requested in a special format. We will get to that later.

Small input
We first present an algorithm for the small input. There are N-1 ways to accommodate the first family, each with probability 1/(N-1). Suppose we assign to them adjacent rooms i and i+1. If either is equal to K, we are done. Otherwise, K falls either in the interval [1, i-1] or in the interval [i+2, N]. Take the former case without loss of generality. What happens in the interval [i+2, N] is now irrelevant. Hence the problem reduces to finding P(i-1, K). Working out the math, we get the following recurrence relation:

P(1, 1) = 0, and
P(N, K) = 1/(N-1) [ μ(K+1 ≤ N) + μ(K-1 ≥ 1) + ΣK+1 ≤ i ≤ N-1 P(i-1, K) + Σ1 ≤ i ≤ K-2 P(N-i-1, N-K+1) ], where 1 ≤ K ≤ N, 2 ≤ N, and μ(condition) = 1 if condition holds and 0 otherwise.
Setting aside the issues of precision and the special format requirements of the output, we can use dynamic programming to compute P(N, K) in time O(N3). This is too slow even for the small case. However, by defining S(n, k) = Σk ≤ i ≤ n P(i, k), the recurrence simplifies as follows:

P(N, K) = 1/(N-1) [ μ(K+1 ≤ N) + μ(K-1 ≥ 1) + S(N-2, K) + S(N-2, N-K+1) ].

The above recurrence can be used for an O(N2)-time algorithm, which is sufficient for the small input.

Modular arithmetic
Before proceeding to a faster solution for the large input, let us address the output format. Let M = 109+7 be the prime specified in the problem statement. For a rational solution p/q, we are asked to output the unique integer y such that 0 ≤ y < M and y*q = p (mod M). In terms of the field (ZM, +, *), we are simply looking for p/q (mod M) = p * q-1, where the latter (i.e., the inverse of q modulo M) equals qM-2 by Euler's theorem (or Fermat's little theorem). This computation requires only logarithmically many operations on integers less than M2. In fact, all the arithmetic can be carried out in this field, hence there is no need for large integers or any floating-point operations. Note that the division operation in the recurrence is well-defined since we never divide by a multiple of M.

Large input
To solve the large input, we observe the following:

Claim: P(N, K) = 1 - F(K) * F(N-K+1), where F(q) = 1-P(q, 1) denotes the probability of the left-most room being unoccupied at the end of the day.

Proof: This can be proved rigorously via induction with the base cases being when K ∈ {1, N}, and using the recurrence relation. A more elegant proof, though, is as follows. Suppose room K is left unoccupied until the end. In that case we essentially have two independent hotels, one formed with rooms from 1 to K, and the other with rooms from K to N. The first hotel gets those guests that are assigned to rooms between (1,2) and (K-1,K), and the second hotel gets those guests that are assigned to rooms between (K,K+1) and (N-1,N). It's easy to see that those two "sub-streams" of guests that are going to each hotel are also uniformly and independently distributed. The only difference between having such two hotels and one big hotel is that room K could be occupied twice in the two hotel case, but since we're considering the case where it's left unoccupied, this does not happen. Now since the two hotels are independent, we just need to multiply the probabilities that room K, which is a border room in both hotels, is left unoccupied. ■

The recurrence for P(N, K) has a simpler form for K=1, as follows:

P(1, 1) = 0, and
P(N, 1) = 1/(N-1) [ 1 + S(N-2, 1) ] for N ≥ 2.
Since the two one-dimensional arrays can be filled at the same time, there is an O(N)-time algorithm to compute all values of P(N, 1), hence F(N), which leads to a constant-time operation for each P(N, K) query in the input.

Final remarks
For the curious, the above recurrence yields the following solution for F(q):

F(q) = Σ0 ≤ i ≤ q-1 (-1)i 1/i!, for q ≥ 1.

This has a combinatorial meaning, as well. Consider a permutation π of 1, …, N-1, where πi denotes the time at which we try to assign pair of rooms i and i+1 (if both are vacant). Convince yourself that the number of such permutations producing a certain outcome of room assignments is proportional to that outcome's probability. Now F(q) is equal to the probability that the length of the decreasing sequence at the beginning of π be even. We calculate this via Principle of Inclusion-Exclusion.

Since the claim above is the crux of the solution for large input, it is worth noting that some contestants got the idea by simply looking at the table of 1-P(N, K) for small values of N and K.

World Finals 2016 - Code Jam 2016

Integeregex (15pts, 15pts)

Analysis
One handy property of regular expressions is that matching one of them is equivalent to being accepted by a special type of machine called a nondeterministic finite automaton (NFA).

An NFA is a graph of states and transitions, with a special initial and final state. Each transition is labeled with a digit or ε. When processing a string, the machine may move from a current state to another state by taking an ε-transition, or by taking a digit transition that matches the current digit of the string and moving to the next digit. After the last digit of the input string is read, only ε-transitions are allowed. If there is a path from the initial digit to the last digit that reads the entire input, we say the input string is accepted or matched, and that path is called an accepting path.

Build an NFA that matches strings to the regular expressions of the problem.
Thompson's construction is one algorithm to construct an NFA that matches regular expressions.
The general outline is start with two special states: the initial state, q, and a final accepting state, f.
Then build up the NFA f(E) recursively:

If E is a digit, then f(E) contains only the two special states linked with a transition labeled with E.
f(E = E1E2) is the union of f(E1) and f(E2), using the initial state of f(E1) as initial state of f(E), the final state of f(E2) as final state, and adding an ε-transition from the final state of f(E1) to the initial state of f(E2).
f(E = (E1|E2|...|EN)) is the union of all the f(Ei) plus an additional initial and final state. Then, ε-transitions are added from the initial state of F(E) to each initial state of an f(Ei) and from the final state of each f(Ei) to the final state of f(E).
f(E = (E1)*) is just f(E1) with an additional ε-transition from the final state to the initial state.
Here's an example NFA built from the Integeregex (13|1)((2)*|3)

Checking that a single string matches
Consider the following example: the input string 1322 has an accepting path on the NFA above that goes through these states: q, s1, s2, s3, s6, s7, s8, s7, s8, f.
Notice that there are multiple paths for each string, some may be accepting and some others may not. Just one accepting path is enough for the string to be accepted.

Start with the set of possible states containing only the initial state (q).
For each character C in the string:
  For each of the last possible states:
    Find all the states that can be reached by ε-transitions then a transition on C.
    Add these to the new set of all possible states.
From the last set of possible states:
  if the accepting state (f) is reached from any of these states by ε-transitions then the NFA (and the regular expression) match!.

For example to check the string 1233 against our example NFA:
the NFA starts in the initial state q
After ε-transitions and a transition on 1, the NFA can be in any of the states {s2, s5}.
After ε-transitions and a transition on 3, the NFA can be in any of the states {s3, s10}.
After ε-transitions and a transition on 2, the NFA can only be state s8.
After ε-transitions and a transition on 2, the NFA can only be state s8.
Because the accepting state f can be reached from s8 with ε-transitions, the NFA, and the regular expression, match 1322!

Quickly counting all numbers that match the NFA
We can now use dynamic programming to quickly check how many numbers less than or equal to X match the NFA.
We keep a map of (is_empty, is_prefix_of_x, possible_states) to memoize the result starting from that state.
We use is_empty to keep from adding zeros at the front of the number.
We use is_prefix_of_x to keep from counting numbers larger than X.
def MatchNFA(X, transitions):
  x_digits = []
  for c in str(X):
    x_digits.append(int(c))

  # Start of numbers with same length  as X.
  count_state = { (True, True, 'p') : 1 }
  for index in range(len(X)):
    # Start of shorter and shorter numbers.
    new_count_state = { (True, False, 'p') : 1 }

    for (is_empty, is_prefix_of_x, states), count in count_state.items():
      for new_digit in range(10):
        if is_empty and new_digit == 0:
          continue # Numbers can't start with 0.

        if is_prefix_of_x and new_digit > x_digits[index]:
          continue # Numbers can't be greater than X.

        # Find all possible states if new_digit was next in the string
        new_possible_states = []
        for start_state in states:
          # Add all states that can be reached from start_state by (ε)* new_digit
          for epsilon_state in transitions[start_state]['']:
            new_possible_states += transitions[epsilon_state][new_digit]
        new_count_state[(False, is_prefix_of_x and new_digit == x_digits[index],
                         set(new_possible_states))] += count
    count_state = new_count_state

  count_match = 0
  for (is_prefix_of_x, states), count in count_state.items():
    for final_state in states:
      if 'f' in transitions[state]['']
        count_match += count

  return count_match
Finally we calculate the number of matching numbers between A and B as MatchNFA(B, transitions) - MatchNFA(A-1, transitions).
As the number of states in the NFA grows, new_possible_states can grow exponentially large (it can theoretically be the powerset of the states). However, the small maximum length of the regular expression and the amount of non-digit characters consumed to include disjunctions or repetitions make it so that the number is actually really small (for a computer) in practice. There are mathematically provable bounds on the number, but the proofs are too long to fit in the margins of this analysis.

World Finals 2016 - Code Jam 2016

Gallery of Pillars (10pts, 30pts)

Gallery of Pillars: Analysis
A convenient way to look at this problem is to set up a coordinate system with the viewer at the origin and the center of every pillar at other coordinates (x,y) with x and y non-negative integers less than N.

Let us focus on a given pillar centered at (x,y). Suppose some other pillar centered at (a,b) intersects the segment [(0,0),(x,y)]. Then, (a,b) blocks at least half the view of (x,y), from the segment towards one of the sides. Then, if we consider the pillar centered at (x - a, y - b), that is, the one symmetric across the midpoint of the segment [(0,0),(x,y)], the distance from that pillar to the segment is the same, so it also covers the segment; it's located on the other side of the segment from (a,b), and so it blocks the other half of the view. It follows that the pillar centered at (x,y) is visible from the origin if and only if the segment [(0,0),(x,y)] does not intersect any other pillar. The following picture illustrates the situation.

Assume the pillar centered at (a,b) intersects the segment [(0,0),(x,y)]. Clearly, a ≤ x and b ≤ y. Moreover, the square of the distance between the point and the segment can be expressed as
d2 = (ay - bx)2 / (x2 + y2).
Since the pillar intersects the segment, d2 × 106 ≤ R.

From this expression, we can tell in constant time whether a given pillar blocks the view of some other pillar. This immediately gives an O(N4) solution to the problem: for each pillar, check every other pillar to see whether it blocks it. This is just shy of fast enough for the Small, but there is a simple improvement: instead of checking every possible (a,b) for each (x,y), it's enough, for each possible a between 0 and x - 1, to try only b = a * y / x, rounded up and down (that is, the two points with that a coordinate that are clearly closest to the segment). This makes the complexity O(N3), which is definitely fast enough to solve the Small dataset.

The Large dataset, as usual, requires a bit more work. Let us pick up where we left off: our expression for the squared distance
d2 = (ay - bx)2 / (x2 + y2).
If x and y are not relatively prime, choosing a = x / gcd(x,y) and b = y / gcd(x,y) yields a pair (a,b) that fullfills every condition to cover (x,y). This is to be expected, as (a,b) clearly covers all pillars (ka,kb) for each k > 1. If x and y are not relatively prime, then there exist positive a and b such that |ay - bx| = 1. Since |ay - bx| is an integer, and it can only be 0 within our constraints when x and y are not relatively prime, choosing a pair that makes it 1 yields the closest distance. Thus, a pillar (x,y) is visible if and only if x and y are relatively prime and 106 / (x2 + y2) < R. That is, we need to count the pairs of relatively prime integers within a circle of radius 106 / R and a square of size N by N.

If N ≥ 106 / R, then the square contains the circle, so let us assume further that N ≤ 106 / R ≤ 106. We can count all points inside the area, then subtract the multiples of 2, 3, 5, ..., etc, then add the multiples of 2 × 3 = 6, 2 × 5 = 10, 3 × 5 = 15, etc, that were subtracted twice, and keep doing the inclusion-exclusion argument for each possible divisor k between 1 and N. The count for points that are multiples of a given k should be multiplied by the Möbius function of k. Calculating that function for each number between 1 and N can be done efficiently with a slight modification of the sieve of Eratosthenes, which runs in linear time. For each k, we count by iterating the possible values of x, k, 2k, 3k, etc, and for each one we find the range of possible y values. We can do that by taking the minimum of N and the square root of (106 / R)2 - x2, or by binary search, or by linear search. Notice that the maximum value of y decreases from one value of x to the next, so if we pick up at the value considered for the previous x, the linear search will only take linear time overall (constant amortized time per each value of x). Linear and binary search are guaranteed to give a precise number, but using the square root also works because we are dealing with fairly small values. For the most efficient versions, the time complexity of the algorithm to get the count for a given value of k is O(N / k). Since the summation of 1 / k up to k = N can be approximated by log N, the overall complexity of the algorithm is O(N log N). Using binary search to find the range of values for y only adds a second log factor, yielding an overall O(N log2 N). Both are efficient enough for N up to 106.

World Finals 2016 - Code Jam 2016

Map Reduce (20pts, 30pts)

Map Reduce: Analysis
Some initial checks
First, calculate two values: the length Li of any shortest path from the start to the finish (using BFS), and the Manhattan distance M from the start to the finish (ignoring walls). Based on those results, we can immediately detect some impossible cases:

We can only remove walls, so we have no way to increase the length of the shortest path above Li. So, if Li is less than D, there is no solution.
Similarly, if M is greater than D, removing walls does not help; even a blank maze consisting of only a border would still have a shortest path length > D.
If the parity of Li and D is different, there is no solution. The lengths of all paths between any two given cells have the same parity, because each step flips the parity of the sum of the indices of the row and column.
Surprisingly, in any other case, there is always a solution. The rest of the problem is to provide a constructive proof of that fact.

A crucial observation
We want to remove walls to change the current shortest path length L to match D. The key to solving this problem is to notice that we can always remove a wall such that the new shortest path has a length of either L or L-2. The proof of this is somewhat difficult, but we can discuss it intuitively (a formal proof follows at the end of this analysis).

Consider a connected component of walls. It either includes the border or it doesn't. Now, pick some wall W within that connected component that is as far as possible away from either the border, or some arbitrary wall within the component if it doesn't include the border. Using the fact that all empty spaces are connected and that no two walls can connect at a corner, we can find that the 3×3 neighborhood of W looks like one of the following cases, up to symmetry (# is a wall, . is a space, ? can be either):

...  ?#?  ?##
.W.  .W.  .W#
...  ...  ..?
Let's consider each case in turn. We will show that if the shortest path proceeded through the 3×3 neighborhood of W, removing W will decrease the length of the shortest path by at most 2:

W has zero neighbors that are walls: The only path that would be shortened by removing W is a path that goes around W. So, removing W will shorten the path by 2, since the path can now go directly through W.
W has one neighbor that is a wall: Say we have the case pictured above, and the shortest path proceeds from the top-left corner around the bottom to the top-right corner. Removing W again shortens the path by 2.
W has two neighbors that are walls: Removing W doesn't shorten the path at all. (The reason we have this case is that removing this wall can open up other walls to be removed.)
Here are some illustrations of the above three cases. (For simplicity, we assume here that all the ?s are walls, but the argument holds regardless.)

To solve the problem, then, we just continually remove walls from the map (keeping in mind that removing a wall may make another wall removable) until the shortest path is equal to D. For the Small, we can repeatedly scan the map for removable walls and remove a wall; we continue this until the shortest path is the required length.

For the Large dataset, scanning the map repeatedly is too slow, so we need a different approach. We can figure out a list of walls to remove, in order, then binary search on the number of walls to remove to make a path of the required length. To find this list, we can scan the map once, then initialize a queue containing all the removable walls. Then, each time we choose a wall to remove next, we scan each of its neighbors to see if it's removable, and add any newly-removable walls to the back of the queue. We also need to scan the neighbors for walls that may have become unremovable, and remove any such walls from the queue. For example, if a connected component is just a 2x2 square, all of its walls are initially removable, but after removing one of them, only two of the three remaining walls can be removed. So we may sometimes need to remove a wall from the queue, and then perhaps even re-insert it later.

With this method, we're only scanning the entire map once, then doing constant additional work per wall we remove (O(N) total), so it's fast enough for the Large. (This method will eventually remove every wall, except that it might leave an extra-thick unremovable border. This border doesn't matter, since removing it wouldn't change the shortest path.)

A formal proof
As before, let Li be the shortest path initially and M be the Manhattan distance. We claim that the problem is solvable for any D (of the same parity as Li) between Li and M. It suffices to show that you can always delete a wall while keeping the maze valid and without decreasing the current shortest path length L by more than 2.

Consider some connected component of walls. If it includes the outer boundary, let B be the set of walls on the outer boundary. Otherwise, let B be an arbitrary wall in the component. Let A be a wall in the component that is adjacent to an empty cell and maximally far apart from B (based on distance staying within the component). We'll delete A.

This adds an empty cell adjacent to another one, and so all empty cells stay connected. We next need to show that it can't make two walls touch only at a corner. By way of contradiction, suppose that X and Y are walls adjacent to both A and an empty cell Z. Z and A are connected by empty cells, so X and Y cannot be connected by walls after deleting A. Thus one of X or Y must be further from B than A is. But X and Y are not on the outer boundary, and are connected to B before A is deleted, and are adjacent to Z, so we have a contradiction.

Finally, we need to show that deleting A cannot make two empty cells greater than two steps closer to each other. The only way this could happen is if A were adjacent to empty opposite cells X and Y, and walls W and Z. As above, X and Y are connected, so Z and W cannot be connected by walls after deleting A. This leads to the same contradiction as before. W and Z may not be adjacent to an empty cell, but they are adjacent to something other than A that is. Either W or Z or this adjacent cell will contradict the choice of A. Note that the key claim here is false if walls are allowed to touch only at corners, but the problem setup disallows that.

World Finals 2016 - Code Jam 2016

Radioactive Islands (25pts, 25pts)

Radioactive Islands: Analysis
This optimization problem touches on topics that are a bit further afield from our other problems! But a wide range of reasonable solution approaches can be successful.

We will start with some general insights, and then move on to a few of the many possible solutions.

Never get too close to an island
The radiation dose rate rises sharply as we approach an island, so any path that passes too close to an island is worse than a path that takes the time to travel around it at a greater distance away.

This is helpful because we can be more confident about numerical precision when finding approximations to the best path, since areas where the dose rate is low are also those where the derivative of the dose rate with respect to position is low.

Never move left
Although the input limits do not allow it, suppose that the boat started 0.000001 km to the left of an island. In that case, the effects of the island would be much worse than the effects of the background radiation, and we'd want to move left to "escape" from the island as fast as possible and eventually make a wide curve around it.

Also, consider a scenario where the difference in the Y-coordinates of the endpoints is large enough that the angle between them is nearly 90° from horizontal, and where there is an island in the middle of the direct path between the endpoints. Then an optimal path could start by moving almost vertically but slightly to the left, because that would give the island a wider berth without significantly increasing the total length of the path.

However, we've made the input "nice" — the boat always starts 10 km to the left of the islands, where the maximum dose rate from the islands is at most 0.02μSv/h, and the maximum angle between the endpoints is 45° from horizontal, so any movement to the left at the start would be misguided.

Similarly, leftward moves in the middle of the path do not help either — a path which zig-zags back and repeats some X-coordinates can be altered to take a more direct route with a lower total radiation dose.

Solution #1: Hill-climbing
This optimization problem is well-suited for hill-climbing because it is easy to iteratively take a reasonable path and "nudge" it towards a better path in such a way that it will converge quickly towards a local minimum.

How many local minima are there? Since we should never move left, and there's only one or two islands, the optimal paths can only take so many forms. With one island, the optimal path will either go above the island, or go below it. With two islands, the optimal path will either go above both islands, between the two islands, or below both islands.

So if we start with a reasonable path for each of these forms and hill-climb from there, we will find each of the two or three local minima.

An easy way to do this is to model a path as a series of many small straight line segments. For each such segment, we can use calculus to find the amount of radiation received along that part of the path. Let's consider one such segment, running from (x1,y1) to (x2,y2), that we travel along between time 0 and time 1 (we've rescaled the times for convenience). Then the x and y coordinates, as a function of time t, are:
x(t) = x1 + t(x2-x1)
y(t) = y1 + t(y2-y1)
Suppose that there are two islands (the one island case is just a simpler version of this), and the y coordinates of the islands are yi1 and yi2. (Recall that the x coordinates of both islands are 0.) Then the total amount of radiation is the definite integral from 0 to 1 of:

[1 + 1 / (x(t)2 + (y(t)-yi1)2)) + 1 / (x(t)2 + (y(t)-yi2)2))] × [sqrt((x2-x1)2 + (y2-yi1)2))] dt

This integral can be solved exactly, but that's not necessary. Multiplying the length of the segment by the average of the dose rates at the endpoints of the segment is sufficiently accurate, if enough segments are used.

The question is how to find the right positions for our segments. We can lay down a rough path with endpoints whose X-coordinates are fixed and evenly-spaced between -10 and +10. Now, the Y-coordinates are a vector of real numbers, and we need to optimize that vector.

We can use gradient descent to do this — we need to iterate finding a direction to move the vector in such that the total radiation decreases.

Adjusting one value at a time doesn't work — even if a particular point needs to be moved upward, if we move only that one point upward, we'll soon be increasing the total radiation dose because the path will have a "kink" in it. But almost any other scheme will do — choosing an interval of points and nudging them all up or down in a triangular shape, moving the points in the middle more than the points at the end, will preserve the smoothness of the path.
Other more sophisticated nonlinear optimization techniques like the BFGS algorithm work also.
Solution #2: Calculus of Variations
We can consider the Y-coordinate of the ship to be a function of its X-coordinate and write the total radiation dose as an integral involving that function, then use techniques from the calculus of variations to minimize it. The Euler-Lagrange equation gives us a condition, expressed as a differential equation, that any solution must satisfy. This condition is entirely local, so if we knew the initial direction the ship should travel, we could find the entire path by using a numerical integration technique such as a Runge–Kutta method, to find a solution to the differential equation using the known initial conditions. However, we don't know the initial direction of the boat — we only know its starting and ending positions!

If we guess an initial direction for the boat that's reasonable, then integrating will trace out a path, but the y-coordinate of the endpoint will probably not be right. So, we can do a binary search to find the initial direction that does lead us to the desired endpoint. If we repeat the binary search for ranges that correspond to each of the forms the path can take, then we will find the optimal path.

Simple numerical integration techniques using finite differences are accurate enough to solve this problem, but one has to be careful of initial directions that point too directly towards an island or that have too large a slope, as those cases can have large numerical errors that might cause the binary search to take the wrong branch.

KalininN used this method to solve the Small dataset, and his solution would have worked for the Large with some minor tweaking.

Solution #3: Dynamic Programming
Another approach is to overlay a grid of points on the map, and use dynamic programming to find the optimal path through them. This is possible because the precision bound for this problem was not too strict, but it is still difficult to get right. A grid with too few points cannot model the optimal path accurately enough to get the right answer, and a grid with too many points would result in a dynamic programming problem that is too big to solve in time.

Gennady.Korotkevich, the only contestant to solve the Large dataset, successfully used this approach. He had two insights that made this method workable.

The ship's path is largely horizontal, with gradual changes in angle; the problem is mostly a question of how to carefully modulate the boat's vertical position. So the grid of points can have coarser horizontal granularity than vertical granularity; Gennady used a ratio of 20 to 1. In the final minutes of the contest, Gennady was testing two solutions, which only differed in their horizontal granularity. The coarser grid turned out to be more accurate, because it was able to model angles more precisely. A grid that was finer in both the horizontal and vertical directions would have been more accurate, but might have been too slow.

The second insight was that the path should have no segments which have a very steep angle, so when computing the optimal value for a point in one column, only points in the previous column within a certain vertical range need to be considered. The size of that range was controlled by a constant called MAGIC in Gennady's code. His solution ran in well under the time limit allowed for the Large, and the answers were within our somewhat generous error bounds.

It is possible to get a very accurate result quickly with this method by starting with a coarse grid, finding the optimal path in that grid, then iteratively improving the path by using finer and finer grids overlaying the space close to the path.

You can download our contestants' submissions to this problem (and others) from the Finals scoreboard.

Qualification Round 2017 - Code Jam 2017

Oversized Pancake Flipper (5pts, 10pts)

Oversized Pancake Flipper: Analysis
Let's start with two simple but key observations. First, the flips are commutative. That is, the order in which you make the flips doesn't matter: flipping at the same positions in any order always yields the same result. Second, you never need to flip at the same position more than once: since flips are commutative, you may rearrange the flips so that two flips at the same position happen consecutively, and then it is clear that they cancel each other out and you might as well not do either.

Small dataset
Applying the observations above to the Small dataset leaves only a really small number of combinations to try. Since there are N-K+1 possible flips, the number of flip subsets is 2N-K+1, which is at most 29 = 512 for this dataset. We can just try all of these combinations, discard the ones that leave at least one pancake blank side up, and get the minimum number of flips or determine that it is impossible.

Even if you don't have the observations above, a breadth-first search of all possible pancake states, using all possible flips as transitions, is also easily fast enough.

Large dataset
Of course, the approach above will take way too long for the Large dataset, which can have up to 1000 pancakes. There is a simple greedy strategy that is not too hard to find. Notice that the left-most pancake p1 is only affected by the left-most flip f1. That means that the initial status of p1 completely determines what do we need to do with f1 if we want to have any chance of leaving all pancakes happy side up: use f1 if and only if p1 is initially blank side up. After deciding on f1, we can notice that there is only one remaining flip f2 that affects the second to the left pancake p2. So, after we have used f1 (or not), the current state of p2 completely determines whether to use f2.

Continuing with this reasoning, we can use the leftmost N-K+1 pancakes to determine all flips. After doing that, we only need to check whether the last K-1 pancakes are happy side up or not. If they all are, then the answer is the number of flips we used. Otherwise, the answer is IMPOSSIBLE. Notice that this reasoning also implies that at most one of the subsets of flips from our Small-only solution above would have worked.

If we carry out the above strategy literally and actually flip all of the pancakes in memory each time, the complexity is O(N2). We can reduce this to O(N) by only flipping one pancake at a time, and keeping a running count of the number of flips that we "owe" the current pancake. For instance, suppose that N = 10 and K = 5, and the first pancake is blank side up. We start with the first pancake, and because we must flip it, we know we must flip the first five pancakes. We increase our flip count to 1, and also make a memo to decrease the flip count by 1 once we reach the sixth pancake. We continue from left to right, and whenever a pancake is blank side up and the flip count is even, or a pancake is happy side up and the flip count is odd, we increase the flip count and make another memo. These memos can be easily stored in another array of length N that is checked before handling each pancake. This optimization is not necessary to solve the Large dataset, but it is a useful trick in programming contest problems (and at least one of your Code Jam engineers has used it in day-to-day software engineering work!)

Qualification Round 2017 - Code Jam 2017

Tidy Numbers (5pts, 15pts)

Tidy Numbers: Analysis
Small dataset
The Small dataset can be solved by a simulation, going backwards through the numbers that Tatiana counted. We start with N and check whether it is tidy. If it is, we are done; if not, we check N-1, and then N-2 if N-1 is not tidy, and so on.

Checking a number for tidiness takes a single pass over its digits. We can bound the number of digits of numbers no greater than N by log10N. Since we iterate through at most O(N) numbers, the overall time complexity of this approach is O(N log N). This is easily fast enough to solve the Small dataset, in which N ≤ 1000, but it will not hold up for the Large dataset, which can have N as large as 1018. If you start at the non-tidy number 111111111111111110, you have a very long way down to count before reaching the answer, 99999999999999999!

To iterate through the digits of an integer, you can use a language-provided utility to convert it to a string, or repeatedly take the value modulo 10 to obtain the last digit and divide by 10 to remove the last digit.

Large dataset: A greedy approach
There is an efficient greedy approach for the Large, but it has some tricky cases.

Let us name the digits of N N1, N2, ..., NL from most to least significant (i.e., left to right, the way in which we usually write digits). Let A be our desired answer: the most recently counted tidy number. We want A to have as many digits from the left as possible in common with N, because that minimizes the value N - A.

Find the first "inversion" in N — that is, the minimum i such that Ni > Ni+1. If there is no such i, then N is already tidy and A = N. Otherwise: no number starting with the sequence N1, N2, ..., Ni can be both smaller than N and tidy. So, we can try making our answer start with A1 = N1, A2 = N2, ..., Ai-1 = Ni-1. The next digit Ai has to be less than Ni, so we can try using Ai = Ni-1. If Ai is greater than or equal to Ai-1, we can make the remaining digits from Ai+1 onward into 9s; this makes A as large as possible while ensuring that it remains tidy. However, if Ai is less than Ai-1, we would just be creating another inversion, and so we should instead try starting A with the digits up to Ni-2. If that doesn't work, we can try the digits up to Ni-3, and so on. We may even start with zero digits of N; for N = 211, we get A = 199. Even starting with zero digits might not work: for N = 100, for example, the answer is 99.

Let's condense these observations into a final algorithm. If there is no inversion, we are done. Otherwise, if the first digit of the inversion is i, find the maximum j < i such that Nj < Nj+1. If there is no such j, set j = 0. Then A starts with N1, N2, ..., Nj, Nj+1-1, and then ends with enough 9s to make it length L. The only exception is if j = 0 and N1 is 1, in which case the answer is L-1 9s.

Since this strategy requires only one pass forward through the digits of N and then another pass backward, it is O(the number of digits in N), which, as we argued above, is O(log N). In this case, we can even get away with not converting the input data strings into integers!

To bypass some of the complexity above, we could have simply tried all combinations of the form SD999...99, where S is each prefix of N (including the empty prefix), D is each possible digit from 0 to 9, and the number of 9s is such that the total length is L. L-1 9s must also be included as a special case. Then the answer must be among those options, and it is the largest tidy number among them that does not exceed N. This is a common trick to simplify the code of greedy solutions: try more options than are needed, as long as the number of options is still tractable. It is often easier to implement finding the best solution among several options than to implement the checks necessary to avoid options we know we do not need.

Large dataset: A combinatoric approach
Another way to solve this with a little bit more math is to notice that there are few tidy numbers. For a fixed length L, the number of tidy numbers is the number of ways to put 8 balls in L+1 bins. Each bin represents a position at the start or end or in between two digits, and each ball represents "increase the number by 1". So, for instance, tidy number 2455 is represented by 1 ball in the first bin (skipping 1), 2 balls in the second bin (moving from 2 to 4), 1 ball in the third (4 to 5), no ball in the fourth (5 is repeated), and the rest of the balls in the last bin (moving from 5 up to 9, but there are no digits left to write). 8 balls in L+1 bins is equal to (L+8 choose 8) which, for the maximum L=18, is less than 2 million. So, we can enumerate all of the tidy numbers, skipping the rest and just return the largest we find which is no greater than N. To enumerate tidy numbers, we can use recursion like in this pseudocode:

best = 1
enum(current_string, current_digit, digits_left):
  if digits_left > 0
    enum(current_string + current_digit, current_digit, digits_left - 1)
    enum(current_string + (current_digit + 1), current_digit + 1, digits_left - 1)
  else
    if best ≤ string_to_int(current_string) ≤ N
      best = string_to_int(current_string)
We can also define a function that finds the next tidy number greedily and use it. We present such a function in the next subsection.

Large dataset: A binary search approach
Our original problem is: given an integer N, find the largest Y ≤ N such that Y is a tidy number. Let's consider a related problem: given an integer X, find the smallest integer Y ≥ X such that Y is a tidy number. That problem turns out to have a significantly simpler solution. Suppose that X is a sequence of digits X1, X2, ..., XL, enumerated from most to least significant, as above. Then Y can be formed by finding the first inversion in the sequence (i.e., the minimum i such that Xi > Xi+1) and creating a new number that is X1, X2, ..., Xi plus enough additional copies of Xi to make Y as long as X. For example, if we start with the number 13254, then the first inversion is at 32, and we replace everything after the 3 with more 3s, forming 13333. If there are no inversions, then X is already tidy and Y = X. This algorithm is O(L) for an X of L digits.

With this related problem solved, we can solve the original problem by binary searching for the smallest range [X, N] such that the Y (as defined above) that corresponds to X is not larger than N. This approach takes O(log2 N) time, because the binary search takes log N steps, and each step requires running the O(L) greedy algorithm above. As argued above, L can be bounded by log N. This approach is less efficient than the greedy approach above, but it is still easily fast enough to pass the Large dataset.

Qualification Round 2017 - Code Jam 2017

Bathroom Stalls (5pts, 10pts, 15pts)

Test set 1
For test set 1, the limits are small enough that you can just simulate the rules outlined in the statement. Most implementations of a simulation will run in O(NK) time and thus finish immediately, but even a slow O(N2K) implementation like "try every possible stall for the next person, and for each empty stall run a loop for each side to check for the closest neighbors" will most likely finish in time.

For test sets 2 and 3, however, something quadratic in the number of stalls won't cut it, so we have to do better.

Test set 2
The critical observation to jump from test set 1 to test set 2 is that only the number of consecutive runs of empty stalls matters at any given time. The next person always chooses the middle stall or the left of the two middle stalls of a longest subsequence of consecutive empty stalls. Moreover, the output format already hints at this: even if you were to choose the rightmost of a set of two middle stalls, or a longest run of stalls other than the leftmost one, the answer would not change. Thus, we can rewrite the algorithm in this equivalent (for the required output) form:

Find any longest subsequence of consecutive empty stalls.
Choose the middle or one of the two middle stalls.
Notice that even though there are still ties to be broken, the output is equivalent for all of them. Since the output is equivalent, so is the multiset of lengths of consecutive runs of empty stalls left behind, so the whole process only depends on that multiset. (As a reminder, a multiset is a set in which the same element can appear more than once.) We can write an optimized simulation that solves test set 2 following this pseudocode:

  S = {N}  - This is a multiset!
  repeat K times:
    X = max(S)
    X0 = ceil((X - 1) / 2)
    X1 = floor((X - 1) / 2)
    if this is the last step:
      we are done; answer is X0 and X1
    else:
      remove one instance of X from S
      insert X0 and X1 into S
If the operations over S are efficient, this will run in quasilinear time. There are many data structures that support insertion, finding the maximum, and removal of the maximum in logarithmic time, including AVL trees, red-black trees, and heaps. Many languages have one such structure in their standard libraries (e.g., the multiset or priority_queue in C++, TreeSet in Java, and heapq module in Python). Since we take O(log K) time for each of K steps, the algorithm takes only O(K log K) time, which is fast enough to solve test set 2. However, for test set 3, even quasilinear time on K is not enough.

Test set 3
The observation required to solve test set 3 is that we are simulating similar steps over and over again. The first time a bathroom user arrives, we partition N into ceil((N - 1) / 2) and floor((N - 1) / 2), which means that numbers between ceil((N - 1) / 2) and N will never appear in S. This hints at a logarithmic number of simulation steps.

Let's divide the work in stages. The first stage processes only N. Then, stage i+1 processes all of the values spawned by stage i. So, stage 2 processes up to 2 values: ceil((i - 1) / 2) and floor((i - 1) / 2). What about the other stages? It is not hard to prove by induction that they also process at most two consecutive values: since stage i processes two consecutive values, they are either 2x and 2x+1 or 2x and 2x-1, for some x (that is, one even and one odd number). Thus, the spawned values for stage i+1 can only be x and/or x-1. Since the largest value in each stage is at most half the largest value of the previous stage, there are a logarithmic number of stages. This all means that there are at most O(log N) different values that go into S at any point. Of course, some of them appear in S many, many times. So, the optimization to get the running time low enough for test set 3 is to process all repetitions of a given value at the same time, since all of them yield the same X0 and X1 values. We can do that by using a regular set with a separate count for the number of repetitions.

  S = {N}  - This is a set, not a multiset!
  C(N) = 1
  P = 0
  repeat:
    X = max(S)
    X0 = ceil((X - 1) / 2)
    X1 = floor((X - 1) / 2)
    P = P + C(X)
    if P ≥ K:
      we are done; the answer is X0 and X1.
    else:
      remove X from S
      insert X0 and X1 into S
      add C(X) to the counts of X0 and X1 in C
Once again, we have structures that implement all the required operations in logarithmic time, yielding an O(log2 N) running time overall. In general, adding any good dictionary implementation to the structure of choice from the test set 2 solution would work, either by plugging the dictionary functionality into the structure (like map in C++ or TreeMap in Java) or having a separate hash-table for the dictionary (which is the easiest implementation in Python).

Moreover, since we proved the population of S is at most 4 at any given time (only values from two consecutive stages can coexist in S), any implementation of set and dictionary will provide all operations in constant time, because the size of the whole structure is bounded by a constant! This makes the overall time complexity just O(log N).

This was a nice problem to put experimentation to work if your intuition was not enough. After solving test set 1, if you print the succession of values for a fixed N, you may spot the pattern of few values occurring in the set S, and from there, you can find the mathematical arguments to support the needed generalization. In harder problems in later rounds, this can become an even more important asset to tackle problems. As you can see in many parts of last year's finals live stream, finalists use experimentation a lot to inspire themselves and/or validate their ideas before committing to them.

Qualification Round 2017 - Code Jam 2017

Fashion Show (10pts, 25pts)

Fashion Show: Analysis
What's really going on here?
The somewhat strange scenario presented in this problem disguises a classic type of chess problem in which pieces must be placed so that they do not attack each other. The eight queens puzzle is one well-known example. Our problem is more complicated, and it involves three types of piece. Let's restate the rules about how the models can interact in more chess-like terms:

The + models are bishops. Two bishops may not occupy the same diagonal.
The x models are rooks. Two rooks may not occupy the same row or column.
The o pieces are queens. Two queens may not occupy the same row, column, or diagonal. Moreover, a queen and a bishop may not occupy the same diagonal; a queen and a rook may not occupy the same row or column.
Observe that our problem does not accord with typical chess rules about "attacks". For example, in our problem, it is fine for a rook and a bishop to share the same row or column. Also, unlike in chess, a piece between two pieces does not prevent them from "attacking" each other. For example, we do not allow two bishops to share the same diagonal even if there is a rook between them.

Decomposing the problem
How will we deal with this variety of pieces? The critical insight is that the rook and bishop parts of the problem are independent; we can solve them separately, placing as many new pieces in each subproblem as possible, and then merge the answers together. A queen is just a rook plus a bishop; we can add each pre-placed queen into the rook subproblems as a rook and into the bishop subproblem as a bishop. Then, once the subproblems are solved, we can turn any cell that is occupied in both subproblem solutions back into a queen.

This strategy is guaranteed not to violate any rules. A rook subproblem solution will never have two rooks in the same row or column, and a bishop subproblem solution will never have two bishops on the same diagonal. Merging the two subproblems' solutions may generate new queens, but it is impossible for them to violate any rules, since that would imply a rule violation in one of the subproblems. For example, we do not need to worry that we will end up with a queen and a bishop on the same diagonal, since that would only be possible if our bishop solution had two bishops on the same diagonal.

Moreover, as long as we place as many rooks as possible in the rook subproblem, and as many bishops as possible in the bishop subproblem, we are guaranteed the maximum possible amount of style points for our test case. Since queens are worth 2 points, merging a rook and a bishop into a queen has no effect on our score.

Let's walk through an example. This case:

+..
+.o
x..
can be decomposed into rook and bishop problems:

... +..
..x +.+
x.. ...
which can be solved independently:

.x. +..
..x +.+
x.. +..
and then merged back together. Note that we have replaced the former x model from the lower left corner with an o model.

+x.
+.o
o..
Solving the subproblems
Now all we need are strategies for the subproblems themselves. The rook problem is straightforward. Each rook removes exactly one row and column from further consideration, so any greedy placement strategy that does not violate the rules will place exactly N rooks.

The bishop subproblem is more challenging. We can approach it differently in the Small and Large datasets.

Bishops: Small dataset
In the Small dataset, any pre-placed pieces are all in the top row. Observe that bishops in the same row or column cannot possibly "threaten" each other, and so we can safely add a bishop to any top row cell that does not already have one. So, if we can come up with a general solution pattern in which the top row is always completely filled with bishops, then we can solve any Small test case, because we can safely turn any pre-placed arrangement into a row packed with bishops.

Once the top row is filled with bishops, where else on the board should we put them? The bottom row is farthest away from the constraints imposed by the top row, and we can try putting a bishop in every bottom-row cell except for the cells on either end (which are "threatened" by the bishops at the two ends of the top row). These bishops do not threaten each other or any top row bishops, so this arrangement is valid, and we have a total of 2N-2 bishops. No additional bishops can be added after that, though. Have we really placed as many as possible?

At this point, we can experiment and convince ourselves that this solution is probably optimal. We can also take a leap of faith; for a Small dataset in a Qualification Round, there is little incentive not to submit the solution and see if it is correct. Or, we can come up with a proof. An N by N board has 4N-2 different diagonals. Moreover, the parallel diagonals of length 1 in opposite corner cells can never both be used simultaneously, so there are really only 4N-4 simultaneously usable diagonals. Since placing a bishop uses up two diagonals, 2N-2 is an upper bound on the number of bishops we can place. So, our method is optimal!

We must still take care, though, to handle a pre-placed rook/queen correctly if one is present, and to merge the rook and bishop solutions appropriately, creating queens when necessary. We must also be careful with the 1 by 1 board, which has no bottom row distinct from its top row.

It is possible to come up with the same construction without realizing that the problem can be decomposed into rooks and bishops; it is just more difficult to justify the optimality of the construction in that case!

Bishops: Large dataset
One helpful observation is that, just as in a chess game, the "white cell" bishops (in our problem, cells for which Ri + Ci is even) are completely independent of the "black cell" bishops (cells for which that sum is odd). So we can consider these as sub-sub-problems.

You can use a bipartite matching algorithm to place the bishops optimally. There is, however, a greedy strategy; unlike in the rook subproblem, however, not just any greedy strategy will work!

Let's consider an 8 x 8 board with no pre-placed bishops. We'll look at just the "black" cells of the board, and tilt the board 45 degrees clockwise. (Here, .s represent black cells. The @s do not represent cells — they are just there to orient the image.)

@@@..@@@
@@....@@
@......@
........
@......@
@@....@@
@@@..@@@
This new board has an important property: any row is a subset of all rows with more black cells than it. For example, the four black cells in the second row are also present in every row with at least four black cells. This property holds regardless of the value of N, or whether we look at "white" or "black" cells. It even holds as we add bishops! Adding a bishop wipes out one entire row and one entire column — notice that we have made this more like the rook problem — and since the remaining rows have all lost the same column, the aforementioned property is unchanged.

The property suggests a greedy strategy: first, sort the rows by the number of available cells. Then, pick a "smallest" row (one with a minimal number of available cells), place a bishop in any column in that row, and wipe out that row and column. This strategy is guaranteed to place an optimally large number of bishops. Suppose that we choose a column C in the smallest row R, and another column C' in some other row R', such that C' is also in R. Then C must also be in R', since all columns in the smallest row are in every other row. We can therefore swap them over, and it is equally valid to choose C' in row R and C in row R'.

One tempting (but incorrect) greedy strategy is to go left to right, top to bottom, and greedily place bishops in all legal places. Here is an example of a board for which that fails.

.@@@@
....@
...@@
..@@@
.@@@@
The incorrect greedy strategy will only place three bishops, as follows:

+@@@@
.+..@
..+@@
..@@@
.@@@@
whereas the correct strategy will place four (here is one optimal placement):

.@@@@
...+@
..+@@
.+@@@
+@@@@
Notice that although we can always place N rooks, the number of bishops we are able to place depends on the pre-placed bishops. This explains why different test cases with the same value of N might have different maximum scores.

Round 1A 2017 - Code Jam 2017

Alphabet Cake (8pts, 13pts)

Attempt 4
check
check
Apr 5 2021, 11:38
remove_red_eye
Attempt 3
WA
remove_circle_outline
Apr 5 2021, 11:34
remove_red_eye
Attempt 2
WA
remove_circle_outline
Apr 5 2021, 11:31
remove_red_eye
Attempt 1
WA
remove_circle_outline
Apr 5 2021, 11:16
remove_red_eye

Alphabet Cake: Analysis
A brute force cell filling Small approach
In the Small dataset, the cake can consist of at most 12 units. A viable brute-force strategy is to try all ways of filling in every blank cell with each letter already appearing on the cake. If L letters are already present, then we have L possibilities for each of the remaining 12-L cells. Then, the number of combinations to try is L(12-L), which is a lot less than 1212. In this case, since L can range from 1 to 12, inclusive, the maximum value that this can take is 57 = 78125, which is a pretty small number for a computer.

One way to check a cake is to first make one pass through the cake, going left-to-right, top-to-bottom, and find each letter's upper leftmost position, lower rightmost position, and frequency. Then, for each letter, check that the rectangle defined by the upper leftmost position and lower rightmost position contains only that letter, and that its area equals the frequency count.

A brute force rectangle-extending Small approach
Another possible strategy is to start at a letter, draw a bounding box around it that includes only ?s, move on to another letter, and so on. In the Small dataset, there are relatively few choices for how to draw these bounding boxes. As long as you use other pre-existing letters to prune the possible bounding boxes for each letter, and you carefully check for overlaps and unused cells, an exhaustive search should be fast enough.

There are non-exhaustive strategies similar to this that have the potential to fail, though. Consider this algorithm: for each letter, start with a 1-by-1 bounding box containing only that letter. Stretch the box as far as it will go to the top and bottom without hitting other letters, then stretch the box as far as it will go to the left and right without hitting other letters. Depending on the order in which you handle the letters, this can fail. For example, consider this case:

  
    A?B
    C??
    ??D
    ?EF
  
If the algorithm proceeds in left-to-right, top-to-bottom order, it will fill up the grid as follows, leaving an unfilled ?:

  
    AAB
    C?B
    CDD
    CEF
  
A recursive Large approach
Fittingly enough for a problem about cake, there is a simple divide-and-conquer strategy for the Large dataset. If the cake has only one letter on it, we can fill it up with that letter. Otherwise, we can make a horizontal or vertical cut that divides the cake into two sub-cakes, such that each part has at least one letter on it; this creates two more instances of the same problem. It is always possible to do this if there are at least two letters on the cake; one surefire way is to make a dividing cut that includes the right border of the leftmost of a pair of letters, or, if they are in the same column, the bottom border of the topmost of the pair.

A greedy Large approach
There is a simple non-recursive approach as well. First, within each row, we can extend each existing letter into all cells to the right of that letter, until we reach another existing letter or the edge of the cake. Then, we can extend the leftmost existing letter (if any) into all cells to the left of that letter.

At this point, any rows that had no existing letters are still empty. We can make a pass through the grid from the second row to the bottom row, replacing each empty row with the row above it. Then we can make one more pass from the next-to-last row up to the top row, replacing each empty row with the row below it.

It is not difficult to prove that it is impossible for this strategy to produce a non-rectangular region for a letter, to produce disjoint regions for a letter, or to leave any cell unfilled.

Round 1A 2017 - Code Jam 2017

Ratatouille (12pts, 23pts)

Ratatouille: Analysis
Small dataset
The number of grams in a given package is not important in and of itself; what is important is which multiples of the recipe that package could be used in. If the base recipe requires 10 g of tomato, for instance, then a 201 g package of tomatoes can be used to make 19, 20, 21, or 22 times the recipe, but not any other multiple. So we can think of this package as the inclusive range [19, 22].

How can we find these ranges? Let Q be the amount in the package and R be the amount of that ingredient required by the base recipe. Then any allowable multiple m must satisfy both 0.9 × R × m < Q and Q < 1.1 × R × m. Simplifying and rearranging this, we get (10/11) × (Q/R) < m < (10/9) × (Q/R); any integer value of m that satisfies this inequality is part of the range, and we can find the lower and upper bounds of this range in constant time. (Note that a range might not include any integer values, e.g., for R = 10 and Q = 15.) It is possible to use integer division here and avoid floating point numbers entirely.

In the Small dataset, there can be at most two ingredients. If there is one ingredient, the problem is very simple: each package with a non-empty range (as described above) forms a kit, and each package with an empty range does not.

What if there are two ingredients? Then we need to pair up the packages into kits, but we must be careful, since not any assignment will be optimal. For example, suppose our recipe requires 5 g of A and 10 g of B, and we have the following packages: 45 and 50 of A, and 110 and 111 of B. Converting these to ranges, we get [9, 10] and [10, 11] for A, and [10, 12] and [11, 12] for B. If we pair the [10, 11] package of A with the [10, 12] package of B, then we will be left with a [9, 10] package of A and an [11, 12] package of B, and those cannot be paired. However, we can make two kits by pairing the [9, 10] package of A with the [10, 12] package of B, and the [10, 11] package of A with the [11, 12] package of B.

We could solve the problem via bipartite matching, but it is not needed. Since there can be at most 8 packages of each ingredient in the Small, it is possible to use brute force to try all 8! possible matchings and find the largest number of kits that can be formed.

Large dataset
The following greedy strategy works for the Large, for any number of ingredients:

Keep making the smallest possible multiple of the recipe that we can.
Whenever we have a choice of packets, we choose one with the smallest upper end of the range. Since we are always making the smallest multiple we can, we do not care about the parts of the ranges up to and including that multiple. We only care about how large the upper ends get. Since the ranges are continuous, a range with a larger upper end is strictly more flexible for future pairings than a range with a smaller upper end.
This strategy would not necessarily be optimal for arbitrary ranges, but it is in this case thanks to the following property: if I1 and I2 are the ranges of valid multipliers for the same ingredient, and I1 is included in I2, then I1 and I2 have at least one endpoint in common. This is not hard to prove: if Ii comes from an original packet of size Si, if S1 ≤ S2, then the lower bound of I1 must be no greater than the lower bound of I2. Otherwise, S1 > S2, so the upper bound of I1 must be no less than the upper bound of I2. Let us call this property 0.

We need some preliminary properties of our greedy algorithm before the main proof:

Property 1: Whenever it chooses to discard a range, then there was no valid kit remaining to use that range for. This follows directly by the condition to discard ranges.
Property 2: Whenever it chooses to use a set of ranges to form a kit, and M is the lower bound of the intersection of all those ranges, there is no other set of ranges left that could possibly make a kit with a multiplier strictly less than M. This follows directly from the order in which the ranges are considered.
Now we can combine all those properties and proceed by contradiction. Let M1, M2, ..., Mk be the smallest multipliers of the kits produced by our greedy algorithm, in non-decreasing order. Let N1, N2, ..., Nk, Nk+1 be the smallest multipliers of an assignment that produces one more kit, in non-decreasing order.

Case A: Let i be the minimum i for which Ni < Mi. By property 2, that can only happen if there is no set of ranges to make a kit with multiplier Ni. That means the greedy algorithm discarded such ranges or used them for some previous kit. Property 1 prevents the discarding. Property 0 and the processing order prevented the algorithm from using it before, because if a range R is chosen at some point, all other remaining ranges for that ingredient have an upper bound that is no less than R's, and therefore, such choice can't prevent a future kit for being formed; all other ranges are, at that point, at least as useful as R for larger multipliers.
Case B: There is no such i. Again, the greedy algorithm must have some range missing not to be able to produce the kit with multiplier Nk+1. However, the same argument as in Case A using properties 1 and 0 shows that the wrong earlier decision that made that range not available couldn't have happened.
We can implement this strategy with a separate list for each ingredient, sorted first by nondecreasing lower end of range and then by nondecreasing upper end of range. We keep looking at the set of earliest ranges of the lists. If they all have an intersection, make them into a kit and remove them. Otherwise, remove the least useful range (the one with the smallest upper limit). We can speed this process up by using a priority queue, but with N × P ≤ 1000, such optimizations are not necessary.

Round 1A 2017 - Code Jam 2017

Play the Dragon (19pts, 25pts)

Play the Dragon: Analysis
Small dataset
The Small limits are large enough to foil pure simulation of all possible choices, so we need to have some insights before proceeding.

The dragon should cure only when it is forced to — that is, when the knight's next attack would defeat it, and attacking or debuffing would not prevent that. Otherwise, it is always better to do something else.
All buffs should come before all attacks, so that each of the dragon's attacks gets the benefit of all of the buffs.
The number of buffs directly determines the number of attacks needed.
All debuffs should come before all buffs/attacks, so that the total amount of damage the dragon must withstand is minimized.
If the knight's first attack will defeat the dragon even if the dragon attacks or debuffs in the first turn, the case is impossible.
If the dragon is forced to cure two turns in a row, then the case is impossible, since that implies that the dragon will have to cure every turn.
These observations add up to a strategy: spend some number D' of turns debuffing, then some number B' of turns buffing, then some number A' of turns attacking, and interleave cures only as needed to not be defeated. Since B' determines A', we only need to consider (D', B') pairs. Since Ak cannot exceed 100, there is no reason to ever do more than 100 debuffs or 100 buffs; moreover, the worst-case scenario can't possibly require more than a couple hundred turns (D' + B' + A'). We can place much smaller upper bounds than those with a little more thought, but it is already clear that direct simulation should be fast enough for the Small dataset.

So, we can proceed with translating the above strategy into code. We must take care to prioritize actions in the right order. In particular, we must avoid curing when we do not need to or failing to cure when we should. Once that is written, we can simulate each possible (D', B') pair and find the global minimum number of turns, or determine that the case is IMPOSSIBLE.

Large dataset
We noted above that all debuffs should come before all buffs/attacks, and that the number of buffs determines the number of attacks. In fact, the buff/attack part of the problem is independent of the debuff part of the problem. Changing the number of debuffs may change the number of cures, but regardless of how many times we debuff, we have nothing to gain by using more than the minimum number of buff + attack turns; that would just make us waste more turns curing.

We can find this minimum number of buff + attack turns as follows. First, we suppose that we will buff 0 times, and we determine the total number of attacks needed to defeat the knight. Then, we can repeatedly increase B' by 1 and calculate the number of attack turns A' required at that new level of attack power. As soon as this causes the total to get larger, we can stop (and take the previous total). It is safe to stop at that point because the total number of turns is given by

B' + ceil(Hk / (Ad + B' × B))

The B' part contributes a line with positive slope; the rest contributes a decaying step function. If that step function were a smooth curve, there would be one point at which the rate of decrease from the curve exactly matched the rate of increase from the linear part, and the function would take on our desired minimum there. Because of the discretization, there may actually be multiple values of B' that yield the minimum number of B' + A' turns, but it does not matter which one we choose; only the total matters.

Finding the minimum B' + A' in this way takes O(sqrt(N)) time, where N is the common upper limit for all of the parameters (109 for the Large). This is because once we have raised B' to about sqrt(N), we can defeat the knight in about sqrt(N) attack rounds, and there is no need to buff further. It is also possible to solve this part of the problem using binary search or ternary search, or by solving a quadratic equation.

What about the number D' of debuffs? The key observation here is that we do not need to consider every possible value of D'. For instance, suppose that Hd is 100, Ak = 50, and D = 1. Reducing Ak to 49 (which takes 1 turn of debuffing) is as good as reducing Ak to 48 or 34; in all of these cases, the dragon has to cure every other turn. However, reducing Ak to 33 (which takes 17 turns of debuffing) means that the dragon only has to cure on every third turn. We might as well only consider these threshold values of D' = 0, 1, 17, 26, 31...; we can leave out the others. We can find each of these values formulaically in constant time.

Once we have these values, we do not even need to simulate them independently. Note that a simulation with D' = 17 begins by looking like a simulation with D' = 1, since we have to do one debuff before we do the other sixteen. So we can perform a single simulation in which we keep track of a number T of turns used so far, and repeat the following:

Pretend that we will do no more debuffing. Figure out how many additional turns are needed to buff + attack while curing enough to survive. Compare the total (T plus that number) to the best total we have seen so far.
Figure out how many turns are needed to increase the number of debuffs to the next threshold value, while curing enough to survive. Add that number to T.
It takes additional turns to debuff more, but that debuffing may "pay for itself" by saving curing turns during the buff + attack phase. Our strategy will find the right balance.

Instead of actually simulating the turns, we can take advantage of the way we have chosen threshold values of D': in the debuffing period between two of our values of D', the frequency of curing remains constant. So we can calculate the total number of debuff turns + cures via a formula, and we can do the same for the number of cures in the buff + attack phase. With this optimization, the complexity of this step is O(sqrt(N)); since we also took O(sqrt(N)) time to find the optimal number of B' + A' turns, the algorithm is O(sqrt(N)) overall.

All that remains is to actually implement the above, which is perhaps even harder than coming up with the algorithm; there are many opportunities to make off-by-one errors! Although this is not generally the case in Code Jam, in this particular problem, the limits do allow solutions with some but not all of the above insights, and additional low-level optimizations, to pass within the 8-minute window.

Round 1B 2017 - Code Jam 2017

Steed 2: Cruise Control (11pts, 14pts)

Attempt 1
check
check
Apr 5 2021, 12:52
remove_red_eye

Pop quiz, hotshots! This problem seems pretty complicated at first glance. What do you do?

One natural strategy is to try binary searching on Annie's speed, but it is difficult to directly determine whether a given speed avoids passing another horse; the input data alone does not tell us where each horse is at any given time, because horses might slow other horses down. In theory, we could figure out when faster horses catch up to slower horses and slow down, determine the exact path of each horse, and check whether our chosen speed crosses any of those paths. With only up to two horses in test set 1, this sort of calculation is feasible, but it would be laborious for test set 2.

However, we can avoid all of that work via some observations. To maximize cruising speed, Annie's horse should reach the destination at exactly the same time as the horse ahead of her (let's call it Horse A); there is no reason to leave a gap. Either Horse A will reach the destination without having to slow down (and so it will be the one that directly limits Annie's speed), or it will be slowed down at some point by the horse ahead of it (let's call it Horse B). The same is true for Horse B: either it will never have to slow down (and so it will be the one that ultimately limits Annie's speed), or it will be slowed down by the horse ahead of it, and so on. So there will be a single "limiting horse" on the road that ultimately determines how fast Annie's horse can reach the destination. We claim that this "limiting horse" is the only horse that matters, and we can disregard all of the others!

It is easy to see that we can ignore the horses to the east of the limiting horse; they will reach and pass the destination before the limiting horse gets there. What about the "intermediate horses" between Annie and the limiting horse? We know from the way we have defined the limiting horse that every intermediate horse will catch up to the limiting horse before reaching the destination. (If one did not, then it would be the limiting horse.) Suppose that Annie chooses a cruising speed that gets her to the destination at exactly the same time as the limiting horse. We certainly cannot go faster than this. Moreover, this speed is safe: it cannot possibly cause Annie to pass any of the intermediate horses. If she were going fast enough to overtake an intermediate horse, then she would definitely be going fast enough to pass the limiting horse, since every intermediate horse will catch up to the limiting horse. This would cause a contradiction. Therefore, we do not need to worry about the intermediate horses or their interactions with each other.

So, once we have identified the limiting horse, the strategy is simple: go at the exact speed that will cause Annie to reach the destination at the same time as the limiting horse. This speed can be found in constant time. We could identify the limiting horse directly via the argument in our third paragraph above, but even this would be unnecessary work. Instead, for each horse in turn, we can pretend that it is the limiting horse and calculate the cruising speed that it would force. Then the smallest of those speeds is our answer. (If any horse allows a faster cruising speed than another, it cannot be the limiting horse, because that cruising speed would cause Annie to pass the true limiting horse.) This takes O(N) time.

Round 1B 2017 - Code Jam 2017

Stable Neigh-bors (13pts, 22pts)

Attempt 4
WA
remove_circle_outline
Apr 5 2021, 13:19
remove_red_eye
Attempt 3
WA
remove_circle_outline
Apr 5 2021, 13:17
remove_red_eye
Attempt 2
WA
remove_circle_outline
Apr 5 2021, 13:09
remove_red_eye
Attempt 1
WA
remove_circle_outline
Apr 5 2021, 12:59
remove_red_eye

Stable Neigh-bors: Analysis
Small dataset
In the Small dataset, all of the unicorns have manes with only a single primary color: red, yellow, or blue. None of these manes share any primary colors, so our only restriction is that two unicorns with the same mane color cannot be next to each other. However, one color might be so common that it is impossible to avoid putting two unicorns with that color next to each other. If we space unicorns with the most common color out, with some other unicorn between each pair of them, then we can accommodate up to floor(N/2) of the most common color. If we have more than that number with any color, the case is impossible.

Otherwise, we can extend this "every other stall" strategy to work for any case. Start at some arbitrary position on the ring of stalls, and place unicorns in the first, third, fifth, etc. stalls, all the way around the ring. When you reach the starting point again, continue to place unicorns in the second, fourth, sixth, etc. stalls. As you do this, place all of the unicorns with the most common mane color, and then all the unicorns with the next most common mane color, and then all the rest. Because all of these colors have a frequency no greater than floor(N/2), it is not possible for this placement strategy to put two unicorns with the same color next to each other.

Large dataset
The Large dataset introduces more complications. Unicorns with a secondary-colored mane (orange, green, or violet) can only have one type of neighbor. An orange-maned unicorn must be next to two blue-maned unicorns; similarly, a yellow must be next to purples, and a red must be next to greens.

How many instances of a primary color do we need to "surround" all unicorns with the corresponding secondary-colored mane? If the secondary color and its primary neighbor are the only two colors available, then there must be equal numbers of those two colors. Otherwise, if there are S instances of the secondary color, we need at least S + 1 instances of the primary color.

Moreover, we might as well put all secondary colors of the same type together (separated by the primary color, of course) in a single chain. Any valid arrangement without that property can be rearranged to have that property. For instance, suppose that we have two separate R-G-R chains separated by two other valid chains Z and Z': R-G-R-Z-R-G-R-Z'-. We can just rearrange this to R-G-R-G-R-Z-R-Z'-, which must also be valid.

A final useful insight is that an R-G-R-G-R chain, for example, acts just like a single R in terms of what can neighbor it on either side. So one strategy for the Large is: first, check for the case mentioned above in which only one secondary color and its neighboring primary color are present. Otherwise, for each secondary color, check that there are enough primary-color unicorns to surround the secondary-color unicorns in a chain. Then pretend that each of these chains is just a single instance of the primary-colored mane from that chain; this reduces the problem to primary colors, and the algorithm for the Small dataset works. You can substitute each chain back in (for some arbitrary instance of the appropriate primary color) once that algorithm has finished.

These Small and Large solutions run in O(N) time, and they are bound by reading the input, checking color frequencies, and printing the output. Other, more complex solutions (e.g., dynamic programming) also exist.

Round 1B 2017 - Code Jam 2017

Pony Express (16pts, 24pts)

Pony Express: Analysis
Small dataset
In the Small dataset, we are given a line of cities and the distances between them, and we want to go from the first to the last city in the line. However, we need to minimize the time and not the distance, and the obvious way to transform between the two (divide by speed) does not work directly, because the horses we can use have different speeds. The solution, as in many minimization algorithms on a path that only moves forward, is to use dynamic programming.

Let us define f(i) as the quickest way to get from city i to the finish line, assuming we start using the horse at city i. Of course, f(1) is the answer to the only query.

We do not know where we need to change horses, but we can try every possible intermediate city j to change horses, thus completely defining f(i) by:

f(i) = min { distance(i, j) / speed(i) + f(j) : for all j such that distance(i, j) ≤ endurance(i) }, for i < N.
f(N) = 0.
This solves the problem in O(N2) because the domain of f is of size N and the iteration to calculate each value of the domain takes time O(N) if we memoize the results. There are other dynamic programming approaches that also work in the same amount of time.

Large dataset
In the Large dataset, we are given a graph G of cities with distances between them instead of just a line, but the setup of the problem is otherwise the same.

As mentioned above, one relatively unusual feature of this graph problem is that the desired result (time) is not in the same units as the weights of the graph's edges (distance). Moreover, the obvious way to transform between the two (speed) is not a constant. This observation leads to a key insight: we should construct a new graph with weights of time instead of distance. That way, we can apply a shortest path algorithm to get the result we want.

We can do this by defining a graph G' that has the same nodes as G, but in which the weight of edge (i, j) is the time that it takes to go from city i to city j. As we said above, there is no fixed speed, but we can fix it. The edge (i, j) then represents the time it takes to go from city i to city j using a single horse, and the obvious choice is to use the horse at the departure city i. Let us call that horse h. In this way, the edge (i, j) on G' represents a path on G, traversed using a single horse. That means the edge (i, j) exists if and only if there is a path between i and j in G with distance less than or equal to h's endurance. Moreover, the weight (i, j) is a time now defined by a single speed, h's speed, and thus it is just the distance between i and j in G, divided by h's speed. You can see that a minimum path from a to b in G' represents a succession of edges in G', that is, a succession of single-horse paths in G, which is exactly what a solution looks like!

Since the limits are low enough, and we need all shortest paths in G to construct G', and many shortest paths in G' to answer many queries, the best option is to use an all-pairs shortest path algorithm. Floyd-Warshall is the easiest and fastest to implement, but others would work too. To also check for existence of paths, you can use the trick of setting weights to "infinity", that is, a distance too large for an actual shortest path (larger than maximum distance × number of total edges). Thus, an infinity distance simply means the edge or path does not exist in the graph.

To summarize, this pseudocode solves the problem:

Apply Floyd-Warshall to input G getting distances between all pairs of nodes.
Create G' by adding all edges (i, j) such that the distance between i and j in G is less than or equal to the horse starting at city i's endurance, and set their weights to that same distance divided by that horse's speed.
Apply Floyd-Warshall to G' to get minimum times between all pairs of nodes.
Read queries and answer immediately from the output of the last step.
This solution takes time O(N3). Both uses of Floyd-Warshall take time O(N3), and creating G' as an adjacency matrix only takes time O(N2).

Round 1C 2017 - Code Jam 2017

Ample Syrup (9pts, 16pts)

Ample Syrup: Analysis
Small dataset
With at most ten pancakes to consider, we can easily enumerate and check all subsets of size K, perhaps using a library function such as Python's itertools.combinations. When we are considering a subset, the statement's rules tell us exactly how to stack them: in nondecreasing radius order from top to bottom. So all we need is a way to calculate a stack's exposed pancake surface area.

Except for the top pancake, any pancake that is not completely covered by the one above it exposes a ring-shaped outer area of its upper surface. It is possible to calculate and sum the areas of these rings, but there is a much easier method. Observe that the exposed top areas of all pancakes in the stack exactly add up to the top area of the bottom pancake in the stack. That is, if you were to look down on the exact center of the stack, ignoring the heights of the pancakes, the view would be indistinguishable from the top of the bottom pancake. So the exposed surface area of a stack is equal to the top area of the bottom pancake, plus the combined areas of all pancakes' sides. This is π × R2 for the bottom pancake, plus the sum (over all pancakes in the stack) of 2 × π × Ri × Hi. The largest such sum that we find after checking all possible pancake subsets is our answer.

Large dataset
For the Large dataset, we cannot afford to check every possible subset, so we need a better approach. Suppose that we choose a certain pancake P to be on the bottom of our stack. Then every other pancake in the stack must have a radius no larger than the radius of P. Recall from our area-calculating simplification above that the other pancakes besides P effectively only contribute their sides to the total exposed surface area, so we do not need to think about their tops. So, out of the pancakes besides P, we should choose the K - 1 of them that have the largest values of Ri × Hi.

So, we can try every pancake as a possible bottom; once we choose a possible bottom, the criterion above tells us exactly which other pancakes we should stack on top of it. We can simply search the list for these each time we try a new bottom, given the low maximum value of N, but we can do better. For example, we can start with one list of pancakes sorted in nonincreasing order by side area, and another list of pancakes sorted in nonincreasing order by radius. Then we can go through the list of possible radii in decreasing order, treating each pancake in turn as the possible bottom; for each one, we choose the first K - 1 pancakes from the list sorted by side area. Whenever we encounter a pancake in the list sorted by side area that comes earlier in the list sorted by radius than our current possible bottom does, we can remove it forever from the list sorted by side area. It is always safe to do this. If that pancake's radius is larger than the radius of our possible bottom, we cannot use it now or ever again, since all future possible bottoms will have a radius that is no larger. If that pancake's radius is equal to the radius of our possible bottom, we have already tried to use that pancake as a bottom previously (since it is earlier in the list sorted by radius), so we have already checked an equivalent stack in which pancakes of the same radius differed only in their order in the stack. Of course, if we encounter our current possible bottom in the list sorted by side area, we should remove that too, because we cannot use the same pancake twice in a stack.

This strategy drops the time complexity to O(N log N (for the sorts) + N × K), which is effectively O(N2) in the worst case. We can further improve upon this by storing our best set of K - 1 as a min heap / priority queue bases on index in the radius list, so that we do not need to check all K - 1 values each time to see whether they have "expired". This drops the complexity to O(N log N + K log K), which is equivalent to O(N log N).

Round 1C 2017 - Code Jam 2017

Parenting Partnering (12pts, 20pts)

Parenting Partnering: Analysis
It is clear that whenever one parent has an activity, the other parent must care for the baby throughout that activity. But how should Cameron and Jamie divide up their remaining time after all activities are covered?

Small dataset
In the Small dataset, there are at most two activities, so we can use casework:

If only one parent has an activity, an optimal solution is to have the other parent care for the baby in a continuous 720 minute block that includes that activity. Then the parent with the activity can handle the other 720 minutes. So only 2 exchanges are required.
If both parents have one activity each, the same idea as above works. It is always possible to choose some way to divide the 24-hour cycle exactly in half, such that Cameron's activity falls completely within Jamie's half and vice versa. So the answer is again 2.
If one parent (without loss of generality, we'll say it's Cameron) has two activities, then they divide the 24-hour cycle such that there is a gap between activity 1 and activity 2, and a gap between activity 2 and activity 1. Jamie has to cover both activities. If one of these gaps is empty (which can occur if the activities are back-to-back) or can be completely filled in by Jamie, then the split the day in half strategy works again and the answer is 2. But if the activities are too far apart and/or too long, Jamie may not have enough remaining time to fill in either gap; in that case, both gaps must contain a switch from Jamie to Cameron and back, so the answer is 4. (We will explain later how we can fill in such gaps without adding more exchanges than we need to, regardless of how much time each parent contributes to filling in the gap.)
It does not take much code to implement the above arguments. However, care must be taken to handle midnight correctly when calculating the lengths of the gaps in the third case.

Large dataset
Let's consider a list of the activities, sorted by time. To gracefully handle the midnight transition and the time before the first activity / after the last activity, we will add a copy of the first activity to the end of the list. In this list, each interval is surrounded by two activities. Some intervals may be instantaneous transitions between activities that are back-to-back (let's call these "empty"); the other intervals represent all of the non-activity time. Each of these other intervals is either surrounded by activities covered by different parents, or by activities covered by the same parent.

We have no decisions to make about the empty intervals, but we must count up the exchanges they add. What about the different-parent intervals? For each, we will have to add some time from one or both parents to cover the interval; we can always do this optimally by starting with the time (if any) from the parent covering the activity on the left, and ending with the time (if any) from the parent covering the activity on the right. This strategy always leaves us with just one exchange. We can do no better than that (since the two different parents guaranteed that there would be at least one exchange), and we have no reason to do worse than that. So we do not need to worry about the different-parent intervals at all! We can assume that whatever time we have available (from one or both parents) can fill them up without creating any new exchanges.

Same-parent intervals require some more thought. If we can fill one in with time from the parent who is covering the endpoint activities, we can avoid an exchange, whereas if we include any time at all from the other parent, we will add two more exchanges. (We can avoid adding more than two by putting the time from the other parent in in one continuous chunk.) We may not be able to avoid the latter, depending on how much available time each parent has left, but we should fill as many intervals with "matching" time as we can, to minimize the number of new exchanges we create. Each interval contributes equally to the number of possible exchanges, but the intervals may have different lengths, and longer ones take up more of a parent's available time. So our best strategy is to sort the list of Cameron-bounded intervals and greedily fill in the shortest ones with Cameron time until we do not have enough Cameron time left to fill in the shortest remaining interval. Then, we do the same for the Jamie-bounded intervals. For every interval we fail to fill in this way, we add two exchanges to our total.

After we have handled the same-parent intervals, we are done and our current total number of exchanges is our answer. Whatever remaining time we have from one or both parents can safely go into the different-parent intervals without creating any new exchanges. As explained above, we do not need to think about the details; we leave those as an exercise for Cameron and Jamie.

This method is O(N log N) (because of the required sorting operations) and is easily fast enough for the Large dataset.

Round 1C 2017 - Code Jam 2017

Core Training (15pts, 28pts)

Core Training: Analysis
Small dataset
In the Small dataset, every core must succeed. The probability that this will happen is the product of the success probabilities of the individual cores: P0 × P1 × ... × PN-1. How can we assign our training units to maximize that product?

We will show that it is always best for us to spend our units on the core with the lowest success probability. Suppose that P0 < P1, and we have some tiny amount Q of units to spend; we will denote P2 × ... × PN-1 as OtherStuff. If we spend the Q units to improve P0, our success probability becomes (P0 + Q) × P1 × OtherStuff. If we instead improve P1, the product becomes P0 × (P1 + Q) × OtherStuff. Expanding those expressions, we find that they are identical, except that the first has a Q × P1 × OtherStuff term where the second has a Q × P0 × OtherStuff term. Since we know that P0 < P1, the first value must be larger. This means that we should improve P0 first.

The argument above has one issue to iron out: what if increasing P0 causes it to rise above P1? By the same argument, we should switch to improving P1 instead at the instant that P0 exceeds P1, and vice versa if they change ranks again, and so on. So, once we have made P0 equal to P1, we should increase both of them at the same time. More generally, we should increase the smallest probability until it exactly matches the next smallest probability, then increase those at the same time until they exactly match the next smallest, and so on. We could try to simulate adding tiny units bit by bit, but it is faster to directly calculate how many units are spent at each step. We must take care to correctly handle the case in which we do not have enough units to fully complete a step.

Large dataset
In the Large dataset, K of the cores must succeed. Now it is no longer necessarily optimal to improve the smallest probability. For example, suppose that N = 2, K = 1, and U = 0.01, and the Pi values for the two cores are 0.99 and 0.01. If we spend all of our 0.01 units on the first core, its success probability becomes 1 and we succeed regardless of whether the second core succeeds. There is no reason to consider spending any units on the second core.

Let's extend this strategy of investing most heavily in a subset of the cores and ignoring the rest. We will sort the cores' success probabilities from lowest to highest, and focus on the ones from some index i onward. As in the Small solution, we will start by improving the success probability of the core at index i to match the success probability of the core at index i+1, then improve those two until they match the success probability of the core at index i+2, and so on. (If all of the success probabilities including and beyond index i become 1, then we can improve the core at index i-1, and so on.) We will show that, for some value of i, this is the optimal strategy. We can then try every possibility for i and keep the one that yields the largest overall answer. Notice that, for one optimal i, at most one "previous" core i-1 needs to be improved, because there is at most one i such that capacity is enough to improve cores i, i+1, ..., N up to probability 1 and have some left to improve i-1 but not up to 1 (otherwise, we can just choose i-1 instead).

First, let us consider whether it is better to improve core i or core i+1. Let Ai and Bi be the probability of exactly K-2 and K-1, respectively, of the cores 1, 2, ..., i - 1, i + 2, i + 3, ..., N succeeding. Let Pi,d be the probability of at least K cores succeding if the success probability of core i is Pi+d and the probability of success of any other core j is Pj. By replacing the definitions and cancelling out some values, we can see that Pi,d - Pi+1,d = (Ai - Bi) × (Pi+1 - Pi) × d. Since (Pi+1 - Pi) × d is positive, improving core i+1 is better than improving core i if and only if Bi > Ai. Moreover, this doesn't depend on the initial success probabilities of cores i+1 and i, but only on their relative values. So, if we improve core i+1 a little bit, it is still better to keep improving core i+1 if we can instead of switching to improve core i.

Now we want to show that there is some i0 such that Bi > Ai if and only if i ≥ i0. That i0 is the core where we want to start improving. Notice that Ai depends on N-2 probabilities, and Ai+1 depends on other N-2 cores, but N-3 of those overlap. Assume fixed N-3 core probabilities and let A(p) be the probability that exactly K-1 of the N-3 fixed cores and an additional core with probability p succed. Define B(p) similarly. We will show that A(p + d) - B(p + d) > A(p) - B(p) for all p and d. Let U, V and W be the probabilities of having exactly K-3, K-2 and K-1 successes out of the fixed N-3 cores. Then, A(p) = U × p + V × (1 - p) and B(p) = V × p + W × (1 - p). Then, B(p) - A(p) is a linear function on p, which means if it ever changes from positive to negative, it must be B(0) - A(0) > 0 and B(1) - A(1) < 0, which implies W > V and V < U. However, this is impossible, because it is a well known fact in probability theory that the function f(k) defined as the number of successes in a given set of independent experiments is convex, so it has no local minimum at K-2.

With the above claim established, we can check all possible values of i and then take the largest overall success probability that we find. To compute the probability of at least K successes, we can use a dynamic programming method similar to the one described in the analysis for last year's Red Tape Committee problem.

It is difficult to prove this method under time pressure in a contest, but we can discover it via brute force simulation, e.g., by dividing up the available units into quanta and exploring all possible ways to partition them among a small number of cores, and seeing which assignment yields the greatest chance of success. It is also possible to arrive at the correct answers via other methods such as gradient descent.

Round 2 2017 - Code Jam 2017

Fresh Chocolate (6pts, 10pts)

Fresh Chocolate: Analysis
The main initial observation to have is that the group sizes are only important modulo P. Then, we can just take the smallest possible equivalent size for each group, and further assume that all of them are within the range [1,P]. Moreover, you may find it easier to map them all to the range [0,P-1], using the modulo operation, as long as you don't mind working with 0, which is not a real group size.

We can see now that a test case with P=2 can be described by 2 integers: the number of groups with odd size, and the number of groups with even size. Similarly, each test case can be described by a tuple of exactly P integers a0, a1, ..., aP-1, where ai is the number of groups of size equal to i modulo P.

The groups counted in a0, i.e., those with a group size multiple of P, are the simplest. No matter when they are brought in for the tour, the number of leftovers after they are given chocolate is the same as before. Therefore, their position won't change how many of the other groups get all fresh chocolate. This implies that it's always optimal to greedily choose a position for them where they get all fresh chocolate, and that we can accomplish that by putting them all at the beginning. Since the starting group gets fresh chocolate, a stream of groups with sizes that are multiples of P will all get fresh chocolate and will leave no leftovers for the next group. That is, we can solve the problem disregarding a0, and then add a0 to the result. After this simplification, there are only P-1 numbers left to consider.

Small dataset
For the Small dataset, there are only two possible values for P, so we can consider them separately.

For P=2, there is only one number to consider: a1. Since all groups are equivalent, there are no decisions to be made. All odd-sized groups will alternate between getting all fresh chocolate and getting a leftover piece. Since the last group will get fresh chocolate if there is an odd number of such groups, the number of groups that get fresh chocolate is ceiling(a1 / 2).

For P=3, there are two numbers to consider: a1 and a2. Intuitively, we should pair each 1-person group with a 2-person group to go back to having no leftovers as soon as possible, and that's indeed optimal: start with an alternation of min(a1, a2) pairs, where we get min(a1, a2) added to our result, and then add pairs, of which min(a1, a2) will get all fresh chocolate. Then add all the remaining M = |a1 - a2| groups (all of size 1 or 2 modulo P, depending on which type had more to begin with). Of those, ceiling(M / 3) will get all fresh chocolate. This last calculation is similar to what we saw for odd-sized groups for P = 2. We will prove the optimality of this strategy below.

Large dataset
The remaining P=4 case is a bit more complicated than just combining our insights from the P=2 and P=3 cases. To formalize our discussion, and to prove the correctness of our Small algorithm, we will introduce some names. Let us call a group fresh if it is given all fresh chocolate, and not fresh if it gets at least one leftover piece. Also, we will call a group with a number of people equal to k modulo P a "k-group".

Given some fixed arrival order, let us partition it into blocks, where each block is an interval of consecutive groups that starts with a fresh group and doesn't contain any other fresh groups. That is, we start a new block right before each fresh group. The problem is now equivalent to finding the order that maximizes the number of blocks. A group is fresh if and only if the sum of all people in the groups that preceded it is a multiple of P. This implies that reordering within a block won't make any other blocks invalid, so it won't make a solution worse (although it could improve it by partitioning a block into more than one block). Also notice that, with the possible exception of the last block, reordering blocks also doesn't alter the optimality of a solution.

Suppose we have an optimal ordering of the groups, disregarding 0-groups as we mentioned above. First, we can see that if a block contains a k-group and a different (P-k)-group, then it only contains those two: otherwise, we can reorder the block putting the two named groups first, and since the sum of people of the two groups is a multiple of P, that partitions the block further, which means the original solution is not optimal. Second, let us show that it is always optimal to pair a k-group with a (P-k)-group into a block. Assume an optimal order with a maximal number of blocks consisting of a k-group and a (P-k)-group. Then, suppose there is a k-group in a block A with no (P-k)-group and a (P-k)-group in a block B with no k-group. We can build another solution by making a new block C consisting of the k-group from block A and the (P-k)-group from block B, and another block D consisting of the union of the remaining elements of blocks A and B. If D doesn't sum up to a multiple of P, that implies that either A or B didn't to begin with, so we can just place D at the end in place of the one that didn't. C can be placed anywhere in the solution. This makes a solution with an additional pair that is also optimal, which contradicts the assumption that a k-group and a (P-k)-group existed in separate blocks. This proves that, for P=3, it is always optimal to pair 1-groups and 2-groups as we explained above.

For P=4, the consequence of the above theorem is that we should pair 2-groups with themselves as much as possible and 1-groups with 3-groups as much as possible. This leaves at most one 2-group left, and possibly some 1-groups or 3-groups left over, but not both. Since we need four 1-groups or four 3-groups to form a non-ending block, and we can use a 2-group to form a non-ending block with only two additional groups of either type, it is always optimal to place the 2-group first, and then whatever 1-groups or 3-groups may be left. Overall, the solution for P=4 is a0 as usual (singleton blocks of 0-groups), plus floor(a2 / 2) (blocks of two 2-groups), plus min(a1, a3) (blocks of a 1-group and a 3-group), plus ceiling((2 × (a2 mod 2) + |a1 - a3|) / 4) (the leftover blocks at the end).

Notice that even though the formality of this analysis may be daunting, it is reasonable to arrive at the solution by intuition, and the code is really short. If you have a candidate solution but you find it hard to prove, you can always compare it against a brute force solution for small values of N to get a little extra assurance. Or there is also ...

A dynamic programming solution
The insight that a case is represented by a P-uple is enough to enable a standard dynamic programming solution for this problem. Use P-uples and an additional integer with the number of leftovers as current state, and recursively try which type of group to place next (there are only P alternatives). Memoizing this recursion is fast enough, as the domain is just the product of all integers in the initial tuple, times P. When the group types' sizes are as close to one another as possible, we get the largest possible domain size, which is (N/P)P × P. Considering the additional iteration over P possibilities to compute each value, the overall time complexity of this approach is O((N/P)P × P2). Even for the largest case, this is almost instant in a fast language, and gives plenty of wiggle room to memoize with dictionaries and use slower languages. A purposefully non-optimized implementation in Python takes at most half a second to solve a case in a modern machine, so it finishes 100 cases with a lot of time to spare. Moreover, just noticing that groups of size multiple of P can optimally be placed first makes the effective value of P be one less, which greatly improves speed and makes the solution solve every test case instantly.

Round 2 2017 - Code Jam 2017

Roller Coaster Scheduling (7pts, 14pts)

Roller Coaster Scheduling: Analysis
Small dataset
There are various ways to solve the Small dataset; for instance, it can be reduced to a flow problem. It is also possible to find a greedy strategy or even a formula: the number of rides needed is max(number of tickets for customer 1, number of tickets for customer 2, number of tickets with seat 1), and the number of promotions needed is max(0, (maximum among counts of i-th seat among all tickets) - number of rides needed). However, these strategies are tricky to prove. The rest of our analysis will shed some light on why they work.

Large dataset
To solve the Large dataset, we can start with observations similar to those in the Small case: the number of tickets held by any one person is a lower bound on the number of total rides, and so is the number of tickets for position 1. This is because the set of tickets held by any one person, and the set of tickets for position 1, share the property that no two tickets in the set can be honored in the same ride, even with promotions. An extension of this is that for the set of all tickets for positions 1, 2, ..., K, at most K of them can be honored by a single ride (possibly using promotions). This means that, if SK is the number of tickets for positions up to and including K, ceil(SK / K) is also a lower bound on the final number of rides.

It is not difficult to see this when there is only one ticket per customer: if the maximum of all those lower bounds is R, for as long as there is some position P such that there are more than R tickets for position P, we can promote any ticket for position P to some previous position that has less than R tickets assigned, which is guaranteed to exist due to the R ≥ ceiling(SP / P). After that, no position is assigned more than R tickets. Since there are no repeated customers, we can just grab one ticket for each position that has tickets remaining and assign them to a ride until there are no more left. This will yield an assignment with exactly R rides, and we proved above that there can't be less than R.

When there are customers holding more than one ticket, our greedy assignment in the last step above might fail. We can still prove that there is an assignment that works with a bit of mathematical modeling. Consider a fixed ride plan. Let us define the ride plan matrix as a square matrix of side S = max(N,C). The first C rows represent customers and the first N columns represent positions. The remaining rows or columns represent fake customers or positions, whose role will be clear in a moment. For each ride in the plan, we construct a one-to-one assignment of customers and positions. Customers that participate in the ride are assigned to their position on it. Customers that do not are assigned to empty positions or fake positions. If there are more positions than customers, each empty position is assigned a fake customer. Then, the value of a given cell of the ride plan matrix is the number of times the represented customer was assigned to the represented position. Notice that the value is an upper bound on the number of times a customer actually rode in the position, but not the exact number.

Notice that for any ride plan consisting of R rides, its ride plan matrix will have rows and columns that sum up to R. This is because for each ride, there is exactly one cell per row and one cell per column that gets a 1 added to it. The most interesting realization is that we can go the other way: for any matrix M such that all its rows and columns add up to R, there is a ride plan consisting of R rides such that M is a ride plan matrix for it. The proof is a simple variation on the Birkhoff–von Neumann theorem, which implies a matrix with that property can be expressed as a sum of permutation matrices, and each permutation matrix corresponds to a possible ride.

A given set of tickets can also be represented by a matrix of side S by having a cell contain the number of tickets a given cusotmer holds for a given position. Let us say that a matrix M is less than or equal to another matrix M', if and only if the cell at row i and column j in M is less than or equal to that the value of the cell at row i and column j in M'. After promotions, we need the matrix representing the tickets to be less than or equal to the ride plan matrix.

Notice now that the greedy promotion algorithm presented in the second paragraph of this section, actually yields a ticket matrix such that no row or column exceeds the established lower bound R, even if there is more than one ticket per customer. The proof is exactly the proof above. What was missing before was a way to know that set of tickets could be turned into an actual ride plan in the case where there is more than one ticket per customer. We now have such a way, which means the originally naive solution is actually a full solution for the problem. Moreover, since we need to report the minimum number of promotions and not the promotions themselves, we can just add SP - R for each position P such that SP > R, and we are done.

Round 2 2017 - Code Jam 2017

Beaming With Joy (12pts, 17pts)

Beaming With Joy: Analysis
Small dataset
There are a lot of ways to solve the Small dataset. The most straightforward way is to use dynamic programming. Since the input contains no mirrors, each laser beam covers a set of non-wall cells that are either horizontally or vertically consecutive. Let us process the columns from left to right. For each row, we will keep track of its current status, which must be one of the following:

It has an incoming beam, which originated in this row in a previously processed column and was not yet blocked by a wall.
It needs a beam — that is, there is at least one cell in this row from a previously processed column that has not yet been covered by a beam, but could be (i.e. there is not a wall in the way).
It does not have nor it can receive a beam, because this would destroy a beam shooter pointing vertically that we saw in this row in a previously processed column.
None of the above. This means that all previous consecutive non-wall cells, if any, are covered with vertical beams, so we can shoot horizontally on this row, but there is no obligation yet to do so.
There are some ways to refine this to fewer combinations, but it's not necessary. Then, we can try all combinations of orientations for the beam shooters in the current column and see if that produces a valid status for the next column (there are more efficient ways to do this part, but this will do). This requires a bit of casework: for each row, we have to combine its incoming status with the current cell. The current cell can be one of five things: a covered or uncovered empty cell, a vertical or horizontal beam shooter, or a wall. Then, since there are 4 possible states for each row, the domain of our recursive function is of size 4R × C and the calculation of each value in that domain requires checking up to 2R combinations of beam shooters' orientations. Each such check requires a linear pass over the R cells, making the overall time complexity bounded by O(8R × C × R). It's not hard to come up with even smaller bounds, given that many combinations of statuses are actually impossible.

There is also a greedy Small solution that relies only on the absence of mirrors:

If a beam shooter can possibly destroy another, then point it the other way. If both directions would destroy another beam shooter, then the case is impossible.
If there are uncovered cells that can only be covered by a single non-yet-fixed shooter, then point that shooter in its direction. If any remaining cells are impossible to cover (because any beam shooters that may have pointed at it had to be fixed in the other direction), then the case is impossible.
Make all beam shooters that are not fixed yet shoot horizontally (or all vertically, which also works).
This works because if any cell remains uncovered after the first two steps, it can only be because there are exactly two remaining non-fixed beam shooters potentially pointing at it: one horizontally, and one vertically. If there had been two or more beam shooters pointing at it in the same direction, we would have already fixed all of them in step 1; if there had been only one beam shooter in total, we would have fixed it in step 2. Then, choosing the same direction for all beam shooters ensures that for all those cells, (exactly) one of the beam shooters is pointed in the correct direction to cover that cell.

Large dataset
The second solution presented above for the Small dataset hints at a possible generalization that solves the Large dataset.

In the Small, each cell is at an intersection of a horizontal line of consecutive non-wall cells, and a vertical line of consecutive non-wall cells. These lines of cells run between walls, between opposite grid boundaries, or between one wall and one grid boundary. Each of these lines of cells is characterized by having either no beam shooter (which forces each cell to be covered from the other direction in step 2), one beam shooter (which leaves the cell on hold until step 3), or more than one (which forces all beam shooters on the line to point in the direction opposite to the line of cells in step 1). In the Large, something similar happens, but instead of simple lines of cells, we have the more complicated notion of paths. A path is a set of pairs (c, d) where c is any cell that is empty or contains a beam shooter and d is either horizontal or vertical. A pair (c, d) and a different pair (c', d') are part of the same path if and only if a beam shooter placed on cell c and pointing in direction d would produce a beam that, ignoring all other beam shooters, would pass through cell c' in direction d'. As an example, the following picture illustrates a grid with 9 paths.

Notice that the red and blue paths pass through the same set of empty cells, although the (cell, direction) pairs that make up those paths are different. Also notice there is an orange path that passes through the same cell twice; that is, the path contains the pair (c, horizontal) and the pair (c, vertical) where c is the rightmost empty cell in the bottom row. Also notice how some paths may contain a single pair, like the turquoise, purple, and brown paths in the picture. Finally, some paths loop onto themselves, like the pink path in the picture, while other paths start and end at a wall or grid boundary. We call the former loop paths.

Paths that go through a number of beam shooters other than one have similar immediate consequences to lines of consecutive non-wall cells in the Small dataset. For convenience, let us define the opposite function o given by o(horizontal) = vertical and o(vertical) = horizontal.

If a path contains two pairs (c, d) and (c, o(d)) where c is a cell that contains a beam shooter, the case is impossible, because that means that beam shooter will destroy itself when pointed in either direction.
If a loop path contains a pair (c, d) where c is a cell that contains a beam shooter, the beam shooter must be fixed in direction o(d).
If a path contains two or more pairs (ci, di) where ci contains a beam shooter, then beam shooter ci must be fixed in direction o(di).
If there is no beam shooter on a path that contains two pairs (c, d) and (c, o(d)) where c is an empty cell, the case is impossible (as c cannot be covered).
If there is no beam shooter on a path, then for each pair (ci, di) it contains: let p be the path containing (ci, o(di)). If p contains a single pair (c, d) where c contains a beam shooter, then fix that beam shooter in direction d. If p contains a number of beam shooters other than one, the case is impossible.
If a beam shooter is required to be fixed in two different directions, by the same or different steps, the case is impossible. This is the generalization of steps 1 and 2 in the Small greedy solution. After applying these, some cells may be left uncovered. As before, those cells are always in the intersection of two different paths, and those paths contain a single pair where the cell contains a beam shooter each. Unfortunately, the way to set the remaining beam shooters so that every cell is covered is not as simple as step 3 for the Small. This might start to look like a (non-bipartite) matching problem, but it is not one!

In the Small, after all the cases that directly fixed the direction of some beam shooter, each empty cell that remained uncovered could possibly be covered by two beam shooters, one on each direction. The same is true in this case: for each empty cell c that remains uncovered there are two different paths that contain the pairs (c, horizontal) and (c, vertical), and each of them passes through exactly one pair that contains a non-yet fixed beam shooter. Say those two pairs containing shooters are (s1, d1) and (s2, d2). Then, covering cell c requires us to either set s1 in direction d1 or s2 in direction d2, or both. If it happens to be s1 = s2, then it must be d1 ≠ d2, and the requirement is fullfilled by any assignment.

If we assign logical variables to the shooters with the logical values true and false corresponding to the two directions, each of these restrictions is a disjunction of two literals. Making all such disjunctions simulatenously true is, then, making their conjunction true. Finding a truth assignment to variables to make a conjunction of disjunctions of up to two literals each true is a problem known as the 2-satisfiability problem or 2SAT. We can use known 2SAT algorithms to solve that problem and get an assignment for the variables, and then translate that assignment back into an assignment of directions for the beam shooters.

The greedy Small-only solution we presented is even simpler to prove correct under this logic model: without mirrors, every disjunction has exactly one literal negated (because the cell requirement is for one beam shooter to be horizontal and/or some other beam shooter to be vertical), so assigning all variables a true value makes all disjunctions true thanks to the non-negated literal, and also assigning them all a false value makes all disjunctions true thanks to the negated literal.

This analysis is written in the order in which one might reason this problem step by step, without any huge leap at any single step. However, for some people it might be faster to have an aha moment and notice that each cell yields a requirement on up to two beam shooters directly: cells without shooters imply a requirement on up to two shooters, one of each direction, to be true. If there is only one of those, that can be encoded as a single-literal disjunction, or made it a two-literal disjunction by simply taking literal L and writing (L ∨ L). Then, beam shooters that possibly point at another beam shooter yield another single-literal disjunction forcing them to point the other way. This makes the first pass unnecessary and encodes all requirements into the 2SAT instance, making the solution more concise.

This problem lends itself to many greedy heuristics plus some kind of bruteforce or backtracking. Notice that 2SAT itself can be solved in polynomial time by some backtrackings and by a graph-theoretical algorithm that has many greedy decisions underneath (both approaches are mentioned in the Wikipedia article). So, many algorithms that don't explicitly use an algorithm for the 2SAT problem are actually correct because they are basically doing the same thing without going through the modeling.

Round 2 2017 - Code Jam 2017

Shoot the Turrets (13pts, 21pts)

Shoot the Turrets: Analysis
Small dataset
The first thing to notice is that, if a given soldier s is not the first to perform her task, there is no reason for s to move at all until all soldiers that are to shoot a turret before s have finished.

Let us call the output a strategy: a pairing between some soldiers and all turrets, in a specific order. Given a fixed strategy, we can check if it's viable with one BFS per soldier, in order. We stop with the first soldier that can't reach its assigned turret, if any, and report the number shot so far. Any strategy that shoots a maximum number of turrets can be extended to a pairing of all soldiers to turrets, so this covers all possibilities. Besides the usual empty spaces and walls, we need to consider some empty spaces enterable but not exitable: the spaces that are in the line of sight of turrets that are to be destroyed by a soldier after the current one. Each check takes time linear in the size of the map, so O(RC) per soldier.

Unfortunately, there can be too many strategies to bruteforce them. There can be up to 10! possible pairings and 10! orderings, which means 10!2 strategies, which is way too big, so we need to do better. We will use precomputing to optimize the check and dynamic programming to optimize the strategy generation.

Let S be the number of soldiers and T the number of turrets. To make the check faster, we can compute, for each soldier and each possible subset of still-alive turrets, the set of reachable turrets to shoot. That requires at most 2T × S BFSes like the ones mentioned above, for a total complexity of O(2T × SRC), which is fine for the Small limits.

Now on to the dynamic programming: consider the function f(s, t) which finds a strategy that shoots a maximum number of turrets for a given set of remaining soldiers s and a given set of remaining turrets t, or decides that there isn't one. Each of s and t can be represented with a binary string of up to S and T digits, respectively. The domain of the function, then, is of size up to 2S+T. To compute the function, we can check every soldier against every reachable turret. Since that list is precomputed, this takes at most S iterations over lists of length up to T. The overall computation of f, if memoized, has a complexity of O(2S+T × ST), which again, finishes comfortably in time under the Small limts.

Large dataset
For the Large dataset, anything with exponential complexity seems doomed to fail, so we need to go a different route. We will reuse the the BFS idea (to check out whether turrets are reachable) from the Small, but there are a lot of additional insights.

Let us build a bipartite graph G with a node for each soldier and turret. G has an edge from soldier s to turret t if and only if soldier s can destroy turret t after all other turrets have been destroyed. A single BFS starting from each soldier can build this graph. We state the following: destroying R turrets is possible if and only if there is a matching in G that covers R turrets. The only if part is trivial, as the conditions for edges in the graph are a relaxation of the conditions for the pairing we need to construct as solution. We concentrate on proving the if part. Moreover, we provide here a computably constructive proof, for which there is an efficient enough algorithm, meaning the proof is also an algorithm that solves the problem.

First, if G is empty, the problem is solvable trivially by vacuity. For non-empty G, we can use the Ford-Fulkerson algorithm to find an initial maximum matching M of G of size R. We will further consider only the soldiers present in M and ignore the others. From now on, we refer only to soldiers matched by M, and we remove the unmatched soldier nodes from G. Let us define the graph G' with the same nodes as G, but an edge between soldier s and turret t only exists in G' if s can destroy t with the other turrets active. Clearly, G' is a subgraph of G. The outdegree of each soldier node in M is at least 1. If an edge (s, t) in G does not exist in G', it is because some other turret t' is reachable by s and blocking the path to t. But then, (s, t') is in G'. Therefore, the outdegree of each soldier not in G' is at least 1.

Consider the graph H that is the union of G' plus the edges in M reversed. If H contains an edge (s, t') where t' is a turret node not covered by M, let M' be equal to M, but replacing (s, t) by (s, t'), where (s, t) was the edge covering s present in M. Of course, M' is a maximum matching of G of the same size as M. If there is no such edge in H, do the following: since H is a supergraph of G', the outdegree of each soldier node in H is at least 1. The inclusion of the reversed edges of M in H means the outdegree of all turret nodes matched in M in H is at least 1. Therefore, starting at any soldier and moving through edges in H, we will always encounter nodes with outdegree 1 of soldiers and turrets covered by M, and eventually find a cycle of them. Let us call this cycle C. Notice that the edges going out of turrets in H are only the reversed edges from M, so C is necesarily an alternation of reversed edges from M and edges from G'. Consider a new matching M' of G consisting of the edges of M whose reverse is not in C, plus the edges in C whose reverse is not in M. That is, M' is M but exchanging the edges present in C in some direction. M' in this case is also a matching of G of the same size as M. If the cycle is of length 2, then M' ends up being exactly the same as M.

In both cases, we constructed a new matching M' of the same size as M with the additional property that at least one edge of M' is present in G'. Therefore, there are some edges in the matching that represent a number A > 1 of actions that we can take now. So, we can just take those A actions, remove all A used soldier nodes and A destroyed turret nodes, and we are left with a smaller graph which also has a maximum matching of size R - A (whatever is left of M'). Rinse and repeat until the graph is empty.

Complexity analysis. Building the original G takes time O(SRC) for S BFSes, and the original matching takes time O((S+T)3) (there are faster algorithms for matching, but Ford-Fulkerson is more widely known, simpler to code, and sufficient for this task). After that, we have at most T steps of altering the matching and removing some parts of it. Each of this steps requires building G', which takes time O(SRC) for S BFSes, and after that, all steps are linear in the size of the graphs G, G' or H, which are all bounded by O((S+T)2). Notice that building G' is by far the slowest step, so you can use less efficient implementations to manipulate the graphs without altering the time complexity. This gives an overall complexity of O(TSRC + (S+T)3). This is enough to solve the problem, but it can be further refined by noticing each time we build G', the BFSes will reach the same places or farther than in the previous step, so, if instead of restarting from scratch we remember the BFSes and continue from where we were stopped before, we can reduce the total time to build G' by a factor of T, down to O(SRC), reducing the final complexity to O(SRC + (S+T)3).

Round 3 2017 - Code Jam 2017

Googlements (3pts, 10pts)

Googlements: Analysis
Small dataset
In the Small dataset, the googlements can have a maximum length of 5. We will discuss googlements of that length; the procedure for shorter ones is the same.

Any string of length 5 that uses digits only in the range [0, 5] and is not 00000 is a googlement; there are 65 - 1 = 7775 of these. One workable solution is to iterate through all of these possible googlements and simulate the decay process for each one until it reaches the sole possible looping end state of 10000. As we do this, for each googlement, we maintain a count of how many decay chains it appears in. When we are finished, we will know how many possible ancestors each googlement has, and so we will have the answer to every possible Small test case!

Later in this analysis, we will show why 10000 is the only googlement of length 5 that is its own descendant, and we will look into how long a decay chain can go on. Even if each chain were somehow as long as the number of googlements, though, with only 7775 googlements, this method would still easily run fast enough to solve the Small.

That approach is "top-down"; a "bottom-up" approach to this tree problem also works. For a given googlement, we can figure out what digits must have been in the googlement that created it (its "direct ancestor"). For example, if we are given 12000, we know that any direct ancestor must have one 1, two 2s, and two 0s to bring the total number of digits to five. Then we can create all such googlements (e.g., 12020) and recursively count their direct ancestors (e.g., 42412) in the same way. This process does not go on forever, because (as we noted above) there are at most 7775 googlements in the tree. The worst-case scenario is 10000, which is a descendant of every googlement.

After thinking through the bottom-up approach, only implementation details remain. The toughest part is generating direct ancestors with particular counts of particular digits. A permutation library can help, or we can write code to generate permutations in a way that avoids repeatedly considering duplicates. We can save some time across test cases by memoizing results for particular nodes of the tree, since the decay behavior of a given googlement is always exactly the same, and some of the same googlements could (and probably will) show up many times in different cases.

Large dataset
In the bottom-up Small approach outlined above, we spent a lot of time generating new direct ancestors to check. For ancestors such as 12020 that themselves had direct ancestors, this was necessary. However, it was a waste of time to construct and enumerate ancestors such as 42412, which have no ancestors of their own. More generally, a googlement of length L with a digit sum of more than L cannot have any ancestors, since it is impossible to fit more than L digits into a length of L.

It turns out that avoiding enumerating these ancestor-less ancestors saves enough time to turn that bottom-up Small solution into a Large solution. We can do that with some help from combinatorics and the multinomial theorem.

Let's think again about the direct ancestors of 12020. Each must have one 1, two 2s, and two 4s. How many ways are there to construct such a string? Starting with a blank set of 5 digits, there are (5 choose 2) = 10 ways to place the 2s, then (3 choose 2) = 3 ways to place the 4s into two of the three remaining slots, then (1 choose 1) = 1 ways to place the leftover 1. These terms multiply to give us a total of 30 ways, so 12020 has 30 direct ancestors. Since each has a digit sum of 13, which is greater than 5, none of them have their own ancestors. So we do not care what they are; we can just add 30 to our total.

At this point, we can either test this improvement on the worst-case googlement 100000000 before downloading the Large dataset, or we can reassure ourselves via more combinatorics. The googlements of length 9 with ancestors are the ones with a digit sum less than or equal to 9; we can use a balls-in-boxes argument to find that that number is (10 + 9 - 1)! / (10! * (9-1)!), which is 92377. This is a tiny fraction of the 999999999 possible googlements of length 9; we have avoided enumerating the other 999900000 or so! As long as our method for generating ancestors doesn't impose too much overhead, this is easily fast enough to solve 100 Large test cases in time, even if most or all of them explore most or all of the tree.

Appendix: Some justifications
Let's prove that there is only one looping state for a given googlement length, and that there are no other loops in the graph. We will start with some observations.

When a googlement decays, the number of non-0 digits in the googlement equals the sum of the digits in the googlement it decays into.
Because of this, any googlement that has a digit other than 0s or 1s will decay into a googlement with a smaller digit sum.
Any googlement consisting only of 0s and 1s will decay into a googlement with the same digit sum. If the googlement is 1 followed by zero or more 0s, it will decay into itself. If the googlement has a single 1 at another position, it will decay into a 1 followed by 0s. Otherwise, it must have at least two 1s, so it will decay into a googlement with the same digit sum but with at least one digit other than 0 or 1.
We can draw some useful conclusions from the above observations:

The only googlement of length L that can decay into itself is a 1 followed by L-1 0s. For any other googlement, decay would either reduce its digit sum or create a new googlement with a digit other than 0 or 1, which would itself decay into a googlement with a smaller digit sum.
It can take at most two steps for a chain of googlements to lose at least one point of digit sum. This puts a bound on the height of the decay tree; it can be at most 2 times the maximum possible digit sum. (Moreover, it is even less than this; for example, a googlement of 999999999 will immediately decay into 000000009, losing a large amount of its digit sum.

Round 3 2017 - Code Jam 2017

Good News and Bad News (7pts, 19pts)

Good News And Bad News: Analysis
The ordered pairs of friends form a directed graph in which each friend is a node and each news path is an edge. Notice that we can work on one connected component at a time, because each can be solved independently of the others. In the following, we assume a connected graph.

One additional observation is that if any edge of the undirected graph is a bridge — that is, an edge not present in any cycle — then the task is impossible. This is a consequence of the news balance on nodes extending to subsets of nodes: if we group nodes on each side of the bridge, the aggregate balance of each subset must be zero, and values in edges internal to the subset don't affect that balance. Since the bridge is the only external edge, this forces its value to be zero. Formally, the news balance property is preserved by vertex identification, and we can identify all vertices on each side of the bridge to reach a contradiction.

Note that since we can pass positive or negative news on each edge, we can forget about the directions of edges in our graph, and consider it as undirected. If the directed version had opposing edges (v, w) and (w, v), the undirected version can just have two edges between v and w. After solving the problem on the undirected graph, we simply adjust the sign of the value we assign to each edge according to its original direction.

Small dataset
The Small dataset can have up to 4 friends, so there can be as many as 12 edges. The bad news is that with up to 2 × 42 = 32 possible legal values for each of those edges, there can be 3212 possible answers to check, which is far too large to handle with brute force. The good news is that there are only on the order of 212 graphs to consider; we can rule out the ones that have bridges (since those are impossible), and then run whatever method we come up with on all of the others to see if we get solutions.

What if we don't really need the full extent of the [-16, 16] range? We can try restricting our values to a smaller set and see whether we get solutions. As Sample Case #5 demonstrates, we might need negative values, so we can try a set like {-2, -1, 1, 2, 3}, for instance. It turns out that this set alone is enough to solve all possible Small cases that do not contain a bridge, so we can use brute force to find all possible assignments.

Large dataset
Judging by the Small solution, we may believe that all bridgeless graphs do have a solution, and that's indeed the case (as we will see in Solution 2, below).

Solution 1. Start by using depth-first search to build a spanning tree of the underlying undirected graph. Assign all non-tree edges a random nonzero value in [-K, K]. Now, the current balance of each node is some integer. In leaf-to-root order, we can go through all nodes using the single non-assigned edge remaining to make the balance zero. Notice that if we fix all F-1 non-root nodes, the root is automatically fixed, because the sum of the balance over all nodes always equals 0 (an edge with value v contributes v to one balance and -v to another). Notice that it is possible that a node's balance is 0 when we need to fix it, leaving us no non-zero choice for the corresponding tree edge. Additionally, if the balance on a node is too high, the value we need for the corresponding edge could be out of range. However, a value for K around F yields a small probability of both, which we can find through experimentation. And, if we happen to run into a problem, we can re-run the algorithm. As long as the probability of success is a not-too-small constant, running it enough times will eventually yield a valid assignment. Since we can implement this in linear time, and the bounds are small, we can afford thousands of runs per test case, or possibly more. So, even with weak experimental data on the probabilities, we can build enough confidence that this method works. Notice that we can avoid explicitly checking for bridges by assuming that failure after 1000 or so retries means the case is impossible.

Solution 2. Again, find a DFS spanning tree of the graph and number the nodes in discovery order. Remember that in a DFS over an undirected graph, every non-tree edge goes from a node to some ancestor in the tree. Direct all edges in root-to-leaf direction (we reverse or split edges after solving, as explained above). We assign edges not in the tree a value of 1, that is, they send positive news from nodes to descendants. On the other hand, we assign negative values to tree edges. We process the nodes in reverse discovery order: as in the previous randomized solution, each node has a single adjacent edge with no assigned value, so we assign it the only value that makes the node balanced.

Let us prove that the values assigned this way are all negative if there is no bridge or contain a zero if there is a bridge: assume a value of 0 for unassigned edges to check the current balance of a node x right before we attempt to balance it. Let A be the set of descendants of x, including x itself. There is exactly one tree edge going into A: the edge that we are trying to assign at this step. Let us call it e. All other edges going into A are non-tree, so they all have a value of 1. The current aggregate balance of A then equals the number of non-tree edges going into A. Since nodes are processed in leaf-to-root order, all nodes in A other than x are balanced, and A's balance equals x's balance. If the number is zero, then there are no edges other than e going into A, which makes e a bridge. If the number is greater than 0 (it is a number of edges, so it can't be negative), the current balance of x is greater than 0 and we can assign its opposite, a negative number, to e. This value is always within the legal [-F2, F2] range, because its magnitude is equal to some number of edges in the graph, and the total number of edges in the graph cannot exceed F × (F - 1), which is less than F2.

Solution 3. We keep the nodes balanced at all times. Start with 0 on all edges. For each undirected edge, if it has a 0 in the current valuation, find a cycle that contains it and then add -K to all edges of the cycle, where K is an integer different from the current values of all edges in the cycle. This choice of K guarantees that the procedure doesn't change a non-zero edge into zero, and it always changes at least one zero edge into a non-zero (the current edge), so it always finishes. Adding a value to edges in a cycle maintains the balance of each node. If we choose K with minimum possible absolute value, and cycles as short as possible, this won't exceed the range. The easy to prove maximum absolute value this can produce is F × P / 2, because K is always in the range [-F/2, F/2] as there can't be more than F forbidden values for it. There are also at most P steps. However, there are many results showing that graphs with many edges have lots of short cycles, so we believe it's impossible to construct a case in which the range is exceeded. Further, if we randomize the order to fix edges and retry many times, we can work around the worst cases that specifically try to overload a given edge, but possibly produce lots of small cycles that help fix the others cheaply. Notice that the idea of using the DFS tree to fix balances in the previous solutions is actually the same idea from this theoretically simpler solution, but using the fundamental cycles of the graph instead of potentially all of them.

Round 3 2017 - Code Jam 2017

Mountain Tour (6pts, 24pts)

Mountain Tour: Analysis
Small dataset
Since each hiking tour can be taken only once, each of the hiking tours arriving at a camp must be followed by a different departing tour. Since each camp has two arriving and two departing tours, there are exactly two ways to “pair” the hiking tours at a camp: the first arrival with the first departure (and the second arrival with the second departure), or the first arrival with the second departure (and the second arrival with the first departure). We can think of a candidate solution as specifying this boolean decision for each camp. The total duration is the sum of each hiking trip's duration and the amount of time waiting at each camp.

When evaluating a possible solution, you must check whether or not every hiking tour is present on the path beginning with your start hike. It is possible for a path through the graph to represent a cycle with fewer than 2C edges. For example, the left side of the figure farther down on the page shows a setup with three disjoint paths through the graph.

Because the time spent waiting at the base camp is calculated differently than the time spent waiting at all other camps, the base camp needs to be handled as a special case. You could simply run the algorithm four times, once for each “start” and “end” hike.

The entire space can be explored in O(2C) time, which is sufficient for the Small dataset with C ≤ 15.

Another way to look at the small dataset is as a special case of the Travelling Salesman Problem (TSP). Consider the directed graph in which each hiking tour is a node and edges are the possible pairings between hiking tours, four for each camp, with edge weights corresponding to the amount of time you would have to wait at the camp in order to make that transfer. For example, if hiking tour A arrived at node 1 at time 02:00, and hiking tour B left node 1 at time 06:00, there would be an edge from A to B with weight 4. Running TSP on this graph, again with special handling for the base camp, can also yield a correct solution.

Large dataset
Let's look more closely at the two possible arrival-departure pairings for each camp. Suppose a camp has tours arriving at 13:00 and 21:00 and tours leaving at 22:00 and 07:00, such that there are no departures between the times that the two tours arrive. If the 13:00 arrival were paired with the 22:00 departure and the 21:00 arrival were paired with the 07:00 departure, the total wait time at the camp would be (22 – 13) + ((7 – 21) mod 24) = 19 hours. If we did the other pairing, the total wait time would be (22 – 21) + ((7 – 13) mod 24) = 19 hours. Both pairings result in the same total wait time at the camp. We will call these camps the “free” camps.

Now consider a camp having tours arriving at 11:00 and 23:00 and tours leaving at 17:00 and 08:00, such that after each tour arrives, there is a tour that leaves before the other tour arrives. If the 11:00 arrival were paired with the 17:00 departure and the 23:00 arrival were paired with the 08:00 departure, the total wait time at the camp would be (17 – 11) + ((8 – 23) mod 24) = 15 hours. If we did the other pairing, the total wait time would be ((17 – 23) mod 24) + ((8 – 11) mod 24) = 39 hours, 24 hours longer than the other pairing.

We mentioned that for the small solution, you must ensure that every hiking tour is present on the path starting from the base camp. What does the graph look like if there are hiking tours that are not present on that path? It is a set of disjoint cycles. Each camp has two paths crossing through it. If those paths are on different cycles, switching the pairing of that camp will in effect merge the two cycles together.

For example, consider the following figure with 5 camps. Camps 2 and 5 are free, and camps 3 and 4 are 24-hour switches. The initial graph has 3 disjoint cycles. As it turns out, to merge all of the cycles together, it is necessary to switch the pairing of camp 5 for free and either camp 3 or camp 4 for a 24-hour penalty. Note that the red path, the one starting and ending at the base camp, does not pass through camp 5 prior to switching the pairings, but switching the pairing of camp 5 is nonetheless required.

A graph with three disjoint cycles requires switching two camp pairings. One of the switches is free, and the other costs 24 hours.

In this initial state, wait times are minimized, but there are several disjoint cycles.

The same example, modified to have only one cycle.
Now there is only one cycle.

Thus, to solve the problem efficiently, add each hiking tour to a Disjoint Sets data structure. Iterate through each camp. If the camp is free, union all four hiking tours connected to that camp into the same set (this is analogous to letting the free camps take either pairing); otherwise, union each arrival with only its respective departure. You will end up with one disjoint set per cycle in the graph. If there are Q > 1 disjoint sets, since all of the free camps have already been accounted for, you will need to switch the pairing of Q – 1 camps with 24-hour penalties. These Q – 1 camps are guaranteed to exist. The base camp still needs special handling; you can afford to run the algorithm once for each of the four possible start and end hikes. The total duration is the sum of all the hiking tour durations plus the lowest waiting time at each camp plus 24 hours for each of the Q – 1 penalties. This solution takes O(C α(C)) time, where α(C), the inverse Ackermann function, is the amortized time per operation on the Disjoint Sets data structure.

Round 3 2017 - Code Jam 2017

Slate Modern (5pts, 26pts)

Slate Modern: Analysis
Small dataset
To formalize the reasoning about this problem and yield both an algorithm and a proof of its correctness we will resort to some graph theory. Consider an undirected graph G where the set of R × C + 1 nodes is the set of cells in the painting plus an additional special node we call root. We add an edge between each pair of adjacent cells with length D and for each cell that has a fixed brightness value v we add an edge from the root to the cell node with length v.

Let p(c) be the length of the shortest path on G from the root to the node that represents cell c.

Property 1. The p(c) is an upper bound on the value that can be assigned to c. Let
root, a1, a2, ..., ak=c
be the path of k edges from the root to c of total length p(c). By construction of G, a1 is a fixed cell. Let v be a1's value. Also by construction, the edge (root, a1) has length v and all other edges (ai, ai+1) have length D. Thus, p(ai) = v + (i - 1) × D. Clearly p(a1) = v is an upper bound on its value v, and by induction, if p(ai) is an upper bound on the values that can be assigned to ai, p(ai+1) = p(ai) + D is an upper bound on the values that can be assigned to ai+1 because it can't differ by more than D with the value assigned to ai because the corresponding cells are adjacent (any edge from G not involving the root connects nodes representing adjacent cells).

Property 2. Let c be a fixed cell with value v. If p(c) ≠ v, then the case is impossible. Since there is an edge (root, c) with length v, p(c) ≤ v. And, if p(c) < v, the case is impossible by Property 1.

Property 3. If p(c) is exactly the value assigned to c for all fixed cells c, then the case is possible and assigning p(c) to each non-fixed cell c is a valid assignment of maximum sum. By the precondition we know that p assigns all fixed-cells their original value, so we only need to check if neighboring cells are assigned values that differ by no more than D. Let c and d be two neighboring cells. Since G contains an edge (c, d) with length D it follows by definition of shortest path that p(c) ≤ p(d) + D and p(d) ≤ p(c) + D. Since p is a valid assignment, and by property 1, it assigns all cells a maximum value, it follows immediately that p is a valid assignment of maximum sum.

This yields an algorithm to solve the Small dataset: calculate the shortest path from the root to all other cells using Dijkstra's algorithm and then use Property 2 to check for impossible cases. If the case is possible, the answer is just the sum of p(c) over all cell nodes c. Dijkstra's algorithm takes O(R × C × log (R × C)) time while both checking for Property 2 and summing take O(R × C) time. Therefore, the overall algorithm takes O(R × C × log R × C) time, that fits comfortably within the Small limits.

Another similar approach that sidesteps the graph theoretical considerations is noticing that by transitivity, a cell at S orthogonal steps of a fixed-cell with value v cannot be assigned a value greater than v + S × D. That means that if two fixed cells are at S orthogonal steps and their value differs by more than S × D the case is impossible. Otherwise, it can be shown that it's possible and a valid assignment of maximum sum results from assigning each cell the minimum of v + S × D over all values for v and S for each fixed cell (which is, of course, the exact same assignment as p() above). Checking all pairs of fixed-cells takes O(N2) time and finding the assignment takes O(R × C × N) time, which means this yields an algorithm that takes O(R × C × N) time overall and it also passes the Small. The claims above can be proven directly, but the graph notation makes it easier, and the claims in both cases are essentially the same. Additionally, the graph theoretical approach directly yields a more efficient Small-only solution.

Large dataset
The size of the grid in the Large dataset is too big to inspect the cells one by one, so we need a different approach. The foundations built while solving the Small dataset, however, are still tremendously useful. We define the same G and function p as before.

Property 4. For every cell c there is a shortest path in G from the root to c of length k:
root, a1, a2, ..., ak=c
where a1 is a fixed cell and there is some i such that each edge (aj, aj+1) come from a horiztonal adjacency if and only if j < i. In plain English, there is path that goes from the root to a fixed cell a1, then moves horizontally zero or more times, and then moves vertically zero or more times. We can prove this easily by noticing that if the last k-1 steps include h horizontal and k-1-h vertical steps of any shortest path, a path that does h horiztonal steps first and k-1-h vertical steps last will reach the same destination (through different intermediate cells), and since the length of the edges of all horizontal and vertical is the same (D), the resulting path is also a shortest path.

Let us call our original R × C matrix M. We can use a technique called coordinate compression to consider only the interesting part. We call a row or column interesting if it is in the border (top and bottom rows and leftmost and rightmost columns) or if it contains at least one fixed cell. The interesting submatrix M' is the submatrix of M that results on deleting all non-interesting rows and columns. Notice that M' contains all fixed cells of M, and possibly some non-fixed cells as well. The size of M' is however bounded by (N+2)2, which is much smaller than R × C in the largest cases within Large limits.

Define the graph G and shortest path function p for M in the same way as for the Small. Define also a smaller graph G' whose nodes are a root and cells that are in M'. G' contains one edge (root, c) with length v for each fixed-cell c with value v. Notice that, since M' contains all fixed cells, the root and all its outgoing edges in G' are the same as in G. G' also contains an edge connecting two cells that are orthogonal neighbors in M'. The length of each such edge (c, d) is S × D where S is the distance in orthogonal steps between c and d in the original matrix M.

In the depicted input matrix there are 3 fixed cells. The 3 interesting rows and 4 interesting columns are highlighted in light red, and the cells in the intersection of an interesting row and an interesting column are highlighted in dark red. Those dark red cells make up M' and the nodes of G' (besides the root). The fixed cell containing a 3 has two neighbors in G'. The edge going to its vertical neighbor has length 2 × D, because it is 2 steps away in the original M. Similary, the edge going to its horizontal neighbor has length 3 × D.

Having G', we can define p'(c) as the shortest path in G' from the root to each cell c that exists in M'.

Property 5. For each cell c that exists in M', p'(c) = p(c). To prove this, consider a shortest path
root, a1, a2, ..., ak=c
in G from the root to c with the hypothesis of Property 4 (that is, it does horizontal steps first, and vertical steps last). First, notice that if the edge going into ai is horizontal and the edge going out of it is vertical (that is, ai is the only corner), then ai is in M', because it shares a row with a1, which is a fixed cell and thus it is in M', and a column with ak=c, which is also in M'. Between a1 and ai all moves are horizontal, so we can "skip" the ajs not in M', and the length of the edges in G' will exactly match the sum of the lengths of all intermediate edges. The analogous argument works for all the vertical moves between ai and ak.

Notice that Property 5 implies that we can use a similar algorithm over G' to distinguish the impossible cases, as we can calculate p' and then know the value of p'(c) = p(c) for all fixed cells c, which we can use to check Property 2. We still need, however, a way to know the sum over all p(c) for the possible cases, which we can't calculate explicitly.

Let a cell c in M be at row i and column j. Let i0 be the largest interesting row that is no greater than i, and i1 be the smallest interesting row that is no less than i. Notice that i0 = i = i1 if i is interesting and i0 < i < i1 otherwise. Similarly let j0 and j1 be the closest interesting columns to j in each direction. We call the up to 4 cells at positions (i0, j0), (i0, j1), (i1, j0) and (i1, j1), which are all in M', the sourrounding cells of c.

Property 6. For each non-fixed cell c in row i and column j of M there is a shortest path from the root to c in G that goes through one of its sourrounding cells. This can be proven similarly to Property 4. After the first step of going from the root to the appropriate fixed cell a1, any path that has the minimum number of horizontal and vertical steps yields the same total length. Notice that there is always a sourrounding cell that is closer to a1 than c (that's why they are "sourrounding"). Therefore, we can always take a path that goes through that sourrounding cell.

Given Property 6, we can build G', calculate p', and then solve each contiguous submatrix of M delimited by interesting rows as a separate problem. Each of those problems is an instance of our original problem in which we have fixed exactly the four corners. There is overlap in the border among these subproblems, but we can simply subtract the overlapping from the total. Calculating the sum of the overlapping part requires a few of the simple observations required to calculate the problem with four corners fixed, so we concentrate on solving the following: for a given matrix size and values in its 4 corners, calculate its sum (modulo 109+7). It's important not to do modulo while calculating p or p', as that can ruin the calculation because we have inequalities in calculating shortest paths, and inequalities are not preserved under modulo operations. Notice that, if U is an upper bound for the given fixed values for cells, the longest path consists of at most 2 × U steps, so the highest value in the image of p is at most U + 2 × U × D, which is at most U + 2 × U2. With U up to 109, that value fits in a 64-bit signed integer, which means we don't need large integers to hold on taking the results modulo 109+7 until the summing part.

Let us call the matrix A and the 4 corners tl, tr, bl and br for top-left, top-right, bottom-left and bottom-right. As we argue in the Small dataset section, each cell's value is determined by one of the fixed cells, in this case, one of the four corners. Given the existence of the shortest path tree, the region of cells that are determined by each given corner (breaking ties by giving an arbitrary priority to corners) is contiguous. For a given corner x and cell c, let us call the influence of x over c i(x,c) to the fixed value x has plus S × D where S is the number of orthogonal steps between x and c. We call lower values of influence stronger. The corner that determines the value of a cell is therefore any of the ones with the strongest influence.

Now, consider the top row: tl has a stronger influence than tr over a left-most contiguous set of cells, and tr has a stronger influence over a right-most set of cells. There may be a single cell where the influence strength is equal. It is not hard to prove that the column at which the strongest influence switches from being tl to tr (if looking from left to right) is the same in this top row as in any other row, because the influence values from tl and tr for the i-th row (from top to bottom) are exactly the same values as the values in the top row plus i × D, so the most influential between tl and tr is always the same across cells in any given column. A similar thing happens with each pair of non-opposite corners. There are 4 such pairs. If we consider the lines that split the influence region of each of those pairs, we have up to 2 vertical and 2 horizontal lines (some of them may overlap), dividing A into up to 9 pieces. All except the middle piece have a single corner that has the most influence, thus they can be solved in the same way.

Consider a matrix of r rows and c columns with a single influential corner with value v. The sum of a row containing the value v is v + (v + D) + (v + 2 × D) + .... This summation can be calculated with a formula. And then, the sum of each other row is c × D larger than the previous one, as we add D to the sum for each column. Again, this yields a summation over a known linear function, which can be reduced by the same known formula.

The middle piece of A has influence from two opposite corners (which pair of opposites depends on the order of the lines). We can again partition A into up to 3 parts: rows with influence from one corner, rows with influence from the other corner, and rows with influence from both. Two of those can be summed with a similar formula as the single influential corner case. The rest is a rectangle partitioned into two stepped shaped pieces where the influence is divided. Those laddered pieces can be summed as the summation over a certain range of a quadratic function, which can also be reduced to a formula.

This finishes the problem. There are lots of technical details, specifically math details, that aren't covered in detail, but we hope this conveys the main ideas. We encourage you to fill in the gaps yourself and ask the community to help out if you can't, as it will be really good practice for your next contest.

World Finals 2017 - Code Jam 2017

Dice Straight (10pts, 15pts)

Dice Straight: Analysis
Small dataset
One brute force approach to the problem is to examine all possible subsets of dice in all possible orders, and look for the longest straights. Since there may be as many as 100 dice in the Small dataset, we need a better strategy.

First, let's create the set of all integers that are present on at least one die, and sort that set in increasing order. Then we will create an interval that initially contains only the first number in that sorted list. We will expand and contract that interval according to the following strategy. As an invariant, the entire interval will always be a straight (a sequence of one or more consecutive numbers).

Check whether it is possible to build the straight using the available dice (we'll get to how to do that in a moment).

If it is possible: Expand the interval to include the next value to the right.
If that value is not one greater than the previous rightmost value, then we no longer have a straight; contract the interval to remove all but that new rightmost value. Then we have a straight again.
If it is not possible: Contract the interval by removing its leftmost value.
To check whether a possible straight can be built, we can find a bipartite matching from the required integers to the dice by using a flow algorithm such as Ford-Fulkerson. Since each die has exactly 6 faces, the number of edges in the graph is 6N, and the running time of one iteration of Ford-Fulkerson is O(N2). We run it up to O(N) times as we adjust our interval, so this solution is O(N3). Other polynomial solutions are possible.

Large dataset
To make the flow algorithm fast enough to solve the Large dataset, we need the additional insight that we do not need to start the algorithm from scratch every time. When expanding by adding a new number, we need to add all O(N) edges into that number (either by adding them outright or changing their flow capacity from 0 to 1), and then find and add a single augmenting path to the existing flow, which also takes O(N) time (since the total number of edges in the graph is at most 6N). So an expansion takes O(N) time overall. When contracting, we need to remove edges and flow from the path associated with that number; this takes O(N) time. Since we have O(N) expansions and contractions, they take O(N2) time in total. Adding this to the O(N2) from completely solving the flow problem the first time, the overall complexity is still O(N2). This is fast enough for the Large dataset.

World Finals 2017 - Code Jam 2017

Operation (10pts, 20pts)

Operation: Analysis
Small dataset
Let us consider the Small dataset first. The number of operations is large enough that trying all possible orderings would time out, but a typical trick might work: turn that N! into a 2N with dynamic programming over subsets of cards. That is, try to reduce all possible orderings of a subset of the cards to only few possible results that are worth exploring further. The first option to try is to define a function f(C), for a subset C of the cards, as the best possible result of applying cards in C to the starting value S. It seems that defining f(C) as the maximum over all c in C of applying c to f(C - c) could be reasonable (with f(∅) being S). However, it doesn't quite work in general. For instance, suppose c is * -1. We are getting the maximum possible result out of f(C - c), only to flip the sign right after. It would have been better to get the minimum possible result for f(C - c) instead. Of course, if c is * 2 instead, getting the maximum for f(C - c) seems like a good idea. It turns out that the best option is always either the minimum or the maximum for f(C - c), which we prove below. Therefore, let g(C, maximum) be the maximum possible result of applying cards in C to S, and g(C, minimum) be the minimum among those results. We can define g(C, m) recursively as the "m" (i.e., the maximum or the minimum depending on m) over each c and each m' in {minimum, maximum} of applying c to g(C - c, m'), with g(∅) = S. This definition of g formalizes our intuition that only the minimum and maximum possible values are needed from each subset. We prove its correctness below. The result is then g(C, maximum) for C = the entire input set. The function has a domain of size 2N × 2, and calculating each value involves an iteration over at most 2 × N possibilities, yielding O(2N × N) operations in total after memoizing the recursion. Of course, those operations are over large-ish integers. The number of digits of those integers is bounded by O(ND) where D is the number of digits of the input operands. That means the time complexity of each operation, which operates on a large integer and an input integer with up to D digits, is bounded by O(ND2), which makes the overall running time of this algorithm O(2N × N2 × D2).

We can prove the definition of g is correct by complete induction. If C is the empty set, then g is correct immediately by definition. Otherwise, assume g is correct for all m' and all sets C' smaller than C, and let us prove that g(C, m) is correct. Let c be the last card used in an ordering of C that gives the "m" result when applied to S. If c is +v or -v, we can commute the operator "m" with the application of c. That is: let T be the result of applying all of the other operations in the optimal order. Then we know that T + v or T - v is "m" over the operations, so if the value of T is not the optimal g(C - c, m), then there is some other ordering that yields g(C - c, m) + v or g(C - c, m) - v, which is better. The same is true for c being a multiplication or division by a non-negative, since those also commute with maximum and minimum. If c is a multiplication or division by a negative, then it can commute with a maximum or minimum operator, but the operator is reversed, that is, max turns into min, and vice versa. Since we try both maximum and minimum in the definition of g, that poses no problem. Notice that this proof also shows that we do not even need to try both options for m'; we only need to check the one that actually works. Trying both is simpler, though, and it doesn't impact the running time in a significant way.

Large dataset
Of course, an exponential running time would not work for the Large dataset, so we need to reason further. As a first simplification, assume all additions and subtractions have positive operands by removing those with a zero operand, and flipping both the sign and the operand of those with a negative operand. This leaves an input with the same answer as the original one.

Suppose all cards are already ordered forming an expression E. We can distribute to "move" all additions and subtractions to the right end creating a new expression E' that contains multiplications and divisions first, and additions and subtractions later. In order to make the value of E the same as the value of E', we change the added or subtracted value on each term. The value of a given addition or subtraction card is going to be multiplied/divided by all multiplications/divisions that are to its right in E. For instance, if E = (((0 + 1) × 4) - 6) / 2, then E' = ((0 × 4) / 2) + 2 - 3. Notice "+ 1" turned into "+ 2" because it is multiplied by 4 and divided by 2. "- 6" turned into "- 3" because it is only divided by 2 (the multiplication in E does not affect it).

If we consider an initial fixed card "+S", we can even move that one to the end and always start with a 0, making multiplications and divisions in E effectively not needed in E'. The final result is then the sum over all additions minus the sum over all subtractions of the adjusted values (in the example above 4 is the adjusted value of the only addition and 3 is the adjusted value of the only subtraction). Notice that this shows the value of S always impacts the final result in the same way regardless of the order: it adds S times the product of all multiplications and divided by all divisions to the result.

Consider a fixed order for multiplications and divisions, and insert the additions and subtractions. As explained in the previous paragraph, inserting operation Z at a given position implies that Z will get multiplied by all multiplications that are to its right, and divided by all divisions that are to its right. Effectively, each position has a fraction F such that operations inserted there get multiplied by F. Given the view of the final result above, it follows that it's always best to insert additions at a position where F is maximal, and subtractions at a position where F is minimal. Even though there could be multiple places with the same value of F, this shows that it is never suboptimal to insert all additions in the same place A, and all subtractions in the same place B. Since additions and subtractions commute, this further shows that it is equivalent to have an input with a single addition and a single subtraction whose operand is the sum of the operands of the corresponding operation (after adjusting the signs to make them all positive).

Given the simplification, we can reverse the point of view. Consider the addition and subtraction fixed and insert multiplications and divisions. Since multiplication and division commute, we just need to decide between 3 places: a) before both addition and subtraction, b) in between, c) after both. What we insert in a) will multiply/divide only S in the final result, what we insert in b) will multiply/divide S and only additions or only subtractions depending on which is earlier, and what we insert in c) will multiply/divide everything. If we fix the positions of multiplications and divisions with negative operands, we can greedily place multiplications and divisions with a positive operand: we place multiplications to apply to the greatest of the 3 values mentioned above (after applying the fixed multiplications and divisions by a negative) and divisions to apply to the lesser of the 3 (after doing the same). This shows that it is never suboptimal to place all multiplications by a positive together in one place, and all divisions by a positive in one place.

To further simplify, divisions can't have a zero operand, but multiplications can. Notice that a "* 0" will nullify the entire thing, so only the placement of the rightmost "* 0" matters, so we can simplify them all into a single card (or no card if there is no "* 0" in the input). This leaves only multiplications and divisions by a negative. If a pair of multiplications by a negative are in the same place a), b) or c) as above, they multiply as a positive, so it is always better to move the pair to the optimal place to put multiplications, as long as we have pairs. If there is an even number of multiplications by a negative in a suboptimal place, then all of them get moved by this. If their number is odd, all but one are moved. Leaving behind the one with the smallest absolute value is optimal. A similar argument applies to divisions by a negative, although the optimal place to move to may of course be different than the optimal place to move multiplications. This shows that we can group multiplications and divisions by a negative similarly to what we did with all other operations, but leaving out the two smallest absolute values of each type, as they may be needed in suboptimal places to perform a sign change (there are 3 places out of which 1 is optimal, leaving 2 suboptimal places).

After all the groupings, we are left with at most 11 cards: 1 addition, 1 subtraction, 1 multiplication by 0, 1 multiplication by a positive, 1 division by a positive, 3 multiplications by negatives and 3 divisions by negatives. This can be refined further, but there's no need for it. With just 11 cards, we can apply the Small solution and finish the problem. There are also other ways of reasoning among similar lines to get the number of cards low enough. Another possibility that doesn't require any dynamic programming is to notice that we can brute-force the order of the addition and subtraction (two options), and then brute-force which place a), b) or c) each multiplication and division (up to 8 cards after all simplifications) should go into. This requires exploring only 2 × 38 combinations in total, and exploring each one is a relatively fast process requiring only O(N2) time (11 operations of quadratic time each, since the operands in the simplified cards can have linear size).

It is possible to reduce the set of cards further with more thought, and it's also possible to follow other lines of reasoning that will lead you to slightly higher card totals that are small enough to make the dynamic programming solution work.

World Finals 2017 - Code Jam 2017

Spanning Planning (30pts)

Spanning Planning: Analysis
In general, for any value of X greater than 2, it is possible to construct a graph that has X spanning trees: connect X vertices to form a single cycle. Then there are X ways to delete a single edge, and each of these deletions leaves behind a different spanning tree. However, in this problem, we are only allowed up to 22 vertices, so we can only use that strategy for values of K up to 22.

The statement guarantees that an answer exists for every K in the range [3, 10000], so we just need to find those answers. Since there are so few possible testcases, we can prepare ourselves by finding one answer for each possible K before ever downloading a dataset.

A good way to start exploring the problem is to create random graphs and count their spanning trees. We can use the beautiful Kirchhoff matrix tree theorem to reduce the problem of counting spanning trees in a graph to the problem of finding the determinant of a matrix based on the graph. We can either carefully implement our own function for finding determinants, or use a library function, e.g., from SciPy. We must take care to avoid overflow; a complete 22-node graph has 2220 trees! Precision loss is also a potential concern if working outside of rational numbers, but one of our internal solutions successfully used Gaussian elimination and doubles.

It turns out that we cannot find all the answers we need via a purely random search, but we can find most of them, which provides some hope that we can reach the others with some tweaking. For example, we can change the graph size and the probability that each edge exists; empirically, 13 nodes and an existence probability of 1/4 work well.

Another strategy is to apply small modifications to a graph, hoping to converge on one of the desired numbers of trees. Specifically, we can remember what numbers of trees we still need to get, pick one of them as a "target", and then repeatedly add edges if we have fewer trees than the target or remove them (taking care not to disconnect the graph) if we have more trees than that target. Once we reach our goal, we pick another still-unreached number as the new goal, and so on. To make this finish quickly, we can mark all visited numbers as reached even if we reach them while in pursuit of a different goal.

In both of the above approaches, generating solutions for K = 13 and K = 22 can take a very long time, since those numbers apparently require very specific graph structures. However, since we're allowed 22 vertices, we can get the answers from those numbers by building a cycle with 13 or 22 vertices, as described earlier.

This problem is inherently experimental, and requires research on a computer; you cannot just write down a solution on paper. (If you do know of a tractable construction-based solution, we'd love to hear about it!) However, this sort of exploratory research skill is valuable in real-world programming, and we like to reward it in Code Jam, particularly in problems in which it is surprising that a randomized strategy works at all.

World Finals 2017 - Code Jam 2017

Omnicircumnavigation (15pts, 20pts)

Omnicircumnavigation: Analysis
The concept of omnicircumnavigation requires a given itinerary to touch every possible hemisphere. Pick any plane P that contains the origin. The plane splits the surface of the sphere into 3 parts — two open hemispheres and a circle between them. If the travel path lies entirely within one of the hemispheres, then it is not an omnicircumnavigation. The problem is to find such a dividing plane P or prove that one does not exist.

Continuing with that reasoning, the travel path touches a plane P if and only if one of the stops is on P, or there are stops on both hemispheres so that the connection between them passes through P. Therefore, another equivalent definition of dividing plane of a travel path is a plane that has all stops strictly on one of the hemispheres. This means the order of the input points is not important: the answer is the same for any valid permutation of a given set of points!

Notice that each actual stop S is given in the input by giving another point S' such that S is the normalized vector of S'. That means the origin, S and S' are collinear, which in turn implies that any plane P that contains the origin leaves both S and S' on the same hemisphere. Then, checking for the actual stops to be on a side of a plane is equivalent to checking the points S' given as input. And since points S' have the lovely property of having all integer coordinates, it is much better (and precise) to use them directly.

To summarize, the problem we are presented is equivalent to a simplified formulation: given a set X of points with integer coordinates, decide whether there exists a plane P that contains the origin such that all points in X lie strictly on the same side of P. Let us call a plane that goes through the origin and leaves all points strictly on one side a dividing plane.

Convex-hull based solutions
Notice that if there exists a dividing plane P, then P also has the convex hull of all points on one side. Moreover, by convexity, a dividing plane exists if and only if the convex hull does not contain the origin. So, one possible solution, that can even work for the Large, is to calculate the convex hull and check whether it contains the origin. If you do this using a library, it might be easier to calculate the convex hull of X plus the origin and check whether the origin is indeed a vertex of it. This solution, however, has two major drawbacks: 1. the algorithm to do convex hull in 3d is pretty hard, and 2. many implementations will have either precision issues, overflow problems, or get slowed down by handling increasingly big integers. This is because the needed plane can be really skewed, with angles within the 10-6 order of magnitude. Moreover, if the entire input is coplanar, then the convex hull might fail. One way to take care of both problems is to calculate the convex hull of X plus the origin plus F where F is a point really far away. F is not going to be coplanar, and it will also make the convex hull not have extremely narrow parts. Of course, the addition of F may make the convex hull contain the origin when the original did not. We can solve that with a second pass using the antipode of F, -F. If the original convex hull contained the origin, then both of the passes will. If the original convex hull didn't, then at least one of them won't (the one where F or -F is on the appropriate side of the dividing plane, since they are necessarily on different sides).

A simplified way to check for this is to notice that, in the same way there is a triangulation for any convex polygon, there is a tetrahedralization of any polyhedron. That means, we can avoid explicitly calculating the convex hull if we check all possible tetrahedra. This can't give false positives because all of them are included in the convex hull, and since some subset of those tetrahedra will actually be a partition of the convex hull, their union is the entire convex hull, and one of them contains the origin. We can conclude that the convex hull of X contains the origin if and only if some tetrahedron with vertices in X does. The coplanar case can be simplified in this case: if the entire input is coplanar, we can check for any triangle to contain the origin. This solution, however, takes time O(N4), which is definitely too slow for the Large dataset, and might even be slow for the Small, given that checking for each tetrahedron to contain the origin requires quite a few multiplications, which takes significant, though constant, time. The coplanar edge case of this solution can also be avoided by adding phantom points F and -F as above.

A speedup of the solution above that is certainly fast enough to pass the Small is to notice we can fix one of the vertices and try every possible combination of the other 3. This is because, for any vertex V, there is a tetrahedralization of the convex hull such that all tetrahedra in it have V as a vertex. This takes the running time down to O(N3), which is definitely fast enough for the Small dataset, even with a large constant.

Solutions based on restricting the dividing planes
As usual in geometry problems, we can restrict the search space from the infinitely many possibilities to just a few. Suppose there is a dividing plane P. If we rotate P while touching the origin, we will eventually touch at least one point S from the input. If we rotate while around the line between S and the origin, we will eventually touch another point from the input. That means we can restrict planes P to those who touch two points from the input. Of course, the plane P is not the dividing plane (since it touches points from the input), but P represents planes epsilon away from those touching points. This means we need to take special care of inequalities to make sure that small rotation doesn't ruin the solution. In short, if there is another point touching P, we can't necessarily rotate P to make all 3 points on P to lie on one side. We need to take care of this coplanar case separately, with either solving the 2-dimensional version of the problem, or using phantom poins. Since 3 points (two points from the input and the origin) completely determine a plane, this restricts the number of planes to try to O(N2) possibilities. For each one, we need another pass through the input to check on which side of the plane each point lies. This yields a solution that runs in time O(N3), which is enough to pass the Small. Even in fast languages, this can be too slow for the Large, as the constant is, once again, very significant.

The solution above can be made run much faster, by randomizing the input, and thus, the order in which we check the points. For most planes, there will be several points on either side, and then when checking in a random order, the expected time to find one on each side (after which, we can stop) can be greatly reduced. Notice, however, that a case in which all the points are coplanar will not have its running time improved by this randomization, as no point will fall strictly on one side. For this to work well, we need to check for the all-coplanar case and special-case that one before launching the general case algorithm. This randomized version is indeed enough to pass the Large.

A bazooka-to-kill-a-fly solution
And finally, we can use linear programming. A plane that contains the origin is defined by an equation Ax + By + Cz = 0, for some A, B and C. A plane that has all points on one side has to satisfy either AX + BY + CZ > 0 for each point (X, Y, Z) or AX + BY + CZ < 0 for (X, Y, Z). Notice that if a triple (A, B, C) satisfies one of the conditions, then (-A, -B, -C) satisfies the other. So, we can restrict ourselves to one of the two cases, and then define a polytope with the set of inequalities AX + BY + CZ > 0 for each (X, Y, Z) in the input. The answer to the problem is whether that polytope is empty. Most LP algorithms figure that as an intermediate result towards optimization, and some libraries may provide functionality to check that directly. For others, you can just optimize the constant function 0 and see whether the answer is indeed 0 or "no solution".

As simple as the description of this solution is, it has a lot of issues to resolve. If using a library, it is highly likely that you run into similar precision / large number problems as the convex hull (and for the same reasons) that may make it either wrong or slow or both. If you want to implement your own algorithm, well, it's long and cumbersome to do it and avoid precision problems. There are tricks here, too. We can catch the possible problems and try to rotate the input to see if the library performs better. We can add additional restrictions like bound A, B and C to absolute values up to 106 to have a bounded polytope to begin with. That being said, judges tried 4 different LP libraries, and only one of them worked, after adding both additional restrictions (the library wouldn't handle unbounded spaces) and rotating the input a few times. Adding far-away phantom points can also help the LP, because it avoids the same problems as in the convex hull case. Of course, if you had a really robust prewritten algorithm or library, this option was best, even enabling a possible 3-line solution that passes the Small and the Large.

World Finals 2017 - Code Jam 2017

Stack Management (10pts, 30pts)

Stack Management: Analysis
This problem requires quite a lot of thinking, but relatively little code in the end!

Let S be the number of suits that appear on at least one card used in the problem. For each of these suits, we will call the card with the highest value the ace of that suit, and the second-highest card (if it exists) the king. We'll say that a card is visible if it is at the top of one of the stacks, and that a suit is visible if a card of that suit is visible.

Notice that once a suit becomes visible, it will stay visible throughout the rest of the game. At any point in the game, if there are N visible suits (recall that N is the number of stacks), then either we have won, or we cannot make any more moves and so we have lost. On the other hand, if there are fewer than N visible suits, then either we have won, or we are able to make a move (because either a suit has two visible cards, or there is an empty stack). In particular, this implies that if S < N, then we are guaranteed to win however we play. However, if S > N, we cannot win, because we can never remove the last card of any suit from the game. This means that S = N is the only interesting case, so we will assume from now on that S = N. In this case, a winning position is a position in which we have exactly one card in each stack, with each one representing a different suit.

Let us assume there is some way to win the game, and we will consider the very last move of that game. Before that move, we had not won, and yet we were able to make a move; this means there must have been fewer than N suits visible. So, the last move must have exposed a card of some suit that had never been visible. This means that this suit contained only one card (the card that now remains as the only card in its stack), and this card started the game at the bottom of its stack.

Also note that it is never disadvantageous to remove a card; the only real decisions made in the game are choosing which cards to move into an empty stack. Thus, as a pre-processing step, we can remove all the cards that can be removed from the initial position. If doing that is enough to win the game, we are done. If doing that leaves us with no empty stacks, we are also "done", because we have lost the game! So, let's assume that when we begin, there is at least one empty stack.

We will now aim to prove that the following condition is necessary and sufficient for a game to be winnable. Let us construct a graph in which vertices are the suits for which the ace begins the game at the bottom of some stack. We say that a vertex (suit) s is a source if the ace is the only card in this suit, and that s is a target if there is another ace (of a different suit) in the stack in which the ace of s is at the bottom. We add an edge from vertex s1 to a different vertex s2 if the king of s2 is in the stack that has the ace of s1 at the bottom.

Now, we claim the game is winnable if and only if there exists any path from some source vertex to some target vertex.

To understand this condition, consider a simple case in which there is a single edge from a source suit s1 to a target suit s2; i.e., suit s1 has exactly one card (an ace), which is at the bottom of a stack A, and suit s2's king is in A. To ensure that s1 is a source but not a target, assume that there are no other aces in A, and to ensure that s2 is a target, assume that its ace is at the bottom of a different stack B, and there is a third suit s3 that has an ace higher up in B, which we will assume is the ace nearest the bottom except for the bottmmost card in B.

The winning strategy in this case is as follows. First, make all legal moves until the only remaining legal move is moving the ace of s3 to an empty pile. Since we won't uncover the ace of s1, there will be fewer than N suits visible, so this state is always achievable. When we reach this state, all of the following are true:

There is an empty stack (since s1 isn't visible, we'd otherwise have two cards visible in one suit, and could remove the one with the lower value).
Stacks A and B are the only stacks with more than one card. (Otherwise, we could move a card from from one of the other stacks into the empty space.)
The other N-3 stacks (aside from A, B and the aforementioned empty stack) each contain an ace of one of the remaining N-3 colors. (We couldn't have removed the aces, and they are not in stack A or stack B).
At this point, we will move the ace of s3 to the empty stack, and then try to remove cards from B, until we get down to the ace of s2. If the top card of B isn't yet the ace of s2, then it's either in suit s2 (and lower than the king, so we can remove it, because the king is visible), or in some other suit (in which case the ace of that suit is visible, and we can remove it). Therefore, we can remove cards down to the ace of s2, then remove the king of s2, and then again dig down to the ace of s1. We can do this because any card in A other than the ace of s1 will be removable, since we now see all the aces other than s1, and there are no cards but the ace in suit s1.

The description above can be extended relatively easily to show how to win the game when a longer path exists. First, we clean up everything but the aces mentioned in the path, and then move the ace from the end of the path into the empty space, and remove all the remaining cards one by one. So, what remains is to prove that if the game is winnable, a path from a source to a target always exists in the graph we constructed.

At the end of a successful game, each of the N stacks will contain one of the N aces. Whenever we move an ace to the bottom of a stack, it will never again be covered. So, before the last move action, N-1 aces will be on the bottom of a stack, and the last move is necesarilly moving an ace to the empty spot. Some of the aces are visible before the last move. Nothing interesting will happen to cards in those suits - we might uncover a card in one of those suits, and then we will be able to immediately remove it, because the ace is visible. The more interesting suits are the ones in which the aces are not visible and are at the bottoms of stacks. Note that once uncovered, an ace cannot be covered again, so these aces had to be at the bottoms of their stacks from the beginning of the game, and the cards on top of them had to be there from the beginning of the game. So, it's enough to prove that we will see a source-target path in the position before the last move. This will mean that the path was there from the beginning of the game.

The cards on top of the other stacks with more than one card have to be in the same set of colors as the covered aces (or else they would have already been removed). We want to prove they are all kings. We will proceed by contradiction: assume that one of them is a lower card (say, a "queen of spades"). Since it is visible and not removed, the king of spades must be somewhere in one of the stacks, and not visible; it cannot have been removed yet, because in this case the ace would have to be visible, and the queen would have been removed as well.

Consider what happens if we move this queen into the empty space. We experience a sequence of removals, which cannot end with removing the queen (that would contradict the assumption that no more moves can be made before the winning one). Thus, it has to end in uncovering the ace of the suit that was not previously visible (causing us to lose the game) - let's call this suit "diamonds".

Now, consider the winning move instead. We also end up with a sequence of removals. After each removal except the last one, we see N-1 suits, and so we have exactly one choice what to remove. So, we deterministically remove cards until we, at some point, uncover the king or the ace of spades, whichever comes first, and that causes us to remove the queen of spades... and then execute the exact same deterministic sequence of removals that, in the end, caused us to uncover the ace of diamonds. Note that the other high spade card (whichever among the king and ace that we did not see) is not uncovered in this sequence. If it were, it would have removed the queen of spades in the previous scenario - so, we end up with at least one card that is not visible, which is a contradiction.

So, we have proven that kings are on top of our stacks with (non-visible) aces on the bottom. At this point, following the graph from the ace that was the source will eventually lead us to the target: the stack with two aces.

After establishing all this, the algorithm to check for the desired condition is fairly simple. After constructing our graph, we can start at sources and perform a depth-first search to see if there is a path from any of them to a target. This is considerably faster than running a backtracking algorithm on the set of moves itself, which works for the Small but not for the Large.

World Finals 2017 - Code Jam 2017

Teleporters (10pts, 30pts)

Teleporters: Analysis
This problem starts off with a strange twist, because instead of the regular Euclidean geometry (also known as L2 geometry), it uses L1 geometry. The reason, if you are curious, is that the distance between two points with integer coordinates is also an integer. A Euclidean geometry version of this had serious precision issues, and L1 geometry has all the properties that are needed. A concept that will come up later is the set of points that are at a particular distance from a center. In Euclidean geometry, that's just a sphere. In L1 geometry, though, is an octahedron with diagonals (segments that connect opposite vertices, passing through the center) parallel to the axis. Luckily, most intuitive properties of spheres in Euclidean geometry also work for spheres in L1 geometry, with the important exception of rotations, that are not used in this problem. If it helps you to visualize, for remainder of the text, you can think of the problem as working in L2 geometry. However, throughout the rest of this analysis, every time we say "distance", we are referring to L1 distance; we will write dist(x, y) for the L1 distance between points x and y. Every time we say "sphere", we are referring to L1 spheres (i.e., regular octahedra).

Small dataset
Let us start slowly: after 0 teleportations starting from a point P, the only reachable place is point P. After 1 teleportation, reachable places depend on which teleporter we used. If we use teleporter t, reachable points are the surface of the sphere with center t and radius dist(P, t). Let Ri be the set of points that are reachable in i teleportations using any set of teleporters. As we mentioned above, R0 = {P} and R1 is a union of N sphere surfaces, one centered on each teleporter. What about R2? If we use teleporter u for the second teleportation, we can land at any point that is at distance d from u, where d can take the value of any distance betwen u and a point in R1. This implies R2, and all other Ri, are also a union of sphere surfaces, possibly infinitely many.

Notice that R1 is a connected continuous set because all the spheres' surfaces intersect at point P. Thus, the values for the distance d in the definition above form an interval, since the distance function is continuous. This implies that R2 is actually a union of sphere differences: for each teleporter t, all points x such that Lt,2 ≤ dist(x, t) ≤ Ut,2 are reachable. (Throughout these explanations, we use L to refer to a lower bound on a reachable range, and U for a corresponding upper bound.) Once again, all the sphere differences contain P, thus, they intersect and R2 is a connected continuous set, which means the distances we use to calculate R3 are intervals. This argument generalizes to prove by induction that each Ri is exactly the union over all teleporters t of all points x such that Lt,i ≤ dist(x, t) ≤ Ut,i.

This yields a dynamic programming algorithm that solves the Small dataset. Keep two arrays L and U representing the values Lt,i and Ut,i for a particular i and use them to calculate the next values Lt,i+1 and Ut,i+1. After each iteration, check whether Lt,i ≤ dist(Q, t) ≤ Ut,i for some t, and if so, i is the answer.

By definition, Lt,i+1 and Ut,i+1 are the distances from t to its closest and farthest points in Ri, respectively. The farthest point in Ri from t is at a distance which is the maximum over all teleporters u of dist(t, u) + Uu,i (this is the distance to the point on the surface of the sphere centered at u with radius Uu,i that is the opposite direction from t). The closest point is slightly more complicated. For each teleporter u we need to consider:

dist(t, u) - Uu,i if dist(t, u) > Uu,i (t is outside the outer sphere centered at u),
Lu,i - dist(t, u) if dist(t, u) < Lu,i (t is inside the inner sphere), or
0, in all other cases (t is in between, that is, it is itself a reachable point).
This means we can calculate each Lt,i and Ut,i in O(N) by comparing the values above for each teleporter u.
Notice that a point reachable in i teleportations is also reachable in i+1, i+2, ... etc teleportations, because you can use a teleporter to move from a point to itself. Thus, Ut,i is non-decreasing with i, and Lt,i is non-increasing with i. Additionally, since the distances dist(t, u) are positive, when N ≥ 2, the maximum over all t of Ut,i is strictly increasing with i, and the minimum over all t of Lt,i is strictly decreasing with i up to the first j where Lt,j = 0. That means, for N ≥ 2, the intervals grow until one of them represents a sphere covering the entire cube of values for Q within the limits. Moreover, since the values are integers, the increase and decrease is at least 1 per iteration, so the number of iterations needed to cover the entire region of valid Qs is bounded by 3M (M on each direction), where M is the number of valid coordinates, which is only 2001 in the Small dataset. This in particular means that for N ≥ 2 the answer is never impossible. For N=1, we can note that using the same teleporter twice in a row is never useful, so after 1 iteration, if Q is not reached, the answer is impossible.

The time complexity of the presented algorithm is O(M N2): up to 3M steps, each of which requires calculating O(N) values, and calculating each one requires an iteration over N other teleporters and constant-time math.
Large dataset
Of course, when M is bounded by 2 × 1012 + 1, a time complexity linear on M won't finish fast enough, so we have to do something else.

Let us focus first on deciding if it's possible to go from P to Q with a single teleportation. That means using a single teleporter t, and due to conservation of distance, it must be dist(P, t) = dist(Q, t). Moreover, this condition is sufficient and necessary for the answer to be 1. We can check for this condition initially with a loop over all teleporters in O(N) time.

As we saw on the Small, checking whether the answer is 1 is sufficient to fully solve cases with N = 1. We assume further that N ≥ 2.

Let us now consider the case where there exists two teleporters t and u such that dist(P, t) ≥ dist(Q, t) and dist(P, u) ≤ dist(Q, u). Consider the sphere A centered at t that passes through P, and the sphere B centered at u that passes through Q. By the assumed inequalities, A contains Q and B contains P, which means A and B intersect. Let x be any point at the intersection, for which dist(P, t) = dist(x, t) and dist(Q, u) = dist(x, u) hold. Then, x is a possible intermediate stop to go from P to Q in exactly 2 teleportations, so, if the inequalities hold, 2 is the answer. Notice there are other cases in which 2 is also the answer, which are covered below.

At this point, we can assume that either P is closer to any teleporter than Q, or vice versa (otherwise, we can choose two teleporters to fullfill the inequalities at the beginning of the previous paragraph). Since the problem is symmetric, swap P and Q if needed to make P the closest of P and Q to all teleporters.

Now recall the definitions of R, L and U from the Small solution. Since P is closest to all teleporters, dist(Q, t) > Ut,1 = dist(P, t) for all t. This means Q is outside the spheres centered in all teleporters. Since Lt,i is non-increasing with i, the inner sphere contracts with each step, which means Q is never inside the inner sphere, so as soon as Q is inside the outer sphere, we are guaranteed that Q is reachable. So, we only need to calculate the Us. By reading its definition above, we note that Ut,i is equal to the longest path from P to t using teleporters as intermediate steps, where the length of each step is simply the distance between the two points.

We can calculate the length of the required longest paths for all t and a fixed i in O(N3 log i) time by using something similar to iterated squaring to calculate the matrix of largest distances from any teleporter to any other in i - 1 steps, and then combining that with the vector of distances from P to each teleporter. The "multiplication" here is not an actual matrix times matrix multiplication, but rather the use of the property that the longest path from t to u in i steps is the longest path from t to v in j steps plus the longest path from v to u in k - j steps, for some v. Taking j = k / 2 for even k shows how to do log k steps overall. This, combined with a binary search on the number of steps, gives an algorithm with overall time complexity O(N3 log2 M). If you have a good implementation in a fast language, this runs in minutes, but it's enough to pass the Large.

It's possible to get rid of one the log factors for an overall time complexity of O(N3 log M), and a program that finishes the Large in a few seconds. This is achieved by starting the binary search on a range [min, max) whose size, that is, max - min, is a power of 2. Each step calculates mid as the averge of min and max, so mid - min and max - mid are also powers of 2, which proves by induction that the range size is a power of 2 in every step of the binary search. Then, since mid - min is also a power of 2 in every step, every distance matrix you need is of a number of steps that is itself a power of 2 (the range keeps being cut in half, so it remains of size a power of 2, so the delta between the min and the midpoint that we need to test is always a power of 2). That means we can precalculate all needed matrices in O(N3 log M) time, since the matrix for 2k+1 steps is the "square" of the matrix for 2k steps. With the memoized matrices, each step of the binary search only takes O(N2) time to "multiply" the matrix and the initial vector.

Practice Session 2018 - Code Jam 2018

Number Guessing (5pts, 10pts)

Test set 1
Since A = 0 and B = 30 in this test set, and since we get N = 30 tries per test case, we can simply guess every number from 1 to 30 until the judge sends back CORRECT.

Test set 2
In test set 2, since the answer could be anywhere in the range (0, 109] and we still have only 30 guesses, we will use binary search.

Initially, we know the answer P is in [1, 109], which is a big range! To cut that range by half, our first guess will be (1 + 109) / 2 = 5×108. If the judge sends back TOO_SMALL, we will know that P is in [1, 5×108). Similarly, if the judge sends back TOO_BIG, P is in (5×108, 109]. Otherwise, P is 5×108 and we are done.

We will cut that range further by making our next guess the middle number in that range. Again, based on the judge response that we get, we will know that either we have guessed P correctly, or P is in the upper or lower half of the range. We will do this repeatedly, until CORRECT is received.

Each time we make a wrong guess, the range that we must examine next will always be at most half the size of our previous range. So, it will take at most log2109 = 29.897353 < 30 tries to guess P correctly.

Sample Solutions
This problem was intended as an opportunity to get used to our interactive judges. Here are some example solutions in all languages that we support so far:

Bash:
read t
for p in $(seq 1 $t); do
  read -a line
  a=${line[0]}
  b=${line[1]}
  read n
  head=$(( a+1 ))
  tail=$b
  while true; do
    mid=$(( (head+tail)/2 ))
    echo $mid
    read s
    if [[ "$s" == "CORRECT" ]]; then
      break
    elif [[ "$s" == "TOO_BIG" ]]; then
      tail=$(( mid - 1 ))
    elif [[ "$s" == "TOO_SMALL" ]]; then
      head=$(( mid + 1 ))
    else
      # Wrong answer; exit to receive Wrong Answer judgment
      exit 0
    fi
  done
done

C:
#include <stdio.h>
#include <string.h>

int main() {
  int T; scanf("%d", &T);

  for (int id = 1; id <= T; ++id) {
    int A, B, N, done = 0;
    scanf("%d %d %d", &A, &B, &N);
    for (++A; !done;) {
      int mid = A + B >> 1;
      char result[32];
      printf("%d\n", mid);
      fflush(stdout);
      scanf("%s", result);
      if (!strcmp(result, "CORRECT")) done = 1;
      else if (!strcmp(result, "TOO_SMALL")) A = mid + 1;
      else B = mid - 1;
    }
  }
  return 0;
}

C#:
using System;

public class Solution
{
  static public void Main ()
  {
    int num_test_cases = Convert.ToInt32(Console.ReadLine());
    for (int i = 0; i < num_test_cases; ++i) {
      string[] lo_hi_s = Console.ReadLine().Split(' ');
      int[] lo_hi = Array.ConvertAll(lo_hi_s, int.Parse);
      int num_tries = Convert.ToInt32(Console.ReadLine());
      int head = lo_hi[0] + 1, tail = lo_hi[1];
      while (true) {
        int m = (head + tail) / 2;
        Console.WriteLine (m);
        string s = Console.ReadLine();
        if (s == "CORRECT") break;
        if (s == "TOO_SMALL")
        {
          head = m + 1;
        }
        else
        {
          tail = m - 1;
        }
      }
    }
  }
}

C++:
#include <iostream>
#include <string>

int main() {
  int num_test_cases;
  std::cin >> num_test_cases;
  for (int i = 0; i < num_test_cases; ++i) {
    int lo, hi;
    std::cin >> lo >> hi;
    int num_tries;
    std::cin >> num_tries;
    int head = lo + 1, tail = hi;
    while (true) {
      int m = (head + tail) / 2;
      std::cout << m << std::endl;
      std::string s;
      std::cin >> s;
      if (s == "CORRECT") break;
      if (s == "TOO_SMALL")
        head = m + 1;
      else
        tail = m - 1;
    }
  }
  return 0;
}

Go:
package main

import (
  "fmt"
  "strings"
)

func main() {
  var t int
  fmt.Scanf("%d", &t)
  for i := 1; i <= t; i++ {
    var a, b, n int
    fmt.Scanf("%d %d", &a, &b)
    a = a + 1
    fmt.Scanf("%d", &n)
    for {
      m := (a + b) / 2
      fmt.Println(m)
      var str string
      fmt.Scanf("%s", &str)
      if strings.EqualFold(str, "CORRECT") {
        break
      } else if strings.EqualFold(str, "TOO_SMALL") {
        a = m + 1
      } else if strings.EqualFold(str, "TOO_BIG") {
        b = m - 1
      }
    }
  }
}

Haskell:
import System.IO

getNum :: IO Int
getNum = do
    x <- getLine
    let n = read x :: Int
    return n

bisect :: Int -> Int -> Int -> String -> IO ()
bisect a b m "CORRECT" = return ()
bisect a b m "TOO_SMALL" = singleCase (m+1) b
bisect a b m "TOO_BIG" = singleCase a (m-1)

query :: Int -> IO String
query m = do
    putStrLn ( show m )
    hFlush stdout
    x <- getLine
    return x

singleCase :: Int -> Int -> IO ()
singleCase a b = do
    let m = (a+b) `div` 2
    response <- query m
    bisect a b m response
    return ()

solve :: Int -> IO ()
solve 0 = return ()
solve n = do
    [a, b] <- fmap(map read.words)getLine
    _ <- getNum
    singleCase (a+1) b
    solve (n-1)

main = do
    hSetBuffering stdout NoBuffering
    t <- getNum
    solve t

Java:
import java.util.Scanner;

public class Solution {
  public static void solve(Scanner input, int a, int b) {
    int m = (a + b) / 2;
    System.out.println(m);
    String s = input.next();
    if (s.equals("CORRECT")) {
      return;
    } else if (s.equals("TOO_SMALL")) {
      solve(input, m + 1, b);
    } else {
      solve(input, a, m - 1);
    }
  }

  public static void main(String args[]) {
    Scanner input = new Scanner(System.in);
    int T = input.nextInt();
    for (int ks = 1; ks <= T; ks++) {
      int a = input.nextInt();
      int b = input.nextInt();
      int n = input.nextInt();
      solve(input, a + 1, b);
    }
  }
}

JavaScript:
var readline = require('readline');
var rl = readline.createInterface(process.stdin, process.stdout);

expect = 'begin';
rl.on('line', function(line) {
  if (expect === 'begin') {
    num_test_cases = parseInt(line);
    expect = 'lo_hi';
    case_counter = 0;
  } else if (expect === 'lo_hi') {
    lo_hi = line.split(' ');
    head = parseInt(lo_hi[0]) + 1;
    tail = parseInt(lo_hi[1]);
    expect = 'num_tries';
  } else if (expect === 'num_tries') {
    num_tries = line; // not used.
    expect = 'solve';
    mid = parseInt((head + tail) / 2);
    console.log(mid);
  } else if (expect === 'solve') {
    if (line === 'CORRECT') {
      ++case_counter === num_test_cases ? rl.close() : 0;
      expect = 'lo_hi';
    } else {
      line === 'TOO_SMALL' ? head = mid + 1 : tail = mid - 1;
      mid = parseInt((head + tail) / 2);
      console.log(mid);
    }
  }
}).on('close',function(){
    process.exit(0);
});

PHP:
<?php

function solve($a, $b) {
  $m = ($a + $b) / 2;
  printf("%d\n", $m);
  fscanf(STDIN, "%s", $s);
  if (strcmp($s, "CORRECT") == 0) {
    return;
  } else if (strcmp($s, "TOO_SMALL") == 0) {
    $a = $m + 1;
  } else {
    $b = $m - 1;
  }
  solve($a, $b);
}

fscanf(STDIN, "%d", $t);
for ($ks = 0; $ks < $t; $ks++) {
  fscanf(STDIN, "%d %d", $a, $b);
  fscanf(STDIN, "%d", $n);
  solve($a + 1, $b);
}
?>

Python2:
import sys

def solve(a, b):
  m = (a + b) / 2
  print m
  sys.stdout.flush()
  s = raw_input()
  if s == "CORRECT":
    return
  elif s == "TOO_SMALL":
    a = m + 1
  else:
    b = m - 1
  solve(a, b)

T = input()
for _ in xrange(T):
  a, b = map(int, raw_input().split())
  _ = input()
  solve(a + 1, b)

Python3:
import sys

def solve(a, b):
  m = (a + b) // 2
  print(m)
  sys.stdout.flush()
  s = input()
  if s == "CORRECT":
    return
  elif s == "TOO_SMALL":
    a = m + 1
  else:
    b = m - 1
  solve(a, b)

T = int(input())
for _ in range(T):
  a, b = map(int, input().split())
  _ = int(input())
  solve(a + 1, b)

Ruby:
$stdout.sync = true

def solve(a, b)
  m = (a + b) / 2
  puts m
  $stdout.flush
  s = STDIN.gets.chomp
  if s.eql? "CORRECT"
    return
  elsif s.eql? "TOO_SMALL"
    solve(m + 1, b)
  else
    solve(a, m - 1)
  end
end

t = STDIN.gets.chomp.to_i
ks = 1
while ks <= t
  a, b = STDIN.gets.split.map &:to_i;
  n = STDIN.gets.chomp.to_i
  solve(a + 1, b)
  ks = ks + 1
end

Practice Session 2018 - Code Jam 2018

Senate Evacuation (5pts, 10pts)

Test set 1
With at most three parties and at most nine senators, various brute force approaches will work. One exhaustive strategy is to generate all possible different evacuation orders, treating senators from the same party as interchangeable, and then try all possible different ways of chopping those into groups of one or two senators.

Another, simpler strategy is to keep randomly choosing and trying one of the nine possible evacuations (A, B, C, AA, AB, AC, BB, BC, CC) as long as the chosen senator(s) exist and the evacuation will not cause a new absolute majority. You may worry that this strategy could get stuck, but the outcome of any legal evacuation will just be another possible test case for the problem, and the statement guarantees that every test case has a solution! With more parties and senators, though, this strategy might bog down in the details of checking the legality of evacuations, so we should come up with a more efficient approach.

Test set 2
Intuitively, it is safest to remove one senator at a time, and to always draw from whichever party has the most remaining senators (or any such largest party, if there is a tie). But this strategy won't always work! For example, if we have two senators from party A and two from party B, and no others, which is a valid test case, then removing one senator from either party will give the other party an absolute majority.

However, this strategy is always safe whenever there are more than two parties present. Suppose that party 1 is currently the largest, or tied for the largest, of at least three parties, and that we remove a single senator from party 1. Clearly, making party 1 smaller cannot give it an absolute majority that it didn't have before. But could some other party acquire an absolute majority as a result? Suppose that the removal of a senator from party 1 were to cause party 2, which currently has X senators, to have an absolute majority. But since party 1 was the largest, or tied for the largest, before a senator was removed, party 1 must still have at least X-1 senators. Moreover, since at least one more party is present, there is at least 1 other senator who is not from party 1 or 2. So there are a total of at least X remaining senators who are not from party 2, which means the X senators of party 2 are not enough to give it an absolute majority, so we have a contradiction.

If we start with three or more parties and keep evacuating a single senator from the largest party in this way, then at some point, we must reach a step in which we go from three parties to two parties. These two remaining parties must have only one senator each. Since we just removed the one remaining senator from the third party, it must have been a largest party, so the other two can be no larger. So we can remove this last pair of senators in a single evacuation as a final step.

What if we start with two parties? Since the problem statement guarantees that no party begins with a majority, these parties must have equal numbers of senators. So, we can evacuate them in pairs, one from each party, until the evacuation is complete.

This approach takes more steps than are needed — most of those single evacuations can be paired up — but it gets the job done.

Practice Session 2018 - Code Jam 2018

Steed 2: Cruise Control (5pts, 10pts)

Pop quiz, hotshots! This problem seems pretty complicated at first glance. What do you do?

One natural strategy is to try binary searching on Annie's speed, but it is difficult to directly determine whether a given speed avoids passing another horse; the input data alone does not tell us where each horse is at any given time, because horses might slow other horses down. In theory, we could figure out when faster horses catch up to slower horses and slow down, determine the exact path of each horse, and check whether our chosen speed crosses any of those paths. With only up to two horses in test set 1, this sort of calculation is feasible, but it would be laborious for test set 2.

However, we can avoid all of that work via some observations. To maximize cruising speed, Annie's horse should reach the destination at exactly the same time as the horse ahead of her (let's call it Horse A); there is no reason to leave a gap. Either Horse A will reach the destination without having to slow down (and so it will be the one that directly limits Annie's speed), or it will be slowed down at some point by the horse ahead of it (let's call it Horse B). The same is true for Horse B: either it will never have to slow down (and so it will be the one that ultimately limits Annie's speed), or it will be slowed down by the horse ahead of it, and so on. So there will be a single "limiting horse" on the road that ultimately determines how fast Annie's horse can reach the destination. We claim that this "limiting horse" is the only horse that matters, and we can disregard all of the others!

It is easy to see that we can ignore the horses to the east of the limiting horse; they will reach and pass the destination before the limiting horse gets there. What about the "intermediate horses" between Annie and the limiting horse? We know from the way we have defined the limiting horse that every intermediate horse will catch up to the limiting horse before reaching the destination. (If one did not, then it would be the limiting horse.) Suppose that Annie chooses a cruising speed that gets her to the destination at exactly the same time as the limiting horse. We certainly cannot go faster than this. Moreover, this speed is safe: it cannot possibly cause Annie to pass any of the intermediate horses. If she were going fast enough to overtake an intermediate horse, then she would definitely be going fast enough to pass the limiting horse, since every intermediate horse will catch up to the limiting horse. This would cause a contradiction. Therefore, we do not need to worry about the intermediate horses or their interactions with each other.

So, once we have identified the limiting horse, the strategy is simple: go at the exact speed that will cause Annie to reach the destination at the same time as the limiting horse. This speed can be found in constant time. We could identify the limiting horse directly via the argument in our third paragraph above, but even this would be unnecessary work. Instead, for each horse in turn, we can pretend that it is the limiting horse and calculate the cruising speed that it would force. Then the smallest of those speeds is our answer. (If any horse allows a faster cruising speed than another, it cannot be the limiting horse, because that cruising speed would cause Annie to pass the true limiting horse.) This takes O(N) time.

Practice Session 2018 - Code Jam 2018

Bathroom Stalls (5pts, 10pts, 15pts)

Test set 1
For test set 1, the limits are small enough that you can just simulate the rules outlined in the statement. Most implementations of a simulation will run in O(NK) time and thus finish immediately, but even a slow O(N2K) implementation like "try every possible stall for the next person, and for each empty stall run a loop for each side to check for the closest neighbors" will most likely finish in time.

For test sets 2 and 3, however, something quadratic in the number of stalls won't cut it, so we have to do better.

Test set 2
The critical observation to jump from test set 1 to test set 2 is that only the number of consecutive runs of empty stalls matters at any given time. The next person always chooses the middle stall or the left of the two middle stalls of a longest subsequence of consecutive empty stalls. Moreover, the output format already hints at this: even if you were to choose the rightmost of a set of two middle stalls, or a longest run of stalls other than the leftmost one, the answer would not change. Thus, we can rewrite the algorithm in this equivalent (for the required output) form:

Find any longest subsequence of consecutive empty stalls.
Choose the middle or one of the two middle stalls.
Notice that even though there are still ties to be broken, the output is equivalent for all of them. Since the output is equivalent, so is the multiset of lengths of consecutive runs of empty stalls left behind, so the whole process only depends on that multiset. (As a reminder, a multiset is a set in which the same element can appear more than once.) We can write an optimized simulation that solves test set 2 following this pseudocode:

  S = {N}  - This is a multiset!
  repeat K times:
    X = max(S)
    X0 = ceil((X - 1) / 2)
    X1 = floor((X - 1) / 2)
    if this is the last step:
      we are done; answer is X0 and X1
    else:
      remove one instance of X from S
      insert X0 and X1 into S
If the operations over S are efficient, this will run in quasilinear time. There are many data structures that support insertion, finding the maximum, and removal of the maximum in logarithmic time, including AVL trees, red-black trees, and heaps. Many languages have one such structure in their standard libraries (e.g., the multiset or priority_queue in C++, TreeSet in Java, and heapq module in Python). Since we take O(log K) time for each of K steps, the algorithm takes only O(K log K) time, which is fast enough to solve test set 2. However, for test set 3, even quasilinear time on K is not enough.

Test set 3
The observation required to solve test set 3 is that we are simulating similar steps over and over again. The first time a bathroom user arrives, we partition N into ceil((N - 1) / 2) and floor((N - 1) / 2), which means that numbers between ceil((N - 1) / 2) and N will never appear in S. This hints at a logarithmic number of simulation steps.

Let's divide the work in stages. The first stage processes only N. Then, stage i+1 processes all of the values spawned by stage i. So, stage 2 processes up to 2 values: ceil((i - 1) / 2) and floor((i - 1) / 2). What about the other stages? It is not hard to prove by induction that they also process at most two consecutive values: since stage i processes two consecutive values, they are either 2x and 2x+1 or 2x and 2x-1, for some x (that is, one even and one odd number). Thus, the spawned values for stage i+1 can only be x and/or x-1. Since the largest value in each stage is at most half the largest value of the previous stage, there are a logarithmic number of stages. This all means that there are at most O(log N) different values that go into S at any point. Of course, some of them appear in S many, many times. So, the optimization to get the running time low enough for test set 3 is to process all repetitions of a given value at the same time, since all of them yield the same X0 and X1 values. We can do that by using a regular set with a separate count for the number of repetitions.

  S = {N}  - This is a set, not a multiset!
  C(N) = 1
  P = 0
  repeat:
    X = max(S)
    X0 = ceil((X - 1) / 2)
    X1 = floor((X - 1) / 2)
    P = P + C(X)
    if P ≥ K:
      we are done; the answer is X0 and X1.
    else:
      remove X from S
      insert X0 and X1 into S
      add C(X) to the counts of X0 and X1 in C
Once again, we have structures that implement all the required operations in logarithmic time, yielding an O(log2 N) running time overall. In general, adding any good dictionary implementation to the structure of choice from the test set 2 solution would work, either by plugging the dictionary functionality into the structure (like map in C++ or TreeMap in Java) or having a separate hash-table for the dictionary (which is the easiest implementation in Python).

Moreover, since we proved the population of S is at most 4 at any given time (only values from two consecutive stages can coexist in S), any implementation of set and dictionary will provide all operations in constant time, because the size of the whole structure is bounded by a constant! This makes the overall time complexity just O(log N).

This was a nice problem to put experimentation to work if your intuition was not enough. After solving test set 1, if you print the succession of values for a fixed N, you may spot the pattern of few values occurring in the set S, and from there, you can find the mathematical arguments to support the needed generalization. In harder problems in later rounds, this can become an even more important asset to tackle problems. As you can see in many parts of last year's finals live stream, finalists use experimentation a lot to inspire themselves and/or validate their ideas before committing to them.

Qualification Round 2018 - Code Jam 2018

Saving The Universe Again (5pts, 10pts)

Test set 1
Since there is at most one C instruction in this test set, we can solve the two cases independently.

If there is no C instruction in P, then none of our swaps will have any effect, so all we can do is check whether the damage of the beam exceeds D.

If there is one C instruction in P, then we can try every possible position for the C instruction in the program. Assuming that there is at least one position for the C instruction that causes the total damage not to exceed D, we can choose the scenario that requires the fewest swaps; the number of required swaps for a scenario is equal to the distance between the original and final positions of the C instruction.

Test set 2
To solve test set 2, we will first derive a formula to compute the total damage based on the positions of the C and S instructions in P. Let NC and NS be the number of C and S instructions in P, respectively. Let Ci be the number of S instructions to the right of the i-th C instruction, where i uses 1-based indexing.

Note that the i-th C instruction will increase the damage of the subsequent beams by 2i-1. For example, in the input program CSSCSSCSS, initially, all of the S instructions will inflict a damage of 1. Consider the damage dealt by the last S instruction. Since the robot has been charged twice, the damage output by the last instruction will be 4. Alternatively, we see that the damage, 4 = 1 (initial damage) + 20 (damage caused by the first C) + 21 (damage caused by the second C). By breaking down the damage by each S instruction in the same manner, the total damage output, D, of the input program is given by:

  D = NS + C1 × 1 + C2 × 2 + ... + CNC × 2NC - 1 .
Next, we investigate how each swap affects the amount of damage. A swap on adjacent characters which are the same will not affect the equation. When we swap the i-th C instruction with a S instruction to its right, the value of Ci will decrease by 1 since now there is one less S than before. On the other hand, swapping the i-th C instruction with an S instruction on its left will increase the value of Ci by 1. Note that in either case, we will only modify the value of Ci, and the other C values will remain the same. This suggests that we should only ever swap adjacent instructions of the form CS.

Therefore, executing M swaps is equivalent to reducing the values of Cis such that the total amount of reduction across all Cis is M. We want the total damage (according to the above equation) to be minimized. Clearly, we should reduce the values of Ci that contribute to the largest damage output, while making sure that each of the Cis is nonnegative.

Intuitively, all of this math boils down to a very simple algorithm! As long as there is an instance of CS in the current program, we always swap the latest (rightmost) instance. After each swap, we can recompute the damage and check whether it is still more than D. If it is not, then we can terminate the program. If we ever run out of instances of CS to swap, but the damage that the program will cause is still more than D, then the universe is doomed.

Qualification Round 2018 - Code Jam 2018

Trouble Sort (8pts, 15pts)

Test set 1
Like bubble sort, Trouble Sort has O(N2) time complexity; the proof is explained below. With N ≤ 100 for test set 1, we can run Trouble Sort to completion and simply iterate over the result list to find the first sorting error, if any (that is, a value that is greater than the value that follows it in the list).

Test set 2
Running O(N2) Trouble Sort to completion is too slow for N ≤ 105.

Instead, let's break down what Trouble Sort is doing at each step. Let's consider an input list of 6 elements. Trouble Sort makes the following comparisons on each pass through the array:

element 0 ↔ element 2
element 1 ↔ element 3
element 2 ↔ element 4
element 3 ↔ element 5
Regardless of the length of the list, this table illustrates the fundamental flaw in Trouble Sort: even-index elements are compared with other even-index elements, and odd-index elements are compared with other odd-index elements, but even-index and odd-index elements are never compared with each other! This means that Trouble Sort is just bubble sort run separately on the even-index elements and the odd-index elements, interleaving them into the output list. Trouble Sort is correct only if interleaving the two sub-lists (the even-index list and the odd-index list) happens to produce another sorted list. Since there are O(N) even-index and O(N) odd-index elements, and since bubble sort is O(N2), Trouble Sort is also O(N2).

To solve test set 2, we can can run our favorite O(N log N) sorting algorithm independently on the two sub-lists described above, interleave the sorted sub-lists, and then find the first sorting error as in our solution for test set 1.

Qualification Round 2018 - Code Jam 2018

Go, Gopher! (10pts, 20pts)

Test set 1 (Visible)
For test set 1, we need to prepare all the cells within a grid-aligned rectangle of size at least 20. Before starting to deploy the gopher, let's choose a rectangular target region of size at least 20. We will try to prepare all the cells within this target region. One option is to choose a 4 x 5 target region. We could have chosen 3 x 7, 5 x 5 etc, but it should not be too big. It does not matter where we place this target region in our initial 1000 x 1000 matrix. So let's place it such that one corner of the rectangle is at (1, 1) and the opposite corner is at (4, 5). Here (r, c) refers to the cell in the r-th row and the c-th column of the original matrix. Now the question is: can we come up with a strategy that will prepare all of the cells in this target region, and no other cells?

Let's visualize our 4 x 5 target region as follows (row and column numbers are given for convenience):

  12345
1 xxxxx
2 x@@@x
3 x@@@x
4 xxxxx

We marked the internal cells with @ and the border cells with x. The gopher should not be deployed on the border cells, because that might cause it to prepare a cell outside of our target region. We will only deploy on the internal cells (cells marked with @ in the above picture). We can deploy the gopher 1000 times in total and we have 6 internal cells. Let's deploy the gopher to each of the internal cells floor(1000 / 6) = 166 times. Will this be enough to solve test set 1?

To answer this crucial question, let us compute the probability that after deploying the gopher as described above, the (1, 1) cell is still unprepared. Notice that this cell can only be prepared by deploying the gopher at (2, 2). Every time we do this, it has a 1/9 probability of preparing (1, 1). Looking at it from another angle, every time we do this, it has an 8/9 probability of not preparing (1, 1). So the probability that (1, 1) will not be prepared after 166 deployments to (2, 2) is (8/9)166 = 3.226 x 10-9, which is quite small. Realistically, we do not need to worry about this happening to any of the four corners (since the probability of this happening to at least one of the four corners is 1 - (1 - 3.226 x 10-9)4 = 1.29 x 10 -8). Other cells are adjacent to more than one internal cell, and thus they are more likely to be prepared than the corners. So this solution is sufficient to pass test set 1.

To convince you further we ran a simulation. We deployed the gopher to the center of a 3 x 3 region until all the nine cells in the region is prepared. We ran this 100000 times and following is the result:

The above figure shows how many of our 100000 simulations (y axis) required each possible number of deployments (x axis). As we can see, out of 100000 simulations the maximum number of times we needed to deploy the gopher was not more than 120. So 166 deployments would be more than enough to prepare an internal cell and all of its surroundings. Moreover, once we have prepared an internal cell and all eight of its neighbors, there is no reason to deploy the gopher there again, so we will almost certainly have more than 166 deployments to use to fill in the last stubborn cell if necessary.

Test set 2 (Hidden)
Now we need to create a rectagular region of area at least 200. If we used the same strategy described above, for, say a 10 x 20 size rectangle, we could make floor(1000/(18 * 8)) = 6 deployments to each of the internal cells. But then the probability of (1, 1) being not prepared would be: (8/9)6 = 0.49327, which is way too high!

How can we improve this strategy? We can observe that most of the cells can be prepared from a number of locations. For example, the cell (2, 2) can potentially be prepared on a deployment to any of the internal cells around it, or to the cell itself. What if we divide our rectangular region into disjoint 3 x 3 regions and only deploy the gopher to the center cells of those regions? This way each of the cells can be prepared from only one cell. To sum things up, our plan is:

Select a large enough rectangle, say 3 x 69.
For convenience, we will place the corner of this 3 x 69 rectangle at (1, 1).
We will divide our initial 3 x 69 region into 69/3 = 23 disjoint 3 x 3 regions. That is, we will deploy the gopher only to (2, 2), (2, 5), (2, 8) ... (2, 68): the center cells of those regions.
We will keep deploying the gopher at (2, 2) until all the cells inside the 3 x 3 grid centered on (2, 2) are prepared.
Then we will deploy at (2, 5) and so on.
Is that enough? As the above simulation showed, sometimes it requires 120 deployments to prepare entire 3 x 3. So in the worst case 23 x 120 = 2760 deployments to prepare entire 3 x 69 region which is more than out limit 1000. However, such worst case will not happen always. We ran another simulation to examine our new strategy:

The above figure shows how many of our 100000 simulations (y axis) required each possible number of deployments (x axis) to prepare all the cells inside 3 x 69 target region. As we can see, the maximum we needed is no more than 850 and our limit was 1000. So we can be confident that this strategy is good enough to pass test set 2.

There are many other strategies. One may be, to deploy the gopher to the cell that has the largest number of unprepared cells in 3 x 3 region centering at that cell. This strategy yields following simulation result (100000 runs), which looks better than the one before:

Qualification Round 2018 - Code Jam 2018

Cubic UFO (11pts, 21pts)

This is an ad hoc geometry problem with many different solutions.

Test Set 1 (Visible)
Suppose the cube is initially axis-aligned. Let us rotate it about the z-axis by angle t, from +x towards +y, and study the shadow:

x
y
z
V
W
x
y
z
V
W
x
y
z
V
W

Key observations:

The shadow is a rectangle aligned to x- and z-axes, starting out as a square for t = 0.
For 0 ≤ t ≤ π/4 (45 degrees): z-length = 1 always, and x-length = Vx − Wx, where Vx and Wx are x-components of vertices V and W in the figure. Therefore this is really a 2-D problem; we can ignore z!
The shadow area is A = 1 × (Vx − Wx) = 2 Vx, since Wx = −Vx.
For this setup, maximal area is attached when t = π/4, which corresponds to Vx = √(½), resulting in A = √2 ≈ 1.414214. This exceeds all Test Set 1 inputs, so the setup is sound.

Next, we find the shadow area A as a function of angle t. From basic geometry, Vx = √(½) × cos(t − π/4). Therefore A = 2Vx = √2 × cos(t − π/4), for 0 ≤ t ≤ π/4.

Given A, naively we'd invert the formula and get t as sum of π/4 and cos−1(A / √2). However, to satisfy 0 ≤ t ≤ π/4, we need the negative branch of cos−1! Therefore the inverse is:

t = π/4 − |cos−1(A / √2)|.

Once t is obtained, the final outputs are the centers of three non-parallel faces. One such face (invariant for all t) is (0, 0, ½). The other two can be obtained from rotating (½, 0), and (0, ½) by angle t, and assigning z to 0. Using the rotation formula yields (½ cos(t), ½ sin(t), 0) and (−½ sin(t), ½ cos(t), 0).

Test Set 2 (Hidden)
Our solution hinges on the following two crucial observations:

The cube will cast the smallest possible shadow, which has a square shape, when one of its faces is parallel to the xz-plane.
The cube will cast the largest possible shadow, which has the shape of a regular hexagon, when one of its vertices is on the y-axis.
To simplify the computations, let's rotate the cube about the y-axis by 45 degrees. (The direction of the rotation does not matter, since the cube would end up in the same orientation either way.) After that, the cube will look like this:

x
y
z
A
B
E
F
D
C
H
G
x
y
z
A
B
E
F
D
C
H
G

It might not be immediately clear why this simplifies our life, but it will make sense soon!

According to the above observations, the cube currently has the smallest possible shadow. To maximize that shadow, we can rotate the cube about the x-axis from +z towards +y, and bring the vertex H (from our diagram above) onto the y-axis. A useful property of this rotation is that the area of the shadow consistently increases throughout this rotation. Since we start with the smallest possible shadow and continuously rotate until we get the largest possible shadow, we achieve every possible shadow area at some point during this rotation. So, we can use binary search to figure out the exact angle by which we need to rotate the cube about the x-axis to achieve the desired area.

x
y
z
A
B
E
F
D
C
H
G
x
y
z
A
B
E
F
D
C
H
G
x
y
z
A
B
E
F
D
C
H
G

However, two questions remain:

If we rotate the cube about the x-axis by a certain angle, what will be the coordinates of the vertices of the cube?
Given the coordinates of the vertices, how can we calculate the area of the shadow?
Rotating a cube about the x-axis
Notice that, since we are rotating the cube about the x-axis, the x-coordinates of the points will remain the same; only the y- and z- coordinates will change. So, instead of performing rotations in 3-D, we will project the point onto the yz-plane (the x = 0 plane) and perform the rotations in 2-D.

For example, suppose that we want to rotate point P = (Px, Py, Pz) about the x-axis from +z towards +x by angle t. First, we project the point onto the yz-plane, where it will have coordinates (0, Py, Pz). We will ignore the x component and treat the point as (Py, Pz) on a 2-D plane.

Now, rotation about the x-axis by angle t in the indicated direction is equivalent to rotating (Py, Pz) about (0, 0) by angle t in a clockwise direction. The resulting 2-D point will be

(Py', Pz') = (Py × cos(t) + Pz × sin(t), −Py × sin(t) + Pz × cos(t)),

which in 3-D becomes (Px, Py', Pz'). We can get this from the rotation formula, or the complex expression (Py + iPz) × e−it.

Shadow area
As H approaches the y-axis, the shadow on the y = −3 plane takes the shape of a convex hexagon. More specifically, the vertices of the hexagon are the projection of points D, C, G, F, E, and A onto the y = −3 plane. For a point P with coordinates (Px, Py, Pz), the coordinates of its projection onto the y = −3 plane are (Px, −3, Pz).

Now, to find the area of the shadow, we can treat the six projected vertices as if they were on a 2-D plane, with only their x- and z-coordinates. By construction, these already form a convex hull with vertices properly oriented (otherwise one would need to explicitly compute the convex hull, although shortcuts exist for the special case). This enables us to apply the standard formula to compute area of a convex polygon. Note that area computation can also be simplified by using symmetry with respect to the z-axis, and apply the trapezoid area formula instead of the general convex polygon.

Once we have a set of coordinates that produces the desired area, we can compute the coordinates of the face-centers of any three non-pairwise-parallel faces, and we have solved the problem. With the setup above, these are simply ½(A + C), ½(C + F), and ½(F + A).

Other approaches
There are many other ways to solve this problem:

Instead of binary searching, the angle of rotation can be solved directly. In fact, final coordinates can be computed using a closed form without using trig functions!
We could avoid computing the area of the shadow by using this amazing cube shadow theorem.
Instead of doing two rotations (one about the y-axis and another about the x-axis), we could rotate the cube about line connecting points (0, 0, 0) and (1, 0, 1), or some other similar axis, to bring a vertex onto the y-axis. The general rotation formula can be found here.

Round 1A 2018 - Code Jam 2018

Waffle Choppers (9pts, 16pts)

Test set 1
In test set 1, we are asked to make only one horizontal cut and only one vertical cut. There are R - 1 possible horizontal cuts and C - 1 possible vertical cuts, so there are a total of (R - 1) × (C - 1) different ways to make the two cuts. Since R and C are both at most 10, there are at most 81 ways, and we can try each one of them separately. For each way, we can count up the chocolate chips in each of the four resulting pieces. If we ever find a way in which that number is the same for all four pieces, the answer is POSSIBLE; otherwise, it is IMPOSSIBLE.

Test set 2
In test set 2, we may have to make many horizontal and vertical cuts, and there are too many ways to do this to check. The worst cases occur when H is close to half of (R - 1), and V is close to half of (C - 1); there could be over 1057 ways of making the cuts! So we need another approach.

Let us consider only the horizontal cuts, ignoring the vertical cuts for the moment. These horizontal cuts break the waffle into two or more horizontal "slices". Each of these slices will turn into exactly V + 1 pieces when we make our vertical cuts. Here is the critical observation: if the case is POSSIBLE, all of those pieces must have exactly the same number of chips, and since each horizontal slice will produce exactly the same number of pieces, all of those horizontal slices must have exactly the same number of chips. Moreover, we know exactly what that number is; if there are a total of C chips in the waffle, each of the H + 1 horizontal slices must have exactly C / (H + 1) chips, or else the case is IMPOSSIBLE.

This observation is so powerful that it tells us where we have to make the cuts if the case is POSSIBLE! We can make a list of the numbers of chips in each row, then turn that list into a cumulative sum of the total number of chips seen so far. For example, for a waffle with seven rows containing 3, 7, 6, 2, 2, 0, and 10 chips, respectively, the cumulative sum list would be: [3, 10, 16, 18, 20, 20, 30]. In that example, if H = 2, we know we have to make cuts immediately below rows that bring the total sum to 10 and 20, and we can do so by cutting immediately below the second and fifth rows. (Note that we could instead make our second cut below the sixth row, but this would make no difference.) If H = 1, though, we need to make our one cut immediately below a row that brings the total sum to 15, and there is no such row. So, after this step, either we will know that the case is IMPOSSIBLE, or we will know exactly where to make our horizontal cuts.

Then, we can consider only vertical cuts, using a similar method, and either learn where to make them, or learn that the case is IMPOSSIBLE. Even if we know where to make the horizontal and vertical cuts, though, we are not done yet! These cuts might not actually create pieces with the same number of chips. For example, for H = 1, V = 1, and this waffle:

..@@
..@@
@@..
@@..
we will learn that if the case is POSSIBLE, we must make our horizontal cut below the second row, and our vertical cut to the right of the second column. But this creates two pieces with four chips each and two pieces with no chips at all, so the case must be IMPOSSIBLE.

To check that each piece has the same number of chips, we can start by turning the list of horizontal cuts into a list of intervals; for example, if we cut below the second and fifth rows in the [3, 10, 16, 18, 20, 20, 30] example from above, then we have the inclusive intervals [1, 2], [3, 5], and [6, 7]. We can do the same for the vertical cuts, and then perform a double iteration over these two sets of intervals, checking each cell within each pair of intervals. We know that each piece must have exactly (total # of chips) / ((H + 1) × (V + 1)) chips if the case is POSSIBLE, so if we ever find a slice in which this is not true, the case is IMPOSSIBLE. Otherwise, we have finally shown that the case really is POSSIBLE. (Once again, note that even if we had a choice of where to make one or more of our cuts, this must have been due to empty rows or columns, which do not influence the number of chips in each piece.)

(There are other ways to check the pieces; for example, we can make one pass through the data, and, for each cell, calculate the number of chocolate chips in the rectangle that has that cell as its lower right corner, and the upper left corner of the waffle as its upper left corner. Then, to find the number of chips in a certain piece, we can add and subtract the appropriate rectangles from this set.)

In summary, this algorithm involves several steps:

Create the row and column sum arrays. We can either make two separate passes through every cell of the waffle, or make one pass and create both arrays at once.
Convert the sum arrays into cumulative sum arrays.
Check the cumulative sum arrays to find our places to cut.
Use the cumulative sum arrays to create interval arrays.
Make one pass through every cell of the waffle in a directed way, using the interval arrays, to check whether every piece has the same number of chips.
Steps 1 and 5 above involve looking at each of the R × C cells of the waffle, whereas steps 2, 3, and 4 only involve looking at R or C values. So steps 1 and 5 dominate the running time, which is O(R × C). (We could not have done better than this anyway, since we clearly have to look at each cell of the waffle at least once to solve the problem.)

Round 1A 2018 - Code Jam 2018

Bit Party (11pts, 21pts)

Test set 1
We might consider enumerating all the ways of assigning bits to cashiers, but with 20 bits and 5 cashiers, there could be as many as 520 ways — too many to check! We need to take advantage of the fact that the bits are interchangeable, and instead find all the ways to partition B bits among R of the C cashiers (since we will only be able to use as many cashiers as we have robots). Then, we can compute how much time each of those ways takes, and pick the minimum of those values.

We can calculate the number of ways to partition 20 bits among 5 robots using this method. It turns out there are only (24 choose 4) = 10,626 ways to check. If we have fewer robots than cashiers, then we need to introduce another multiplicative factor of (C choose R), but this cannot be larger than 10 in test set 1. Each check takes O(R) time, which is very small given that R is at most 5. So, test set 1 should be solvable well within the 15 second time limit, regardless of your language choice.

Test set 2
To solve this test set, we need to be able to answer the following question: Given a time limit T, is there a possible assignment of bits such that all the robots can finish interacting with their cashiers in no more than T seconds? Let f(T) be the answer to this question.

How can we find f(T)? The maximum number of bits that the i-th cashier can process in not more than T seconds is max(0, min(Mi, floor((T - Pi) / Si))). Let us call this value Capacityi.

Then, we want to know whether a total of B bits can be assigned to R robots, and each of those robot to a cashier, such that the number of bits processed by the i-th cashier is not more than Capacityi. To do this, we can greedily sort the Capacity values into nonincreasing order, and then assign the R robots to the first R cashiers. f(T) is true if and only if the total number of bits that can be processed by the first R cashiers is at least B. Therefore, we can compute the value of f(T) for any T in O(C log(C)) time (which is the time it takes to sort the Capacity values). (Aside: We can even avoid the sort, and instead partition in O(C) time, by using introselect, for example.)

Since we want to minimize the time taken for all robots to interact with their cashiers, we want to find the minimum possible value of T such that f(T) is true. That value of T will also satisfy the following:

f(x) is false for all x < T.
f(x) is true for all x ≥ T.
Therefore, we can find the value of T using binary search. Since the maximum answer will not be more than O(max(S) × B + max(P)), this solution will run in O(C log(C) log(max(S) × B + max(P))) time.

Round 1A 2018 - Code Jam 2018

Edgy Baking (14pts, 29pts)

Cookie-cutting
A cookie with width W and height H has a perimeter of 2 × (W + H). If we make a straight cut of length C that divides the cookie in half, each piece will have one side of length C, and the remaining sides of the pieces will represent the perimeter of the original cookie. So, after cutting, we will have a combined perimeter X = 2 × (W + H + C).

Clearly X is smallest when we leave the cookie alone. However, if we want to minimize X given that we are making a cut, we must minimize C; no cut can be smaller than the smallest of W and H, so we should cut through the midpoints of the two larger sides, which gives the cut a size of the smaller of the two sides. Then X = 2 × (W + H + min(W, H)). On the other hand, if we want to make a cut that maximizes X, we must maximize C by cutting through two opposite corners of the cookie. Then C = sqrt(W2 + H2), so X = 2 × (W + H + sqrt(W2 + H2)). If we start our cut somewhere between one of those larger-side midpoints and a corner, we will get a value of X somewhere between these two extremes; since we can cut anywhere we want in that interval, we can get any intermediate value of X that we want.

So, depending on what we do with our cookie, its contribution to our overall perimeter sum will either be 2 × (W + H), or a value in the range 2 × (W + H) + [2 × min(W, H), 2 × sqrt(W2 + H2)]. From now on, we will abbreviate the extremes of that range as [L, R]. (We could divide all values in the problem by 2 as well, removing that factor, but we will retain it in this analysis for clarity.)

Test set 1
In test set 1, all of the cookies have the same dimensions; we will call them W and H. Let us define P' as P - 2 × (W + H) — that is, the amount of extra perimeter that we need to add to reach the target P. So, we can reformulate the problem as starting with a total perimeter of 0, and trying to reach P'. For each cookie, we can choose to either not cut it, which leaves our total perimeter unchanged, or cut it, which increases our total perimeter by some value of our choice in the range [L, R]. If we cut K cookies, then our total perimeter can be anywhere in the range [K × L, K × R], depending on how we make our cuts. We will think in terms of this range and not in terms of the individual cuts.

How many cookies should we cut? Of course, we should never cut so many that K × L exceeds P'. However, we might as well keep cutting cookies up until that point, since doing so both moves our range closer to the target and makes the range wider. So, we can solve for X in the equation X × L = P', and then choose K to be the floor of X — that is, floor(P' / L). (Since L is an integer representing one of the original side lengths, and P' is also guaranteed to be an integer, we should use integer-based division here to avoid the usual problems with flooring floating-point numbers!)

Then, we can check whether the range [K × L, K × R] includes P'. If it does, the answer is 0; otherwise, it is P' minus the right end of the range, i.e., P' - K × R. Even though R is generally not an integer, we do not need to worry about floating-point issues; even if P' is very close to K × R and we mistakenly conclude that it is not in the range, we will still get an answer that is sufficiently close to 0 under our floating point rules.

Test set 2
In test set 2, different cookies might have different dimensions, so we get different total ranges depending on which ones we cut. We have no a priori basis for deciding which cookies to cut, and we cannot directly check all 2100 possibilities, but we can use dynamic programming to ensure that we find the best possibility without explicitly checking them all; the problem is very similar to the knapsack problem. For each cookie, we are deciding whether to leave it as is, or cut it, which adds L to our total perimeter and gives us up to R - L units of additional "slack". Once we have finished deciding which cookies to cut, we can include as much or as little of this "slack" as we want. All other things being equal, having more slack is always better for us. The large limit on P may seem daunting, but you can convince yourself that the problem space is actually small enough for methods like this to work.

However, the problem can also be solved in a different way. As in the previous solution, each cookie corresponds to a range [L, R] by which we can increase the total perimeter if we decide to cut that cookie. Let S(K) be the list of intervals we can reach after processing the first K cookies. We start with S(0) = {[0, 0]}.

When we consider the K'th cookie, we can choose to cut it or not. Suppose that this cookie corresponds to the interval [L, R]. If we do not cut it, we can obtain any perimeter that is already in an interval in S(K - 1). If we do cut it, we can reach any interval in the set S' given by S' = {[l + L, r + R] : [l, r] is an interval in S(K - 1)}.

To get S(K), we first take the union of S(K - 1) and S', followed by merging overlapping intervals. For example, if S(K - 1) = {[0, 0], [3, 6]}, and we add the interval [L, R] = [1, 2], we obtain S' = {[1, 2], [4, 8]} and S(K) = {[0, 0], [1, 2], [3, 8]}. Note that the intervals [3, 6] from S(K - 1) and [4, 8] from S' merge into a single interval [3, 8] here.

We can drop any intervals that start after P', so that after processing all N cookies, the answer to the question will be the distance from P' to the last interval.

Without any further analysis, we might expect the size of S(N) to be 2N, as it could double in each step. It turns out that we can provide a much stronger bound of (log(P') / log(sqrt(2))) + 1.

We first need the observation that any interval [L, R] corresponding to a cookie will satisfy R ≥ sqrt(2) × L, where equality holds for a square. Using induction, we can infer that every interval [l, r] in each S(K) also satisfies r ≥ sqrt(2) × l.

The second observation is that all intervals in S(N) are disjoint. Let us now sort intervals in S(N) and enumerate them starting from [l0, r0]=[0, 0]. Then the observations together imply that l{i+1} > ri ≥ sqrt(2) × li. Since the lower bound of each interval is an integer, we also have l1 ≥ 1. Hence we have li ≥ sqrt(2)i-1. Since we can discard any intervals starting after P', we may assume that li ≤ P'. We conclude that there are at most (log(P') / log(sqrt(2))) + 1 intervals in S(N).

This solution takes O(N log(P')) time: for each of the N cookies, we have to perform one merging step that takes O(|S(K)|) time.

It can also be shown that |S(K)| ≤ 2K + 1. We leave this as an exercise to the reader!

Round 1B 2018 - Code Jam 2018

Rounding Error (5pts, 9pts, 11pts)

Test set 1
This test set can be solved using a complete search. We can try every possible partition of N voters among N languages. If two partitions differ only in the order of their languages, then we consider those partitions equivalent.

Therefore, we can consider a partition as an N-tuple (x1, x2, ..., xN), where xi ≥ xi + 1 and Σ xi = N.

Even with N = 25, there are no more than 2,000 different partitions. For each partition, we can use the following greedy algorithm to check whether the partition can be achieved by only adding voters: let us sort the Ci values in non-increasing order — that is, such that Ci ≥ Ci + 1. Then the partition can be achieved by only adding voters if and only if xi ≥ Ci for all 1 ≤ i ≤ L. Among all such partitions, we can find the largest percentage sum, which is our answer.

Test set 2
For this test set, we can remove our assumption that a partition and C must be sorted non-increasingly. Therefore, we consider a partition of N voters to N languages as (x1, x2, ..., xN), where Σ xi = N.

To solve this test set, we can use dynamic programming (DP). We define a function f(a, b) as the following:

Among all partitions (x1, x2, ..., xa) such that Σ (1 ≤ i ≤ a) xi = b and xi ≥ Ci for all 1 ≤ i ≤ a, what is the maximum Σ (1 ≤ i ≤ a) round(xi / N × 100) possible? If there is no satisfying partition, then f(a, b) = -∞. We can assume Ci = 0 for i > L.

We can first handle the base case of the function. We can easily compute f(1, b) since there is only at most one satisfying partition. Therefore, f(1, b) = round(b / N × 100) if b ≥ C1, or -∞ otherwise.

The recurrence f(a, b) of this function can be computed by considering all possible values of xa. Let i be the value of xa. Therefore, xa contributed round(i / N × 100) to the total percentage, and there are (b - i) votes left to be distributed among x1, x2, ..., xa - 1. Therefore, for a > 1, f(a, b) = max (Ca ≤ i ≤ b) (round(i / N × 100) + f(a - 1, b - i)).

Since we want to distribute N voters to x1, x2, ..., xN, the answer for the problem is f(N, N).

Function f has O(N2) possible states and each state takes O(N) time to compute. Therefore, this solution runs in O(N3) time.

Test set 3
We can solve this test set with a greedy strategy. For each language, we will either be rounding the percentage up or down. We get the maximum answer when as many of these as possible are rounded up.

Therefore, we can ignore any languages that are already being rounded up. Since there can be arbitrarily many languages, nothing ever forces us to disturb these languages by adding another vote—it's no worse to add that vote to some new language instead. We figure out how many more votes each language (including languages nobody has even mentioned yet) would need in order for it to be rounded up.

We greedily satisfy as many of these as possible, starting with the ones that take the fewest additional votes. This solution runs in O(N × log(N)) time.

Round 1B 2018 - Code Jam 2018

Mysterious Road Signs (10pts, 20pts)

Test Set 1 (Visible)
First, let's break down what the problem is asking us to find. We are asked to identify sets of contiguous signs, where each set is defined by four variables:

The index of the first sign in the set, i (inclusive)
The index of the last sign in the set, j (exclusive)
The destination for eastbound travelers, M
The destination for westbound travelers, N
In order for a set to be valid, each sign in the set must be truthful to eastbound travelers, westbound travelers, or both. Given the four variables above, we can evaluate a set of signs for validity in O(j–i) time. We can bound the number of possible values for M and N by S by taking the set of all westbound or eastbound destinations displayed by at least one sign. Since i and j are also bounded by S, we would need O(S5) time for this solution, which is not sufficient for Test Set 1.

To improve our brute-force solution, we can be more clever with how we pick M and N. The first sign in a set tells us a lot of information: namely, it defines what either M or N must be (one of the two destinations on the sign, or else the sign wouldn't be able to be in the set). Suppose we fix M based on the first sign. Now, we can walk through the rest of the signs in the set until we find a sign whose eastbound destination is not M; use that sign's westbound destination as N. Continue walking until we reach a sign that does not share either M or N, in which case the set is invalid, or until we reach the end of the set, in which case the set is valid. We can perform the analogous process by fixing N based on the first sign. This “two-pass” set evaluation algorithm runs in O(S) time, since sets have size O(S).

Since there are O(S2) sets of signs and we can evaluate each set for validity in O(S) time, we have a O(S3) algorithm, which is fast enough for Test Set 1.

Test Set 2 (Hidden)
Clearly the cubic solution won't work on Test Set 2. It turns out that a quadratic solution coded in a fast language with a sufficient amount of pruning can pass Test Set 2. However, in this analysis, we will describe a O(S log S) solution followed by a linear-time O(S) solution, both of which comfortably finish in time when evaluating Test Set 2.

The O(S log S) solution is a classic application of divide and conquer. Cut the list of signs in half, except for a single sign at the center, which we will call the “midpoint” sign. Recursively apply the algorithm to the western half and the eastern half of signs. Then, use a modified version of the previously described two-pass algorithm to find the best set containing the midpoint: first, fix M to the midpoint's eastbound destination. Since we are looking for long segments, we can greedily add as many signs to the set as possible. Walk the list of signs both westward and eastward until reaching a sign on each end that does not share the same eastbound destination (M value) with the midpoint. Let N1 equal the westbound destination of the western boundary (the first sign to the west that did not match M) and let N2 equal the westbound destination of the eastern boundary (the firsts sign to the east that did not match M). Find M1 and M2 using an analogous procedure: return to the midpoint, walk westward and eastward until reaching signs that do not share a westbound destination (N value) with the midpoint, and let M1 and M2 be the eastbound destination of the signs at the western boundary and eastern boundary, respectively. We now have four possible M/N pairs: (M, N1), (M, N2), (M1, N), and (M2, N). We can greedily walk east and west from the midpoint with each of the four pairs to find the longest set containing the midpoint. This “four-pass” step for D&C runs in linear time. Now that we have found the longest set(s) containing the midpoint as well as (recursively) the longest sets from the western and eastern halves of signs, we can combine these results to get the longest sets for the whole dataset. This yields an algorithm requiring T(S) operations to compute an input of size S, for a function T that satisfies T(S) = 2T(S/2) + O(S). Using the Master Theorem we get that T(S) = O(S log S).

The linear-time solution does a single pass, identifying all possible long segments. Start by walking forward from the first sign. Maintain two “candidates”, called the “M-candidate” and the “N-candidate”, with the following properties:

Two destinations M and N.
An index start corresponding to the westernmost sign in the contiguous segment of signs that includes the current sign and that satisfies M or N.
An index xstart corresponding to the westernmost sign in the contiguous segment of signs that includes the current sign and whose eastbound destinations are all M (for the M-candidate) or whose westbound destinations are all N (for the N-candidate).
With these invariants, the set of signs starting at start and ending after the current index is guaranteed to be a valid set.

To maintain the invariants when reading a new sign, we can use the following procedure to create the new the M-candidate (the procedure for creating the new N-candidate is analogous):

If new sign's eastbound destination equals previous sign's eastbound destination, copy the previous M-candidate to become the new M-candidate.
If the new sign's eastbound destination equals the previous N-candidate's M value, copy the previous N-candidate to become the new M-candidate, and set xstart to the new sign's index.
Otherwise, copy the previous N-candidate to become the new M-candidate, set M to the new sign's eastbound destination, set start to xstart, and set xstart to the new sign's index.
Consider the following illustration:

eastbound destinations (Dᵢ+Aᵢ)
westbound destinations (Dᵢ–Bᵢ)
6
-1
8
-1
7
0
8
2
8
2
9
0
M-candidate:M=9, N=2start=3xstart=5
N-candidate:M=8, N=0start=1xstart=5
The illustration shows six zero-indexed signs and the destinations for those six signs. The signs are connected by lines that show which candidates are used when calculating the candidates for the next step. The candidates after reading the sixth sign (index 5) are shown at the end. On top we have the M-candidate with M=9 and N=2. The candidate starts at index 3, because the sign at index 2 does not have a compatible westbound or eastbound destination for that candidate. The illustration also demonstrates how xstart=5 is the start of the most recent string of westbound destination M matches, which in this case is only the current sign. The N-candidate goes all the way back to start index 1 with M=8 and N=0.

Since calculating the new candidates takes constant time, and we have to read each sign in sequence, this is an O(S) solution.

There are two more tips that can aid in solutions to this problem. First is that we don't ever need to remember Ai, Bi, and Di directly; we only care about the westbound and eastbound destinations. You could therefore compute those destinations upon reading in the data and store them in a list of pairs. The second tip is that there is a lot of opportunity for pruning: if you maintain a global list of the best known sets of signs, you never need to look at any candidate sets that are smaller than the current best, allowing you to prune away a lot of segments that you no longer need to evaluate. All of the solutions described here except for the O(S) solution can take advantage of pruning.

Round 1B 2018 - Code Jam 2018

Transmutation (15pts, 18pts, 12pts)

This is a very tricky problem with lots of pitfalls to consider.

Test set 1 (Visible)
Plain brute force is sufficient to pass this data set. There are many ways to approach it that would work, though. One intuitive approach would be to create one gram of lead (metal 1) as often as possible. Suppose Create(i) is a function that takes a metal i as parameter and returns true if it can create one gram of the i-th metal and false otherwise. Then, we can call Create(1) repeatedly until it returns false, and output the number of calls that returned true. An implementation of Create(i) works as follows:

First, check if the remaining amount of the i-th metal is positive. If it is, we consume 1 gram of this metal and return true.
Otherwise, let P and Q be the two metals that can be combined to create the i-th metal.
We recursively call Create(P) and Create(Q).
If both of the function calls return true, we return true. Otherwise we return false.
However, there is slight problem with the above function. When we are not be able to create metal 1, this function will never end, it will keep trying to create the metals. Consider a simple example: there are 3 metals and for each of them the other two are the ingredients. Suppose, the amount of the metals are now 0 but we are trying to create lead. In such case, it will try to create the other two metals recursively, which will eventually try to create the lead again and so on. One way to fix it would be to check if we tried to create this i-th metal in the current call stack. If we already tried to create the i-th metal and while doing so we looked into its ingredient elements and again arrived at the i-th metal, that means it is not possible to create the i-th metal (and eventually the lead) anymore. Another way that is simpler to code is to limit the depth of the recursive call to M, because after M recursive calls we are guaranteed to repeat at least one metal.

The runtime of the above solution is not very large. The recursive call stack is up to M calls deep and at each level we call Create twice recursively. We may imagine it as a binary tree. The number of leaf nodes is at most 2M which makes the total less than 2M+1. So, a Create(1) call takes O(2M) time. There may be at most 8 grams of each of these metals. So we will call Create(1) function about 8M times. For M = 8, this means the body of the function executes 8 × 8 × 28, which is not too large.

Test set 2 (Hidden)
The approach above is of course too slow for the second test set. We can do a top-down approach as follows: we maintain a current "recipe" to create lead. The initial recipe is just use 1 gram of lead to create 1 gram of lead. The invariant is that the current recipe is always optimal, that is, any other way to create lead that can be done with the current supply is an expansion of the recipe. An expansion of a recipe is the result of zero or more applications of replacing 1 gram of a metal in the recipe by 1 gram of each of the two metals required to make it. This invariant is clearly true for the initial recipe.

As a first step, we make as much lead as we can with the current recipe, which we already mentioned is optimal. After doing that, the supply of one or more of the metals in the recipe is less than the recipe requirements of each. That means that any recipe that works is an expansion of replacing 1 gram of each of those metals for its ingredients. We perform one of those replacements to get a new optimal recipe and repeat.

Notice that the total amount of grams of metal in the recipe only increases and the same number in the supply only decreases or stays the same (if we make no lead in a step). So, when the amount in the recipe surpasses the amount in the supply we can stop, since we won't be able to make another gram of lead.

Let S be the sum of all Gi, that is, the amount of grams of metal in the initial supply. Each step above takes O(M) if we represent the recipe as a vector of size M with the required grams of each metal in the recipe. Checking how much lead we can do requires a single pass to find the limiting ingredient, and finding an ingredient that we need to replace requires another pass. Making the replacement of a single ingredient takes constant time. Since after each step the total amount of grams of metal in the recipe increases by at least 1, and the supply does not increase, the number of steps until the stopping condition is at most S. This makes the running time of this algorithm O(MS) which is enough to pass for M ≤ 100 and S ≤ 10000.

Test set 3 (Hidden)
Since S can be up to 1011 for test set 3, we can't really use the approach above. Adding a lot of prunning to it to prevent really long cycles of replacements to happen can work, but it's hard to know exactly how much prunning is required unless we take a systematic approach. Fortunately, using binary search can simplify this a lot. First, we consider the simplified problem of deciding if it is possible to make L grams of lead, for an arbitrary L. If we can solve that efficiently, a simple binary search within the range [0, S] finished the problem, and multiplies the time complexity by a (relatively) small log S.

For a fixed L, we start by adjusting our supply by making the amount of lead G1 - L. That may leave us with lead debt instead of lead supply. We now iterate paying off debt until either we cannot or we have no debt left. If we cannot pay off some debt, then we cannot make L grams of lead. If we find ourselves with no debt left, we can make L grams of lead. While we iterate, we will adjust the supply G and the recipes R, that start as given in the input.

For each step of pay off, find an ingredient i such that Gi < 0. If there are none, we paid off all debt and we are done. If there is, we go through the recipe to make metal i. If the recipe contains metal i itself, we can't pay off the debt and we are done. Otherwise, for each k grams required of metal j in the recipe, we do Gj := Gj + k × Gi (remember Gi is negative). That is, we push the debt of metal i to requiring amounts of metals from its recipe. Finally we can set Gi := 0. If we ever need i in the future, we know we will again need to go through its recipe, so we replace any k grams of i in the recipe of any ingredient by i's recipe multiplied by k. In this way, metal i will never be required in the future.

A step of payoff takes O(M2) time: finding a metal in debt takes time linear on M, and so do adjusting all ingredients (remember we represent recipes by a vector of size M with the grams required for each metal). Replacing metal i in a single recipe takes time linear on M too, and we have to do it for each other of up to M metals, yielding O(M2) time for the step.

During each step of payoff one metal disappears from the recipes. Since at most M - 1 metals can disappear (when there is only one metal i left, its recipe is guaranteed to contain metal i itself and we'll stop, since recipes only grow and thus are never empty) the total number of payoff steps is O(M). This makes the overall time to check an arbitrary L O(M3), and the overall time for the entire algorithm O(M3 log S), which is fast enough to solve the hardest test set.

Round 1C 2018 - Code Jam 2018

A Whole New Word (11pts, 17pts)

Test set 1
Since L ≤ 2, this test set can be solved using a complete search. First, we collect the letters that appear among the first characters of the input words in a set C1 and the letters that appear among the second characters of the input words in a set C2. Any candidate new word has the form c1c2, where c1 is in C1 and c2 is in C2. For each candidate new word, we check whether this word is in the input. We can output any candidate new word which does not appear in the input as our answer. If every candidate new word already appears in the input, the case is impossible.

Since there are only at most 262 different candidate words that we need to try, this solution will run very quickly.

Test set 2
In early rounds of Code Jam, a complete search will often work for the first test set, but will generally not work for subsequent test sets. This problem is an exception! Our approach above will work just fine for test set 2.

We can create sets C1, C2, ..., CL as in the solution above, and then use them to generate candidate words as before, one at a time. If we encounter a word that is not in the input, we can return it as our answer. If it turns out that there are exactly N candidate words (which implies that every word that could be generated is already in the input), the case is impossible. Otherwise, we can be sure that we will have found an answer by the time we generate and check the (N + 1)th candidate word, since there are only N words in the input list.

Round 1C 2018 - Code Jam 2018

Lollipop Shop (29pts)

This problem's acceptance condition is based on competitive analysis. The problem is a bit tough to test locally, even for an interactive problem. It is easy to generate a particular set of customer preferences, but determining the maximum possible number of lollipops we can sell requires bipartite matching. However, we do not necessarily need to do that; since there is only one test set, which is Visible, we have some room for experimentation and for small leaps of faith.

A solution that just picks random legal flavors is not good enough to pass, so we need a couple of insights:

Selling a lollipop, if possible, is never worse than not selling a lollipop. Assume that there is some good solution that involves not selling a lollipop now, and selling L lollipops in total later. Selling a lollipop now can only stop at most 1 lollipop from being sold later, so we can still sell L-1 more lollipops.
If we have a choice of lollipops to sell, the best we can do is sell a lollipop that has the lowest probability of being liked. Intuitively, this saves the flavors that are more likely to show up later. (This intuition can be proved mathematically.)
But we don't know the true probability for each flavor. The best estimate we can get for a flavor at any point is the number of customers that have liked that flavor so far, divided by the total number of customers seen so far.
Therefore, our best strategy is to always sell a lollipop if possible; whenever there are multiple flavors to choose from, we choose one of the flavors that we have seen the minimal number of times among previous customers' preferences. In problems like this, when we have to break a tie, we should do so at random just in case the problem setters have tried to anticipate and thwart particular strategies like always choosing the smallest ID number. (In this problem, it would have been impossible for the setters to penalize that, though!)

Notice that our solution would not work for an arbitrary list of probabilities, even with 200 customers. If every probability is 0.02, for example, our strategy loses its power (since all flavors are equally likely), and each flavor appears frequently enough to make our strategy do much worse than matching, but infrequently enough that we have very few chances to make the right choices. However, the problem guarantees that the probabilities are drawn randomly from the range [0.005, 0.1]. This should give us enough confidence that the probabilities will be different enough for our strategy to exploit.

The algorithm we have described can be shown to be at least as good as any other solution. But what if we had been less certain? In a problem with only Visible test sets, it is generally better to try (at the risk of a 4 minute penalty) than to spend more than 4 minutes worrying.

Round 1C 2018 - Code Jam 2018

Ant Stack (16pts, 27pts)

Test set 1
To solve this test set, we can use dynamic programming (DP). We define a function f(x, y) as the maximum number of ants that can form a stack (following the stack requirements given in the problem statement), in which we only consider ants from the first ant to the x-th ant, inclusive, and the sum of the ants' weight is not more than y.

We can compute the value of f(x, y) by considering two cases:

Suppose we don't put the x-th ant on the bottom of the stack. Then we can ignore the x-th ant, and so the value of f(x, y) from this case becomes f(x - 1, y).
Suppose we put the x-th ant on the bottom of the stack. Then we need to maximize the number of ants to be put on top of the x-th ant, which is the value of f(x - 1, min(6Wx, y - Wx)). Counting the x-th ant as well, the value of f(x, y) from this case becomes f(x - 1, min(6Wx, y - Wx)) + 1. Note that we only consider this case if y ≥ Wx.
Between the two cases (or only one case if y < Wx), we take the larger value as the value of f(x, y).

The answer for the problem is the value of f(N, ∞). Since there are O(N) possible values for x, O(max(W)) possible values for y, and O(1) iterations for each computation of f(x, y), this solution runs in O(N × max(W)) time.

There is also another solution involving another DP formulation f', where f'(x, y) only considers ants from the x-th ant to the N-th ant (instead of the first ant to the x-th ant).

Test set 2
To solve this test set, we need to find the value of K, the maximum possible answer to the problem. To have a stack with as many ants as possible, where the upper-bound of the ants' weight is fixed by a constant (i.e. 109 in this problem), we greedily put the lightest ant possible on the bottom of the stack. In other words, a stack with as many ants as possible will have ants with weights something like (1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 6, ...). We stop as soon as we need to add an ant with a weight more than 109. By creating a simple script, we can determine that the value of K is 139, much smaller than N.

Therefore, we can solve this test set with another DP formula. We define a function g(x, y) as the minimum sum of the weights of the ants that can form a stack of y ants where only the ants from the first ant to the x-th ant are considered, or ∞ if no such stack exists.

Again, we can compute the value of g(x, y) by considering two cases:

Suppose we don't put the x-th ant on the bottom of the stack. Then we can ignore the x-th ant, and so the value of g(x, y) from this case becomes g(x - 1, y).
Suppose we want to put the x-th ant on the bottom of the stack. We first need to check whether this is possible. We can compute the value of g(x - 1, y - 1), the minimum sum of the weights of the ants that can form a stack of y - 1 ants where only the ants from the first ant to the (x - 1)-th ant are considered. If g(x - 1, y - 1) ≤ 6Wx, then it is possible to put the x-th ant on the bottom of the stack. The value of g(x, y) from this case becomes g(x - 1, y - 1) + Wx.
Between the two cases (or only one case if g(x - 1, y - 1) > 6Wx), we take the smaller value as the value of g(x, y).

The answer for the problem is the largest possible value S where g(N, S) < ∞. Since there are O(N) possible values for x, O(K) possible values for y, and O(1) iterations for each computation of g(x, y), this solution runs in O(NK) time.

Round 2 2018 - Code Jam 2018

Falling Balls (5pts, 12pts)

Test set 1
We can narrow down the scope of Test set 1 with only a couple of insights:

We cannot put any ramps in the leftmost and rightmost columns, so the balls dropped into those columns must fall straight down to the bottom. So, if either of those columns has no balls, the case is IMPOSSIBLE. (Later on in this analysis, it will become clear that this is the only way for a case to be impossible!)
Suppose C has its maximum value of 5. Since we know the leftmost and rightmost columns must have at least one ball each, the only variation among cases comes from where the other 3 balls end up. It turns out that there are only 35 ways to partition 3 balls among 5 columns, and some of these cases are mirror reflections of each other, so there are really only 19 distinct cases to consider. When C is smaller than 5, there are even fewer cases.
So, we can experiment offline, generating potential ramp arrangements via simulation or by hand, until we are confident in our solutions for all cases, and then submit; since Test set 1 is Visible, we have little to lose. However, we can avoid this extra work, as follows...

Test set 2
Consider the paths that each ball might take through the toy, and notice that whenever two balls' paths intersect, those balls must end up in the same final column. (Because we can never have a \ ramp immediately to the left of a / ramp, paths cannot cross over each other.)

So, for example, if your friend says there are exactly K balls in the leftmost column, they must have been the balls that were originally dropped into the K leftmost columns. Suppose that one of those balls had instead come from, e.g., the (K+1)-th column; then the path taken by that ball would have crossed all of the paths taken by the balls in the K leftmost columns, so all K+1 of the balls would have ended up in the leftmost column, which is a contradiction.

With those observations in mind, let us think of the i-th column as both a source of one ball and a potential target of Bi balls (if Bi > 0). We can scan our friend's list from left to right, and when we encounter a positive Bi value, we allocate the Bi leftmost source columns that we have not used so far to target column i. So, for example, if we have the data 3 2 0 0 0 0 2 1, we map the source column range [1, 3] to target column 1, the source column range [4, 5] to target column 2, the source column range [6, 7] to target column 7, and the source column range [8, 8] to target column 8.

Once we have this mapping, we can move on to designing the toy to ensure that every source column's ball will end up at the bottom of the appropriate target column. We will start with an empty top row, and add ramps and more rows as needed.

For each source column range, we check whether the left endpoint is to the left of the target. If it is not, we do nothing. Suppose, on the other hand, that it is L columns to the left of the target. Then we add L \ ramps, starting in the top row and the left endpoint's column, and proceeding diagonally, moving one cell to the right and one cell down each time. Notice that this line of ramps will catch all balls in that source column range that need to move to the right, so we do not need to explicitly worry about those balls. Then, we draw similar diagonal lines of / ramps running down and to the left from our right endpoints. Finally, if our bottom row is not empty, we add an empty bottom row, as the rules require.

Can we convince ourselves that this construction is optimal? We have already shown that the mapping of source columns to target columns is forced. Consider the largest absolute difference, in columns, between a target column and one of the endpoints of the range of source columns mapped to it. (In our example above, this value is |5 - 2| = 3.) The toy must have at least this many rows (in addition to the empty bottom row), because a ball can only interact with one ramp per row, and each ramp only moves a ball one cell in the desired direction. Observe that our construction uses exactly this many rows, plus the required empty bottom row. For our example, our method produces:

.././\..
././....
../.....
........

Notice that there is no danger of creating a \/ pattern within a range, since no target position could cause that.

Round 2 2018 - Code Jam 2018

Graceful Chainsaw Jugglers (7pts, 17pts)

We can identify each juggler by a pair of integers (b, r) where 0 ≤ b ≤ B and 0 ≤ r ≤ R. The pair (0, 0) does not identify a valid juggler, although we could assume that it does and, since we can always add (0, 0) to any valid configuration that does not contain it, just solve the modified problem and subtract 1 to obtain the final answer. We can then focus on finding the size of the largest subset of V = [0; B] × [0; R] such that the sum of the first component of each element is B, and the sum of the second component of each element is R. Let us call b,r-valid to a set of distinct pairs that fulfills the sum conditions for specific b and r.

Test set 1
For Test set 1, we can first notice that the size of V is small. Therefore, we can do a dynamic programming iterating through V and deciding, for each pair, whether to include it or not. As part of the state we have to also include how many blue and red jugglers we can still include. Therefore, we enumerate V in any order v1, v2, ..., v(B+1) × R+1, and then we compute a function f(i, b, r) defined as the size of the largest b,r-valid subset of {v1, v2, ..., vi}.

We can see that f has also a recursive definition as follows:

f(0, 0, 0) = 0.
f(i + 1, b, r) = max(f(i, b, r), 1 + f(i, b - left(vi), r - right(vi))) for all b, r, i.
The definition above is incomplete because it's not checking for indefinitions. A simple solution is to just define every undefined case as -infinity, or for this particular problem, - B - R.

Memoizing this recursive definition leads to an implementation that takes time proportional to the size of the domain of f, which is (B+1)2 × (R+1)2, which is likely fast enough to solve Test set 1. If your implementation and language of choice are both on the slow side, it's possible that you need some additional insight to make it through the time limit. One simple trick is that the function f actually solves all possible test cases, since the set V is fixed. So, you can reuse the memoization table for all test cases and save 99% of the work compared to solving each test case from scratch. You can even compute the entire table before submitting, and include it in your source code, although you should be wary of this strategy in general since it might cause your source file to exceed the 1 MB limit specified in our rules.

Test set 2
For Test set 2, we will definitely need the additional insight of not resetting the memoization table for each test case. However, the important additional insight is more specific to the problem.

First, let us define weak-b,r-valid sets as sets of distinct pairs of non-negative integers such that the sum of the values of the left sides of each pair is less than or equal to b, the sum of the values of the right sides of each pair is less than or equal to r. Of course, every b,r-valid set is also a weak-b,r-valid set, but the converse is not true.

One important property of weak-b,r-valid sets is that for fixed b and r, the size X of the largest weak-b,r-valid set is equal to the size Y of the largest b,r-valid set. We can prove that by noticing: (1) Y ≤ X because every b,r-valid set is a weak-b,r-valid set; and (2) X ≤ Y because if we let S be a weak-b,r-valid set of size X and then take an element (i,j) of S such that i is maximum, and j is maximum among those with maximum i, we can construct S' = S - {(i,j)} ∪ {(i + db, j + dr)} where db and dr are the difference between the sums of the left/right values of the pairs and b/r, respectively. By construction, S' is b,r-valid, and by the maximality of the choice of (i,j), (i + db, j + dr) is not in S - {(i,j)}, and thus, the sizes of S and S' are equal.

Let us now define minimal-weak-b,r-valid sets as weak-b,r-valid sets S such that for any (i,j) in S with i > 0, (i-1, j) is also in S, and analogously, for any (i,j) in S with j > 0, (i, j-1) is also in S. Similarly to the above, we can prove that the size of the largest minimal-weak-b,r-valid set for fixed b and r is the same as the size of the largest weak-b,r-valid sets by constructing a minimal-weak-b,r-valid sets of size X from a given weak-b,r-valid set S of size X. Let (i,j) in S be such that the minimality of the definition is violated (if such exists). Then, replace (i,j) in S for either (i-1,j) or (j-1,i) as appropriate to obtain another weak-b,r-valid set of the same size. This step strictly decreases the sum of the values of all pairs in S, thus, at some point, there are no such (i,j) to choose for and we successfully obtained a minimal-weak-b,r-valid set of size X.

The problem that remains is to find the size of the largest minimal-weak-b,r-valid set for given b and r. By the minimality condition, if (i,j) is in such largest set, so are (i-1, j), (i-2,j) (i,j-1), (i,j-2), (i-1, j-1), .... To formalize, (i,j) being present in a minimal-weak-b,r-valid set prescribes the presence of (i+1) × (j+1) pairs including itself. We can bound the sum of the left side values of all those pairs by (j+1) × (1 + 2 + ... + i) = (j+1) × i × (i+1) / 2, and analogously the sum of the right sides by (i+1) × j × (j+1) / 2. This means that instead of the set V from the solution above, we can use V', composed of the pairs in V that are under these bounds. This makes the size of V' be approximately O(B1/3 × R1/3) when B and R are close, which is a big reduction from the size of V which is O(B × R). The overall complexity of the resulting algorithm for all test cases is then O(B4/3 × R4/3), which is fast enough to pass Test set 2.

Round 2 2018 - Code Jam 2018

Costume Change (8pts, 19pts)

Test set 1
Trying every possible costume on every dancer will be too slow, even for this test set. Therefore, we need another solution.

To solve this test set, we can first observe that this problem is equivalent to the following: Find the largest subset of dancers such that no two dancers in the subset have the same costume type and share the same row or column.

If we find such a subset, we can change the costume types of the dancers not in the subset so that they follow the rules. We can iterate through the dancers in any order (e.g. row-major). If the current dancer is in the subset, we leave the costume as it is. Otherwise, we pick a new costume type for that dancer. Since a dancer can be in the same row or column as at most 2N - 2 other dancers, and since there are 2N costume types, it will always be possible to find a valid type to change to.

So, we can iterate through every subset of of dancers and check whether there are two dancers in the subset having the same costume type and sharing the same row or column. This solution runs in O(2N2 × N2) time.

Test set 2
To solve this test set, we need a much more efficient way of finding the subset mentioned above. We can solve the problem independently for each costume type. Let f(x) be the largest subset of dancers who are wearing a costume with type x, such that no two dancers in the subset share a row or column. Then the size of our desired subset will be Σ f(i) for -N ≤ i ≤ N, i ≠ 0.

How can we find f(x)? Let us create a bipartite graph as follows:

{Ai} is a set of vertices, where each vertex corresponds to a row.
{Bi} is another set of vertices, where each vertex corresponds to a column.
If and only if the dancer in the i-th row and the j-th column wears a costume of type x, we add an edge connecting Ai and Bj.
To find the largest valid subset of dancers, we need to find the maximum independent set of this graph, since we can only pick one dancer for each row and column. We can use a maximum cardinality bipartite matching algorithm to solve this problem.
A simple maximum cardinality bipartite matching algorithm runs in O(VE), where V is the number of vertices and E is the number of edges. Since V = O(N) and the sum of E among all costume types is N2, this solution runs in O(N3), which is fast enough to solve test set 2.

Round 2 2018 - Code Jam 2018

Gridception (10pts, 22pts)

Test set 1
This test set can be solved using complete search. We can try enumerating every possible connected pattern. For each possible connected pattern, we check whether that pattern exists in the grid which has been deepened twice. Among all connected patterns which exist in the grid which has been deepened twice, we take the one with the largest number of cells. The correctness of this algorithm will be proved later in this section.

Note that it is not sufficient to check the pattern in the grid which has been deepened only once. The first sample case in the problem description shows that there is a pattern which does not exist in the grid which has been deepened once, but the pattern exists in the grid which has been deepened more than once.

Why is it sufficient to check the pattern in the grid which has been deepened twice? We can first observe that a grid which has been deepened X times consists of blocks of cells. Each block is a square of cells of the same color with side length 2X. This does mean any pattern of size at most 3 × 4 can fit in the same block in the grid which has been deepened twice, whereas this might not be the case in the grid which has been deepened once.

Moreover, any pattern of size at most 3 × 4 overlaps with at least one and at most four blocks in the grid which has been deepened twice. This is also the case in the grid which has been deepened more than twice. Therefore, the set of patterns of size at most 3 × 4 which exists in the grid which has been deepened twice is equivalent to the set of patterns of size at most 3 × 4 which exists in the grid which has been deepened X times for any X > 2. Therefore, it is sufficient to check the pattern in the grid which has been deepened twice.

There are at most O(2R × C) patterns. Therefore, there are not more than O(2R × C) possible connected patterns. For each pattern, we can first check whether it is connected, and if it is, we can then check whether that pattern exists in the grid which has been deepened twice in O(R × C) time. Therefore, the total complexity of this solution is O(2R × C × R × C) which is fast enough to solve test set 1.

Test set 2
Since R and C can be up to 20, the exponential solution will not run in time. Therefore, we must not enumerate every possible connected pattern. To solve this test set, we can first observe that a block of cells in the grid which has been deepened at least a googol times will have a side length larger than the size of any possible pattern.

The observation in the previous paragraph ensures that every possible pattern can overlap with at most four blocks of cells in the deepened grid. This means that Codd's pattern needs to be divisible into four quadrants by a horizontal and a vertical line where each cell of the pattern in the same quadrant has the same color. Moreover, that particular combination of four colors has to exist in the original grid.

Therefore, we can consider every possible quadrant center and combination of colors (there are up to O(24 × R × C)). For each quadrant center and combination of colors, we want to get the largest connected component where each cell in this connected component has the same color as the color assigned to the quadrant it belongs to.

For each possible quadrant center and combination of colors, we need O(R × C) time to find the largest connected component. Therefore, this solution will run in O(24 × R2 × C2) time.

Round 3 2018 - Code Jam 2018

Field Trip (4pts, 10pts)

Test Set 1 (Visible)
Since the number of kids and the grid dimensions are small in Test set 1, it is sufficient to develop a correct greedy heuristic for where to move the teacher and then run the simulation to completion.

The first observation is that the problem can be solved independently in each dimension (x and y). At each time step, kid i always moves exactly 1 step closer to person i-1 in both x and y until both people occupy the same position in that dimension. Diagonal movement happens when the two people have different positions in both dimensions; orthogonal movement happens when the two people have the same position in one dimension but different positions in the other dimension; and no movement happens when the people share the same position in both dimensions.

The second observation is that when we take the one-dimensional perspective, the time is limited by only the most distant kids from the teacher in each dimension and direction. Let K be the number of the most distant kid in a certain dimension and direction; if multiple kids share that same position, pick the one with the lowest number. By assumption, all kids with a number lower than K are either closer to or on the opposite side of the teacher than kid K, including kid K-1. Therefore, kid K will move one step in the direction of the teacher. The effect inducts to kids with numbers higher than K; now that kid K has moved closer to the teacher, the first kid with number greater than K that has a distance to the teacher equal to kid K's original distance becomes the most distant kid and is guaranteed to step toward the teacher.

Hence, the correct heuristic is to minimize the distance between the teacher and the most distant kid, treating each dimension independently. To do this, calculate the most distant kid in each dimension, and move the teacher toward each of those kids in their respective dimension. If two kids are equally distant but in opposite directions, do not move the teacher.

An equivalent statement of the heuristic is that the teacher always moves toward the average of the minimum and maximum coordinates in each dimension, not moving if the teacher is already at that center point. If the maximum difference in a certain dimension is odd, the teacher can oscillate between the two central spaces in that dimension; this does not affect the correctness of the heuristic.

A few incorrect greedy heuristics include:

Move the teacher toward the single most distant kid. This is wrong because x and y need to be independent.
Move the teacher toward the average or median kid position. This is wrong because it biases the teacher toward a cluster of kids instead of dealing with outliers.
Move the teacher toward first kid that is not on their current space. This is wrong because the teacher could waste time making a cluster smaller before moving toward outliers.
Test Set 2 (Hidden)
Running the simulation to completion clearly does not work with the higher bounds.

By the greedy heuristic described above, at each time step, since the most distant student in each direction and dimension moves one step closer to the teacher, in each dimension, the minimum coordinate increases by 1 and the maximum coordinate decreases by 1. The problem in each dimension is solved once the minimum and maximum coordinates equal each other. Therefore, we can calculate the total time by taking the difference between the minimum and maximum coordinates in each direction and dividing by 2 (rounding up), choosing the larger of the value in the x dimension and the value in the y dimension. This requires looking at each kid once and therefore takes O(N) time.

Round 3 2018 - Code Jam 2018

Name-Preserving Network (9pts, 16pts)

How hard can this be?
At first glance, test set 1 may seem to be approachable even by hand, since only one design is required, and we have considerable flexibility in the number of computers to use. But it turns out to be difficult to satisfy all of the following at once:

Each computer must use exactly 4 links.
The network must be connected.
The computers must be uniquely distinguishable even after their IDs are permuted, and we must know how to distinguish them.
For example, we might try creating a ring of computers in which each one is linked to its two neighbors and neighbors' neighbors. This satisfies the 4-link and connected conditions. But this design is highly symmetric in a way that makes the computers less distinguishable. We can try introducing some irregularities (by swapping some edges around; more on this later) to disrupt the symmetry, but then it becomes even harder to eyeball the design — and how can we "find" our computer again after the judge has permuted their identities?

Harder than it looks
We need a way to give each computer a unique label. Labels need to be such that:

They are id-agnostic — that is, the label of a computer depends only on the link topology, and not on its own or other computers' ids.
There exist networks such that each computer has a unique label, and moreover, we can somewhat efficiently find those networks.
We can efficiently compute the labels of a given network.
If we have a labeling scheme that has all these properties, the solution writes itself: find a network of appropriate size, compute the labels on it and the labels on the shuffled network given by the judge, and match the computers by label.

There are many possible labeling schemes, and experimentation can go a long way to find them. We describe two below, one for the theoretically inclined, and one more manual. There also exist ways to explicitly construct networks that guarantee a particular labeling scheme. Both the constructions and labeling schemes are somewhat complicated and require casework, and only those constructions with labeling schemes that are quick to compute will work for this problem. Constructions are of course significantly easier for Test set 1, as we only need to build one network and we and we can use whichever network size (between 10 and 50) we want.

Let us start with the theoretical one, since it's shorter to write. We can model the network as an undirected graph in which each node has degree 4. If we take the adjacency matrix of the graph and compute its p-th power, we obtain a matrix in which cell (i,j) gives the number of paths of length p between nodes i and j. In the context of this problem, we do not particularly care about these paths per se, but we do care that the count of paths for a pair of nodes (i,j) depends on the entire topology of the network. So, if we are lucky enough, the set of values (i,1), (i,2), ... will be unique for each i. As it turns out, most graphs of a given size yield unique levels for p = 7, and quite possibly for other values as well. We generated graphs for every size between 10 and 50 and only needed to try a second time twice, and never a third time.

For a more manual approach, we can start by noticing that all computers look similar if we focus on them in isolation; they all use four outgoing links. The same is true of each of a computer's four neighbors. But what if we look at how the neighbors are connected to each other? Specifically, let us label a computer X via a multiset of four values Yi, one for each computer linked to X, where Yi is the number of other computers that are neighbors of both X and Yi. For example, suppose that computer 1 is directly linked to computers 2, 3, 4, and 5, and that there are additional direct links 2-3, 2-4, and 4-5 (and all other links from 2, 3, 4, and 5 are to other computers). Then our multiset for computer 1 would be {1, 1, 2, 2}.

This labeling will not get us far enough in distinguishing computers, since there are not many possible labels of the form described above. But we can take this approach deeper! Let us label each computer as described above, and call those the level 1 labels. Then, we give each computer a level 2 label that is defined as the multiset of the level 1 labels of its neighbors — that is, each level 2 label is a multiset of four multisets. We can even go on to add a level 3 label that is the multiset of the neighbors' level 2 labels, and so on. It turns out that level 4 labels give us enough power to tell computers apart, at least within the 10 to 50 computer range. Moreover, since labeling a computer only requires looking at a computer's four neighbors and a finite number of possible connections between them, one round of the labeling process takes O(computers) time, as does the full process of getting the level 4 labels. This is more efficient than other possible approaches that, e.g., label a computer by the smallest or largest cycle it is part of.

Armed with either labeling method, we just need a way to create network designs to try them on. Generating edges at random has a really low probability of making all degrees exactly 4, so we need something better. One simple possibility is to shuffle a list of exactly 4 times each id, and link the first two ids, the third and fourth, and so on. If this creates a self-loop or a repeated link, repeat until it doesn't. The probability of self-loops or repeated edges is small enough for this to finish quickly for every size.

Another way to generate graphs that is less chaotic is to start with the ring design mentioned above, and then repeatedly mutate it as follows:

Randomly select a computer A and one of its neighbors B.
Randomly select a neighbor C of A and a neighbor D of B, such that C is not already a neighbor of B, and D is not already a neighbor of A.
Delete the links A-C and B-D, and add the links A-D and B-C.
This mutation method guarantees that each computer always has four links, and that the network remains connected. (Since we had C-A-B-D before and we have C-B-A-D after, any two computers that were directly or indirectly connected before are still connected.)

Most networks generated in this way (after, e.g., 1000 mutations) turn out to have unique labels on the computers. When we get one that does not (either because the network has some inherent symmetry, or because our labeling method isn't powerful enough for the network), we can just throw it away and try again. Our strategy should take at most a few seconds to discover usable network designs for all possible test cases, and then the rest is just implementation of interactions with the judge.

Finally, notice that, if necessary, graphs for all necessary sizes could be pre-computed offline and hardcoded into the solution. As long as the labeling method is efficient, it is OK for the process of finding the graphs to be a bit slow, as long as it finishes within the contest time! Nevertheless, all ideas presented above are fast enough to require no pre-computation.

Round 3 2018 - Code Jam 2018

Raise the Roof (7pts, 19pts)

Test set 1
In Test set 1, the number of columns is small enough to allow a brute force solution. The problem guarantees that there is at least one correct order, so we can try all possible orders (there are at most 10! = 3628800) until we find a correct one. To check a given order, we check every prefix of length at least 3, build the roof implied by the last 3 columns of the prefix, and then check that all other columns are below it. Since the statement guarantees no 3 points are collinear, any subset of 3 points determines a single plane. There are several ways to check if a roof is valid; here is one that involves only integer arithmetic:

Let p1, p2, ..., pk be the k points in the prefix. Let q be the plane that contains pk-2, pk-1 and pk. For each i between 1 and k - 3, inclusive, we need to check whether pi is above q. Notice that we can subtract pk from all points and the answer to the question doesn't change. Thus, we further assume pk = (0, 0, 0). Let v be a normal to the plane q. Project pi onto v via dot product and check whether v and the projection are on the same side of q. If they are, then pi is above q, otherwise, it is not.

Since checking a prefix takes O(N) time, and there are O(N) prefixes to check for each of the N! orders, this yields an O(N! × N2) algorithm that may or may not be fast enough depending on your language and implementation. We can speed it up in several ways:

Generate permutations by adding columns one at a time, and check each prefix as soon as it is generated. This reduces the complexity to O(N! × N) because each prefix is not re-checked for each permutation that starts with it. Additionally, when we find a prefix that does not work, we do not waste time checking other permutations beginning with that prefix, which can further reduce the search space in practice.
Try a dynamic-programming-on-subsets approach to change the complexity to something like O(2N × N4), but whether or not that makes the solution faster in practice depends heavily on implementation details.
Test set 2
For Test set 2, any solution that is exponential or factorial in N is doomed from the start, so we need a different strategy. One possibility is to approach the problem in reverse: start with all columns, find a roof that works, remove a column, and repeat.

To formalize this a bit, we start by fixing the last two points q and p from the set of column tips. to be the last two columns, in that order. We discuss how to do this later. Then, we define s as the set of all column tips except p and q, and repeat the following until s is empty:

Find a point r such that the plane that contains p, q and r is above all current points.
Remove r from the set of current points and set p = q and q = r.
The final output is the reverse of the order the points were taken from s, with p and q at the end.

To do the first step above (find r), we could just try every possible point in s and use the process described above to check the condition. This would yield an O(N2) algorithm for the step. To improve upon that, picture the plane containing the last roof, but now being supported by the remaining 2 fixed columns. For the very first iteration, the "last roof" needs to exist for this algorithm to be valid, and how to define it depends on how we choose p and q. For the choice of p and q we explain below, the existence of a possible "last roof" is straightforward. We can rotate the plane around the axis defined by the segment connecting the tips of those two columns, and we want to find one of the columns that it touches first. (There can be up to two such columns, since we can rotate the plane in either direction.) In other words, we should choose a column tip r that minimizes the angle of rotation. That is equivalent to choosing the r that maximizes the angle between a normal to the plane that contains p, q and r and the normal to the "last roof". This yields a linear time algorithm for this step.

To choose the initial p and q, we can try every possibility, which adds an N2 factor to the overall time complexity of the algorithm, or do something better. The last column with tip p can always be chosen as one of the tallest columns. Imagine a plane parallel to z = 0 that passes through p. The plane clearly passes above or touches all other columns, but it's not "pierced" by any column, so we can move it around to create a suitable roof. Then, we can choose q as a column such that the segment pq has minimal angle with the plane. This is similar to what we did above, except we are trying rotating the plane around a single point p until it touches another column q. This can be done by using the same method of comparing angles against a plane we described in the previous paragraph. This involves a one-time linear cost to find q, but then we only do the iteration for a single pair. Notice that, for this choice of p and q, there is a clear choice for the "last roof", which is the imaginary plane that we rotated to find q.

The optimal implementation of the ideas above yields a quadratic algorithm, which is the intended solution. Depending on which of the optimizations you skip, you can end up with a complexity between O(N2) and O(N5), which, together with the constant factors of your implementation and choice of language, determines whether your solution passes both sets, just Test set 1, or neither set.

Round 3 2018 - Code Jam 2018

Fence Construction (12pts, 23pts)

For both solutions discussed below, we model the situation as a planar graph G with endpoints as nodes and fences as edges. Getting a representation of this graph as an adjacency list takes quasilinear time on the input (because we need to identify endpoints, which may add a logarithmic factor or an expected constant factor depending on whether we use trees, hashing or sorting to do it).

Test set 1
Let us assume for a brief moment that there is no prescribed partial ordering (for instance, when K = 1). There are many greedy algorithms that would retrieve a correct ordering of the fences. You can try working the fences from the outside in or from the inside out, cleverly using convex-hull-type algorithms. Each way has its own pros and cons, but many of them also have the property that the reverse of the ordering they produce is also a valid ordering. This is enough to deal with K up to 2: retrieve a greedy order. If the partial order is obeyed, return that; otherwise, return the reverse. One of the two is guaranteed to work. Another way that works for some greedy alternatives is that they may allow us to fix a single fence as the first one to be built and then work from there. If you fix fence 1 to be the first one built, the partial order is guaranteed to be respected.

We will describe a different option than the one discussed above, because it is simpler to describe, prove and implement. Build any spanning tree T of G that contains fence 1. Build the fences in T first in any order that starts with fence 1. This guarantees that we respect the partial order. Since we are building fences that close no cycle, there are no restrictions to moving the printer so far: any point not on a fence is reachable.

For each the remaining fences f, consider the cycle that is formed when f is added to T. That cycle represents a simple polygon. If we simply build these remaining fences in increasing order of area of that polygon (breaking ties arbitrarily) we obtain a valid order. The reason is that we can always keep the printer on the "outside" of all closed cycles. Since the next cycle to be closed at any time has area no less than any already closed cycles, it cannot be contained in any of them, which means the fence that is closing it is also on the "outside", and so the printer can get arbitrarily close to the fence's position and find a point from where the fence can be built.

Notice that this algorithm can be implemented in quadratic time. Building a spanning tree takes linear time. For each fence outside of the spanning tree, we take linear time to get the area of the polygon, which leads to quadratic overall. Then, we only need to sort the remaining fences, which can be done in faster-than-quadratic time.

Test set 2
For Test set 2, a bit of theoretical knowledge is required. Build the dual graph of G and call it H. Moreover, let G(S) and H(S) be the graph corresponding to a given set of fences S and its dual, respectively. As usual, when we name faces or nodes of H, we include the face that is on the outside (i.e., the only one with infinite area).

When we finish our work, the printer will be left in one of the faces of G, that is, one of the nodes of H, and the last fence we built will be one of the edges that are adjacent to that face. If we step through the build order backwards, right before building the last fence f, the printer is on a face of G(F - {f}), where F is the set of all the fences. For any set S and any fence f, there is a strong relationship between G(S) and G(S - {f}), and correspondingly, H(S) and H(S - {f}). G(S - {f}) is the result of removing f, and possibly its endpoints if they become isolated, from G(S). Correspondingly, H(S - {f}) is either equal to H(S) or the result of merging two nodes of H(S) into one: f may have a single adjacent face in G(S) or two, which leads to nothing changing or two nodes merging in H(S), respectively.

Focusing back on G(F - {f}), we know that the next-to-last fence g to be built has to be one of the fences that is adjacent to a face adjacent to f. The one right before that is adjacent to a face adjacent to either f or g, and so on. That means any ordering of the fences is a possible search on the graph X of fences as nodes where two fences are adjacent in X if they are adjacent to the same face of G. A search on a graph is an ordering of the nodes such that any prefix of the ordering is connected. Breadth-first-searches and depth-first-searches are examples of searches, but not the only ones.

Now that we have identified valid orders with searches on a graph, we can greedily build one that respects the partial order. From this point on, we will build the reverse of the output. Let us fix the first fence of our order f. From here, we can keep track of the reached fences. Any reached fence that is not in the partial order can be added immediately to our order, as it only adds more reachable fences and it cannot violate the partial order. Fences that are in the partial order, however, need to be added respecting that, which we can also do greedily. If at some point all the reachable fences are in the partial order but neither is the next one, we have failed and we need to choose a different f.

The explanations above lead to a quadratic algorithm: building G and then H takes quasilinear time. Building X from H takes quadratic time (since it may have a quadratic number of edges). The greedy algorithm takes constant time for each fence it adds to the order, and we may need to try a linear number of starting fences f, so that time is quadratic overall.

The algorithm can actually be improved to quasilinear overall: there is no need to build X explicitly. We can use X implicitly from H and only inspect adjacencies to fences that haven't been reached. That removes the quadratic cost of explicitly building X. As for trying every f, we can prove that fixing f to be the first fence in the partial order (that is, fence K, since we are building a reverse order) is always optimal. For a valid search of X that respects the partial order, reversing the prefix that ends on fence K is also a valid search that respects the partial order, and has fence K first.

World Finals 2018 - Code Jam 2018

Jurisdiction Restrictions (5pts, 23pts)

Test set 1
We begin modeling the problem as a flow network. We construct a directed graph G where the set of nodes is {source, sink} ∪ S ∪ B where S is the set of blocks that contain a station and B is the set of blocks that do not contain a station and are within reach of some station. The edges of the graph are {source → s : s ∈ S} ∪ {s → b : s ∈ S, b ∈ B} ∪ {b → sink : b ∈ B}. Notice that each path from source to sink links a specific station with a specific block. If we refine G by assign capacity |B| to each edge source → s, and capacity 1 to each other edge, a valid flow of the network is a union of paths from source to sink, and due to the capacities, each station will belong to at most one of those paths. Moreover, a maximum flow will cover all stations, so there is a bijection between maximum flows and station assignments. In this way, we can reframe the problem as minimizing the difference between maximum and minimum flows going through edges that come out of the source in a maximum flow of G.

If we want to set an upper bound U for the flow going through edges coming out of the source, we can adjust the capacities of those edges to U. If we want to set a lower bound L on those quantities, we can add a second copy of each of those edges with capacity L and have a cost of 0 in the new edges and 1 in the max-capacity version (plus, cost 0 for all other types of edges). Using min-cost max-flow instead of max flow will give preference to flows that use at least L capacity through each station, and if the retrieved flow doesn't use the full capacity, it means that one doesn't exist.

With the above, we can just try every possibility for L and U, discarding the combinations that don't achieve a max flow of |B| that saturates all the cost 0 edges coming out of the source. The non-discarded combination with minimum U-L is the answer. Both U and L are bounded by |B|, which is bounded by R × C. The size of G is O(S × R × C), and min-cost max-flow on a graph with only 0 and 1 costs takes only linear time per augmenting path, so quadratic time overall. This means the described algorithm takes time O(S2 × R4 × C4). This can be too slow even for test set 1, but there are some optimizations that will shed some of that time, and you only need one of them to pass test set 1.

Instead of trying all combinations of L and U, you can try only the ones that have a chance to improve: if a combination (L, U) doesn't work, then we know (L+1,U), (L+2,U), etc will also not work, so we can move on to (L, U+1). If a combination (L, U) works, we know that (L, U+1), (L, U+2), etc will also work but will yield a larger difference, so we can skip them and move directly to (L+1, U). This requires us to test a linear (instead of quadratic) number of combinations of L and U.
An even simpler version of the optimization above is to try all possibilities for only one of L or U, and then use binary search to find the optimal choice for the other. This doesn't take the number of combinations down to linear in |B|, but it does make it quasilinear.
A flow for the combination (L, U) is also a valid flow for the combination (L, U+1), so instead of starting the flow from scratch, we can use it, dramatically reducing the number of augmenting paths we need to find overall.
Test set 2
For test set 2, the values of R and C are too high to appear linearly in the running time of the algorithm. That means we have to improve upon the solution above in at least two fronts: the size of G, which is linear on R and C and ultimately impacts any algorithm we want to run on G, and the number of combinations of L and U we try, which is also linear or more.

We start by defining G for test set 2 a little differently, using a technique commonly known as coordinate compression. We define B' as a set of sets of blocks that form a partition of B above. The way to partition is by taking the full grid and cutting through up to 2S horizontal and 2S vertical lines at the boundaries of the jurisdiction of each station. The resulting rectangles have the property that the set of stations that can reach any block of the rectangle is the same. Rectangles only represent the blocks within them that do not contain a station. We can now define G as having nodes {source, sink} ∪ S ∪ B' and edges {source → s : s ∈ S} ∪ {s → b' : s ∈ S, b' ∈ B'} ∪ {b' → sink : b' ∈ B'}. Note that each node in B' now represents a set of blocks, so we also have to fiddle with capacities. Edges source → s have capacity |B| (effectively infinity) as before, and so do edges s → b'. Edges b' → sink, however, have capacity |b'|. In this way, each node b' is the union of many nodes b from the previous version of G, so its outgoing edge has the sum of the capacities of the outgoing edges from those nodes b. The size of this G is O(S3) with O(S2) nodes.

Now, in G, each path source → s → b' → sink that carries flow X represents that X blocks from set b' are assigned to station s. Notice that there is no longer a bijection between flows and station assignments, but there is a bijection if we consider permutations of blocks within a rectangle to be equivalent. Since all those blocks are equivalent for the purpose of this problem, this bijection is enough. As before, we now have to find a maximum flow in G such that the difference between the maximum and minimum flow in edges going out of the sink is minimized.

Since we cannot try all possibilities for L and U, we will use a little theory. Let GC be a copy of G, but with the capacities of all edges source → s changed to C. G = G|B|. Notice that a valid flow in GC is also a valid flow in any GC' with C' ≥ C.

Now, let U be minimum such that GU allows a flow of total size |B|. Let L be maximum such that there exists a maximum flow of size |S| × L in GL, that is, there is a flow that saturates all edges of type source → s. Any maximum flow in G has at least one edge of type source → sink carrying flow at least U (otherwise, such maximum flow is a valid flow in GU-1, contradicting the definition of U). Also, any maximum flow in G has at least one edge of type source → sink carrying at most L (otherwise, we can subtract flow from any path sink to source that carries more than L+1 to obtain a flow that carries exactly L+1 through all nodes, and thus it would be a valid flow in GL+1 of size |S| × (L+1), contradicting the definition of L). Finally, if we start with the flow in GL that justifies the definition of L and use it as a valid flow in GU, we know that it can be extended to a maximum flow F in GU via augmenting paths. Since the augmenting paths don't contain cycles, they don't decrease flow in edges of type source → s. Therefore, F is a maximum flow in GU such that the difference between the maximum and minimum flow going through edges coming out of the source is exactly U - L. And by definition, a maximum flow of GU is a maximum flow of G. The previous observations show that there is no flow in G with such a difference less than U - L, so U - L is the answer to the problem. We can binary search for L and U independently using their original definitions, which yields an algorithm that takes time O(S5 × log (R × C)).

World Finals 2018 - Code Jam 2018

Two-Tiling (28pts)

The restriction on tile size limits the problem to just 35 distinct tiles, and therefore "only" 35 * 34 / 2 = 595 possible cases. We might consider whittling away at this number via individual insights — e.g., any case with a 9-cell tile and an 8-cell tile must be impossible, since the least common multiple of 9 and 8 is 72, which is larger than the allowed board size. We may even be tempted to cut out some paper tiles and do some solving by hand, but if so, we will quickly discover that this is harder than it seems, and it is particularly hard to be sure that a case is impossible. We need to either write a Master's thesis on properties of n-ominoes, or take an exhaustive approach.

We cannot simply find all of the ways to tile part or all of the board with each tile, and then intersect those two sets; there could be too many possible tilings to generate (e.g., 264 for the 1x1 tile). Even if we could come up with special-case solutions to get around this issue for enough of the smaller tiles, we would still waste a lot of time enumerating sets that are obviously incompatible with the other tile.

Instead, we can use a backtracking strategy that takes advantage of our constraints, using the placement one type of tile to guide how we place the other type. The strategy does not have to be lightning-fast; we know that every case will appear in the test set, so we can enumerate and solve them all offline and then submit once we have an answer for each one. However, it must be fast enough to generate all 595 answers before the World Finals end!

Let us number the cells of the 8x8 board in row-major order, which is the same order in which our backtracking search will explore. We can translate each tile (and all of its rotations and reflections) around the 8x8 board to exhaustively find all possible placements. Then, we can assign each of those placements to the lowest-numbered cell that the placement uses. For example, one placement of the 2x2 square tile can cover cells 1, 2, 9, and 10 of the board, and the lowest-numbered of those cells is 1, so we assign that placement to cell 1. Now we have a helpful map from each board cell to the tile placements that fit there. Notice that it would be redundant to list our example 2x2 square tile placement under cell 10. For example, by the time our search reaches cell 10, it will have already considered that same placement at cell 1, so there would be no need to consider it again.

Then, starting in the upper left cell and going in row-major order, we try to build a set of cells that can be tiled by both tiles. For each cell c, we decide whether to fill it. If it is already covered by a previous tile placement, we must fill it; if we decide to fill it, and it is not already covered by one or both types of tiles, then we have our search explore all of the tile positions assigned to that cell in our map above.

Since the board is 8x8, we can use the bits of a 64-bit data type to represent each tile's coverage of the board, and we can use bitmasking to quickly see whether desired cells are occupied, or generate one state from an existing state. It is also very quick to check whether two states are equal; if this is ever the case, we stop immediately, step back through our backtracking to figure out exactly how to divide that set up into tiles of each type, and output that tiling. If the backtracking process finishes without finding such a set, then the case is impossible.

This solution turns out to be fast in practice, running in seconds. Intuitively, small tiles are more "flexible" and it is easier to find solutions quickly for cases involving them, whereas large tiles fill up the board's cells rapidly and do not open up as many choices. That is, the recursion tree for the backtracking is wide but short in the case of small tiles, and tall but narrow in the case of large tiles.

This approach does not necessarily find minimal sets of cells, but it can generate some artistically pleasing shapes and tilings, and/or puzzles that really would be fun to solve! Here are a couple examples generated by one of our solutions:

These angular tiles create a solution with a surprising mix of dense and holey areas!

aaa.bbb. AAA.BBB.
a..cbcb. A..CCCB.
aaacbcb. ADDDECB.
...ccc.. ...DEC..
..dddeee ..FDEEEG
..d....e ..F....G
..dddeee ..FFFGGG
........ ........
We hope you "loved" this problem! Notice that a test case is not necessarily any easier when the two tiles differ in only a very small way.

aa...bb. AA...BB.
aaa..bbb AAA..CBB
acacdbdb DDACCCBB
.cccddd. .DDCCEE.
.cceedd. .DDFFEE.
..eefff. ..FFEEG.
..eeeff. ..FFGGG.
....ff.. ....GG..

World Finals 2018 - Code Jam 2018

Go, Gophers! (10pts, 38pts)

Let M = 25 be the maximum possible number of gophers.

Test set 1
Our approach to test set 1 will be as follows: first, we will find the minimum taste level among all gophers, and then we will use it to determine the total number of gophers N.

To find the minimum taste level, we can simply binary search using the question "is the minimum taste level strictly greater than X?" This is equivalent to asking: "Would a snack of quality X go uneaten by all gophers?" To answer such a question, we can offer 2M-1 snacks of quality X consecutively, which guarantees that each gopher is exposed to at least one of them. If none of those snacks are eaten, every gopher's taste level must be greater than X, but if at least one snack is eaten, then there is at least one gopher with taste level at or below X. We can even stop offering snacks as soon as one gets eaten, if we want to save some snacks. This requires ceil(log 106) × 49 = 980 snacks at the most.

Once we know that there is a gopher g with level exactly L, and that L is the minimum taste level, we can use our snacks to answer a query "is it g's turn?" by offering a snack of quality L, because g is the only gopher who would eat it. If we offer that many times in a row and calculate the fraction of eaten snacks, that should approximate 1/number of gophers fairly well. At this point, we might as well use our enormous number of leftover snacks to estimate, and then just answer with the N such that the result is closest to 1/N. It turns out that M3 tries of this experiment guarantee that the answer is correct, even in the worst case. The proof is included below as part of the explanation of the solution for test set 2.

Test set 2
The solution from test set 1 does not extend naturally to test set 2. In particular, it no longer suffices to find out what fraction of gophers have the minimum (or maximum) taste level, because there could be multiple gophers with that taste level; if we find a fraction of 1/K, the number of gophers could be any multiple of K. So, we need to investigate other levels. Investigating taste levels other than an extreme one brings about the problem of the result being impacted by gophers with taste levels other than the one we are interested in. We can still use the idea of answering general queries by repeating snacks of the same quality, but they are significantly more complicated than a simple disjunction of the snack outcomes.

The first query Q we describe is somewhat natural: What is the exact fraction of gophers with taste level ≥ X?

Notice that this query alone is enough to solve the problem: we can do a binary search of sorts: given a range [A, B] of at least two levels, and known fractions of gophers of those taste levels X and Y, respectively, we can calculate the fraction of gophers Z for taste level C = (A + B) / 2. We recurse over the range [A,C] if X ≠ Z and over the range [C,B] if Y ≠ Z, because the fractions are different if and only if there is a gopher with a taste level in the range producing the change. This algorithm allows to identify all levels at which at least one gopher exists. For each of them we calculate the fraction of gophers at level L using our query and then subtracting the fraction of gophers at each other level < L. Finally, we can take the least common multiple (LCM) of the denominators of all those fractions to find the answer.

This algorithm requires about N × ceil(log 106 - N) queries like Q to be made. Unfortunately, we see below that this would be too many.

One way W1 of solving Q is to try enough snacks and then round to the nearest feasible fraction. If we use enough snacks, the rounding will give the exact answer. Note that if we give X consecutive snacks, we are guaranteed to have the right answer in all but a prefix and a suffix of the tries, both of which have length up to M-1. This bounds the error by 2M-2. Moreover, since our experiment has a binary answer, the farthest we can be from the true number of positive (or negative) answers is M-1, in the case when there are exactly ceil(M/2) answers of one type and floor(M/2) of the other, and we happen to hit both a prefix and suffix of size floor(M/2) giving the same answer (this is for odd M, the worst case would be M for an even M).

Additionally, since we only consider fractions with denominators up to M, the distance between two different results is bounded by 1 / (M × (M-1)). This means that if our experiment has an error of less than half of that, rounding is guaranteed to produce the right answer. Putting the total error of M together, this implies that we need M3 snacks to get a perfect answer for Q. A total number of snacks of M × ceil(log 106 - M) × M3 exceeds the allotted total by a large margin.

A different way W2 to answer Q is to always use R consecutive snacks, where R is the LCM of all the possible results. That means the error is always zero, since we give each gopher exactly the same number of snacks. Unfortunately, LCM(2,3,...,25) is also much too large, so this doesn't work either.

Another strategy to reduce the number of queries is to notice that we only need an exact number for the final levels, right before doing the LCM to get the result. For all intermediate values of our "multi-leaf binary search", we only need to decide whether there is some gopher in a range or not. One way W3 to do it would be to, instead of using exact fractions X, Y and Z for A, B and C above, have approximations X', Y' and Z' that are good enough that we can decide whether the real numbers are equal. Using 2M2 snacks guarantees that we will encounter each gopher at least 2M times, which means that if X and Z are different, their approximations of the number of total positives will differ by at least 2M. Since we showed that the total error of both approximations is at most M-1, the error of the difference is at most 2M-1, which means comparing that difference with 2M is enough to determine whether the real fractions are equal or not. This significantly reduces the total required number of snacks, since we only need to use M × ceil(log 106 - M) × M2 for the multi-leaf binary search, and then M × M3 or M × R to get the final precise answers. However, this is still too many.

The final algorithm requires us to use all 3 of the variants above: we start the multi-leaf binary search using W3. Each time we find a level, we use either W1 or W2, whichever is cheapest, to get the real fraction of the found level. If we recurse on the larger intervals first, we'll find the levels from highest to lowest, so we can do the subtraction. Once we have the real fraction, we potentially restrict the number of possible results N to the multiples of the denominator. This reduces R. Eventually, R might be small enough that we can use W2 for the binary search instead of W3 as well. Notice that each time we use W2, since we started with other methods, we need an additional R to "align" ourselves, depending on how many snacks we have used so far. However, since we eventually use R for everything, the additional cost of these alignments is extremely small.

We leave the precise analysis of total number of snacks needed to the reader, but it's possible, with some careful bounding, to prove that it never exceeds S, and we couldn't find a case that gets above ~85% of S. The reason is that if the denominator of a fraction is larger than M/2, then there's only one possible result left and we are done. If it's less than M/2 but somewhat large, the number of possibilities for the result is reduced significantly, and its LCM is reduced even more because the GCD of those possibilities is more than 1. If the denominator is small, it means that the found level has multiple gophers, which reduces the total cost of the multi-leaf binary search.

World Finals 2018 - Code Jam 2018

Swordmaster (10pts, 38pts)

The only way we can fail to become the Swordmaster is to get stuck in a situation where for all remaining duelists (duelists that we have not beaten yet) we will not defeat them without learning additional skills. At any point in time, the remaining duelists can be divided into four groups:

D1: Duelists we certainly can defeat.
D2: Duelists that have an attack we cannot defend against and a defense for every one of our attacks.
D3: Duelists that do not have a defense for one of our attacks, but have an attack we cannot defend against.
D4: Duelists that do not have an attack we cannot defend against, but have a defense for every one of our attacks.
If there is any remaining duelist in D1, we should just defeat them. If there is someone in D3 and there is someone in D4, then we can make some progress (i.e. reducing the number of remaining duelists, or moving a remaining duelist to D1) by doing the following:

We choose any duelist in D3 (call them A) and any duelist in D4 (call them B).
We ask A for a duel. Since A is in D3, A has an attack that we don't defend against. Either A will use an attack that we don't defend against (thus we will learn the attack), or we can defend and win the duel (thus learning all of A's attacks). At the end of the duel, we will learn an attack which we can't defend against. Let us call this attack is a.
If B cannot defend against a, then B has moved to D1 and we have made some progress. Otherwise, we ask B for a duel and use attack a. If B cannot defend, then we will win the duel and make some progress. Otherwise, we will learn defense a.
Now that we have learned defense a, A might have moved to D1, or might still be in D3 (by having another attack that we can't defend against). If A moved to D1, then we have made progress. Otherwise, we repeat from step 2 until we make some progress. We cannot loop indefinitely because we are learning additional moves each time through the loop, and there are only a finite number of them.
Therefore, the only possibilities in which we might fail to become the Swordmaster are when either every remaining duelist is in D2 or D3, or every remaining duelist is in D2 or D4.

If every remaining duelist is in D2 or D3, then every remaining duelist has an attack we can't defend against. From this point, every remaining duelist can just attack us with an attack we can't defend against and choose to not defend against, which means that we will not learn any new defenses. Therefore, we can't defeat any more duelists and are certain to fail to become the Swordmaster. Notice that we can actually detect this situation up front — if there exists a group of duelists G1 (not containing us), such that every duelist in G1 has at least one attack against which nobody outside G1 can defend, then we are doomed. The strategy for the duelists is that everybody in G1 can just successfully attack us and choose to not defend (thus we are not learning any new defense known by duelists in G1).

We can check the existence of a G1 by starting with G1', the set of all duelists except us. We can iteratively remove from G1' any duelist we can defend against regardless of their attack, and get all their defenses. If this process ends with a non-empty G1', then this set is a valid G1; otherwise, no G1 can exist. This can be done in O(N × P) time. This complexity is also bounded by the sum of the number of attacks and defenses known to all duelists, which is linear in the size of the input.

Test set 1
For test set 1, the situation when every remaining duelist is in D2 or D4 is similar. This means every remaining duelist can defend against all of our attacks. Therefore, there exists a group of duelists G2 (not containing us), such that every duelist in G2 has a defense for every attack available outside G2. Therefore, we are also doomed, since everybody in G2 can just defend against our attack and always use attack 1 (thus we are not learning any new attack known by duelists in G2). We can check the existence of such G2 up front with an algorithm analogous to the one presented above to check for G1.

Test set 2
For test set 2, the situation when every remaining duelist is in D2 or D4 is complex. In test set 1, our opponents could choose to attack us with attack 1 and be sure they would not teach us anything new. However, in test set 2, the attack an opponent uses potentially causes some duelist to move from D4 to D1, or from D2 to D3. Therefore, the fact that every remaining duelist is in D2 or D4 is not a sufficient condition for us to fail to become the Swordmaster.

Now imagine we are in a situation where we are actually doomed — we are in a position where, finally, every remaining duelist is in D2 or D4 and we will never defeat any duelist, nor will we learn a new attack or defense. Such a situation has to be reached at some point since there is only a finite number of steps of learning (attack or defense) to be made. In this situation, we could fight a duel with every remaining duelist and learn the attacks they use. By definition, we already knew all these attacks, and so, also by definition, all our opponents can defend against all these attacks. So, for us to be doomed through this path, there must exist a group G2 of duelists and an assignment of an attack for each of them a : G2 -> Attacks, such that every duelist d in G2 knows attack a(d) and knows how to defend against every attack known by someone outside G2 and every attack a(d') for d' in G2.

This condition is harder to check. Let us assume there exists such G2. Consider any two duelists X and Y. If X has an attack that Y cannot defend against, then Y being in G2 implies X must be in G2 as well. This defines a directed graph on the duelists, with the edge going from Y to X.

We find the strongly connected components (SCCs) of this graph. If any duelist of a SCC is in G2, then the whole SCC is in G2. Therefore, G2 will be an upper subset (a subset with no edges going out of the subset) of the directed acyclic graph (DAG) of the strongly connected components of the graph. Notice that if we can choose some upper subset of duelists to be a valid set G2, then any subset of G2 which is also an upper subset will also be a valid G2 with the same attack selections. Therefore, it is enough to check only leaf SCCs in the DAG. If any valid G2 exists, then a G2 which is a leaf in the DAG also exists.

Therefore, for each leaf SCC in the DAG, we can check whether each duelist in the SCC can choose an attack that every duelist in the SCC can defend against. To do this, we can first find the set D which is an intersection of the defenses known by every duelist in the SCC, and see whether every duelist in the SCC has an attack in D. We also check that every attack we already know is in D.

To construct the graph in O(N × P) time, we can introduce nodes for attacks as well. We add an edge from a duelist to an attack if the duelist cannot defend against the attack, and an edge from an attack to a duelist if the duelist knows that attack. The overall solution runs in O(N × P) time.

overtroll was the only contestant to solve this problem during the contest.

World Finals 2018 - Code Jam 2018

The Cartesian Job (10pts, 38pts)

The first thing to notice about the problem is that all lasers rotate at the same speed. That means that after each second, regardless of direction, the lasers will be back at the configuration given in the input, and each subsequent second will just be a copy of the previous one. Then, we only need to check what happens within a second. In what follows, we assume the configuration given in the input happens at time = 0 seconds and we only care about what happens for times in the range 0 to 1 seconds.

Test set 1
In Test set 1 there are such a low number of lasers that we can try every possibly combination of directions and add 2-N to the result for each one that leaves the segment uncovered some of the time.

For fixed directions of rotation, we can check whether the segment is covered by first mapping each laser to the interval of time during which it will cover the segment and then seeing if the union of all those intervals covers the full second. We refer to intervals in the modulo 1-second ring; that is, an interval can start at t0 and end at t1 < t0, covering the time from t0 to 1 second and from 0 seconds to t1. There are several ways to check if the union of intervals, even intervals in a ring, cover the full second, but a simple one is to split the intervals that wrap around into two non-wrapping-around ones, sort those intervals by start time, let t = 0, and do for each interval (t0, t1), if t0 > t, then there's a hole; else, t = max(t, t1). If we reach the end, there's a hole if and only if t < 1.

To obtain the covering interval for a given laser with endpoint p and second point q we check the angle qp(0,0) and qp(0,1000), and then divide by 2π. Notice that if we do this with floating point arithmetic, it is highly likely that we will have precision problems. We can do it with integer only arithmetic by representing the times symbolically as the angles represented by the original vectors. To simplify the presentation, we assume we computed actual times, but all the operations needed for the rest of the solution can be implemented for an indirect representation of those times intervals.

Since we need to check every combination of clockwise and counterclockwise rotations, and evaluating a particular combination requires looping over each laser, this solution takes O(N × 2N) time, which is sufficient for Test set 1.

Test set 2
We start by observing that the two time intervals corresponding to each direction of rotation of a given laser are symmetrical; that is, they are of the form (t0, t1) and (1-t1, 1-t0). Notice that the symmetry is both to the 1/2 second point and to the extreme 0 seconds / 1 second, because we are in a ring. Adding the fact that intervals are less than 1/2 long, we can notice that the pair of intervals coming from a given laser can be categorized into one of three types:

Two non-overlapping intervals, one within (0, 1/2) and the other within (1/2, 1). Example: [0.2, 0.3] and [0.7, 0.8].
Two intervals that overlap around 1/2. Example: [0.3, 0.6] and [0.4, 0.7].
Two intervals that overlap around 0 a.k.a. 1. Example: [0.8, 0.1] and [0.9, 0.2].
For the last two types, the overlapped area is an interval of time during which the segment is guaranteed to be guarded. Therefore, we can remove the overlapped part from further consideration and assume that the two intervals are just their symmetric difference. In the first example above, we'd remove [0.4, 0.6] from consideration and keep the pair of intervals [0.3, 0.4] and [0.6, 0.7]. After we do this with all intervals, the part that remains to be considered is some subinterval of (0, 1/2) and some subinterval of (1/2, 1), and each pair of intervals is exactly two symmetrical intervals, one on each side.

At this point the problem we need to solve is, given two symmetrical intervals u1 within (0, 1/2) and u2 within (1/2, 1), and a set of pairs of intervals (a, b), (1 - b, 1 - a), what is the probability that the union of picking one from every pair uniformly at random does not cover both u1 and u2? Notice that because of symmetry, one particular split (a collection of one interval from each pair) covers u1 if and only if the opposite split covers u2. So, an alternate way to frame the problem is: given the list of intervals from each pair that are inside u1, what is the probability at least one part of a random split of them doesn't cover u1? This problem we can solve with a dynamic programming approach over the list of intervals.

The dynamic programming algorithm is, in a way, a simulation of the algorithm we presented above to check if the union of the intervals covers the universe, over all combinations at once. We iterate the intervals in non-decreasing order of left end. We also keep a state of which prefix of u1 each side of the split has already covered. Formally, if u1 = (v, w) we calculate a function f(i, x, y) = probability of a split of intervals i, i+1, ..., N not covering both the remainder of the first side (x, w) and the remainder of the second side (y, w). Equivalently, this is the probability of the cover not being full given that intervals 1, 2,..., i are split in a way that the union of the intervals from one side of the split is (v, x) and the union of the intervals from the other side is (v, y). The base case is if min(x, y) ≥ w, then f(i, x, y) = 0, or if i > N or min(x, y) > the left end of the i-th interval, then f(i, x, y) = 1. For all other cases, we can calculate f(i, x, y) = (f(i+1, max(x, b), y) + f(i+1, x, max(y, b))) / 2, where (a, b) is the i-th interval in the order. Finally, the answer to the problem is f(0, v, v).

Noticing the values x and y for which we need to calculate f are always either s or the right end of an interval bounds the size of the part of f's domain that we need, and thus the running time of the algorithm, by O(N3). We can further notice that max(x, y) is always equal to the maximum between s and the maximum right end of intervals 0, 1, ..., i-1. This reduces the domain size and running time to O(N2). One further optimization needed is to notice that if min(x, y) is not one of the largest K right ends of intervals 0, 1, ..., i-1, then the result of f(i, x, y) is multiplied by 2-K or less to calculate the final answer. For values as small as 50 for K, that means the impact in the answer of the value of f is negligible in those cases and we can just approximate it as 0, making the size of the domain of f we have to recursively calculate only O(K × N).

Implementing the dynamic programming algorithm as explained above can be tricky, especially if you want to memoize using integer indices over the intervals and largest K last values, as opposed to floating point numbers. However, doing a forward-looking iterative algorithm can be a lot easier. We maintain a dictionary of states to probability, where a state is just the two values x and y, always sorted so that x ≤ y. We start with just {(s, s): 1}. The, we consider each interval (a, b) iterative and for each state (s1, s2) with probability p in the last step's result, we add probability p / 2 to new states sorted(max(s1, b), s2) and sorted(s1, max(s2, b)) if a ≤ s1. If a > s1, we add p to our accumulated result and don't generate any new state, since state (s1, s2) is guaranteed to leave some unguarded time. This is a simple implementation of the quadratic version, but the real trick is when making the optimization to bring the time down to linear, which in this case is simply ignoring states with too low probability (i.e., if p < ε, do nothing).

Qualification Round 2019 - Code Jam 2019

Foregone Solution (6pts, 10pts, 1pts)

There are at least three different approaches to this problem, and they will have different degrees of success!

Brute force
In test set 1, N is less than 100000, so if we want to find two numbers A and B such that A + B = N, there are fewer than 100000 possible choices for A. (Once we choose a candidate value for A, we know that B = N - A.) We can try all possible choices of A until we find one for which neither A nor B contains a 4. However, we cannot apply this strategy to test set 2, in which we might have to check almost 109 values!

Randomized
We can repeatedly choose a candidate value of A uniformly at random from the inclusive range [1, N-1], and check whether both it and B = N - A are free of the dreaded 4s. But will this randomized approach find a solution fast enough?

How many numbers less than 109 do not contain a 4? Consider the set of all 9-digit numbers, with leading zeroes allowed; notice that this set therefore includes all 8-digit, etc. numbers. If we choose a number at random from this set, the probability that it lacks a 4 is the probability that its first digit is not a 4, times the probability that its second digit is not a 4, and so on. Since the probability is 9/10 for any one digit, and the probabilities for different digits are independent, we have an overall probability of (9/10)9, which is about 0.387. In practice, depending on the actual value of N, we might have fewer valid choices, but the size of this answer should give us some hope that we can find our A and B quickly. If we presume that the probability that A has no 4s is not too much smaller than our estimate, and that A not having a 4 does not substantially increase the odds that B has a 4, there should be many working values of A out there to find.

We can argue more rigorously. Let D be the number of digits in N; we will consider the set of all D-digit numbers less than N, with leading zeroes allowed. Suppose that we choose A randomly from that set, and let B = N - A, padding with leading zeroes as needed to bring A and B up to D digits.

Consider the last digit of A. If it is a 4, or if it is (N - 4) mod 10 (which means that the last digit of B is a 4), then we have failed. But at least 8 of the 10 possible values for the last digit are OK. Then, assuming we have not yet failed, consider the next-to-last digit of A. There is at most one value for that digit that would imply that the next-to-last digit of B is a 4, so at least 8 of the 10 possible values for the next-to-last digit of A are OK. Notice that even though the value of the last digit of A might determine which particular value for the next-to-last digit of A would imply that the next-to-last digit of B is a 4, we can still be sure that there exist at least 8 OK values.

And so on, for all D-1 digits up to the first. We only need to worry about the first digit of A or B being a 4 if the first digit of N is at least 4, in which case there are certainly at least five possible values for the first digit of A, of which (just as before) at most two can be bad. So the overall probability that a randomly chosen A is OK is no smaller than (8/10)D-1 × (3/5). For D = 9, this is around 0.1, so our randomized approach should only need an expected 10 or so tries to find an answer, and the probability that it will fail to find one after e.g. 1000 tries is vanishingly small (around 10-46).

If you check all values with D digits for some small D, you will find that the worst case (i.e. the one with the fewest solutions) is a 4 followed by D-1 9s. For N = 49999999, the fraction of possible choices that will work is around 0.1258, so our bound of 0.1 was not particularly conservative!

However, this approach is highly unlikely to work for test set 3, in which N can be on the order of 10100. Our lower bound on the probability that a randomly chosen solution is correct becomes (8/10)99 × (3/5), which is about 1.5 × 10-10. Even if we try a more clever randomized approach in which we build a random A (less than N) out of non-4 digits and then check the corresponding B, which would change those 8/10 factors to 9/10, that bound would be on the order of 10-5, which seems too low to work well on 100 test cases. Can we find a deterministic solution?

Constructive
Observe that we can write N in terms of a sum of powers of 10 times its digits. For example, 4837 = 4 × 1000 + 8 × 100 + 3 × 10 + 7 × 1.

Notice that every digit can be written as a sum of two non-4 digits. In particular, we can write 4 = 2 + 2, and for any other digit X, we can write X = 0 + X.

We can apply these observations to solve our problem digit by digit! In the above example, we can decompose 4 × 1000 into 2 × 1000; and 2 × 1000; 8 × 100 into 0 × 100 and 8 × 100, and so on. In this way, we are building up the digits of our desired A and B; A is 2 × 1000 + 0 × 100... = 2000, and B is 2 × 1000 + 8 × 100... = 2837.

In summary, we can set A to be the same as N with every 4 replaced with a 2, and every non-4 replaced with a 0. B is simply N with every 4 replaced with a 2. Even though we would normally need a "big integer" implementation to handle numbers as large as a googol, in this case we can implement the solution using only strings!

Qualification Round 2019 - Code Jam 2019

You Can Go Your Own Way (5pts, 9pts, 10pts)

Test set 1
In the first test set, we can construct all possible paths from the northwest cell of the maze to the southeast cell using backtracking, and see which of them satisfy our requirements (we know from the Output section that at least one answer exists). When we find one that works, we output it as an answer for the test and stop looking any furthter.

Test set 2
Let's assess the number of possible paths from the northwest cell to the southeast cell. For a maze of size N by N, every possible valid path makes N - 1 moves to the east and N - 1 moves to the south. Notice that the order in which these moves are made does not matter – we will always arrive at the southeast cell after making all of them and we are guaranteed not to leave the maze in the process.

So we need to make 2N - 2 moves total, out of which N - 1 are to the east and N - 1 are to the south, and the order does not matter. Using combinations, we can see that there are C(2N - 2, N - 1) possible options. For N ≤ 10 in the test set 1 there are at most 48620 available paths that we need to check. For test set 2, however, in which N = 100, there are 22750883079422934966181954039568885395604168260154104734000 (or approximately 2.28 × 1058) possible paths to take. This is too much to process in the time limit, so let's think of an alternative solution.

We can think of a maze as a graph, where the unit cells are nodes and there is an edge between every pair of nodes that represent neighboring cells. Now instead of moving between two neighboring cells, we will move between the corresponding nodes along the edge connecting them. Because we cannot reuse Lydia's moves, the edges that she used before are no longer available to us, and we remove them from the graph. After that, our problem of finding a valid path reduces to the problem of finding any path from the node representing the northwest cell to the node representing the southeast cell. This is a standard graph problem that can be solved using either Depth-first search or Breadth-first search in O(N2) time, which is fast enough to pass this test set.

Test set 3
With N ≤ 50000 now, we must think of a different approach to the problem.

To solve this test set, let's just invert all of the moves in Lydia's path. That is, every time she moves east, let's move south, and every time she moves south, let's move east. For example, if Lydia's path is EESSSESE, then our path will be SSEEESES.

Let's understand why this inverted path is a correct answer to the problem.

First, notice that we still make N - 1 moves to the east and N - 1 moves to the south, so we will arrive at the southeast cell in the end as required, and we will not step out of the bounds of the maze.

Now let's see why we will not reuse any of Lydia's moves. Suppose this is not the case, and we reuse a move from the position that is X moves to the east and Y moves to the south in some order from the northwest cell. Recall that the order of moves does not matter, and there may be many ways to get to this position, but all of them will require exactly X moves to the east and Y moves to the south. What will be the next move? We know that Lydia's next move is the (X + Y + 1)-th symbol in the string representing her path (with indexing starting from one), and our next move is the (X + Y + 1)-th symbol in the string representing our path. But since our path string is an inverted version of Lydia's path string, we know that (X + Y + 1)-th symbols of the two strings will be different, which contradicts our assumption that we will have the same next move. By the same logic, we can see that the two paths will not reuse any other moves along the way.

Qualification Round 2019 - Code Jam 2019

Cryptopangrams (10pts, 15pts)

Yeah, but what about decrypting?
The statement tells us how the plaintext is encrypted, but it says nothing about the decryption mechanism! Figuring that out is part of the problem. Since this is a Qualification Round problem, there is slightly less time pressure and competitive urgency, and you have some extra time to think about how this cryptosystem is supposed to be used.

Suppose that Cameron and Jamie are members of the Code Jam team who know the secret list of 26 primes, and suppose that Cameron has just sent the ciphertext to Jamie. Each value in the ciphertext is the product of two of those primes, so Jamie can try dividing each value by each known prime to find the value's factors. (Notice that a value in the ciphertext might be the square of a prime, if the plaintext has the same letter twice in a row.)

After getting the list of factored ciphertext values, how should Jamie recover the plaintext? We might be tempted to say that the second letter of the plaintext is the one whose prime appears as factors of both the first and second ciphertext values, the third letter is the one whose prime appears as factors of both the second and third ciphertext values, and so on. Then the remaining factors in the first and last ciphertext values give us the first and last letters of the plaintext.

This is almost correct, but we (and Jamie) would have to deal with one significant annoyance. If the plaintext starts with something like ABABA..., for example, then the first, second, third, and fourth ciphertext values will all be the same, being the product of the same two factors (the primes corresponding to A and B). In particular, the start of BABAB... looks just the same as the start of ABABA...! The good news is that we know that this kind of pattern must terminate somewhere in the message; eventually, either we will get the same letter twice in a row, or (since the plaintext uses more than two different letters) three consecutive different letters. As soon as either of these things happens, we have a "break-in point", and we know which factors of a particular ciphertext value go with which of the two letters of the plaintext that it encodes. Then, we can "unzip" the rest of the plaintext from there, working backwards and forwards.

For instance, if the plaintext starts with ABABAABAB, the first four ciphertext values will all be the same: the product of the prime corresponding to A and the prime corresponding to B. The fifth ciphertext value will represent the square of A, so we will know that the fifth and sixth plaintext letters are both A. We can then reason that the fourth plaintext letter must be the fourth ciphertext value divided by the prime corresponding to A, the third plaintext letter must be the third ciphertext value divided by the prime corresponding to B, and so on going backwards. We can also determine that the seventh plaintext letter is the sixth ciphertext value divided by the prime corresponding to A, and so on going forwards.

Similarly, if the plaintext starts with ABABCBABA, when we inspect the third and fourth ciphertext values, we will see that they are different, but both have the prime corresponding to B as a factor. Then we can unzip from there, as above.

However, we must remember that Jamie has an advantage that we do not have: we do not know the 26 secret primes! We need to find a way to get them.

Test set 1
In test set 1, the ciphertext values are products of small primes. Each prime is less than 104, so each ciphertext value is no larger than 108. It is straightforward to factor these values by testing every possible (prime) factor between 2 and 104. Once we have all of the factors, we will have our set of 26 primes, since each prime will be represented in at least one factor of one ciphertext value. We can assign them in increasing order to the letters A through Z.

Then, to recover the plaintext, we can use a bit of brute force to sidestep the need to unzip as described before. Consider the two factors that contribute to the first ciphertext value; arbitrarily choose one of them. Let us first assume that that factor corresponds to the first letter of the plaintext, and the other corresponds to the second. Then we can take the remaining factor and try to divide the second ciphertext value by that factor. If we cannot, we have a contradiction, and we should go back and make the other factor correspond to the first letter of the plaintext. Otherwise, the quotient is the factor corresponding to the third letter of the plaintext, and so on. For one of our choices, this method will work and will give us the correct plaintext; for the other choice, we will reach a contradiction, since (as described above) it is guaranteed that there is only one possible decryption.

Test set 2
In test set 2, the primes can be enormous (as large as a googol), and the product of two such primes can be even more enormous. We should realize that it is hopeless to try to factor such a product. If we could do that, we could also break modern cryptosystems that depend on the assumption that factoring large numbers is intractable! The Code Jam servers do not run on quantum computers (yet...), so there is no way for us to try to use a quantum algorithm, either.

To solve this seemingly impossible test set, we need to find a different vulnerability of this cryptosystem. The key insight is that any two consecutive values in the ciphertext have at least one factor in common. Factoring very large numbers may be intractable, but we do know how to efficiently find the greatest common divisor of two very large numbers! A method like Euclid's algorithm will be fast enough.

Notice that if we have a plaintext like ABABC..., it is possible that the prime corresponding to A will not appear in any of the pairwise GCD values. So, we should compute GCDs of consecutive ciphertext values until we get a value that is not 1; at least one such value must exist, as described in the introduction to the problem. At that point, we can unzip the rest of the ciphertext, as described previously, finding all of the primes as we go. Then we can proceed as above. And we do not even have to know a bevy of DP flux algorithms, whatever those are!

A note on language choice
An essential skill in Code Jam is picking the right tool (language) for the right job. For some problems, a fast language such as C++ may be needed to obtain a solution within the time limits. For this problem, it is probably better to choose a language like Python that can handle very large numbers without any extra hassle, even at the cost of some speed, or a language like Java that has a built-in "big integer" implementation.

Qualification Round 2019 - Code Jam 2019

Dat Bae (14pts, 20pts)

Test set 1
In this problem, we need to somehow identify which workers are not returning the bits that we send to them. Let's see how strings of bits change when some of the data is lost.

Imagine that we have ten workers, and we send them the following five random strings of bits (the i-th bit in each string is sent to the i-th worker):

  0101010110
  0101010101
  0010100100
  0110110101
  0100100100
Also, imagine that workers 3 and 6 are broken. In this case, we will lose the following bits (highlighed in bold):

  0101010110
  0101010101
  0010100100
  0110110101
  0100100100
In the result, we will receive the following bit strings:

  01001110
  01001101
  00110100
  01111101
  01010100
Notice, how in this representation, we lost columns of bits because of the broken workers. It would be nice to be able to tell which columns we lost – then we will be able to determine which workers did not return the data!

Let's see if we can make all the columns different – then it will be easy to tell which ones are missing. In test set 1 we have up to N = 1024 workers, so we will need up to 1024 different columns. We can send up to F = 10 bit strings, which means our columns will consist of up to 10 bits.

Fortunately for us, 210 = 1024, so we can make each column represent a unique number in the range from 0 to 1023. For example, we could make the i-th column represent the number i, in which case the first five columns, that represent numbers from 0 to 4, could look like this:

  01010...
  00110...
  00001...
  00000...
  00000...
  00000...
  00000...
  00000...
  00000...
  00000...
Now we can use a construction like this to see which workers are broken. If the i-th worker is broken, the bit column representing the number i will be missing in the result we receive.

Test set 2
In the second test set, we can only send up to F = 5 bit strings, which means our columns will consist of only up to 5 bits. 25 = 32, so we can no longer use a previous approach of making each column unique.

Notice how we also know that B ≤ min(15, N-1), even though we have not used this fact in our solution so far. How does this additional constraint change the problem? The first thing to notice is that only a small fraction of columns will be missing when N = 1024, but these columns can still be in any positions.

Let's see what we can do with 32 numbers that we can represent with 5 bits. If we put these numbers in an order, that is

0, 1, ..., 31

we can notice that since B, which is less than 15, is also less than 32, this whole block of 32 numbers will never disappear completely. Let's see how we can make further use of this fact. If we have several blocks like this:

0, 1, ..., 31, 0, 1, ..., 31, 0, 1, ..., 31,...

none of the blocks of numbers from 0 to 31 will disappear completely, and for each the remaining numbers, we will always be able to identify which block it is from.

Let's examine this in more detail. Numbers inside each block are in an increasing order. Notice that even after some numbers disappear, when one block ends and the next one starts, numbers go down:

0, 1, ..., 31, 0, 1, ..., 27, 5 , ..., 31,... (numbers between 27 and 5 disappeared, but 27 is still bigger than 5)

Assume this is not the case, and after some number X from one block goes a number Y from the next block such that X ≤ Y. But this is impossible since there were at least 31 numbers between any such X and Y, and these numbers could not disappear altogether.

With these observations at hand, we are ready for a final solution. Let the bit columns of the strings we send to the database represent repeating blocks of numbers from 0 to 31:

0, 1, ..., 31, 0, 1, ..., 31, 0, 1, ..., 31, ... (N numbers total)

After we receive the responses from the database, we can iterate through the remaining numbers, noting that the new block starts when the current number is smaller than the previous one, and keeping track of how many blocks we have seen so far. Knowing the position of the numbers inside the block and the number of blocks we have seen, we can uniquely identify all the numbers we see: for example number 16 in the fifth block is (5 - 1) * 32 + 17 = 145-th number in the whole sequence. And if we know which numbers we have seen in the whole sequence, we can find out which numbers are missing, and output them as the numbers of the missing workers.

Finally, we used the fact that B < 25 to make the approach above work. But B < 24 too! That means we can use the same approach with only four strings. In this case, it is possible that two consecutive numbers from different blocks are equal, since there are 24 - 1 = 15 other numbers in between them, which might all get deleted. Therefore, to detect block changes, we must use ≥ instead of > between consecutive numbers.

Round 1A 2019 - Code Jam 2019

Pylons (8pts, 23pts)

Test set 1
There are a few impossible cases for this problem. If we experiment with some small grids, we can find that in addition to the 2 x 2 grid given as a sample case, the 2 x 3, 2 x 4, and 3 x 3 cases have no solution. (Because of symmetry, the 3 x 2 and 4 x 2 cases are also impossible.)

In the 2 x 3 grid, we can notice that the middle cell of the top row shares a row, column, or diagonal with every other cell; the same is true of the central cell of the 3 x 3 grid. In each case, we cannot go from that cell to any other cells, or vice versa, so the grid is unsolvable.

In the 2 x 4 grid, we can try starting in the second cell of the top row; then we find that we are forced into a series of three moves that eventually take us to the third cell of the bottom row, from which there are no other moves; we can never move out of that set of four cells that we visited. Since the same is true of the other set of four cells, there is no solution.

The other cases in test set 1 are all solvable, though. One strategy is to make mostly "knight moves" — two unit cells in one direction, and one unit cell in the other. We may not be able to solve a grid using only knight moves — see this article for more information — so we can sprinkle in other legal moves as well. For instance, we can solve the 3 x 4 case by visiting the cells in the following order:

02 05 10 07
09 12 01 04
06 03 08 11
And here is a solution for the 3 x 5 case, which makes many steps that are wider than knight moves:

04 14 09 12 07
06 11 01 15 03
02 08 13 10 05
Because of symmetry, the only remaining cases are 2 x 5, 4 x 4, and 4 x 5, and we can solve these by hand if we choose, or we can use a brute force algorithm.

Test set 2
There are multiple strategies for dealing with test set 2. One class of approach is constructive. For example, we can devise general solutions for 2 x N and 3 x N grids, for arbitrary N, and then divide up the grid into horizontal strips of height 2, plus one more strip of height 3 if needed. Unfortunately, this can be tricky to get right. We need to make sure that the solutions to the subproblems do not violate the rules — the last move in one subproblem cannot be in the same row, column, or diagonal as the first move in another. Moreover, we might struggle to come up with our general 2 x N and 3 x N solutions.

The Code Jam team's first successful constructive solution included multiple cases for 2 x N (depending on whether N is odd, 0 mod 4, or 2 mod 4), and multiple cases for 3 x N (which entailed adding pairs of columns to the left and right of our hardcoded 3 x 4 and 3 x 5 solutions, bouncing between those columns to avoid breaking rules). It also made each such solution start near the left edge of each strip and end near the right edge, to avoid diagonal interactions among subproblems / strips.

Is there an easier way? Let's take a step back. The problem imposes some constraints, but one can observe that it is not too difficult to solve the larger test set 1 cases by hand. This suggests that there are many possible solutions, and we might expect even more as the grid's dimensions get larger. (We could use Ore's theorem to establish the existence of at least one solution for sufficiently large grids.) So, our intuition might suggest at this point that the best approach is some kind of brute force.

We might consider using backtracking solutions. One possible concern is that these solutions, whether they are breadth-first or depth-first, proceed in an orderly fashion that might be more likely to leave us with tough "endgames" in which the constraints are impossible. For example, if our solution somehow solves all but the bottom row of the grid, then all hope for the universe is lost!

We can try these solutions anyway, or we can rely on our occasional friend, randomness! We can pick a random starting cell, repeatedly choose valid moves uniformly at random from the space of all allowed moves from our current cell, and, if we run out of available moves, give up and start over. For any case except for the impossible ones mentioned above, this approach finds a solution very quickly.

Many problems cannot be approached with random or brute-force solutions, but identifying the problems that can be (in Code Jam or the real world) is a useful skill!

There's even a greedy algorithm
We are aware of other approaches. For example, we know that there is at least one implementation of the following idea that solves every possible test in the test set 1 and 2 limits: repeatedly greedily select an unvisited cell that has the largest count of unvisited "neighbors", where we define a cell's neighbors as those cells that share a row, column, or diagonal with that cell. (We can either rule out specific impossible cases beforehand, since there are not very many, or infer impossibility by comparing the number of unvisited cells with the largest count of unvisited neighbors.)

Although we do not provide a proof here, intuitively, it is advantageous to prevent any one row, column, or diagonal from having too many unvisited cells, relative to other rows, columns, and diagonals. For example, if we are close to the end of our journey, and a majority of our remaining unvisited cells are in the same column, we are doomed.

Given the relatively small number of possible test cases for this problem, it is not too hard to check all of them to verify that a solution works, and that may be much easier than proving correctness! That is a rare luxury to have for a Code Jam problem!

Round 1A 2019 - Code Jam 2019

Golf Gophers (11pts, 21pts)

Test set 1
The difficulty of this problem stems from the fact that the windmills can loop around. If we have a windmill with 5 blades, and we find it with blade number 2 pointing downward in the morning, does that mean that 2 gophers rotated it? or that 7 did? or 12? In general, if a windmill has B blades, and we find it in position P, all that we can say for sure is that some number (P + K × B) of gophers (for some integer K) tampered with it during the night.

Because of this, we might reason that we should put as many blades as possible (that is, 18) on each windmill, and then hope that none of those windmills is visited by more than 17 gophers, in which case the total number of rotations of all blades equals the total number of gophers. This turns out to be a reasonable hope! It is hard to directly calculate the probability that none of the 18 blades will be turned more than 17 times on a given night, but a quick simulation can tell us that even in the worst case of 100 gophers, the probability of this happening is about 0.00017. Since we can run this same experiment 365 times and take the maximum result that we find, the chances of a wrong answer (due to getting a misleading result all 365 times) are infinitesimally small.

Test set 2
In test set 2, we only have 7 nights to work with, and the number of gophers can be quite large, so we cannot reasonably hope that the windmills will not loop. Now what? We might consider using different numbers of blades on each windmill during a given night, but it's hard to see how that buys us anything.

Suppose that, on one night, we try putting two blades on every windmill. At first this doesn't seem to help — shouldn't the resulting data should be almost pure noise? However, we can observe that if the number of gophers is odd, the total number of turns (across all windmills) will be odd, and if the number of gophers is even, that total number will be even. So we have a way of determining the parity of the number of gophers, no matter how they happen to turn the blades!

We can extend this idea to find out how many gophers there are modulo any number of our choice between 2 and 18. Since we only get 7 nights, though, we should choose our numbers carefully. One promising idea is to make them all prime, and there are seven primes in the range we can use: 2, 3, 5, 7, 11, 13, and 17. Then we can try to use the construction suggested by the Chinese remainder theorem to uniquely determine the number of gophers. However, this method would only work for any number of gophers up to 2 × 3 × ... × 17 = 510510; it cannot distinguish 510511 gophers from 1 gopher! We are in trouble, since we might have up to 106 gophers.

The final insight that we need is that the Chinese remainder theorem only requires its moduli to all be relatively prime, not necessarily prime. So we can use 16 in place of 2, and 9 in place of 3. This lets us identify any number of gophers up to 5 × 7 × 9 × 11 × 13 × 16 × 17 = 12252240, which easily covers what we need. (We can also get away with using the numbers 12 through 18, for example; we leave the details to the reader.)

Notice that we do not really need to do any calculations based on the Chinese remainder theorem; since the number of possible answers is relatively small, we can check all of them until we find the one that has the appropriate residue for each of our chosen moduli.

Test set 1 appendix
To prove that the probability of at least one blade rotating more than 17 times is about 0.00017, we can solve a related problem:

"Count the number of integer arrays of length L such that each array element is one of 1, 2, ..., U and each number appears at most B times in the array."

This related problem is equivalent to counting integer partitions with constraints on the size of each partition.

We can solve this related problem using Dynamic Programming. First, we choose how many times U appears in the array; say it appears K times, with 0 ≤ K ≤ min(B,L). Once we know K, we have to choose where those K values go. Using combinations, we can see that there are C(L, K) possible options. For each option, we can treat the L-K other spaces as an array that must contain the numbers 1, 2, ..., U-1 such that no number appears more than B times, which is a subproblem of our original problem, so we may recurse. If we sum over all possible values for K, we have the total number of valid arrays.

If we solve the above problem with L = 100, U = 18, B = 17, we see that there are 336647783248234011860927063629187654598455062446560501834487820535956663161762533555609870639313859125191714928476256971520000 (or approximately 3.366 × 10125) arrays such that no number appears more than 17 times. These arrays can represent which windmill the gophers select in a good configuration out of the 18100 possible configurations. This gives us the probability quoted above.

Round 1A 2019 - Code Jam 2019

Alien Rhyme (10pts, 27pts)

Test set 1
In the first test set there are only up to N = 6 words with up to 50 characters in each of them. We can simply use brute force to try all ways of grouping words into pairs (allowing some words not to be in any pair – we will effectively discard these words), try all choices of an accent-suffix for every pair, and check that none of the pairs have the same accent-suffix. Finally, we choose the maximum size across all valid groupings.

Test set 2
Let's notice how the size of an accent-suffix affects the chance of multiple words sharing it. In case we have words CODEJAM, JAM, HUM and HAM, accent-suffix JAM can be part of only two words, whereas a shorter accent-suffix M fits all four words. This leads to the following observation: for two words that we want to pair, it is never suboptimal to choose their longest available common suffix as the accent-suffix – this way we are still making sure that they rhyme, and we are allowing shorter accent-suffixes to be used by other pairs. Notice that any other pair that could use the longer suffix can also use any shorter suffix. For example, if we want words CODEJAM and JAM rhyme, we should choose JAM as their accent-suffix, and allow suffix M to be potentially used by HUM and HAM.

In this problem it is all about common suffixes of the words. In order to better operate with word suffixes, let's actually reverse the words first (so now original word suffixes are prefixes of reversed words), and build a trie (also often called prefix tree) on the reversed words. This is how a trie containing the words CODEJAM, JAM, HUM and HAM looks:

M
U
H
H
A
J
E
D
O
C
Let's also mark the trie nodes where some of the input words end. Since we are guaranteed that all the words are unique, we can use a simple boolean flag. In the picture above, trie nodes where a word ends are marked in red.

Now we can solve the problem as follows: for a trie node v, let f(v) be the minimum possible number of unpaired words that use accent-suffixes whose reverses end in the node v or the subtree under it. The answer to the problem is then N - f(root), since f(root) represents all usable accent-suffixes.

How do we calculate the values of f(v)? If v does not have any children nodes, we set f(v) to be 1, since we know that in our trie all leaf nodes are the end of a word. If node v has children, we can calculate f(v) with the following algorithm assigning the result to r:

  r = sum(f(c) for all c where c is a child node of v)
  if node v is marked (there is a word that ends at v): r = r + 1
  if v is not the root and f(v) ≥ 2: r = r - 2
First, we simply count the number of unpaired words recursively. Finally, we let two of those words be paired using the prefix that ends in the node v, which represents suffixes of original words, as the accent-suffix.

In our example trie we would get the following values of f(v):

M
U
H
H
A
J
E
D
O
C
f(v) = 1
f(v) = 1
f(v) = 1
f(v) = 1
f(v) = 0
f(v) = 1
f(v) = 1
f(v) = 0
f(v) = 0
f(v) = 1
f(v) = 1
Proving the algorithm above correctly calculates f(v) is straightforward. First notice that only words that are represented in the trie at or below v are pairable with the set of accent-suffixes represented by v or its subtree. Then we can proceed by induction: f is pretty clearly correct for a single node tree, as there is no pairing possible. Assume by the inductive hypotheses f works correctly on all proper subtrees under v. The pairing implied by the construction of f(v) — adding any remaining pair of words to the recursive result — is valid: we are only pairing two words with the accent-suffix represented by v, and the rest is valid by the inductive hypotheses. To show that the pairing it is also of maximum size, notice that, by inductive hypothesis, there is no way to pair more than sum(f(c) for all c where c is a child node of v) words with accent-suffixes that are represented by the subtree but not by v directly. This is because words in different subtrees of v cannot be matched with a longer accent-suffix than the one represented by v, and the accent-suffix represented by v can add at most a single pair to the total.

Note how we calculate the values of f(v) in a recursive manner, and f(v) is calculated exactly once for each possible v. Since the algorithm itself takes constant time in addition to the time of the recursion, we can calculate all f(v) values in O(T) time, where T is a total number of nodes in the trie. We can bound T by the total length of all words, or by N × m where m is the maximum word length.

Finally, there are less efficient but simpler implementations that also work. For example, sort the reversed words alphabetically and take any two adjacent words with a longest common prefix, pair them, remove them from the list, and repeat. This simple-to-implement algorithm basically constructs the same pairing our recursive formulation does. This shifts some implementation complexity onto the correctness proof. If you are faster with proofs than with code, it might be an overall gain in solving speed.

Round 1B 2019 - Code Jam 2019

Manhattan Crepe Cart (9pts, 18pts)

Test set 1
Test sets 1 and 2 differ only in how large the available grid area is. In the first test set, people can only be standing in cells with coordinates between 0 and 10, inclusive, in either dimension. Moreover, we can notice that the cart can only be in a cell within this area. The cart's horizontal and vertical coordinates must both be nonnegative per the rules of a problem. Moreover, it cannot have a coordinate larger than 10 (in either dimension). Suppose, for example, that the cart had a horizontal coordinate greater than 10; then that would imply that there must be at least one person standing at horizontal coordinate 10 and facing east. Otherwise, placing the cart at horizontal coordinate 10 would be even better, per the tiebreaker rules. But the rules of the problem do not allow people at (horizontal/vertical) coordinate 10 to face (east/north).

Therefore, to solve test set 1, we can create an array to represent all of the blocks in the allowed area, and initialize each cell's value to 0. Then, for each person, we increment all of the cells of the array that they are walking toward. Finally, we find the cell of the array with the maximum value, using the tiebreaker rules as needed. This solution takes O(P × Q2) time.

Test set 2: a quadratic solution
In problems that involve multiple dimensions, it is often worth checking whether those dimensions are independent. In this case, they are! A person heading west, for example, gives us a "vote" for the crepe cart being to the west of them, but tells us nothing at all about the north-south location of the cart. So we can solve the two dimensions as separate one-dimensional problems; the horizontal problem includes only the people heading west or east (and their horizontal positions), and the vertical problem includes only the people heading south or north (and their vertical positions). Let us consider the horizontal dimension for now; our arguments also apply to the vertical dimension.

Even if our people are widely spread out along the horizontal axis, the crepe cart can only possibly be in a limited number of horizontal positions. In fact, it must be either at position 0, or at a cell that is one cell to the east of some person. To see why, suppose that the cart is in some other cell 1 ≤ C ≤ Q. Let W denote the cell one unit to the west of C. We know by assumption that W does not contain a person. But then if we move the cart from C to W, we will not be losing any votes (i.e. any person voting for C is also voting for W), and we many even gain votes (if there were people in C who were heading west). (Notice that a person does not vote for the cell they are in.) Even if we do not gain votes, our tiebreaker gets better. So we should always make this move, and, therefore, we should always move the cart west until it is at 0 or immediately to the east of someone. Observe that this might find only a locally optimal solution, but if we check all such cells, we will surely find the globally optimal solution.

These observations reduce the number of cells we need to check to O(P) rather than O(Q). To check a cell, we can make a linear pass over all of the people, and count whether each one is voting for that cell. One such check takes O(P) time. Then we choose the cell that got the most votes, breaking a tie if needed by choosing the westernmost of the tied cells. The overall time complexity is O(P2) for the one-dimensional problem, and solving it twice (for two dimensions) is still O(P2).

Test set 2: a (nearly) linear solution
Although the above solution is fast enough to solve test set 2, we can do even better by avoiding making a linear pass over the data for each person.

We can first process our data into a set of tuples like the following: (coordinate, number of people at that coordinate facing west, number of people at that coordinate facing east). Let us denote these as (Ci, Wi, Ei). (Remember that for the purposes of the horizontal subproblem, we are ignoring people facing north or south.) Using a hash-based dictionary, this processing takes time linear in P. As we do this, we should also determine the total numbers W and E of people facing west and east.

Then, we sort these tuples in ascending order of their first values. It is probably most convenient to use a common sorting algorithm (or one built into your language), making this step nonlinear. (We will leave a spirited discussion of which sorting algorithms are truly linear for another day.)

Once we have sorted the tuples, we start by considering cell 0 as a candidate location, and we determine the votes for that cell. We know that quantity is equal to W minus the number of people in cell 0, if any. We make a note of this number of votes and set 0 as our best candidate so far. Then, we look at the first tuple in our sorted list, which represents the people at some cell C (which might be cell 0). The number of votes for cell C + 1 (the cell one unit to the east of C) should be the same as the number of votes we found for cell 0, plus the number of people in cell C facing east (which is all of them in this case), minus the number of people in cell C facing west. If cell C is a better candidate, we store it and its number of votes.

We can do this for every tuple in our list to get our final answer. If we have one or more people on the eastern border, we do not need to check the cell one unit to the east of them, since (as explained at the start of the analysis) the cart can never have a coordinate larger than Q. Since the check for each entry of the tuple takes constant time, the full pass takes O(P) time.

Round 1B 2019 - Code Jam 2019

Draupnir (10pts, 21pts)

In this problem, we need to determine the number of each type of X-day ring by querying the total number of rings in existence at the end of a certain day.

Let R1 be the number of 1-day rings, R2 be the number of 2-day rings, ... etc. at the end of day 0. The number of X-day rings doubles at the end of every day that is a multiple of X. Thus, the total number of rings on day i is

    R1*2i + R2*2floor(i/2) + R3*2floor(i/3) + R4*2floor(i/4) + R5*2floor(i/5) + R6*2floor(i/6).
  
Test set 1
In the first test set, we are allowed six queries to determine the six unknown variables. Note that if we query any day number larger than 63, then the number of 1-day rings on that day will be a multiple of 263 (equivalently, it is 0 modulo 263). Similarly, if we query any day number larger than X*63, then the number of X-day rings will be 0 modulo 263 on that day.

Thus, on day 315 (=5*63), the total number of rings, modulo 263, is R6*252 (since the number of 1-day, 2-day, ..., 5-day rings are all 0 modulo 263). Since R6 ≤ 100, we know that R6*252 does not exceed 263, so we can directly determine R6. Then, on day 252 (=4*63), the total number of rings, modulo 263, is R6*242 + R5*250. Since we already know R6 and we know this sum cannot be more than 263, we can solve for R5. We may continue this process by querying days 189 (=3*63), 126 (=2*63), 63 (=1*63) and 1 to determine R4, R3, R2 and R1, respectively.

Alternative
Note that since we are querying six days and attempting to solve for six variables, we may choose to query (for example) days 1,2, ... , 6. This gives us a system of equations with six equations and six variables. Since these equations are linearly independent, we can solve this system, via Gaussian elimination for example, to get the solution.

Test set 2
The second test set requires another insight. We are only given two queries, so we must be able to solve for multiple Ri at once.

Let's think about what information we get by querying day 189 (=3*63). This gives R6*231 + R5*237 + R4*247, modulo 263. However, it is possible that there is some overlap between R6*231 and R5*237. For example, all other things being equal, we would not be able to differentiate between the case where R6=64 and R5=0 and the case where R6=0 and R5=1. Both of these would have 237 total rings.

We must use the fact that Ri ≤ 100. In order for the number of i-day rings to not interfere with the number of (i-1)-day rings on day d, we need 2floor(d/(i-1)) > 100 * 2floor(d/i) (note that this is equivalent to floor(d/(i-1)) ≥ floor(d/i)+7 since 27 > 100). If we ignore the modulo 263 restriction, then we could solve the question in a single query of (for example) 1000. This would give us

    R1*21000 + R2*2500 + R3*2333 + R4*2250 + R5*2200 + R6*2166.
  
Since Ri ≤ 100, we coud determine R6 by taking this value modulo 2200 to get R6*2166, then by dividing by 2166. We then could iteratively determine R5, R4, ..., R1. However, this idea does not work since the problem is modulo 263. There is no single query that can give us all of the information we need, modulo 263.
We will use one query to determine the values of R4, R5, and R6, followed by a second query to determine the values of R1, R2, and R3. We saw above that a query of 189 (=3*63) will not work. However, a query of (for example) 200 will work:

    R4*250 + R5*240 + R6*233,
  
and then solve for each in the same way as for the 1000 case. We can then make a second query of (for example) 56:
    R1*256 + R2*228 + R3*218 + R4*214 + R5*211 + R6*29.
  
We know the value of R4, R5, and R6 from the first step, so we may substitute those in and then solve for R1, R2, and R3 one-by-one.
Note that the choice of 200 and 56 above were not the only possible options. In this problem, we could either guess-and-check for these values offline or (preferably) write a loop in the program to find two values that satisfy the criteria. We also need to ensure that the term with the largest exponent is not too large. For example, we cannot use a query of 250 in the first step since this gives R4*262 + R5*250 + R6*241, because the first term (R4*262) may be at least 263.

Round 1B 2019 - Code Jam 2019

Fair Fight (14pts, 28pts)

Test set 1
Let us call a pair (L, R) fair if choosing it produces a fair fight. Notice that since there are only 100 entries, there are at most 100*101/2 = 5050 possible intervals. For each interval, we may simply search for the maximum value is in each array via a linear search and check if those maximum values are close enough.

Test set 2
Notice the random choice of sword in case of ties does not change whether (L, R) is fair or not, so we can further assume that, if there are ties, they are broken by choosing the sword with smallest index. To simplify the write-up below, we assume that each Ci is distinct.

Let us consider a sub-problem: for each sword i that Charles can choose, how many fair intervals (L,R) are there which Charles chooses sword i? Let us call this value Fi. The answer to the original problem is simply the sum of the Fis. For each interval (L,R), there are three properties we are concerned with:

(P1) Charles chooses sword i. That is, L ≤ i ≤ R and max(CL,CL+1,...,CR) = Ci.
(P2) Charles' sword is good enough. That is, max(DL,DL+1,...,DR) ≤ Ci + K.
(P3) Charles' sword is too good. That is, max(DL,DL+1,...,DR) < Ci - K.
So we have Fi = (# of intervals that satisfy P1 and P2) - (# of intervals that satisfy P1 and P3). These two quantities are computed very similarly since they just have a different bound on the right-hand side of the inequality in P2 and P3. We explain below just how to compute (# of intervals that satisfy P1 and P2) and leave computing (# of intervals that satisfy P1 and P3) to the reader.

Note that if an interval (L,R) satisfies P2, then any subinterval of (L,R) also satisfies P2. Similarly, if (L,R) satisfies P1, then any subinterval of (L,R) that still contains i also satisfies P1. Thus, we are really only interested in how far left L can go with R=i (and how far right R can go with L=i). One option is to do a linear search for how far left L can go, but this is too slow. Instead, we binary search for how far away the left-endpoint is. A left-endpoint is too far left if P1 or P2 are no longer true. Otherwise, the left endpoint can be pushed further left. Once we know the furthest left L can go (let this index be Li) and the furthest right R can go (let this index be Ri), then (# of intervals that satisfy P1 and P2) = (i - Li + 1) × (Ri - i + 1). Calculating the maximum element in a range can be computed efficiently using a range minimum (maximum) query-type data structure in O(1) time per query, meaning O(log N) total time for the binary search.

In terms of time complexity, for each i, we need to perform 4 binary searches, which take O(log N) time each, for an overall O(N log N) total time. Setting up the range maximum query data structure takes an additional O(N log N) time, yielding an overall O(N log N) time for the full algorithm. There are solutions that can compute all of the required Li and Ri values (defined above) in O(N) total time by doing a couple of clever sweeps of the data to count the number of unfair intervals, but this was not needed for this problem.

Round 1C 2019 - Code Jam 2019

Robot Programming Strategy (10pts, 18pts)

With A competitors, there are A! possible initial setups for the tournament bracket, and A is itself exponential in the number of rounds of the tournament. A factorial of an exponential is terrifying enough, and we haven't even started the tournament yet!

Fortunately, we can ignore almost everything about the structure of the tournament. For any opponent, there is at least one possible initial setup in which we will play our first match against that opponent. Since we have to guarantee that we will win the tournament regardless of the initial setup, we must be able to defeat every opponent. We cannot tie an opponent, since that coin flip might not come up in our favor!

So all we have to do is find a program that beats every opponent's program. To check a program, we can just check it against each opponent, without worrying about the tournament setup.

Test set 1
In test set 1, there are at most 7 opponents, and their programs can be at most 5 characters long. Our program can be longer than that if need be, but how long does it need to be? We can observe that an optimal winning program should never waste a turn by tying with all remaining opponents, since then it could have instead chosen the move that would have beaten all of them; therefore, it should eliminate at least one opponent per move. So in test set 1, if there is a winning program, it is at most 7 moves long. We can comfortably check all 37 + 36 ... + 3 possible programs of length up to 7.

When simulating a match, we cannot wait around for a googol of moves; by the same argument above, an optimal winning program should take no more than 7 moves to defeat all opponents, so we only need to simulate at most 7 moves. If we are still tied with the opponent at that point, we can safely give up on that program.

If we find that no program of length at most 7 beats every opponent's program, then we can declare that the case is IMPOSSIBLE.

Test set 2
In test set 2, there can be up to 255 opponents, and we cannot generate and check all programs of length up to 255. We must find a way to construct a winning program if it exists.

Let's imagine playing against all opponents at once. How do we choose our program's first move? We must win or tie against every opponent, so we will consider the set of their first moves. If it includes all three possible moves, we are doomed; no matter what we pick, at least one opponent will defeat us. If it includes only one move (e.g. every opponent starts with R), then we can pick the move that defeats that move (in this case, P) and instantly defeat everyone. Otherwise, the set includes two of the possible moves, and we should pick the move that ties one and beats the other. For example, if the opponents all lead off with S or P, we should pick S.

Eliminating any defeated opponents and proceeding to the next move of this combined "match", we can apply the same strategy, but considering the set of remaining opponents' second moves (looping through their programs as needed), and so on. We will eliminate at least one opponent with each move, so after A moves, we will either have our winning program or know that the case is IMPOSSIBLE. Notice that this limit holds regardless of the lengths of the opponents' programs.

Round 1C 2019 - Code Jam 2019

Power Arrangers (11pts, 21pts)

Test set 1
We can look at up to 475 figures in Test set 1, but there are 595 figures, so we cannot check them all individually. We can observe that when we investigate any particular set, we only need to look at any four of the figures (say, the first four), since we can infer the identity of the fifth. For example, if the first four figures in a set are D, E, A, and B, then we already know that the last figure in that set is C. This reduces our required number of guesses to 4 × 119 = 476. But that is still one more guess than we are allowed!

Observe that once we have determined the orders in all but one of our sets, there are only two sets remaining: the one we have not inspected, and the one we are missing. These two sets must have different figures for at least two indexes, and we can look at any such index to know which set we have. For example, if we know the last two possible sets are CADEB and CDEAB, we can look at the second index; if we see an A, we have the first set, and if we see a D, we have the second set. So, by the time we get to the 119th set on our shelf, we only need to look at one of its figures (but which one to look at will depend on what we learned earlier). After that, we will know which 119 sets we have, and therefore we will also know which set we lack.

Combining these insights, we can guarantee that we will use at most 4 × 118 + 1 = 473 guesses.

Test set 2
In Test set 2, we only have 150 guesses, which is not much more than one guess per set! How can we possibly succeed?

We need to take further advantage of the fact that we are missing only one of the possible sets / permutations. We can start by checking the first figure in each set, using 119 guesses. When we do this, we will find that four out of five of the letters occur 24 times each, and the other one occurs only 23 times. That letter must be the one on the first figure in our missing set!

Now we can restrict our search to only our 23 sets that begin with that letter, and look at the second figure in each of those; this uses 23 guesses. One of those letters will appear only 5 times compared to 6 for the others; then we can narrow our search to those 5 out of the 23 sets, and use 5 guesses to look at the third figures of those. We will find one unique third letter, and then we only need to check the fourth figure in that set to learn what the other, missing set is. This uses a total of (5! - 1) + (4! - 1) + (3! - 1) + (2! - 1) = 119 + 23 + 5 + 1 = 148 guesses, which fits within the limit of 150.

Round 1C 2019 - Code Jam 2019

Bacterial Tactics (15pts, 25pts)

Test set 1
On a player's turn, the grid will be in some state, according to whether any bacteria have already been placed, and how they have spread. We can determine whether a state is losing via the following recursive definition:

the player has no moves because there are no empty cells
any of the player's moves would either cause a mutation, or give the opponent a winning state.
A winning state is a state that has at least one winning move — that is, a move that would leave the other player in a losing state.
Observe that if a state is not losing, it must be winning, since it has at least one move that does not cause a mutation and does not give the opponent a winning state.

To find the number of winning opening moves (if any) for Becca, we can check each move to see whether it is a winning move. Of course, to do this, we have to investigate the resulting state recursively per the above definition. However, since there are up to two moves per empty cell per state, the naive implementation that recursively counts the number of winning moves for each state may not be fast enough to handle even the 4 × 4 grids in test set 1, so we should optimize it.

Notice that whether a state is winning or losing does not depend on who the player is or on any previous moves. Since the same state may come up multiple times, we should consider memoizing our findings about each state to use in the future. It may be daunting that the number of possible states is intractably large. However, for any given case, there can be at most 16 initially empty cells, each of which can be either filled in by bacteria or not. (After a colony has been placed and has spread, it no longer matters what type it was.) So, we can put an upper bound of 216 on the number of states per case. In practice, there will be even fewer because not all states are reachable.

Moreover, we can save some time by not computing the exact number of winning moves for every state we examine. We only care about this value for an initial state; for every other state, it suffices to determine whether it is winning or losing. If we are investigating a non-initial state's moves and we find a winning move, we can declare the state to be winning, and stop. This optimization alone may be enough to solve test set 1.

Test set 2
When a player makes a legal move, the bacteria spread across the entire width or length of the row or column, up until the line of bacteria reaches reaches the edge of the grid or a cell that is already infected. Therefore, each move creates up to two subproblems that are independent in the sense that a move in one subproblem does not affect the state of the other.

Each subproblem can be expressed as a rectangle contained within the full grid. There are therefore at most O(R2C2) subproblems. How can we use the results of these subproblems to determine the overall winner of the game?

The goal of the game is to force the opponent into a situation in which there is no move they can make that leads them down a path to victory. The game is impartial: both players have access to the same set of moves. It is therefore apt to draw a comparison between Bacterial Tactics and the ancient game Nim, an impartial game with similar types of decisions. The mathematics of Nim are well-studied. A discovery particularly useful to us is the Sprague–Grundy Theorem, which says that any impartial game can be mapped to a state of Nim. Every state in Nim corresponds to a non-negative Grundy number, or nimber, where any nonzero nimber indicates a winnable game.

According to nimber addition, the nimber of a game state after we place a colony is equal to the XOR of the two subproblems. The nimber of a game state before we place a colony is the minimum excludant, or MEX, of the set of possible nimbers after placing colonies. We can therefore solve Bacterial Tactics recursively using the following pseudocode:

let solve(state) be a function:
  let s = Ø
  for each legal colony placement:
    add [solve(first subproblem) XOR solve(second subproblem)] to s
  return MEX(s)
Given this general framework, we can now optimize our implementation.

First, as in test set 1, we can memoize the game states, which are now defined using rectangles of various sizes within the original grid. Note that it is not possible to have bacteria from previous moves in a subproblem, because we always cut the rectangle along the row or column of cells infected by a colony placement. We may also want to pre-compute the nimbers of all sizes of an empty rectangle (no radioactive cells), which is information that can be shared across all test cases.

Second, observe that if it is legal to place a V colony in a cell, then it is also legal to place a V colony in any cell in that column, and similarly for H colonies in a row, within the boundary of the current subproblem's rectangle. We therefore need to check only the rows and columns for legal colony placements, not each individual cell.

Third, we can construct a data structure that allows us to determine whether a colony placement is legal for any row or column in a given rectangle in O(1) time, allowing us to evaluate any game state in O(R+C) operations. For each row and column in the full grid, create an array. Check the cells in the row or column in ascending order, appending the 1-indexed position of the most recently seen radioactive cell, or 0 if a radioactive cell has not been encountered yet. For example, for the row .#..#, the array would be [0, 2, 2, 2, 5]. Suppose we have a rectangle that includes the third and fourth cells of that row. The fourth entry of the array is a 2. Since cell 2 is not in our rectangle (we have cells 3 and 4 only), we can conclude that it is safe to place an H colony in this row of our rectangle. This data structure can be pre-computed for each test case in O(RC) time.

To summarize, there are O(R2C2) subproblems, and each subproblem takes O(R+C) operations. If we let N be max(R, C), this leads to O(N5) total time complexity, sufficient for test set 2. Less efficient solutions might still pass, depending on their implementations.

Round 2 2019 - Code Jam 2019

New Elements: Part 1 (8pts, 14pts)

Test set 1
In the first test set, there can be at most six molecules, so we can easily check all 6! = 720 possible orderings of them. The nontrivial part is figuring out how to check an ordering. We cannot check all possible atomic weights, since they can be arbitrary integers, and those integers might need to be quite large (since the Ci and Ji values can be as large as 109).

For each ordering of molecules, we must determine whether there is at least one valid pair of atomic weights wC and wJ for Codium and Jamarium, respectively, such that the molecules are in strictly increasing order of molecular weight. We do not need to find particular values; we only need to show existence or non-existence.

Let k = wJ / wC. Then, if wC and wJ are valid for our ordering, we can substitute wJ = k × wC in the requirement for any pair of molecules (Ca, Ja), (Cb, Jb) such that a < b:

Ca × wC + Ja × kwC < Cb × wC + Jb × kwC

If Ja = Jb, then this reduces to Ca < Cb, which is a simple check; if it is false, then no set of atomic weights is valid, and we can stop checking this ordering. Otherwise, rearranging some terms and dividing, our expression becomes:

(Ca - Cb) / (Jb - Ja) < k, if Ja < Jb, or
k < (Ca - Cb) / (Jb - Ja), if Ja > Jb.

So, we can start by assuming that k can take on any value in an infinite range. Then, whenever we check a pair of molecules, we get a new (non-inclusive) upper or lower bound on the value of k, and we can update one end of the range accordingly. If we find that constraints force the range to be empty (e.g., because the upper end is forced to be smaller than the lower end), then no valid set of atomic weights exists. Otherwise, at least one does. Notice that even though the atomic weights of Codium and Jamarium must both be integers, any rational value of k corresponds to some pair of integers.

This is a quick set of checks, and it is even quicker if we realize that we only need to check consecutive pairs of molecules. However, as in any problem involving comparisons of fractional quantities, we must be careful to compute exact values instead of using floating-point approximations.

Test set 2
Considering the solution of Test set 1 in reverse can lead us to a solution for Test set 2. If we consider two arbitrary molecules with indices a and b, they can appear in two possible orderings. If one of them is impossible according to the math above, it means that the other is the ordering of the molecules for every possible assignment of atomic weights. If that's the case, we can ignore the pair of molecules (a, b). However, if both orderings are possible, we obtain a range of valid values for the ratio k of the form (0, Ra,b) for one ordering and a range of the form (Ra,b, +∞) for the other, where Ra,b = |(Ca - Cb) / (Ja - Jb)|. That means, for all orderings that correspond to atomic weights that yield a ratio strictly less than Ra,b, the molecules a and b appear in a specific relative order, and for all orderings that correspond to values yielding a ratio strictly greater than Ra,b, the molecules a and b appear in the other relative order. If the ratio is exactly Ra,b, the two molecules weigh exactly the same, so no ordering is valid.

By the previous paragraph, if we consider a function f from ratios into orderings, f is a piecewise constant function that is undefined at ratios Ra,b for any pair a, b and is constant in any interval that doesn't contain any such ratio. Moreover, the image of f over (0, Ra,b) consists of orderings with molecules a and b in one relative order and the image of f over (Ra,b, +∞) consists of orderings with molecules a and b in the other relative order. Therefore, no two elements in f's image are the same. The number of elements in f's image, which is the answer to the problem, is the number of different non-zero Ra,b values, plus 1. Notice that it is possible that Ra,b = Rc,d for different molecules a, b, c, d, and we need to count the number of unique values.

The algorithm to count the number of different values that are equal to Ra,b for some a, b is immediate: try every pair of molecules, calculate the ratio as explained for the Test set 1 solution (if there is one), and add it to a set of fractions to discard non-uniques. This algorithm takes a quadratic number of insertions into a set, which makes it quadratic or barely above quadratic overall depending on the set implementation that is used.

Round 2 2019 - Code Jam 2019

Pottery Lottery (23pts)

A simple (but incorrect) approach
Our tempting first thought might be: why not just put one of our tokens in each vase? Then we can sit back and just wait to win, no matter which vase gets chosen. Who needs the other 80 days?

The flaw in this reasoning is that if multiple vases are tied for having the fewest tokens, there is no winner. A quick simulation can show us that there is only a unique winner around 57% of the time. So this approach has no hope of passing, and we have to work harder!

A slightly more complex (but also incorrect) approach
It might still seem that we can succeed without inspecting any vases. What if we just choose one vase up front — without loss of generality, we will pick vase #20 — and try to make it the winning vase? Over the course of the first 99 nights, we distribute fake tokens among the other 19 vases, as evenly as possible (to prevent any one of those vases from becoming more of a threat than the others). The labels on these fake tokens do not matter, since we are banking on vase 20 winning. Then, on the 100th night, we place our token in vase 20. Since each of the other 19 vases will be burdened with 5 (or maybe even 6) extra tokens, the odds are good that vase 20 will have the fewest, right?

Unfortunately, those odds aren't so good. Again, we can write a quick simulation to verify that our strategy will only succeed around 53% of the time — it's even worse than the strategy above! The average "burden" that we add to the other vases just isn't large enough compared to the random variation in the number of tokens per vase.

A better approach
We will need to take advantage of our inspection ability, but when should we use it? There is not much value in inspecting early on in the lottery, since relatively few tokens will have been placed. On the other hand, if we inspect too late, we may not have enough nights left on which to react to whatever information we discover.

We can devise a version of our first approach that also incorporates inspection. In this approach, V and N are parameters that we will have to discover:

Choose to "give up on" the first V vases. We will assume that none of these will win, and so the labels on the tokens we add to them will not matter. Spend the first N nights sabotaging them (evenly).
Spend the next 20 turns inspecting every vase, in order.
Based on the results of our inspections, choose the vase with the fewest tokens (out of the remaining 20-V vases) to be our candidate winning vase.
On each of the remaining 99 - 20 - N nights, choose a vase (other than our candidate vase) with the smallest number of tokens in our inspection results, and add a token to it. Then, update the inspection results to reflect this.
On day 100, add our own token to the candidate vase.
We might worry that our inspections are only snapshots in time that grow stale. By the time we inspect vase 20, our estimate for vase 1 is already out-of-date — what if more tokens have been added to it since then? To compare the estimates a little more fairly, we could adjust them by the expected number of tokens that have been added since the night of the estimate (i.e., the number of additional nights, divided by 20). This adjustment will be at most .95, which is less than a single full token, so the adjustments could only possibly be used to break ties among vases with the same number of tokens... and we can get the same effect by just choosing the vase with the highest-numbered ID in the event of a tie.

It is not too hard to simulate this process, so we can try out different values of V and N to see what yields the best results, and find that V = 14, N = 60 works about 95% of the time. That gives us, per the binomial distribution, about a 99.96% chance of solving at least 225 out of 250 cases correctly. Since the problem only has one Visible test set, we should just submit and hope for the best!

Can we do even better?
If we want to experiment further, there are even better solutions out there! Here is a variant of the above strategy; it succeeds over 98% of the time:

Days 1-60: Put 4 tokens into each of vases 1-15.
Days 61-80: Inspect every case. Find the two vases with the smallest number of tokens (breaking ties in favor of higher-numbered vases, as we will throughout this strategy), and designate them as candidates.
Days 81-94: Greedily put a token into the non-candidate vase that has the smallest number of tokens, per our inspection results. Update those results.
Days 95-96: Inspect the two candidates again and commit to the one with fewer tokens.
Days 97-99: Sabotage the other candidate.
If we hang onto two candidates, it is less likely that both of them will accumulate a lot of tokens via bad luck during the second sabotage phase. We benefit from delaying our decision point for as long as possible, while still leaving enough time to deal with our runner-up.

Round 2 2019 - Code Jam 2019

New Elements: Part 2 (10pts, 16pts)

Let wC be the atomic weight of Codium and wJ be the atomic weight of Jamarium according to the rules given in the problem statement. Let ΔCi equal Ci + 1 - Ci and ΔJi equal Ji + 1 - Ji, for all 1 ≤ i < N. As in our analysis for New Elements, Part 1, we have:

-ΔCi / ΔJi < wJ / wC, if ΔJi > 0.
wJ / wC < -ΔCi / ΔJi, if ΔJi < 0.
-ΔCi × wC < 0, if ΔJi = 0.
Therefore, we can get the lower bound and upper bound of wJ / wC just by looking at consecutive indices. We can initially set the lower bound (let us represent it with the reduced fraction LN / LD) to be 0 and the upper bound (let us represent it with the reduced fraction UN / UD) to be ∞. We update either LN / LD or UN / UD for each pair of consecutive indexes, just as in our analysis from Part 1.

Once we have LN / LD and UN / UD, we want to find a rational number wJ / wC such that LN / LD < wJ / wC < UN / UD. If LN / LD ≥ UN / UD, then there is certainly no solution. Otherwise, there must be at least one solution; for example, the mediant (LN + UN) / (LD + UD) is certainly between the bounds. However, the problem asks us to minimize wC and wJ (first wC, and then wJ).

Test set 1
Since ΔJi ≤ 99 in this test set, we get LD + UD ≤ 198. Therefore, we know that a solution with wC ≤ 198 exists. We can try all possible values from 1 to 198 as wC. For each choice of wC, we can derive the smallest wJ such that LN / LD < wJ / wC, and then we can check whether wJ / wC < UN / UD.

Test set 2
For each integer C (from 1 to LD + UD), we can check whether there is a rational number that is strictly between LN / LD and UN / UD, and has a denominator that is not more than C. To do that, we can find a rational number with denominator not more than C closest to the average of LN / LD and UN / UD. This is because all rational numbers that are strictly between LN / LD and UN / UD are closer to the average of LN / LD and UN / UD than all rational numbers that are not strictly between LN / LD and UN / UD. We can do so by using a library function like Python's fractions.limit_denominator, or by implementing our own approximation using continued fractions.

Once we can solve the problem given in the previous paragraph, we can use binary search to find wC as the smallest C such that a rational number with denominator not more than C, and strictly between LN / LD and UN / UD exists. Just as we did for the previous test set, we can derive the smallest wJ such that LN / LD < wJ / wC.

Round 2 2019 - Code Jam 2019

Contransmutation (7pts, 16pts, 6pts)

Test Set 1
In Test Set 1 everything is small. One possible solution is to simulate the process. There is never a reason to not convert elements other than lead, so we should just apply the transformations over and over until no more lead can be created. If after a long time the amount of lead keeps growing, that means the total is unbounded. In the Test Set 2 section we do a formal analysis on bounds, but for this one, using an intuitive definition of "a lot" should suffice. If the amount is bounded, 1 gram of a metal can transform into at most 512 grams of lead, since it can only go through 9 splits before getting into a cycle. That means if we ever see more than 5120 grams of lead, the amount is unbounded.

However, the first example shows that we need to do a little more. What if lead itself can be used to get more lead? If that's possible, and we can get any lead at all, the final amount is definitely unbounded. If it is not possible, then we have no reason to ever consume lead. One way to check is to do the simulation above starting with 1 gram each of the two elements that lead is turned into. If that leads to 0 or 1 grams of lead, then lead cannot be used to get more lead. Otherwise, it can.

Test Set 2
Test Set 2 is a lot larger, so a hand-wavy analysis and implementation will not work. However, we can use the same idea above more carefully. Since each metal takes at most M steps to turn into lead, we only need to do M iterations of a simulation, where a simulation involves going over all non-lead metals and converting all grams of them. This takes O(M2) time. To prevent the numbers from growing too big, we can simply do this process twice: the first pass creates as much lead as possible from non-cyclical sources, so if the second pass creates even more, then it must come from cyclical sources that ultimately yield an unbounded amount of lead. The second part, to check for lead creating more lead, is the same as before.

Notice that the result does not fit in 64 bit integers, though, and doing modulo all the time as usual doesn't work right away, since we cannot afford to equate "no X" with "109+7 grams of X" if X has the capability of creating unbounded amounts of lead (with X being lead itself, or some other metal). There are multiple ways to get around the problem, including using big integers, storing an additional bit for each result that represents whether it is an actual 0 value, and doing calculations modulo 109+7 × K for some large random prime K until the very end.

There are other solutions that only work for Test Sets 1 and 2, that are less efficient implementations of the solution for Test Set 3, that is explained below.

Test Set 3
For Test Set 3, the solution looks quite different but it is actually similar. We check for cycles that may generate unbounded lead, and if there are none, we can do the simulation with a single iteration, ordering the metals appropriately. All of those things can be done in linear time, and we describe how below.

One useful thing to notice early on is that if we can determine that the amount of lead we can produce is bounded, the only formulas that are worth using are those that can eventually (perhaps in conjunction with other formulas) produce lead. Moreover, if the amount of lead is bounded, there is no point in using lead as an input to any formula, because even in the best case, every unit of lead will turn into one unit of lead and one unit of something else that cannot be converted to lead. (Otherwise, we could simply apply that series of formulas and generate as much lead as we want).

These observations imply that we can only get an unlimited amount of lead if some metal can produce itself and lead at the same time. In other words, there must be a series of formulas that takes one unit of some metal X and turns it into one unit of X and one unit of lead. In the process, we may end up with some other metals as well, but those don't matter. Notice that if that metal X directly produces metals A and B, then we require that either A produces X and B produces lead, or vice versa. This is because even if X is lead itself and either of A or B is also lead, without considering the other result, we are just destroying 1 unit of lead to get 1 unit of lead, so the total amount of lead doesn't change. In other words, in the unbounded case, after applying each formula, we need to work on both of the outputs, not just one of them.

Let Q be a graph in which each metal is represented by a node, and there is an edge from node u to node v if, by consuming 1 unit of u, we can produce 1 unit of v. Notice that if a metal i is initially not present (that is, Gi = 0), and it is not reachable in Q from some j with Gj > 0, then it is impossible to produce metal i. Therefore, we can remove any such metals at the outset. In what follows, we assume that Q contains no such metals.

Let Q' be the transpose of Q, and let L denote the node representing lead. Then, traversing Q' from node L defines a subgraph Q'L, where every node is a metal that can produce lead after a series of transformations. Now, let Pu be the number of simple paths (paths without cycles) from L to any other node u in Q'L. If the amount of lead is bounded, then Pu is the number of units of lead we can get from every unit of u. This follows from the fact that every (reversed) edge is a valid transformation where we start with 1 unit of some metal and produce 1 unit of some other metal, so along the path, we are using the results of previous transformations and not the very first unit of metal we started with. For example, let Q'L be the following graph:

1 → 2
1 → 3
2 → 4
3 → 4
Which means the valid transformations are:
4 turns into 2 and 3
2 turns into 1 and something else
3 turns into 1 and something else
There are 2 simple paths from 1 to 4, namely, 1 → 2 → 4 and 1 → 3 → 4. Now, 1 unit of metal 4 can be turned into 2 units of lead (1) as follows:

Turn one unit of 4 into one unit of 2 and one unit of 3
Take the one unit of 3 from step 1 and turn it into one unit of 1 and one unit of some other metal
Take the one unit of 2 from step 1 and turn it into one unit of 1 and one unit of some other metal
On the other hand, notice that the nodes that form a strongly connected component (SCC) in Q are a set of metals that can produce each other. More formally, if u and v belong to the same SCC, we can produce 1 unit of v from 1 unit of u, and vice versa. With these definitions, we can say that the amount of lead is unbounded if, for some node u in Q with v1 and v2 being the two outputs of node u's formula, either:

v1 is in Q'L and v2 and u are in the same SCC, or
v2 is in Q'L and v1 and u belong to the same SCC.
Otherwise, there is a limit on the amount of lead we can get, which can be computed by multiplying Pu by the initial number of units Gu for each u in Q'L.

We can compute all the SCCs in O(M) time using Tarjan's algorithm or Kosaraju's algorithm , since both the number of nodes and the number of edges in the graph are linear on M. Walking over the graphs defined above also takes O(M) time, and so does checking the unbounded condition. We can also compute all Pu in O(M) time by taking advantage of the fact that if the amount of lead is not unbounded, then there are no cycles in Q'L. (If a cycle existed in Q'L, then by reversing the edges, we get at least one metal for which the unbounded condition holds.) Therefore, the nodes on Q'L can be topologically sorted. That means we can compute a function F from vertices to the number of simple paths in linear time, using dynamic programming with this recursive definition:

F(L) = 1
F(u) = sum(F(v)) for each v, such that there is an edge from v to u in Q'L
Since the number of edges in every graph described above is also O(M), the whole problem can be solved in linear time.

Round 3 2019 - Code Jam 2019

Zillionim (1pts, 5pts, 6pts)

There may not be a zillion possible solutions to this problem, but there are more than we can discuss in this analysis! We'll only touch on a few here, some of which work, and some of which don't. You're welcome to share yours in the Code Jam Google Group!

Random play
What if we always choose a move uniformly at random from among all available moves, just like the AI does? Intuitively, since the overall number of coins is large relative to the size of any one move, going first or second is not very important (given that in this case, both sides are playing randomly and not using any strategy), and every game is close to a fair coin flip. Suppose that we have a 50% chance of winning a game with this strategy; then, per the binomial distribution, we have only about an 0.0004% chance of winning 300 or more games out of 500. So this approach won't even get us the 1 point for test set 1!

A mirror strategy
If we were allowed to go first in this game, we could guarantee a win using the following strategy. Imagine a "center line" drawn between the (1012 / 2)-th and ((1012 / 2) + 1)-th coins, dividing the coins into two regions of equal size. On the first turn, we could take the group of 1010 coins bisected by this line — that is, the coins numbered from ((1012 - 1010) / 2) + 1 to ((1012 + 1010) / 2). Then, after every move by the opponent, we could make the same move, but reflected across this center line. For example, if the opponent were to take the 1010 coins starting from coin number 2, we would respond by taking the group of 1010 coins starting from the next-to-last coin and going left. We would always be guaranteed a move, by symmetry, so the opponent would have to eventually lose.

Sadly, we cannot move first, but we can try to adapt this strategy by just making the mirrored version of the opponent's move. This will always be possible unless the opponent makes a move that crosses the center line, in which case we cannot mirror it. But if the opponent happens to move close to the center line without crossing it — specifically, taking a coin fewer than 1010 / 2 coins away from the center without crossing the center — then they will ensure their own defeat. If they do happen to make a move that crosses the center line, we can abandon our strategy and move randomly.

We can run a local simulation and find that this strategy does better than randomness, winning around 57.5% of the time. Unfortunately, this only gives us about a 14% chance of passing test set 1. Moreover, because our moves depend on the judge's moves, we cannot easily tweak the judge's randomness; we only get some control over that once the judge has made a move that hurts our strategy. So, all in all, it's probably best to abandon this approach.

A 2 × 1010 strategy
Let's call a remaining set of (at least 1010) contiguous coins a "run". Observe that a "run" of exactly 2 × 1010 remaining coins can be very useful for us. We have the option of taking the first 1010 or the last 1010 coins from that run, and leaving a run of exactly 1010 coins (and therefore one possible move) behind. On the other hand, if we take any other group of 1010 coins from that run, we will leave no moves behind, "destroying" the run. Also notice that if the AI happens to move within this run, it will virtually always make the second type of move; the odds of it happening to choose the first or last 1010 coins are negligible.

Because of this, a run of exactly 2 × 1010 coins lets us control the parity of the game. Suppose, as a thought experiment, that the only remaining runs are of exactly 2 × 1010 coins each, and that it is our turn. If the number of remaining runs is odd, we can move in a way that destroys one of the runs, and then the AI will do the same, and so on until we leave the AI with no moves. If the number of remaining runs is even, we can take the first 1010 coins from one run, leaving the other 1010 behind as a smaller run. Now the AI is in the same bad "even number of remaining runs" situation that we were just in.

We can set up a situation like this by making many moves early on that leave behind runs of exactly 2 × 1010 coins. For example, we can repeatedly choose the largest remaining run of size 3 × 1010 or greater, and start from the (2 × 1010 + 1)-th coin from the left in that group, leaving a run of exactly size 2 × 1010 behind (in addition to any leftover piece to the right of our move). Then, as long as there are still runs larger than 2 × 1010 but smaller than 3 × 1010, we can use our moves to destroy them by moving in their centers. Our goal is to eliminate all runs larger than 2 × 1010 before the AI randomly destroys all of our runs of 2 × 1010. Intuitively, we do not need to worry too much about this, since the AI is more likely to move within some remaining larger runs than within our last remaining run of 2 × 1010 runs, so it will usually be helping us out! Once all runs are of size 2 × 1010 or smaller, and we have at least one run of size 2 × 1010, we have imposed the near-lockdown situation described above.

We hope you will forgive us for not trying to calculate an exact success probability for the above strategy, but one can use a local simulation to show that it wins, e.g., all 100000 of 100000 games. The chances of it losing more than one of 500 games are vanishingly small... and even if the worst somehow happens, in this case, we do have some control over the overall randomness, and we can try again with a slight tweak.

Other parity-based strategies
In Round 1C this year, we had Bacterial Tactics , another problem about a turn-taking game. You may recall that the analysis brought up the Sprague–Grundy theorem ; can we use a similar parity-based strategy here?

As in our 2 × 1010 strategy, we can try to keep the number of remaining runs even for the AI by moving in the middle of a run if there is an even number of runs, or otherwise taking the left end of some other large run. Empirically, this strategy solves TS1, and it even solves TS2 if we always take the largest remaining run. It ends up being similar to our best strategy described above, even though it does not allow for such fine control.

We can even achieve a perfect solution for this problem by exhaustively calculating Grundy numbers and storing them using run-length encoding! However, it is possible to arrive at the 2 × 1010 idea above, which is good enough, without knowing anything about this theory.

Round 3 2019 - Code Jam 2019

Pancake Pyramid (5pts, 17pts)

We can make a couple of useful observations at the outset. First, if we have an interval of length 1 or 2, we do not need to add any pancakes for it to have the pyramid property, so we can ignore the restriction of length ≥ 3 in the problem. Second, for any interval, the "peak" in the optimal answer is the largest stack in the original interval (we leave this for you to think about). If there are multiple largest stacks in an interval, we will take the leftmost largest stack as the peak.

O(S3) — Too slow
For each of the ((S + 1) choose 2) intervals, determine where the peak will be located once the interval is turned into a pyramid. Once we know where the peak will be, we have two smaller problems: we need a non-decreasing sequence to the peak's left and a non-increasing sequence to the peak's right. To compute how many pancakes we need in the non-decreasing interval, we may simply sweep from the leftmost point and add pancakes until no stack in the interval to the left of the i-th stack is strictly taller than the i-th stack. By maintaining the running maximum as we sweep, we can compute the number of needed pancakes needed per interval in O(S) operations. Since there are O(S2) intervals, this algorithm requires O(S3) operations in total.

O(S2) — Test Set 1
The ideas above lay the framework for a quicker solution. Instead of independently recomputing the number of pancakes needed to make an interval non-decreasing (or non-increasing), we can use the results from other intervals. Say we know the index of largest stack of pancakes in the range [L, R] (call this index M[L, R]) and the smallest number of pancakes needed to make the interval [L, R] into a non-decreasing sequence (call this number X[L, R]). We can compute both M[L, R+1] and X[L, R+1] in O(1) time since the height at M[L, R+1]=max(PM[L, R], PR+1) and X[L, R+1] = X[L, R] + (PR+1 - PM[L, R+1]). Similarly, we can store Y[L, R], which is the smallest number of pancakes needed to make the interval [L, R] into a non-increasing sequence.

For any interval [L, R], the smallest number of pancakes needed to turn the interval into a pyramid is simply X[L, M[L, R]] + Y[M[L, R], R]. The precomputation takes O(S2) time and memory and the second step uses O(1) time per interval to compute the answer. Thus, in total, this is O(S2).

O(S log S) — Test Set 2
The above strategy will be too slow and require too much memory to handle the larger bounds. For this test set, we will still use the same underlying idea of needing the number of pancakes to make an interval non-decreasing or non-increasing. But instead of computing X and Y, we will compute cumulative values: define X'[L, R] = X[L, R] + X[L+1, R] + ... + X[R, R] and Y'[L, R] = Y[L, L] + Y[L, L+1] + ... + Y[L, R]. Now, instead of focusing on the left and right endpoints, we will base our strategy on the peaks of the intervals.

Initially, we do not know the value of X' or Y' for any interval. In our analysis, we will assume that we only know the X' and Y' values for maximal intervals. That is, if two intervals are side-by-side, we will merge them (we will never have intersecting intervals). For example, if we know X'[L, k] and X'[k+1, R], we will merge these together into X'[L, R] and forget about X'[L, k] and X'[k+1, R]. The full process of how to merge is explained below. In particular, this means that any given stack is in at most one known interval for X' and one known interval for Y'.

We will process the peaks from smallest to largest. When we process the i-th stack, we are only interested in intervals that have stack i as their peak. If X'[L, i-1] is computed for some value of L, then L must be the furthest left index such that PL, PL+1, ... , Pi-1 are all less than Pi (since we are processing the stacks from smallest to largest). Similarly, if Y'[i+1, R] is computed for some value R, then R must be the furthest right index such that Pi+1, Pi+2, ... , PR are all at least Pi. If such L and R exist, then we can compute the number of pancakes needed over all intervals that have i as their peak:

X'[L, i-1] * (R - i + 1) + Y'[i+1, R] * (i - L + 1)

Note that if we don't know X'[L, i-1] for any L, then Pi-1 ≥ Pi, so i cannot be a peak with any interval that includes both i-1 and i. The answer for those intervals will be computed later when we consider stack i-1 as the peak (and likewise for stack i+1 if we do not know Y'[i+1, R] for any R). In these cases, we may use X' = 0 (or Y' = 0). Note that since our intervals are maximal and we are computing from smallest to largest, PL-1 ≥ Pi (similarly, PR+1 > Pi).

Now we want to merge X'[L, i-1], X'[i+1, R] and stack i into X'[L, R]. We will do this in two steps. First, note that X'[L, i] = X'[L, i-1]: since we are processing the stacks from smallest to largest, Pi can be freely added as the right endpoint of any non-decreasing sequence in this range. Now let's merge X'[L, i] and X'[i+1, R]. Observe that X'[L, R] sums over intervals that end at the R-th stack. If an interval starts in the range [i+1, R], then it is already counted in X'[i+1, R]. If an interval starts in [L, i], then we can start with some sequence in [L, i], but since Pi is the peak, every value on the right must be exactly Pi. The number of pancakes needed to bring every value in [i+1, R] up to Pi can be computed in O(1) time using cumulative sums. Thus, the full merge is:

X'[L, R] = X'[i+1, R] + (X'[L, i-1] + Pi × (i-L+1) × (Pi+1 + ... + PR))

The Y' values can be computed similarly. In terms of complexity, we need to sort the stacks at the beginning in O(S log S) time and the remaining steps take constant time per peak, so O(S) overall. This means the algorithm takes O(S log S) time.

O(S)
Although the O(S log S) solution is fast enough to solve test set 2, an O(S) solution is also possible! We present a sketch of the idea here, which can be read independently of the solution above.

To make things easier, we pretend that the stacks all have different heights. Each time we compare the heights of two stacks of identical height, we break the tie by assuming that the stack with the larger index is higher.

Let us think through the solution starting from the end. For each stack, we want to compute the pyramidification cost for all the ranges in which this stack is the highest; in such cases, it will be the peak of the pyramid. Then we can sum all of those values for all of the stacks, and that will be our overall result.

In order to compute those pyramidification costs, we can compute the following for each stack s:

The nearest higher stack to the left of s (possibly an infinitely high "guard" stack appended to the beginning of the sequence); call this the "left blocker". Let DL be the absolute distance (in stacks) from s to the left blocker.
The nearest higher stack to the right of s (or guard stack) stack added after the sequence); call this "right blocker". Let DR be the absolute distance (in stacks) from s to the right blocker.
The pyramidification cost for all the ranges that end with s and in which s is the highest; call this "left pyramidification cost" CL.
The pyramidification cost for all the ranges that start with s and in which s is the highest; call this "right pyramidification cost" CR.
Then the pyramidification cost for s can be calculated as CL × DR + CR × DL.

Now, how can we compute CL and DL for each stack? (The solution is analogous for CR and DR; the only major difference is that the inequality used to compare stacks is strict on one side and non-strict on the other, due to tie-breaking.)

We can traverse the stacks from left to right, keeping a Stack structure (capital S to avoid confusion) X that remembers the longest decreasing sequence of stacks ending on the current stack. We iterate through the stacks and when seeing a stack s we consume from X all stacks that are lower than s, adding their contributions to the current left pyramidification cost. Let t' = s at the beginning, and later t' = the previously consumed stack. The contribution of a consumed lower stack t is the number of pancakes missing between t and t' (calculated in constant time, if we precompute the cumulative sum of stack sizes) multiplied by the distance to the left blocker of t'. The left blocker of s is the first stack we can't consume from X, because it's higher than s. Once we're done with s, we add s to X and keep going.

Round 3 2019 - Code Jam 2019

Datacenter Duplex (10pts, 13pts)

We can start by modeling the problem as a graph where each cell of the input matrix represents a node. Orthogonally adjacent cells with the same label are connected by edges, and we can optionally add edges connecting diagonally adjacent cells with the same label. The goal is to end up with exactly two connected components.

Test set 1
Test set 1 can be solved with dynamic programming, iterating over columns. When considering the i-th column from left to right, we can summarize the state of the connections we have added in the submatrix S of columns 1 through i by recording: (1) whether we have seen (at least one cell of) each of A and B so far, and (2) whether any two cells on the i-th column that contain the same label but are not orthogonally adjacent are connected by a path in S. Then, we can use brute force, and consider all possible choices of how to connect each of the up to 3 cell corners between columns i and i+1 (there are at most 4 rows in Test set 1). There are 3 possibilities for each corner, \, / and no connection, but it can be reduced by noticing if at least one of the connections is valid, it is always optimal to use one, which reduces the number of choices per corner to 2 at the most. If at any point we discover a new isolated A component that cannot be connected to previously seen As, we have failed, and same is true for B. If we finish, we can then use a second pass to reconstruct one choice of diagonal connections that led to solving the problem.

This seemingly simple idea has several technical details that we are omitting here. Some of them can be simplified by starting and ending the process in columns that contain both As and Bs and preprocessing to check if any leftmost or rightmost columns that contain only one type already disconnect the other type.

The overall time complexity of this solution is exponential in R, because we are trying all combinations of O(R) corners and recording the connected status of O(R) cells at each state, and linear in C, with the exact formula depending on how the technical details are handled. As long as the base of the exponential factor is not too large, this is fast enough to pass within the time limit.

Test set 2
The key observation is: after we decide to add some edges (diagonal adjacencies), two cells c and d containing the same label X end up separated regardless of any future decisions if and only if one of the following two conditions holds:

1. There is a cycle of cells labeled Y ≠ X in the graph, with one of c and d being inside the cycle and the other one being outside.
2. There is a path of cells labeled Y ≠ X in the graph with the first and last cells of the path being border cells of the matrix, with c and d being on opposite sides of the path.
Let us use G to denote the graph with no diagonal edges added and H to denote the final graph with all edges added. Consider the border of the matrix. Suppose that it contains two cells c and d labeled X that are not connected in G. Since they are not connected in G, that means, going around the border, there are cells e and f labeled Y ≠ X such that e is in between c and d in clockwise order and f is between c and d in counter-clockwise order. That means neither c and d nor e and f can be connected in H with a path of only border cells. Therefore, if c and d are connected in H, the path connecting them disconnects e and f, and vice versa. So, by the second condition, having two cells on the border with the same label that are disconnected in G results in an impossible case.

Notice that there is never an incentive to generate an edge from a diagonal adjacency to connect two cells that are already connected. Per the paragraph above, if a case is possible, then all border cells with the same label are already connected in G. Therefore, if an algorithm never adds an edge from a diagonal adjacency that connects two cells that are already connected, it will never generate a path between two border cells. Additionally, if we never connect cells that are already connected, any cycle in H is a cycle that was already present in G, so again, if we end up in a disconnected situation, the case must have been impossible from the start, before we added any connections.

These observations directly suggest the following algorithm: consider each diagonal adjacency and generate an edge if and only if it will connect two previously disconnected cells. If both choices work, we can choose either, since we already established that the decision of not connecting previously connected cells is enough to guarantee an algorithm will not generate an H with more than 2 connected components when a different one with exactly 2 was possible. After this process, check if there are 2 connected components or more than 2, and print the results.

If we implement the algorithm above using a union-find to maintain the connected components, we need O(R × C) checks for whether two cells are connected and O(R × C) connections (joins). Since union-find provides almost constant amortized time for both operations, the overall time complexity of the algorithm is quasilinear.

An equivalent algorithm is to calculate the connected components of G first and then use shortest paths to join any two components of the same label until we cannot do it anymore. Since shortest paths cannot create new cycles, an argument similar to the one above proves that this solution is also correct. This solution can be implemented in linear time if the partial minimum path tree graph is reused for each new connection we need.

Round 3 2019 - Code Jam 2019

Napkin Folding (4pts, 39pts)

Test Set 1
For Test Set 1, we want to find exactly one folding segment such that when we fold the napkin across the segment, the regions on either side of the segment line up perfectly. Because the folding segment must split the napkin into two symmetric regions, we can show that each of the folding segment's endpoints either coincides with a vertex of the polygon defining the napkin, or is the midpoint of an edge of the polygon. If we tried to make a folding segment connecting any other points, the two parts of the split edge could not possibly line up perfectly. Thus, all we need to do is try every line segment connecting any pair of points that are vertices or midpoints of the polygon's edges. Then, for each potential folding segment, we check whether it is valid by making sure it is fully contained within the polygon and that the two regions it creates are symmetric across the folding line segment.

Checking for intersections between line segments can be done by using only integers. Reflecting points across a line or taking the midpoint of a side normally could produce points with non-integer coordinates. But we can scale up our points initially such that if the reflection were to produce a point with non-integer coordinates, it could not possibly be one of the valid endpoints for folding line segments. Of course, we can also choose to work with fractions.

With N points in our polygon, we have 2×N points to choose from for our folding segment's endpoints. Each potential folding segment can be checked in O(N) time. Because there are O(N2) possible segments to check, this results in an O(N3) overall time complexity.

Note that in order to check for symmetry across a folding segment, we cannot just check that the sets of vertices of the polygons defining each region are symmetrical. Rather, we must show that for every edge of the polygon of one region, there exists exactly one edge in the polygon defining the second region which is symmetric across the folding line segment. In other words, the order in which the points appear on each side matters.

Finally, we can note that if a given line is indeed an axis of symmetry of the polygon, then it is guaranteed that it doesn't intersect the polygon more than twice. This means that we don't need an explicit check in the code for the folding segment not to intersect the polygon outside its endpoints. A similar simplification can be applied to the solution of Test set 2, coming up next.

Test Set 2
Since we want to draw a neat folding pattern of K-1 non-intersecting line segments, we must partition our napkin into K regions of identical size and shape, all of which are symmetric with other regions sharing common line segments. Each of these K regions is a polygon with edges that are made up of folding line segments and/or edges or parts of edges from the polygon defining the napkin. It can be shown that at least two of these regions are only adjacent to one line segment in our folding pattern, with the other edges that define the region's polygon coming from the original polygon. Let's call these regions corner regions.

If we have a corner region, we can reflect that region across its one folding line segment to find the region that must be adjacent to it. If we keep reflecting these regions across the edges of their polygons, being careful not to create intersecting regions or leave the polygon defining the napkin, we can reconstruct the neat folding pattern. Thus, every neat folding pattern can be uniquely defined by just one folding line segment connecting two points on the border of the polygon defining the napkin. That segment cuts off a corner region which can be successively reflected to give us the entire folding pattern.

We need to consider pairs of points on the napkin's border defining a folding line segment. For a given candidate folding line segment, we can successively reflect the corner region we form to get the full folding pattern. If we use more than K-1 reflections, or, if after we finish reflecting we don't end up creating the original polygon, we know that the chosen line segment does not give us a neat folding pattern.

Now all that remains is to select all pairs of points on the napkin's border. Clearly we cannot test every pair of points with rational coordinates, because there are infinitely many such points. Rather, we can show that the endpoints of the line segments in our neat folding pattern must be either vertices or points on the polygon's edges that are X/Y of the way between consecutive vertices, where 1 ≤ X ≤ Y-1 and 2 ≤ Y ≤ K (the proof for this is below). Therefore, we can create all of these candidate points and check every pair. With K ≤ 10 and N ≤ 200, there are at most 6400 candidate points for the vertices of the line segments in our neat folding pattern. This puts an upper bound of 64002 on the number of pairs that we might need to check. But, we can reduce this significantly by only considering pairs which create a corner region with an area equal to 1/K of the napkin's area. Every point can pair with at most 2 other points to create a line segment which is fully within the polygon and splits off a corner region with the proper area. Therefore, we only need to check these pairs.

We can check if a single line segment from a pair of points gives us a valid folding pattern in O(N) time if we are careful to stop once a reflection creates a point which is not on the border of one of our polygon's line segments. We can precompute all the valid points and use a hash table for this check. Because all of the points are of the form X/Y × p, where p is a point with integer coordinates and Y is between 2 and K, we can multiply everything by lcm(2, 3, ..., K) at the beginning and then work in integers, dividing and simplifying fractions only for the output.

To summarize, the algorithm requires these steps:

1. Compute all possible endpoints. (O(N × K2))
2. Find candidate segments to create a corner region. (O(N × K2)).
3. For each candidate, fold it up to K-1 times, checking that everything is valid.
There are up to O(N × K2) possible endpoints. If we fix one endpoint P and iterate the other, we can compute the area as we advance, finding the up to 2 segments that have an endpoint in P in O(N × K2) time. So step 2 takes O(N2 × K4) time in total. For step 3, each unfolding requires reflecting the current region and finding new folding segments. All of that is linear in the size of the region, and the sum over the sizes of all regions is at most the total number of endpoints plus 2 × K shared ones, so this takes O(N × K2) time overall. Putting it all together, the algorithm takes O(N2 × K4) time in total. It is possible to make it significantly more efficient than that, but the implementation is complicated enough as it is.

Appendix
To prove that all folding segments endpoints are X/Y of the way between consecutive vertices for 1 ≤ X ≤ Y-1 and 2 ≤ Y ≤ K, we can prove that the number of folding segments that are incident on a non-vertex point of the input polygon is odd or zero. If that's true, let P be an endpoint and Q and R be the two vertices and/or folding segment endpoints that are closest to P towards each side over the same polygon edge E as P. Then, by reflection PQ and PR are equal. By repeating this over E we can see that E is divided into equal length sections by every point that is a folding segment endpoint.

Now we prove that the number of incident folding segments in a non-vertex point of the polygon is odd or zero. Assume that P is a point over an edge E of the polygon with I > 1 of incident folding segments. Let Qi for i = 1, 2, ..., I be the non-P endpoints of each of those segments, in clockwise order. Let Q0 be the reflection of Q2 across PQ1 and QI+1 the reflection of QI-1 across PQI. Notice both those points have to be on E. Now, the I+1 angles QiQi+1 for i = 0, 1, ..., I are all equal. Let Ri be the reflection of Qi across E. Now, because Qi must reflect onto Qi+2, the length of PQ0, PQ2, PQ4, ..., PQI are all equal, and equal to the lengths of PR2, PR4, ..., PRI. Since the angles between two consecutive segments of those are also all equal, Q0Q2Q4...QIRIRI-2...R2 is a regular polygon. All points have rational coordinates because they are either input points, endpoints of folding segments, or reflections calculated from other points with rational coordinates. It is known that the only regular polygon whose vertices have rational coordinates in 2 dimensions is the square, which means I / 2 + 1 + I / 2 = 4, which implies I = 3 is odd.

The following picture illustrates the above proof for the case I = 4. P is the point in the center, and line segments of the same color are guaranteed to be of the same length.

R0 = Q0
Q1
Q2
Q3
Q4
Q5 = R5
R4
R3
R2
R1
A consequence of this proof is that for any non-vertex point on the original polygon, it must be adjacent to exactly 0, 1 or 3 folding line segments.

World Finals 2019 - Code Jam 2019

Board Meeting (5pts, 22pts)

Test set 1
In the first test set, there is only one king. There are various ways for us to deduce its position; we will describe one that requires three queries and is relatively easy to understand.

First, we query (-1000000, 1000000), which is the upper left corner of the board; let A1 be the result. It tells us which J-shaped region of the board the king is in, illustrated as follows starting from the upper left corner:

    0123...
    1123
    2223
    3333
    .   .
    .    .
    .     .
  
Next, if we did not happen to get lucky and find the king, we query (-1000000 + A1, 1000000 - A1). That is, we query the corner of that J-shaped region. Call the result A2; it tells us how far away from the corner the king is, within that region. However, we do not know whether the king is to the left of or above the corner.

Finally, if we have still not found the king, we query (-1000000 + A1 - A2, 1000000 - A1). That is, we guess that the king is to the left of the corner. If the result is 0, then we have found the king; otherwise, we know the king is above the corner, at (-1000000 + A1, 1000000 - A1 + A2).

Test set 2
With more than one king, we find that it is not possible to determine the exact location of each king. For example, we cannot distinguish between a case with kings at (+1, 0) and (-1, 0) and a case with kings at (0, +1) and (0, -1). So it must be possible to answer queries with less complete information about the kings' locations.

The "L∞" metric in the problem, for which we take the maximum of the absolute differences of two coordinates, is a little inconvenient to work with directly, which results in the fairly ad-hoc nature of the test set 1 solution above.

Converting to diagonal coordinates makes things simpler. If a point is at (x, y) in the original coordinates, let (u, v) = (x + y, x - y) / 2 be its diagonal coordinates. Then if the diagonal coordinates of two points are (u1, v1) and (u2, v2), the distance between them is |u1 - v1| + |u2 - v2|. So if the kings are located at diagonal coordinates (ui, vi) for i= 1, ..., N, then the result of a query for the point (u, v) is Σi (|u - ui| + |v - vi|) = (Σi |u - ui|) + (Σi |v - vi|)

We can handle these two sums separately. To compute the first one, we need to know which "downright-sloping" diagonal lines the kings lie on, and to compute the second one, we need to know which "upleft-sloping" diagonal lines the kings lie on.

Now we need a method to compute these diagonals. Consider the point (-2M, 0). This point is far enough to the left that, if we propose this point, the total distance we get will be the sum of the differences in x coordinates. If we propose (-2M, 1), we will get the same answer, unless there is a king at (-M, -M), in which case we will get an answer one higher.

More generally, for a positive integer K, let d(K) for a positive integer K be the difference in the results of proposals of (-2M, K) and (-2M, K-1). d(K) will be equal to the number of kings below the diagonal x + y = -2M + K. We can use binary searches to find the places where d(k) changes value, and the magnitudes of those changes, to find out which diagonals of the form x + y = c contain kings, and how many they contain.

Similarly, by proposing points (-2M, -K), we can find out how many kings lie on the diagonals that go in the other direction. Once we have this data, we are able to compute the sums above and respond to queries from the judge.

World Finals 2019 - Code Jam 2019

Sorting Permutation Unit (5pts, 22pts)

A rotation-based solution
For simplicity, let's assume that none of the arrays we want to sort contain any repeated entries. If there are repeatred entries, we can arbitrarily choose a correct sorted order for them.

To illustrate the sorting algorithm, let's first ignore the constraint on the number of permutations, and use N-1 permutations: the i-th permutation (1 ≤ i ≤ N-1) swaps the i-th element with the N-th element.

For example, when N = 5, we use the following 4 permutations:

5 2 3 4 1
1 5 3 4 2
1 2 5 4 3
1 2 3 5 4
With this setup, we can use the N-th element as a "buffer" to sort the first N-1 elements. The algorithm is as follows:

If the element at position N is not the largest one, swap it to the correct position.
Otherwise, there are 2 cases:
The array is sorted, and we are done.
The array is not sorted, so we swap the N-th element with any element that is at an incorrect position (so that we can continue using it as buffer).
Repeat from step 1.
For example, let's consider the array [30, 50, 40, 10, 20]. We can:

Swap 20 to the correct position: [30, 20, 40, 10, 50].
Now 50 is at the last position, but the array is not sorted, so we swap it with any element that is at an incorrect position, for instance, 30: [50, 20, 40, 10, 30].
Swap 30 to the correct position: [50, 20, 30, 10, 40].
Swap 40 to the correct position: [50, 20, 30, 40, 10].
Swap 10 to the correct position: [10, 20, 30, 40, 50].
The array is now sorted.
Note that this solution uses N-1 permutations and 1.5N operations:

N operations to swap N elements to their correct positions,
At most N/2 to swap the N-th element in case it is the largest. This is because after we swap the largest element with an element at the wrong position, we won't need to do so again on the next step.
Reducing the number of permutations
To fit within the limit on the number of allowed permutations, we can instead use the following 5 operations:

One permutation that swaps elements N-1 and N — the next-to-last and last elements.
4 permutations that rotate each of the elements from 1 to N-1 by 1, 3, 9 and 27, respectively. (Notice that element N remains the same.)
With these 4 permutations, when N ≤ 50, we can rotate the first N-1 elements by any amount (from 1 to N-2) in at most 6 operations. For example, to rotate by 26 = 9 + 9 + 3 + 3 + 1 + 1, we rotate by 9 two times, rotate by 3 two times, and then rotate by 1 two times. To rotate by 47, we use 27 + 9 + 9 + 1 + 1. (Equivalently, we can express any number less than 50 using a base three string with trinary digits summing to 6 or less. The smallest number that would take 7 or more is 53.)
Our new algorithm is similar to the one above.

If the element E at position N is not the largest one, swap it to the correct position as follows:
Apply rotations to the first N-1 elements until the (N-1)-th position is the slot where E should go.
Swap the (N-1)-th and Nth positions, moving E into place (and some other element into the N-th position.)
Otherwise, E is the largest element. We need to temporarily move it out of the N-th position. We can swap in some element that is not already in its correct relative position among the first N-1 positions. We can use the same strategy as above to rotate to that element. If there is more than one element in the wrong relative position, we choose the element such that the amount we need to rotate to get it in place is minimized.
We repeat the above until all of the elements in the first N-1 positions are in the correct relative order. Then, we may need to do some final rotations to put them in the correct absolute positions.
For example, let's consider the array [30, 50, 40, 10, 20]. We will use 3 permutations:

Swap the last 2 elements: 1 2 3 5 4
Rotate the first N-1 elements by 1: 4 1 2 3 5
Rotate the first N-1 elements by 3: 2 3 4 1 5
The algorithm works as follows:

We first want to swap 20 to the relative position after 10:
Rotate the first N-1 elements by 3: [50, 40, 10, 30, 20]
Swap the last 2 elements: [50, 40, 10, 20, 30]
Now we want to swap 30 to the relative position after 20:
Rotate the first N-1 elements by 3: [40, 10, 20, 50, 30]
Swap the last 2 elements: [40, 10, 20, 30, 50]
The largest element, 50, is now at the last position. We need to swap it with some element at the wrong position. In this case, there is exactly one such element: 40.
Rotate the first N-1 elements by 3: [10, 20, 30, 40, 50]
Swap the last 2 elements: [10, 20, 30, 50, 40]
Finally, we want to move 40 to its correct position:
Swap the last 2 elements: [10, 20, 30, 40, 50].
The number of operations we use are:

1.5N operations for swapping (as shown in our first algorithm)
6N operations for rotating the first N-1 elements, to swap them to correct relative positions.
For the case where the largest element is at the N-th position, we need an addition of N operations for rotating. This is because we always rotate the minimum amount, after which the first step will rotate the array until it's back at the same relative position. So in total, the rotations in this step will rotate the array at most one full cycle.
Thus we are using total of 8.5N = 425 operations.
An even tighter solution
For each N ≤ 50, there also exists a set of 4 rotations that allows us to rotate the first N-1 elements by any amount (from 1 to N-2) in at most 4 operations, making use of the fact that rotating by k is the same as rotating by k+N-1. As an example, for N = 50, {1, 3, 12, 20} is one such set.

This yields a solution that uses just 325 operations in the worst case.

A randomized variant
In an alternative solution, instead of four rotations of the other= N-1 elements, we have four random involutions which swap randomly selected pairs of the N-1 elements.

Now we use the same algorithm as in the rotation solution, but instead of rotating elements into place, we use a sequence of the random permutations to move the right element to where it can be swapped with the buffer, and then apply the sequence in reverse to move everything back to where it was.

(We can use a breadth-first search to find the shortest sequence to move any given element into place.)

This process may result in a set of permutations that can't solve the input or can't solve it in few enough permutations, but we can simply retry by choosing a new set of random involutions, as getting a solution with under 450 permutations is highly likely.

World Finals 2019 - Code Jam 2019

Won't sum? Must now (5pts, 22pts)

The fact that we never need more than 3 palindromes is not obvious, and it was proved in a long, complicated paper. Numberphile made a video that explains the basics of the paper's argument. Knowing this upper bound is useful, but is not necessary to solve the problem.

Even if we haven't read every paper on arXiv for fun, we might also reasonably guess that the number of palindromes required is unlikely to be large because there are approximately sqrt(S) palindromes smaller than S, which means that there are approximately S palindrome pairs, which should result in Θ(S) numbers smaller than S that should be representable as sums of palindrome pairs. The number of triples is larger by another factor of sqrt(S), and their sums are all smaller than 3S, which does not leave much space to hide a number that could not be represented by a triple of palindromes, unless many triples add up to the same number.

The first step is to check if S is itself a palindrome. If it is a palindrome, then we are done. In the remaining explanation, we will assume that S is not a palindrome. Checking if a number is a palindrome takes O(log S) steps, one step per digit.

Test set 1
Key insight: There are only roughly 2 × 105 palindromic terms up to 1010. In the analysis below, we will call this number P.

Sum of 2 palindromes?
To check if S is the sum of 2 palindromes, we simply loop through all palindromes, p, less than S and check if S-p is a palindrome. This takes O(P × log S) steps.

Sum of 3 palindromes!
We were told that every number is the sum of 3 palindromes, so we must find such a sum! On first glance, it seems that doing the same as above for 3 palindromes is too slow since there are O(P2) pairs of palindromes to check. However, for all numbers up to 1010, one of the 3 palindromic terms is always at most 404 (which is needed for 1086412253, for example). Thus, as long as we search the pairs of palindromes in a way that searches triples containing a small value first, we will be fine.

Test set 2
Unfortunately, there are as many as 2 × 1020 palindromes less than 1040, so it is hopeless to even iterate through the palindromes. Something quicker will be needed.

S = X + Y?
We will assume X ≥ Y. The first step is to fix the numbers of digits in X and Y. Note that the number of digits in X must be either (1) the number of digits in S or (2) one less than the number of digits in S (otherwise, X + Y < S).

Let's assume that X and Y have a different number of digits (we will deal with the same-number-of-digits case below). The trickiest part about this problem is dealing with the pesky carries that occur while adding. For example, if we want to find 2 palindromes that add up to 2718281828 with lengths 10 and 8, then the first digit of X can be either 1 or 2 (depending whether there is a carry on the most significant digit). Because we don't know, let's try both! First, let's assume that we do not need a carry (so the first digit of X is 2). This also means that we can determine the least significant digit of Y by considering the sum modulo 10 (which also fills in the most significant digit of Y since it is a palindrome).

  ..........        2........2        2........2
+   ........  ->  +   ........  ->  +   6......6
============      ============      ============
  2718281828        2718281828        2718281828
At this point, to fill in the remaining unknown digits, we need a 9 digit palindrome and a 7 digit palindrome that sum to 2718281828 - (2000000002 + 60000006) = 658281820. This is a subproblem and we may recurse (with leading zeroes now allowed). The other option is that there is a carry on the most significant digit. In this case, the first digit of X must be 1.

When the numbers of digits in X and Y are different, there is always at least one unknown digit that we can determine. However, if X and Y have the same number of digits, this is not necessarily true. However, we can combine two simple observations to determine viable options for the first (and last) digits of both numbers. First, we consider the sums modulo 10. This narrows the possible values we may choose. Second, the number of digits in S compared to the number of digits in X (and Y) tells us if we want the sum of the most significant digits to be at least 10 (causing a carry). For example, if S has 5 digits, while X and Y only have 4 digits, then we must provide a carry in order for this sum to work. Similarly, if the number of digits in S and X are equal, then we cannot have a carry, so the sum must be at most 9. Even after these constraints, we still have some options (for example, if we need a sum of 8, we can use 0+8, 1+7, ... , 7+1, 8+0). Note that it doesn't matter* which of these options we take, since the only way they interact with other digits is with the carry.

* - There is one case where it matters: we must avoid creating a leading zero in a number.

Let D be the number of digits in S. There are O(D) possible lengths for X and Y. For each pair of lengths, we must decide if there is a carry on each digit on the left-hand side of S. Depending on implementation, this is O(2D/2 × D) operations. This leads to a total of O(2D/2 × D2) operations. Note that instead of just computing one digit at a time, we can determine the whole "overhang" values. In the example above, we can determine that the first two digits are either 26 or 27:

  ..........        27......72        27......72
+   ........  ->  +   ........  ->  +   65....56
============      ============      ============
  2718281828        2718281828        2718281828
This means that we only need to make D/overhang choices for the carries instead of D/2 choices. Thus, when the overhang is large, those computations do not contribute heavily to the complexity. This reduces the total complexity to O(2D/2 × D) operations.

Sum of 3 palindromes
In the paper listed above, the authors describe an algorithm to write any number as the sum of 3 palindromes is described. This was obviously not needed to solve this problem, but it does require an algorithm to find the sum of three palindromes.

To solve this problem, we will use the fact that there are O(S sqrt(S)) triples of palindromes less than S, which means that each number can be written with an average of O(sqrt(S)) different triples. This allows us to randomly select palindromes as one of our three values, then use the algorithm above to check whether the remaining value can be wrritten as the sum of 2 palindromes.

It is possible that there is a number that can only be represented in one different way as a sum of three palindromes, but we were unable to find any case that was not the sum of triples in many different ways. In fact, we were unable to find an input number such that one of the three numbers was more than 10801 (though, we expect that many such cases exist).

World Finals 2019 - Code Jam 2019

Juggle Struggle: Part 1 (5pts, 30pts)

Test Set 1 (Visible)
We are given a set of 2N points that we are to pair into N line segments such that they all intersect each other. That makes the set of line segments magnificent.

We can extend any line segment S into a full line L that divides the plane in two. In a magnificent set of line segments, since S intersects all other segments, all those other segments have one endpoint on each side of L (no other point can be on L or it would be collinear with the endpoints of S). This means that if we pick a point P in the input, it must be paired with another point Q such that the line that goes through both leaves exactly the same number of other points on each side. This hints at an algorithm: choose a point P, find a Q according to the definition above, pair P and Q, and repeat. However, what if there is more than one such Q?

One way to deal with the problem is by choosing P intelligently. For example, if we choose a leftmost point as P, all candidates for Q end up in the right half of the plane cut by a vertical line through P (with at most one other point possibly on the line). Consider a line rotating clockwise centered on P, going from vertical to vertical again (rotating 180 degrees). At the starting point, there are no points on the left side of the line, because P is leftmost. As it rotates, it will pass over all points, repeatedly adding one point to one side and removing one from the other. Since there are no collinear triples of points, this shift is always by 1. This means there is exactly one such line that has P and one more point on it, with N-1 points on each side. Moreover, if we sort all non-P points by the angle they form with P (choosing 0 to coincide with the vertical line), the point that ends up on the line is exactly the median of the sorted list.

Choosing P as a leftmost point gives a unique choice of another point Q to pair P with. We can then remove both and continue. The algorithm requires N iterations, and in each of them we must find a leftmost point, and sort O(N) points by angle. Calculating the cosine of the angles to compare without loss precision takes constant time. Picking the median and removing the pair of points all can be done in O(N) time or better. Therefore, the overall algorithm requires O(N2 log N) time, and it is enough to solve Test Set 1. If one uses a linear algorithm to find the median instead of sorting, it would improve the time complexity to O(N2). However, sorting to find the median is likely to be faster in practice due to constant factors.

A consequence of the reasoning above is that the solution is actually unique. Another observation is that any point in the convex hull of the points has the same property as a leftmost point, because the definition is invariant through rotations and any point in the convex hull can be made a leftmost point through a rotation. But a leftmost is one of the easiest to find.

Test Set 2 (Hidden)
There is another approach requiring an additional proof but leading to a slightly simpler algorithm for Test Set 1. Most importantly, it leads us one step closer to solving Test Set 2.

The additional key observation for this approach is that if a set of 2N points admits a magnificent pairing, then for every point P in the set (not just those in the convex hull) there is exactly one Q such that the line through P and Q leaves half the points on each side. The fact that there is at least one is immediate from the existence of a magnificent arrangement. The argument that there cannot be more than one requires more thought.

Assume there is a point P and two other points Q1 and Q2 such that both of the lines that go through P and each Qi have half of all the other points on each side. Moreover, without loss of generality, assume Q1 is the one that actually pairs with P in the magnificent pairing (we know it is unique). The following picture illustrates the situation.

2
1
B
A
C
D
Q
Q
P
In the picture, we have labeled the four areas into which the two lines divide the plane. Let A, B, C and D be the sets of input points contained in each area (not including P, Q1 or Q2). Notice that any point in A must be paired with a point in C in order for the produced segment to intersect PQ1. Similarly, any point in D must be paired with a point in B. All points below the line that goes through P and Q2 are in either A or D, which means there are |A| + |D| of them. The number of points above, on the other hand, is |B| + |C| + 1. Since we showed |A| ≤ |C| and |B| ≤ |D|, |B| + |C| + 1 ≥ |A| + |D| + 1 > |A| + |D|. This contradicts the assumption that the line that goes through P and Q2 has the same number of input points on each side.

This observation by itself only allows us to avoid the "find a leftmost point" step of the previous algorithm, which does not change its overall time complexity. The definite improvement is to more quickly shrink the set of points that we must consider, so that all steps within the iteration take less time. To do that, consider that after we have paired M pairs of points, the lines through each pair divide the plane into 2M areas, with only the outside areas (the ones with unbounded area) possibly containing leftover points. Moreover, each point is to be paired with one in the area that is opposite in order to cross all the lines, which is a necessary condition to intersect all line segments. When we create a new pair, exactly two of the areas are split in half. The idea is then: instead of removing the new pair and continuing with the full set, continue recursively with two separate sets. If we consider sets X and Y coming from opposite areas, split after pairing into X1 and X2 and Y1 and Y2, respectively, then we need to solve recursively for X1 and Y1 separately from solving for X2 and Y2 (assuming the areas are labeled such that X1, X2, Y1, Y2 appear in that order when going through them in clockwise order).

The last thing to do is to make sure we split X and Y more or less evenly with the new line. Notice that if we restrict ourselves to start with a point P that is part of the convex hull of X or Y or X ∪ Y, this might be impossible (i.e., all those pairs may split X and Y, leaving most remaining points on one side). Hence the need for the property with which we started this section. However, notice that if we sort all pairs to be made between points in X and points in Y by the slope of their produced segments, the first and last will split X and Y into an empty set and a set with |X| - 1 points (notice that |X| = |Y|). The second and next-to-last will split them into a set with 1 point and a set with |X| - 2 points. The i-th will split them into a set with i - 1 points and a set with |X| - i points. Therefore, if we choose randomly, we end up with a recursion similar to quicksort's, in which the reduction on the size of the sets is expected to be close enough to always partitioning evenly. Since the time it takes for the non-recursive part of a set of size M is O(M log M) — notice that calculating the partitions is only an additional linear time step — the overall expected time complexity of this recursive algorithm is (N log2 N).

World Finals 2019 - Code Jam 2019

Juggle Struggle: Part 2 (5pts, 30pts)

We can represent each pair of jugglers from the input as the endpoints of a segment. The problem then asks us to find two of those segments that do not intersect, or report that there is no such pair.

Test Set 1 (Visible)
Test Set 1 can be solved by checking every possible pair of segments to see if they intersect. Let pq be the segment between points p and q, and rs be the segment between points r and s; then these segments intersect if and only if

(r - p) × (q - p) has the same sign as (s - p) × (q - p), and
(p - r) × (s - r) has the same sign as (q - r) × (s - r),
where × stands for the cross product. This works only when no three of the points are collinear, but that is the case in this problem. Since each check takes constant time, this algorithm runs in O(N2) time.
Test Set 2 (Hidden)
Let Li be the line that fully contains segment Si. Let Si and Sj be two segments that do not intersect. If Li and Lj are not parallel, at least one of Si and Sj does not contain the intersection between Li and Lj. We can find for every input segment Si, we can find whether its line Li intersects some other line at a point outside of Si (or not at all). Let F be the set of all such segments. Then, every non-intersecting pair contains at least one segment in F and the size of F is at most 25. We can do a linear pass over all other segments to see whether or not they intersect each segment in F to find the segments not in F that also participate in non-intersecting pairs. We focus now on finding all segments to put in F.

We now present three algorithms. The first two are ultimately similar: the first one uses less advanced theory, but requires more problem-specific proofs because of it. They both require somewhat large integers. For C++, __int128 is enough to represent all values, but since we need to compare fractions that are the ratio of two __int128s, we'll need special comparison code. For Java, BigInteger will do. The third algorithm is a separate approach that uses a more advanced data structure, and requires only 64-bit integer arithmetic.

A solution using less advanced previous knowledge
First, assume there is a purely vertical segment (that is, its two endpoints have the same x coordinate). If we find more than one of those, we add all of them to F, since they don't overlap. If we find a single one, we can check it against all others like in the previous solution in linear time. In what follows, we assume no vertical segment is present.

Consider the extension of each segment Si to a full line Li. We will find the x coordinates of the leftmost and rightmost intersection of Li with all other Ljs and check that they are inside the range of x coordinates where Si exists. If one of those intersections is not inside that range, then we found one or two segments to put in F. Notice that finding all rightmost intersections is equivalent to finding all leftmost intersections in the input symmetric to the y axis, so if we have an algorithm that finds the leftmost ones, we can just run it twice (and reflecting the input takes linear time). Moreover, suppose we restrict ourselves to the leftmost intersection of Li such that Li is above the other intersecting line to the left of the found intersection. Let us call these "leftmost above intersections". We can use an algorithm that finds those intersections once on the unchanged input and once on the input reflected across the x axis to find the analogous "leftmost below intersections". In summary, we develop an algorithm that finds "leftmost above intersections" and then run it 4 times (using all combinations of reflecting / not reflecting the input across each axis), to find all combinations of "leftmost/rightmost below/above intersections".

To find all "leftmost above intersections", the key observation is that if two lines L1 and L2 intersect at x coordinate X, and L2 is below to the left of the intersection, then L2 cannot participate in any leftmost below intersection at coordinates X' > X. L2's own intersections at coordinates X' > X are not leftmost. If L2 intersects an L3 that is below L2 to the left of their intersection at X' > X, then L3 intersects L1 to the left of X' because of continuity: L1 is below L2 to the right of X.

This leads to the following algorithm: let X0 be the minimum x coordinate among all endpoints. Sort the lines by y coordinate at X0. Let Li be the line with the i-th highest y coordinate. We iterate over the lines in that order, while keeping a list or ranges of x coordinates and which previously seen line is below all others there, since that is the only one that can produce leftmost below intersections in that range. We keep that list as a stack.

At the beginning, we push (X1, 1) onto the stack, where X1 is the maximum x coordinate among all input points. This signifies that L1 is currently below in the entire range of x coordinates. Then, we iterate through L2, L3, ... LN. When processing Li, we find the x coordinate of its intersection with Lj and call it X, where (j, X') is the top of the stack. We check the intersection to see if it is within the x coordinate range of the two corresponding segments. Then, if X < X', we simply push (i, X) onto the stack. Otherwise, we pop (j, X') from the stack and repeat, since j was not the line below all others at X. Notice that this keeps the stack sorted increasingly by line index and decreasingly by intersection coordinate at all times.

Since every line is processed, pushed onto the stack and popped from the stack at most once, and everything else can be done in constant time, this iteration takes linear time. Other than that, the sorting by y coordinate takes time O(N log N), which is also the overall time complexity of the entire algorithm, since it dominates all other linear steps.

Notice that the way we use the stack in the above algorithm is quite similar to how a stack is used in the most widely known algorithm to compute the convex hull of a set of points. As we show in the next section, that is no coincidence.

Using point-line duality to shortcut to the solution
In this solution we change how we find leftmost intersections. Treating vertical lines and reflecting to find rightmost intersections, and the way to use leftmost/rightmost intersections to find the solution to the problem, are the same as in the solution above.

To find the leftmost intersections, we can apply the point-line duality to the input. With duality between points and lines, a line y=mx+b in the original space can be represented as the point (m, -b) in the dual space. Similarly, a point (a, b) in the original space can be represented as a line of the form y=ax-b in the dual space. Notice that the dual space of the dual space is the original space. Vertical lines have no corresponding point. This duality has the property that when two lines L1 and L2 intersect in the original, their intersection point P corresponds to the line dual(P) in the dual space goes through the points dual(L1) and dual(L2).

Thus, if we take all lines that are extensions of input segments and consider the points that correspond to them in the dual space, the leftmost intersection for a given line L1 occurs when intersecting L2 such that the slope of the segment between dual(L1) and dual(L2) is minimal.

We now work on the dual space with an input set of points, and for each point P we want to find another point Q such that the slope of PQ is minimal. For each point in the convex hull of the set, the minimal slope occurs between that point and the next point in the convex hull. For points not in the convex hull, however, the appropriate choice is the temporary "next" point of the convex hull as calculated by the Graham scan. This leads to similar code as for the solution above, but using the duality saves us quite a few hand-made proofs. Just as for the algorithm above, all of the steps of this algorithm take linear time, with the exception of the sorting step needed for the Graham scan, yielding an overall O(N log N) algorithm.

A solution using incremental convex hull
Another solution requires more code overall, but some of that code might be present in a contestant's comprehensive library. It uses an incremental convex hull, which is a data structure that maintains a convex hull of a set of points and allows us to efficiently (in logarithmic time) add points to the set while updating the convex hull if necessary.

The algorithm checks for a condition that we mentioned in the analysis for Part 1: each segment has the two endpoints of all other segments on different sides. The algorithm uses a rotating sweep line. Assume the endpoints of all input segments are swapped as necessary such that the segments point right (the first endpoint has an x coordinate no greater than the x coordinate of the second endpoint). Then, we sort the segments by slope and consider a rotating line that stops at all those slopes — that is, we iterate through the slopes in order. If we number the segments S1, S2, ..., SN in that order, S1 must have all left endpoints on one side, and all right endpoints on the other. S2 is the same, except the left endpoint of S1 goes with the right endpoints of all others, and vice versa. In general, for Si we need to check for the left endpoint of all segments S1, S2, ..., Si-1 to be on one side together with the right endpoints of all segments Si+1, Si+2, ..., SN, and all other endpoints are on the other side. If we find an endpoint of Sj on the wrong side of Si, then Si and Sj do not intersect. If we find no such example, the answer is MAGNIFICENT.

If we knew the convex hull of all the points that are supposed to be on each side, we could use ternary search on the signed distance between the convex hull and the line to efficiently find the point from that set whose signed distance perpendicular to the current Si is smallest (for the side where the distances are supposed to be positive) or largest (for the other side). If one of those finds us a point on the wrong side, we are done; otherwise, we know all other points are also on the correct side. Unfortunately, to keep those two convex hulls as we rotate would require us to both add and remove a point from each set. Removing is a lot harder to do, but we can avoid it.

When considering the slope of Si, instead of using the convex hull of the full set on one side, we can use the convex hull of the left endpoints that are on that side, and separately, the convex hull of the right endpoints on that side. That leaves us one additional candidate to check for that side, but one of those is the optimal candidate. Since we are calculating left and right endpoints separately, the 4 × N - 1 convex hulls we need are the ones of the set of left endpoints of the segments in a prefix of the list of segments, the left endpoints of the segments in a suffix of the list of segments, and similarly, the right endpoints of the segments in a suffix or prefix of the list of segments. We can calculate all of those convex hulls with a data structure that only provides addition of a point by calculating the convex hulls for prefixes in increasing order of segment index, and the ones for suffixes in decreasing order of segment index. Notice that this means we will calculate the convex hulls in an order different from the order in the original form of the algorithm.

We are doing O(N) insertions into the convex hull data structure and O(N) ternary searches, and each of these operations takes O(log N) time, making the time complexity of this algorithm also O(N log N).

For this particular use, we only need one half of the convex hull: the half that is closer to the line being inspected. In this half convex hull, the points in the hull are sorted by y coordinate, so a tree search can yield us the tentative insertion point, and we can maintain the convex hull by searching and inserting in a sorted tree. This is simple enough that it does not necessarily require prewritten code. Additionally, we can further simplify by using binary search on the angle between the convex hull and the line instead of the ternary search mentioned above.

World Finals 2019 - Code Jam 2019

Go To Considered Helpful (19pts, 30pts)

There are two kinds of programs: those that terminate, and those that don't!

For any set of instructions for Marlin that does not loop indefinitely (because it reaches the end of the program), there is an equivalent program that has no goto instructions, and this is the shortest way to write that program.

Similarly, if a program loops indefinitely, there is an equivalent program that consists only of move instructions with a single goto at the end of the program, and this is the shortest way to write that program.

So we only need to consider those two cases: programs that consist only of move instructions, and programs with only a single goto, as the last instruction. Then we take the shortest of all of these.

We can easily find a minimal program of the first type with a breadth-first search (BFS) which starts at M and stops when it reaches N, avoiding #s. If the search terminates without reaching N, then we can output that the case is impossible.

Finding a minimal program of the second type is harder. The program can be split into two sections which compute two separate paths: an initial path P produced by the instructions which do not repeat, and then a path Q which repeats until N is reached.

Let B be the point where P ends. We can use a BFS, as in the previous case, to find a minimal P for all possible points B.

Optimizing Q is slightly more involved. The first time through Q, Marlin walks through some set of grid cells. The second time through, Marlin walks through grid cells of the same pattern, but offset by some displacement vector D. This continues for some number of iterations K until Marlin reaches the N cell. So the first iteration of Q takes Marlin from B to B+D, but the path must avoid not only cells with a #, but also cells which have an equivalent cell in a later iteration which is a #. Additionally, the first iteration of Q must include a cell which, in a later iteration, will be N.

It is not easy to satisfy these constraints unless we already know the displacement D, and the number of iterations, K. So, we loop over all possibilities for these.

Inside this loop, we determine the shortest program for all possibilities for B, for the given values of D and K. We split Q into two parts, Q1 and Q2. Q1 contains the instructions which repeat K times, and reach cell N on the final iteration. Q2 contains the instructions which only repeat K-1 times. (If we reach N at the end of an iteration, Q2 is empty.)

Let N be the location of the cell containing N. Q1 is a path from B to N-(K-1)×D, and Q2 is a path from N-(K-1)×D to B+D.

The path for Q1 can only touch cells X such that X+i×D is empty for 0<i<K. We do a BFS from N-(K-1)×D, using these cells, to find an optimal Q1 for all possibilities for B.

The path for Q2 can only touch cells X such that X+i×D is empty for 0<i<K-1. We do a BFS from N-(K-1)×D, using these cells, to find an optimal Q2 for all possibilties for B.

Finally, we loop over all possible cells for B, and sum up the lengths of the shortest P, Q1 and Q2, plus one for the goto statement.

Running time
Let max(R,C)=N. For the outer loop that iterates over D and K, there are O(N2) choices for D and O(N) choices for K. However, not all combinations are possible. If the number of iterations, K, is large, then the displacement D for each iteration must be small; otherwise Marlin's path would have to leave the grid to complete K iterations. The total number of valid combinations of D and K is O(N2) — we leave the proof as an exercise for the reader.

Inside the loop, setting up the grids for the BFS, running the two BFSs, and trying all values for B all take O(N2) time. So the loop takes O(N4) time.

The BFS outside the loop that computes all optimal paths P takes O(N2) time, and so does the search for the optimal solution with no goto statements. So the overall solution is O(N4), and runs fast enough to solve both test sets.

Being less careful can result in an O(N5) solution — for example, by doing O(K) work to check whether a cell is valid for the two BFSs inside the loop. This is sufficient to solve test set 1.

An exponential-time solution that naively tries all possibilities for Q is unlikely to work even for test set 1, since the maximum length of Q is too large.

Qualification Round 2020 - Code Jam 2020

Vestigium (7pts)

Attempt 8
S
check
Mar 21 2021, 12:03
remove_red_eye
Attempt 7
Sample Failed: WA
Mar 21 2021, 12:01
remove_red_eye
Attempt 6
Sample Failed: WA
Mar 21 2021, 12:00
remove_red_eye
Attempt 5
Sample Failed: WA
Mar 21 2021, 11:46
remove_red_eye
Attempt 4
Sample Failed: WA
Mar 21 2021, 11:42
remove_red_eye
Attempt 3
Sample Failed: WA
Mar 21 2021, 11:41
remove_red_eye
Attempt 2
Sample Failed: WA
Mar 21 2021, 11:41
remove_red_eye
Attempt 1
Sample Failed: CE
Mar 21 2021, 11:40
remove_red_eye

One simple way to check whether the values in a row or column are a permutation of the values from 1 to N is to sort them and then step through them, checking whether the sorted list starts at 1 and increases by 1 each time. Another option, which avoids the sort and takes time linear in N, is to look at the values one by one and store each one in a hash table-based data structure. If we ever find that a value is already in the set, then that row or column contains a repeated value. Because there are N values and the problem guarantees that they are integers between 1 and N, inclusive, the absence of duplicates implies that we have a permutation as desired.

Finding the trace is also straightforward — iterate through the rows taking the i-th value from the i-th row, and add the values together.

Qualification Round 2020 - Code Jam 2020

Nesting Depth (5pts, 11pts)

Attempt 1
S
check
check
Mar 21 2021, 15:00
remove_red_eye

Test Set 1
To solve Test Set 1, we can put an opening parenthesis before each group of 1s and a closing parenthesis after.

We can use the following trick to simplify the implementation: prepend and append one extra 0 to S. Then the implementation is just replacing 01 with 0(1 and 10 with 1)0, which can be written in one line of code in some programming languages. Don't forget to remove the extra 0s from the end of the resulting string!

Test Set 2
For convenience, let's once again use the trick described above: prepend and append extra 0s to S, and then scan S from left to right.

Suppose we see some number A immediately followed by some larger number B and suppose all of the previously inserted parentheses would leave A at the right nesting depth — that is, there are exactly A unmatched opening parentheses preceding A, and no unmatched closing parentheses. For B to be at nesting depth B we need to add at least B - A opening parentheses. We can just do that and nothing else, to keep the final string length minimal. Any additional opening parentheses we would add would need to be closed before B, which would needlessly lengthen the string. Similarly, if we see some number A immediately followed by some smaller number B, we can just insert A - B closing parentheses. And in the case when A is equal to B, we don't need to add anything.

We don't need any parentheses before the temporary 0 in the beginning, or after the one in the end, so we can just drop them before printing the result.

Since we only add p parentheses when at least p are needed, the resulting string is of minimum length.

An inefficient but fun solution
The problem can be solved using only string replacements. First, replace each digit D with D (s, then the digit itself, then D )s. Then eliminate all instances of )(, collapsing the string each time, until there are no more to remove.

Here's a Python3 implementation:

for C in range(int(input())):
  rawstr = ''.join([int(x) * '(' + x + ')' * int(x) for x in str(input())])
  for _ in range(9):
    rawstr = rawstr.replace(')(', '')
  print("Case #{}: {}".format(C+1, rawstr))

Qualification Round 2020 - Code Jam 2020

Parenting Partnering Returns (7pts, 12pts)

Test Set 1
We can solve this test set by naively trying every possible subset of activities to be covered by Jamie and assign the rest of the activities to be covered by Cameron. For each subset of activities, we can check whether a pair of activities overlap for each pair of activities. An activity with start time s1 and end time t1 overlaps with another activity with start time s2 and end time t2 if the time intersection is not empty (i.e., max(s1, s2) < min(t1, t2)).

The running time of this solution is O(2N × N2), which is fast enough to solve Test Set 1.

Test Set 2
We can solve this test set by greedily assigning the activities in increasing order of start time. For each activity (in increasing order of start time), we can check whether Jamie or Cameron can be assigned to cover the activity and assign the activity to whomever can be assigned to (or arbitrarily if both partners can be assigned). The check can be done by iterating all activities that have been previously assigned to Jamie and Cameron.

The greedy assignment is correct because the only way that the assignment fails is when there is a time that is covered by three activities. In such a case, there is indeed no valid assignment. When deciding who to assign an activity with start time s, only activities with start times no later than s have been assigned. Therefore, if both Jamie and Cameron have some activity assigned with end time later than s, it means that there are three activities that use the time between s and s + 1, and therefore, there is no possible assignment. If an assignment is possible, there cannot be any set of three activities that pairwise overlap, so by the contrapositive of the the previous argument, we will be able to assign the activity to at least one of Jamie or Cameron at every step.

The running time of this solution is O(N2), which is fast enough to solve this test set. To optimize the solution to O(N log N) time, we can efficiently check whether an activity can be assigned to Jamie or Cameron by keeping track of the end time of the last activity assigned to each partner and comparing this to the start time of the new activity. In this case, only O(N) extra time is needed after sorting the activities by their start time.

Graph approach
Another possible approach to solve this test set is to construct a graph with N nodes, each representing one activity. We add an edge connecting a pair of nodes if the pair of activities represented by the nodes overlap (see Test Set 1 section for details on how to check if two intervals overlap). This graph is commonly known as an interval graph.

Therefore, the problem is equivalent to finding a partition of nodes C and J such that every edge connects a node in C and a node in J, as we can assign all activities represented by nodes in C to Cameron and all activities represented by nodes in J to Jamie. The running time of the algorithm to find the partition (or report if one does not exist) is linear on the size of the graph. The graph has N nodes and O(N2) edges, which means the solution requires O(N2) time to build the graph and O(N2) time to run the partition algorithm, so also O(N2) time overall.

Qualification Round 2020 - Code Jam 2020

ESAb ATAd (1pts, 9pts, 16pts)

Test Set 1
In Test Set 1, there are only 10 positions in the string. We can query for each of them and then submit the complete string, without having to worry about any quantum fluctuations (which would only happen if we submitted an 11th query).

Test Set 2
Here is one of various ways to solve the second test set. We begin by querying for the first ten positions in the real string, then create a "possibility set" containing all 1024 20-character strings that begin with those 10 characters. Then we update our "possibility set" to contain all strings that could have arisen from those strings after the next quantum fluctuation. The correct answer is in here somewhere — now we need to narrow the set down!

Before making each subsequent query, we first find the string index (between 1 and 20) at which the proportion of 0s and 1s among the strings in our possibility set is most nearly even. Then we query the real string at that index, and eliminate from the possibility set any strings that are not consistent with that information. Whenever we can indeed find a position with even proportions, we are guaranteed to cut the size of the set in half, but if there is no such position, we may not be able to eliminate that many possibilities. We can continue in this way, remembering to expand the possibility set every time there is a quantum fluctuation, until only one possibility remains, which must be the answer.

It is not easy to prove that this strategy will converge upon an answer. Intuitively, we can observe that a quantum fluctuation increases the size of the possibility set by at most 4, and even if we somehow only cut the possiblity set by 20% with each pruning, we would still easily beat that factor-of-4 increase and make enough progress to finish within 150 queries. Moreover, it would not be possible for the strings in the possibility set to all be distinct while being so similar at every individual position (recall that we always pick the position that will be most useful to us in the worst case). Also, Test Set 2 is a Visible Verdict set, so we might as well just submit our answer and see.

Test Set 3
The above strategy will not work for 100-character strings, since the possibility set would be astronomically huge. Fortunately, there is a much simpler approach.

Observe that if we can find two positions that are equidistant from the center of the string and have the same value, we can use them to detect when a quantum fluctuation has included a complementation (with or without a reversal). Suppose, for example, that the two ends of the string are 0 just before a quantum fluctuation. After the fluctuation, we can check the first one. If it is 1, then there was a complementation; if not, there wasn't one. This is true regardless of whether that quantum fluctuation included a reversal.

Now suppose that we continue to check pairs of positions in this way, moving inward one step at a time. After every quantum fluctuation, we must spend one query to check for complementation so we can update our existing knowledge about the string if there has been one. If every pair turns out to be a "same pair" like the first pair, then we never needed to care about reversals anyway (since the string is palindromic), and we are done.

But what if, in the course of this, we find a "different pair"? Such pairs are helpful in their own way! If we query the first position of a "different pair" after a quantum fluctuation and we find that that bit has changed, then we know that either a complementation or reversal has happened, but not both.

Once we have such a "different pair", we can use it in conjunction with the "same pair", spending 2 out of every 10 queries to learn exactly what happened in each quantum fluctuation. For example, if the first position of our "same pair" stayed the same but the first position of our "different pair" did not, we know that the quantum fluctuation included a reversal but no complementation.

In the above analysis, we assumed we would encounter a "same pair" first. If the first pair is different, though, we can proceed until we encounter a "same pair"; if we never encounter one, then we do not care about the distinction between complementation and reversal, because the operations are equivalent for that particular string. If we do encounter a "same pair", though, then we can proceed as above.

How many queries will we need in the worst case? We can use all of our first 10 to gather data, since whatever happened in the quantum fluctuation at the start of the problem is unknowable and does not matter. After that, we may need to use up to 2 out of every 10 queries to reorient ourselves before spending the remaining 8 gathering data. So, to be sure we can find the entire string, we will need 10 queries, plus 11 more sets of 10 queries in which we learn 8 positions each time, (to get us to 98 positions known), plus 2 more queries for a final reorientation, plus 2 more to get the last two positions. That is a total of 124, which is well within the allowed limit of 150.

Regarding the name...
Last year, we had the Dat Bae problem about deletions from a string in a database; the name was Data Base, altered in a way that reflected the theme. ESAb ATAd is similar, with case change serving as a rough equivalent of complementation. (Imagine how much the Code Jam team has enjoyed trying to type the name correctly each time!)

Qualification Round 2020 - Code Jam 2020

Indicium (7pts, 25pts)

Test Set 1
There are a few different options for solving test set 1. Since there are only 44 possible cases, one option is to generate all answers by hand or via a program that is run locally, then submit a program that dispenses those. Another approach is to notice that there are not many different Latin squares for N ≤ 5 (see the number of Latin squares here), and check them all. To generate all Latin squares, we can recursively fill in the cells one by one. For each cell, we try all N possible values. For each one, we ensure that it does not conflict with any cells in the same row or same column. Since there are at most 161280 Latin squares to consider, this is quite quick.

Test Set 2
Unfortunately, once N gets even slightly large, there are way too many Latin squares to generate them all (for N = 11, for example, there are 776966836171770144107444346734230682311065600000 different Latin squares).

There are many creative ways to solve this test set. The Code Jam forum is a good place to share and discuss different solutions! For example, we can directly create Latin squares with the appropriate trace by modifying structured Latin squares (for example, by modifying circulant Latin squares). Below, we discuss an easy-to-implement idea which is a little tricky to come up with and uses a graph algorithm in the middle!

First, we start by dealing with the impossible cases. If K = N+1, then the only possible diagonals have exactly one 2 and N-1 1s. However, if N-1 of the diagonal elements are 1, then the only location for the 1 in the remaining row must be on the diagonal, so we cannot make a sum of N+1. Similarly, we cannot make a sum of N2-1 since the only possible diagonal is one N-1 and N-1 Ns.

We will now show a construction which works for every other case (with 2 additional small cases that don't work, see below). One of the main insights needed is that all possible sums are achievable using a diagonal with almost all values the same. In particular, we may assume that at least N-2 values are the same: AAAA ... AABC for some A, B, C (not necessarily all different).

For example, if N = 10 and K = 20, we can choose A = 2, B = 2, and C = 2. If N = 10 and K = 55, we can choose A = 6, B = 4, and C = 3. We already showed above that A = B if and only if A = C. We leave it as an exercise to show that all values for K between N and N2 are possible with these constraints. (Note: you have to be a little careful with N = 3. If B = C, then A = B = C for a similar reason; so with N = 3, neither K = 5 nor K = 7 will have solutions). To find the appropriate values of A, B, and C, we can brute force all possible triples and check whether the chosen diagonal will work.

Now that we know what the diagonal looks like, how do we actually find a Latin square that has this diagonal? To do that, we will fill in the unfilled cells row by row. We will use bipartite matching to find a valid row. In one bipartition, we have N vertices for the N cells in that row. In the other bipartition, we have N vertices for the N numbers that can be placed into the cell. Make an edge between the cell vertex on the diagonal and the number vertex that was decided on. For every other cell, make an edge between a cell vertex and a number vertex if that number can be put into that cell without breaking the Latin square properties.

We can greedily pick any perfect matching for each row starting with the rows with B and C on their diagonal. Once we have filled in these two rows, we can use Hall's Marriage Theorem to show that we will never run into any issues (so long as the conditions above about A, B, C are met).

Hall's Theorem
This section is dedicated to proving the above claim that Hall's theorem holds. We will assume in this section that the reader is comfortable with Hall's theorem. A one sentence high-level reminder of Hall's theorem: All subsets of one bipartition have a neighborhood that is at least as large as the original subset if and only if the graph has a perfect matching.

For the explanation here, we will make the top two rows with B and C on their diagonal as the top two rows. We'll assume that these two rows are already filled in (and leave the proof you can do this to the reader). The important part is that the top-left 2 × 2 submatrix is CA/AB. Now imagine that we have filled in N-k rows (and have k mostly empty rows). Consider this example with N = 8 and k = 3. (? means filled in, but it doesn't matter with what and _ means not filled in yet):

CA??????
AB??????
??A?????
???A????
????A???
_____A__
______A_
_______A
For each of the N-1 non-A "cell vertices", the N-k vertices on the left of the diagonal have a degree of k and the k-1 vertices on the right of the diagonal have a degree of k-1 (because the number A is also restricted). For each of the N-1 non-A "number vertices", each number originally had degree N and we have removed at least N-k of those edges since the number appeared once in the top N-k rows. Thus, the maximum degree of the "number vertices" is k.

We will ignore the "cell vertex" and the "number vertex" corresponding to the forced diagonal entry since that will be forced in our matching (and leaving it out makes our math below easier).

Let X be a subset of "cell vertices". Let m = |X|. We must show that |N(X)| ≥ m in order to utilize Hall's theorem (where N(X) is the set of "number vertices" that are adjacent to at least one vertex in X). We have 2 separate cases:

Case 1: m ≤ k-1.

Since the degree of each vertex in X is at least k-1, the number of edges leaving X is at least m × (k-1). Consider the "number vertices" that these edges are absorbed into. Since the maximum degree of "number vertices" is k, there are at least (m × (k-1))/k "number vertices" that absorb these edges. That is, |N(X)| ≥ (m × (k-1))/k = m-m/k. Since m ≤ k-1, we have that m/k < 1. So |N(X)| > m-1. Since |N(X)| is an integer, we have |N(X)| ≥ m as desired.

Case 2: m ≥ k.

Consider the edges leaving X. At most k-1 of them have degree k-1, and the remaining have degree k. Thus, the number of edges leaving X is at least (k-1) × (k-1) + (m-(k-1)) × k. Since the maximum degree of "number vertices" is k, there are at least ((k-1) × (k-1) + (m-(k-1)) × k)/k "number vertices" that absorb these edges. That is, |N(X)| ≥ ((k-1) × (k-1) + (m-(k-1)) × k)/k = m - (1 - 1/k). Since 1 - 1/k < 1, we have |N(X)| > m-1. Since |N(X)| is an integer, we have |N(X)| ≥ m as desired.

Thus, in all cases, the conditions for Hall's theorem are satisfied, so there exists a perfect matching and we can iteratively complete the Latin square.

Round 1A 2020 - Code Jam 2020

Pattern Matching (5pts, 5pts, 18pts)

Test Set 1
In Test Set 1, each pattern forces our answer to have a certain suffix, and we first need to check whether the patterns introduce conflicting requirements for that suffix.

Consider the letter strings coming after the initial asterisk in each pattern. We can find the longest of those strings (or any longest, if there is a tie); call that string L. Then at least one answer exists if (and only if) every other string is a suffix of L; note that we are considering L itself to be a suffix of L. We can check each other string against L by starting at the ends of both strings and stepping backwards through them in tandem until we find a discrepancy or run out of letters to check. If we ever find a discrepancy, then the case has no answer, but otherwise, we know that L itself is an acceptable answer.

Test Set 2
In Test Set 2, we can divide the patterns into (1) patterns that start with an asterisk, (2) patterns that end with an asterisk, and (3) patterns with an asterisk in the middle.

A type (1) pattern requires the output word to have a certain suffix, just as in Test Set 1. A type (2) pattern requires the output word to have a certain prefix. A type (3) pattern introduces both of these requirements, and we can split it into a suffix requirement and a prefix requirement, and then handle those separately.

Then, we can apply our strategy from Test Set 1 twice: once for the prefix constraints (with the algorithm modified to compare prefixes instead), and once for the suffix constraints. We can concatenate the two results together to obtain a valid answer that is certainly short enough (since it can be at most 99+99 characters long).

Test Set 3
We can generalize the idea above into a solution for Test Set 3. Each pattern p in Test Set 3 also prescribes a prefix of the output word (the prefix of p up to the first asterisk) and a suffix of the output word (the suffix of p starting after the last asterisk). If we allow empty prefixes and suffixes, we get exactly one of each for every pattern. We can handle those in the same way we did for Test Set 2, ending up with a prefix P and a suffix S for the output as long as we do not find a discrepancy in either phase.

However, for patterns that have more than one asterisk, we can also have a middle part, which imposes a new type of requirement. Suppose we parse the parts between the asterisks of a pattern so that X is the prefix up to the first asterisk, Y is the suffix after the last asterisk, and M1, M2, ..., Mk are the strings in between the asterisks, in order. After checking that X is a prefix of P and Y is a suffix of S, as before, all that remains to ensure is that the pattern M1*M2*...*Mk is present somewhere within the output word, strictly between P and S.

Let us call M1M2...Mk — that is, the word that occurs in between the first and last asterisks with any other asterisk removed — the middle word. If we make sure a pattern's middle word occurs in the output word outside of P and S, then we fulfill the extra requirement. We can then build a full valid output then by starting with P, then adding the middle word of every pattern in any order, then appending S. We make sure to correctly handle words with a single asterisk or only consecutive asterisks by making their middle words the empty string. Since each middle word contains at most 98 characters, and the prefix and suffix contain at most 99 characters each, the output built this way has at most 99 × 2 + 50 × 98 characters, which is within the limit of 104.

Round 1A 2020 - Code Jam 2020

Pascal Walk (3pts, 11pts, 21pts)

Test set 1
The answer for N ≤ 500 can always be constructed by walking along the leftmost positions of the rows of the triangle: (1, 1), (2, 1), ..., (N-1, 1), (N, 1).

We can handle the case N = 501 by making a detour to pick up the 2 in the third row before returning to the left side. That is, our walk takes us through the positions (1, 1), (2, 2), (3, 3), (3, 2), (3, 1), (4, 1), ..., (498, 1).

Test set 2
With only 1000 possible test cases to consider, we can find an answer to each one before submitting. One way to do this is to walk over the triangle in e.g. a breadth-first or random way, taking care never to reuse a position or visit more than 500 positions. We can check our cumulative sum at each position, and whenever we encounter a sum that we have never seen before, we record the sequence of positions that we used to obtain it. When our sum exceeds 1000, we backtrack in our search or start over with a new random path, and so on, until we have an answer for every possible N. It's difficult to prove a priori that this will work, but we can be optimistic given that the top few rows of the triangle have many small values to work with. Indeed, this method finds a full set of solutions very quickly.

An alternate method is to observe that the positions immediately to the right of the leftmost positions — named (x, 2) for each x ≥ 2 — are 1, 2, 3, 4, 5, and so on. We can move from the top position in the triangle down to the 1 at position (2, 1), then follow that line to the 2 at (3, 2), the 3 at (4, 3), etc., until our next move would cause the cumulative sum to exceed our target. Then, we can instead make a move to the left to reach a 1 on the left edge of the triangle, and then proceed downward along that edge, taking as many extra 1s as we need. Since the sum of the first 45 natural numbers is over 1000, we can be sure that we will never need to take more than 45 of these extra 1s, or visit more than 45 of the positions along this line of natural numbers. This ensures we never use more than 90 positions in total.

Test set 3
There are various ways to solve the third test set, but one of our favorites takes advantage of a special property of Pascal's triangle: the entries in the r-th row (counting starting from 1) sum to 2r-1. This is to be expected from the way in which each row is constructed from the previous one: each element in a row contributes to two elements in the next row, so the sum doubles with each row.

This observation suggests a strategy for constructing any N that we want: write N in binary, and then look through that representation, starting from the least significant bit. If the r-th least significant bit (counting starting from 1) is a 1, our path should consume all of the elements in the r-th row. If the r-th least significant bit is a 0, we should skip that row somehow. We only need the first 30 rows of the triangle, since the 31st row's sum is 230, which is larger than the maximum possible N of 109. Even if we used every element in these first 30 rows (which we would never need to), that would only use 465 positions, which is within the limit of 500.

This doesn't quite work as written, though, because our path must be continuous, and so there is no way to skip a row. But we can get close via the following variant: proceed down one side of the triangle (that is, the diagonal line of 1s) as long as we keep encountering 0 bits. Whenever we encounter a 1 bit, we send our path horizontally across the corresponding row, sending us to the other side of the triangle. This almost gets us the number we want, but we probably overshoot because we have consumed some extra 1s from the rows corresponding to 0 bits. We can be sure that there are fewer than 30 of those, though, since we visit at most 30 rows.

So, we can use that variant, but instead of constructing N, we construct N-30 instead. Once we finish, we can tack on additional 1s from our current edge of the triangle until we reach N. (We can handle cases with N ≤ 30 specially, and just go down one edge of the triangle.)

There are other less elaborate ways to solve this test set, though! One is to walk down the centers of the rows (e.g., going from (1, 1) to (2, 1) to (3, 2) to (4, 2) to (5, 3)...). This is where the largest numbers live, so we will grow our sum as rapidly as possible. At some point, though, when moving downward would make our sum grow too much, we can instead move to a one step away from the center and continue zigzagging downward, and so on. Eventually we will reach the edge full of 1s, and we can take as many more as we need. We must be careful not to increase the sum so fast that we will not be able to "escape" to the edge of 1s. Nor should we reach the edge too early, since we can only take so many 1s. However, it's not too difficult to get this method to work.

Round 1A 2020 - Code Jam 2020

Square Dance (9pts, 28pts)

Test Set 1
Test Set 1 can be solved by simulating the competition:

Iterate through all the cells of the floor. For each cell, if there is a dancer in that cell:
Add their level to the interest level of the competition.
Loop through their row and column to find all of their compass neighbors.
Count the number of their compass neighbors, and sum up their levels.
If the average level of the neighbors (calculated as sum of their levels divided by how many neighbors there are) is greater than the level of the current dancer, add this dancer to an elimination list.
Go through the elimination list and eliminate the dancers.
If the elimination list is empty, we are done. Otherwise, start again from step 1.
Each iteration of the above algorithm has a time complexity of O(R×C×(R + C)), and there are as many iterations as there are rounds in the competition. The number of rounds in a competition cannot be larger than the total number of dancers, R × C. So the total time complexity is O(R2×C2×(R + C)).

Test Set 2
Let's optimize our Test Set 1 solution — specifically, the following parts of it:

For each round, we iterate through all R×C cells and check whether each dancer is eliminated.
For each dancer, we may have to iterate through a significant part of their row and column in order to find their compass neighbors.
To improve the situation described in the first point above, we can observe that for i > 1, if dancer d is not eliminated in round i-1 and they are eliminated in round i, their set of compass neighbors must have changed in between. For that to have happened, at least one compass neighbor of d must have been eliminated in round i-1.

Using this observation, instead of checking every dancer in round i, we can limit the check to the compass neighbors of those eliminated in round i-1. Since a dancer has at most 4 compass neighbors at the time of elimination, the length of the list of candidates to check in round i is at most 4 times the length of the elimination list for round i-1. The sum of the lengths of all elimination lists is at most R × C; therefore, the sum of the lengths of all lists of candidates to check is at most 4 × R × C. With the initial check of all dancers, this means that we do O(R × C) checks overall with this improvement.

To optimize finding compass neighbors, notice that when we remove a dancer, their eastern neighbor becomes the eastern neighbor of their western neighbor and vice versa. The same thing happens to the southern and northern neighbors. So, instead of finding the neighbors each time we need them, we can maintain the specific location of the neighbor in each direction for each dancer, and update their locations in constant time whenever we eliminate one. This is effectively like keeping a linked list to represent each row and column, and this structure allows us to remove and find neighbors in constant time.

Since we do O(R × C) checks thanks to the first optimization, and each check can be done in constant time thanks to the second optimization, this yields an O(R × C) time algorithm that solves the problem fast enough for Test Set 2.

Round 1B 2020 - Code Jam 2020

Expogo (5pts, 8pts, 16pts)

Attempt 2
Sample Failed: CE
Mar 29 2021, 14:40
remove_red_eye
Attempt 1
S
check
check
check
Mar 28 2021, 10:55
remove_red_eye

Test Set 1
Test Set 1 is small enough to solve by hand. We can speed this up with a couple of observations:

We can notice that every position with an even (X + Y) (apart from the origin) — hereafter an "even" position — seems to be unreachable. We can prove to ourselves that this is true: our initial X and Y coordinates of (0, 0) are both even, but only the first of our possible jumps (the 1-unit one) is of an odd length, and all jumps after that are of even lengths. So there is no way to reach any other "even" position starting from the origin, no matter how much jumping we do.
We can find that all "odd" positions, on the other hand, can be reached using no more than 3 moves.
To speed up solving the "odd" positions, we can take advantage of symmetry, as suggested in the explanation for Sample Case #2. For example, if we learn that EEN is a solution for (3, 4), then we also know that WWS is a solution for (-3, -4), and EES is a solution for (3, -4), and so on. Because of all the horizontal, vertical, and diagonal symmetry, there are really only six fundamentally different cases!
We can check that our solutions are optimally short by using an argument like the one in the explanation for Sample Case #1. Any position with a Manhattan distance (that is, |X| + |Y|) of 1 cannot be reached in fewer than one jump; positions with Manhattan distances up to 3 and 7 require at least two or three jumps, respectively. If our solution lengths match these lower bounds — and they probably do unless we have jumped in an unusually indirect way — then they are valid.
Test Set 2
Based on the observations above, we may think to try a breadth-first search of all possible jumping paths, and continue until every "odd" (X, Y) position (with -100 ≤ X ≤ 100 and -100 ≤ Y ≤ 100) has been reached. It turns out that each such position is reachable in no more than 8 moves. We know that these solutions are optimally short because of the breadth-first nature of the search.

Test Set 3
Suppose that (X, Y) = (7, 10). In what direction should we make our initial 1-unit jump? As we saw above, we need our final X coordinate to be odd, but it is currently even, and we have only one chance to go from even to odd. Moving north or south will make our Y coordinate odd, but then we will never have another chance to make that even and the X coordinate odd. So we should either move west or east. For now, let's guess that we will go west; we will revisit the other possibility later.

That jump will take us to (-1, 0), and we will next need to make a 2-unit jump. Notice that we can make this look identical to the original problem setup, if we make two changes:

Shift (-1, 0) to be the new (0, 0). Then the goal becomes (8, 10) rather than (7, 10).
Transform the scale of the problem such that a 2-unit jump (to a "neighboring" cell) becomes the new 1-unit jump. Then the goal becomes (4, 5) instead of (8, 10).
With this in mind, let's revisit our original decision to jump to the west. If we had jumped east instead, we would have ended up at (1, 0), and if we had changed the problem in the same way we did above, our new goal would have been (3, 5). But that would be an "even" position (after rescaling), which cannot be reached! So we had no choice after all; we had to move west to be able to eventually reach the goal. It's a good thing we were so lucky!

So now the problem has "reset", and we are at (0, 0) and trying to get to (4, 5). In what direction should we make our "first" jump? Now we know we must move vertically, since 5 is odd and we will only have "one chance" to go from even to odd. If we jump north, the next rescaling will have a target of (2, 2), but if we jump south, the target will be (2, 3), which is the "odd" position that we want. From there, we should jump south to change the target to (1, 2), then east to change the target to (0, 1). At that point, we have a choice between jumping north and reaching the goal, and jumping south (which could still allow us to reach the goal after some further moves, e.g. one more to the south and then one more to the north). But the problem requires that we choose the shortest solution, so we should jump right to the goal! Therefore, the answer in this case is WSSEN.

Notice that this method is deterministic: we always have only one choice out of the four possible directions. We can rule out two of them because they will not make the correct coordinate odd. Of the other two, the new goal states they would leave us with must differ only in one of the coordinates and only by exactly 1 unit, and therefore one must be an "odd" position and the other must be an "even" position. It is possible that that "even" position is the goal, in which case we should jump there, but otherwise, we must choose the "odd" position.

The above analysis also shows that the only time we have a choice is when one of those options is to jump directly to the goal, in which case we obviously should. So we can be confident that our method produces the shortest possible solution. (We also know that that solution is unique, since if we were to choose to not jump directly to the goal when we had that option, we would only end up with a longer solution.)

Our method has a running time which is logarithmic in the magnitudes of the coordinates, so it will solve this test set blazingly fast!

A Code Jam callback!
This problem is a riff on the Pogo problem from Round 1C in 2013. If you were familiar with that problem, the analysis might have helped a bit with this one... but, like a well-designed pogo stick, Expogo is not too difficult to get a handle on anyway.

Round 1B 2020 - Code Jam 2020

Blindfolded Bullseye (3pts, 12pts, 19pts)

Test Set 1
In Test Set 1 the radius is both really large and known in advance. The fact that it is large gives us a major restriction: plugging in the value for R in the limits for X and Y shows that the hidden center is restricted to a square with side length 10 nanometers, centered on the origin. That means that there are only 112 = 121 possible positions for the center! We can simply try throwing a dart at each of them, stopping as soon as we are told that we have hit the CENTER.

Test Set 2
In Test Set 2 the radius is also large and known, but small enough that the square of candidate center points has side 101 nanometers. This means that there are 10201 possible centers, which is a lot more than the number of darts we get. We need to restrict that further.

We can use the fact that we know the exact radius to reduce this number. Every dart we throw can give us a clue. If a dart thrown at point p hits, that means the the distance between p and the center c is no more than R. If a dart thrown at point q misses, that means that the distance between q and c is more than R. If we find two points p and q that are close to each other, the points that are no more than R away from p but more than R away from q form a narrow crescent-shaped region of width equal to the distance between p and q. Intersecting that with the original square of possibilities from the first paragraph can give us a small enough range for c that we can just throw a dart at each possibility without running out of darts.

We can find points p and q that are close to each other by finding the edge of the dartboard. Since the dartboard is so large, the edge has to be close to the edge of the wall. If we inspect points (x, 0) for the possible values of x, most points are guaranteed to be inside the dartboard: any point (x, 0) with -109 + 101 ≤ x ≤ 109 - 101 is no more than R away from all possible centers, and is thus guaranteed to be inside the circle. Therefore, we can try all possible x on any one side of that guaranteed interval to find a hit point and a miss point that are 1 nanometer away from each other. Since there are only 101 points on each side that are not guaranteed to be in the circle — for example [-109, -109 + 100] — this requires at most 101 darts.

After finding those points, we know that the center has to be inside a narrow crescent-shaped band of width 1. Intersecting that with the 101-by-101 square of candidates leaves at most 202 candidates, since there can't be more than 2 candidates that share an x value. Moreover, for most x values, the rounding ensures that there are less than 2, so the 199 darts we have left are enough to cover all candidates.

Notice that no complicated geometry is involved to find the candidates; the square is small enough to iterate over all points within it and check the distances to our hit and miss points to see if they are candidates or not.

Test Set 3
For Test Set 3, we can take a more typical geometric approach. To find the center of a circle, we can find 3 points on the edge of the circle, and then calculate a center from those. This has an issue: we only have nanometer precision, which means we would typically find points near the edge of the circle, but not exactly in it. The error that this introduces in our calculations could be significant. We can overcome this by bounding the error and then checking not only our calculated center, but also other nearby points too. The math to calculate the center, and most importantly, to bound the error of it, can be hard and time consuming, but it is doable. In addition, you can wing it by not bounding the error and instead starting to throw at your calculated center and then spiral around it on nearby points until you find the actual center. This requires a leap of faith that you won't run out of tries, but it's a reasonable assumption. Fortunately, there is a simpler related approach that is more clearly correct, that we describe below. Of course, we still need to solve the problem of finding "almost-edge" points of the circle, which we also need for our simpler approach.

To find a point in the edge, we can't use the approach in Test Set 1, because if the dartboard's radius is not very large, those points could be really far away from the wall's edge. A different way of doing it is via a binary search. Say we know that point (x0, y0) is within the dartboard. Then, we know that for all x in the range [-109, x0] the points (x, y0) are grouped such that there is a (possibly empty) interval having all misses, and then an interval having all hits. The same holds (in reverse) for [x0, 109]. Analogously, points (x0, y) for y in [-109, y0] or [y0, 109] are grouped by hits/misses. Then, for each of those ranges, we can do a binary search to find the switching point — that is, a point in the edge of the circle. This is exactly what we did in Test Set 2, except we fixed x0 = 0, because the radius is so large that some points like (0, 0) are guaranteed to be inside the dartboard, and the range is small enough that we can scan it entirely instead of using binary search.

The searches above get us leftmost and rightmost points inside the dartboard for a given y-coordinate y0, and topmost and bottommost points for a given x-coordinate x0. Notice that the dartboard is symmetric with respect to the horizontal and vertical lines that go through its center. Therefore, the leftmost and rightmost points inside the dartboard in any fixed y-coordinate mirror each other, and the x-coordinate of the center is the midpoint between their x-coordinates. Similarly, we can find the y-coordinate of the center as the midpoint between the y-coordinates of the topmost and bottommost points inside the dartboard at any fixed x-coordinate.

The remaining task is to find a single point inside the dartboard. Since the area of the dartboard makes up a significant percentage of the wall area (at least π / 16), we could do this by throwing darts at random until we hit one point (the probability of hitting could be slightly less than π / 16 because we consider only points with integer nanometer coordinates, but the difference is negligible). This has an overwhelmingly high chance of hitting in 100 throws or fewer for all cases. However, there is a deterministic way that also works, which is to divide the wall into squares of side A. If the center of the dartboard is inside a square, it is guaranteed that at least one of the corners of that square is inside the dartboard. Therefore, we can simply try all those corners (there are only 25, and it is even possible to restrict it further), and we will find a hit point.

So, we need a fixed small number of darts (up to 100, depending on our method of choice) to find a point inside the dartboard, and each binary search needs at most 31 = ceil(log2 (2 × 109 + 1)). That is at most 4 × 31 + 100, which is a lot less than the 300 limit. In the first version, you need one fewer binary search — 3 edge points are enough to find a unique center — so after finding the center with error, you could have 150 darts left to explore its vicinity. Notice that if your initial point is actually one of the points (X - R, Y), (X + R, Y), (X, Y - R) or (X, Y + R), you would end up with two points instead of three and be unable to find the center. If we use the random procedure to find (x0, y0), the probability of this happening is negligible. Otherwise, we can detect the case and know that the two found points are opposite each other, so the center is their middle point.

Round 1B 2020 - Code Jam 2020

Join the Ranks (14pts, 23pts)

Test Set 1
Test set 1 contains the 12 possible inputs allowed by the limits. We can try to solve the problem via a breadth-first search (BFS), on the graph of states, where a state is the current arrangement of cards in the deck. Notice, however, that cases with 14 total cards would yield an enormous graph and make the solution run too slowly — probably even too slowly for us to run the code locally to precompute answers that we can then hardcode!

However, one observation will help us: since the success condition does not involve the suits of the cards at all, we can ignore them and work only with the ranks. That dramatically cuts down the number of possible states, by going from (R × S)! to (R × S)! / ((S!)R). This allows a BFS to finish fast enough for this test set. We can also use this observation while solving the next test set...

Test Set 2
For test set 2, the worst case (R=40, S=40) has around 1.8 x 102517 unique orderings. This means that the brute force solution will not work.

Our first important observation is that the reordering operation can decrease the number of adjacent cards of different ranks by at most two. In the starting configuration there are (R × S)-1 adjacent cards of different ranks. In the ending configuration there are R-1 adjacent cards of different ranks. So to get from (R × S)-1 to R-1 we need at least ceil((R × S - R) / 2) operations.

Now that we know ceil((R × S - R) / 2) is a lower bound on the answer, if we can come up with a method that is guaranteed to use no more than ceil((R × S - R) / 2) steps, then it will always produce a valid answer.

Now we will outline a way to sort the cards using exactly that many operations. The invariant we maintain is that at all times, for the ranks X and Y of any two consecutive cards, either Y = X, or Y = (X + 1) mod R. This is of course true for the initial ordering of the deck.

We repeatedly perform the following operation, as long as the number of adjacent pairs of cards of the same rank is less than R - 1 and the operation would not pick up the bottom card of the deck: find the largest block of consecutive cards from the top that contains exactly 2 different ranks to use as pile A. By the invariant, this will be one or more cards with rank X, followed by one or more cards with rank (X+1) mod R. Then, starting from the first card from the top that is not on pile A, take as pile B the largest block of consecutive cards that does not contain any cards of rank X, plus all consecutive cards of rank X that immediately follow that block. Notice that at least one such card of rank X must exist; otherwise, by the invariant, the number of adjacent pairs of cards of different ranks would already be R-1.

We can show that this operation reduces the number of adjacent cards of different ranks by 2 every time it does not pick up the bottom card of the deck. To show this, notice that the bottom of pile B is a card of rank X and the first card left over in the deck is, by the invariant, (X + 1) mod R. That means that the new adjacent pairs are two cards of rank X (the bottom of pile B and the top of pile A) and two cards of rank (X + 1) mod R (the bottom of pile A and the top of the leftover deck). The broken adjacent pairs are — by definition of piles A and B — both of cards of different rank. Therefore, the number of adjacent pairs of cards of different rank decreases by 2 with this operation.

Suppose that performing the operation would pick up the bottom card of the deck. That means that all cards of rank X are in two contiguous blocks at the top and bottom of the deck before the operation is performed. In addition, since this is the first time the bottom card of the deck is picked up for an operation, X = R. Because of the invariant, that requires every other rank to be in a single contiguous block. In this ordering, there are exactly R adjacent pairs of cards with different ranks. Instead of the operation above, we finish by making pile A consist of the largest block of consecutive cards of rank R, starting at the top, and pile B be the rest of the deck. After performing the operation, R - 1 pairs of adjacent cards of different ranks remain (by a similar argument as before, ignoring the broken and created pairs that involved the leftover deck, since there is none leftover) and the final card of the deck is still R.

After performing the repeated operation floor((R × S - R) / 2) times, the number of adjacent pairs of cards of the same rank decreases to R - 1 if R × S - R is even or R if it's odd. Notice that the number remains greater than R before each such operation, so we would never have picked up the bottom card of the deck. In the even case, the number of adjacent pairs of cards of the same rank is now minimum and we never picked up the bottom card of the deck, so we are at exactly the target ordering. In the odd case, we arrive at the case in which we do pick up the bottom card of the deck with our last operation, but as argued above, that operation also leaves the deck in the target order.

Round 1C 2020 - Code Jam 2020

Overexcited Fan (4pts, 6pts, 12pts)

Test Set 1
In Test Set 1, we can try every possible path we can take. During any given minute we have 5 options: walk a block in one of the 4 directions, or stay still. Since Peppurr's tour is at most 8 minutes long, we can only walk for at most 8 minutes. That is at most 58 possibilities, which is a pretty small number for a computer. For each combination, we simulate our own path and Peppurr's path, recording any encounters. After trying all possibilities, the solution is IMPOSSIBLE if we did not record any encounters, or the earliest time of those if we did.

Test Set 2
In Test Set 2, just as in Test Set 1, Peppurr's tour will remain within a single north-south street. Notice that if we want to meet Peppurr at an intersection (a, b), the order in which we walk the blocks doesn't matter as long as we walk east a times more than west and north b times more than south. So we may assume that we can finish all of our eastward walking, for example, before walking in any other direction. This means an optimal strategy can begin with X blocks of walking east. After that, we are in the same north-south street as Peppurr, so we can walk towards the tour until we meet or we are just 1 block away, in which case we need to stand still for 1 minute to avoid crossing paths with the tour in the middle of a block.

Test Set 3
For Test Set 3, we can simulate Peppurr's tour. If after R minutes it is XR blocks to the east of us and YR blocks to the north (XR and YR can be negative to represent being west or south), we only need to check whether we can reach that intersection in R minutes. Fortunately, this is easy to check: the intersection is reachable in R minutes or less if and only if |XR| + |YR| ≤ R. That is, the intersection must be within an L1 distance (also known as Manhattan Distance) of R.

Therefore, we can solve the problem by simulating Peppurr's path, and for the i-th intersection visited, check if it's reachable in i minutes. If it is, then i is the answer; otherwise, we need to keep looking. If none of the intersections is reachable within the required time, we answer IMPOSSIBLE.

Round 1C 2020 - Code Jam 2020

Overrandomized (9pts, 10pts, 17pts)

Test Set 1
In Test Set 1, the range for possible M values is so small compared to the number of records that each combination (Mi, Ni) has a somewhat large probability 1 / (99 × Mi) of appearing as a particular record, and an even larger probability of being present as at least one record.

Suppose there exists at least one record with Mi = Ni = x for each x in the range 1 through 9. From the record with Mi = Ni = 1 we know that the only letter in Ri represents 1. Then, from all the records with Mi = 2, we can discard the ones where Ri represents 1. The leftovers must be the ones with Mi = Ni = 2, and in those, the only letter in Ri represents 2. In general, after we have decoded the letters for 1 through x, we can take the records with Mi = x + 1 and discard the ones with letters in Ri that are already assigned, and the Ri values of the remaining records will contain the letter that should be assigned to x + 1. Finally, the only remaining unassigned letter should be assigned to 0.

This process works as long as records with Mi = Ni = x exist for each x in the range 1 through 9. The probability of that happening is hard to calculate, but the least likely of those combinations is Mi = Ni = 9, with a probability of only 1 / (99 × 9) per record. The probability of that appearing at least once in 10000 records is greater than 99.999%. The smaller values of x have even higher probability. Of course, the probability of all 9 coexisting is smaller than that, and the probability of that happening in all 10 cases is even smaller, but still decent enough. In addition, there is a really small probability that the letter representing 0 doesn't appear at all in the input, but if that were the case, no algorithm could find it. Given that this is a Visible Verdict test set, we can give the solution a try, and confirm that it passes.

There are additional heuristics we could add to the algorithm. For example, if we can't find the value for 9 in this way and we need to distinguish the two remaining letters to assign to 9 and 0, just having a record whose Ri starts with one of the letters is enough to know that that one should be a 9, since 0 cannot be a leading digit. This further increases the probability of the method working. We can add more and more heuristics to cover the remaining cases, but at some point it's easier to just try something more general.

Test Set 2
In Test Set 2, the probability of Mi being a single digit is small, and we cannot rely on that happening, let alone several times and with extra conditions. However, we can treat records that have Mi and Ri of the same length similarly, by simply using their first letters and then using those (digit, letter) pairs as we did in the solution for Test Set 1.

Test Set 3
At this point, it seems like we may have to throw all of the above insights away, because they are all predicated on knowing Mi. However, the "use the leading digit/letter" insight we used to solve Test Set 2 is actually the first step toward solving Test Set 3 as well.

In Test Set 3, each record's information comes from a single integer, not two, so we cannot use the association between the two parts as before. We can start by checking the distribution used to generate the only piece of information we have. The probability of any particular Ni being equal to x is the sum of 10-16 / y for y in [x, 1016 - 1], which can be approximated by 10-16 (ln 1016 - ln x). In other words, the probability is decreasing in x. Because of this, leading digits are more likely to be smaller than larger. Moreover, even though the decrease in probability between the actual result being x and x + 1 is small, the decrease in probability between the leading digit being d and d + 1 is large, because it aggregates the differences between the probability of dS being more than the probability of (d+1)S for each possible suffix S. This is a version of Benford's law.

Therefore, a possible solution is to calculate the frequency with which each letter appears as a leading digit, and assign the highest frequency to 1, the second highest to 2, etc. The only letter that never appears as a leading digit, and therefore has the minimum frequency, should be assigned to the digit 0.

Round 1C 2020 - Code Jam 2020

Oversized Pancake Choppers (10pts, 16pts, 16pts)

Test set 1
In the first test set, we are only asked to produce 2 or 3 equal slices. Let us consider these cases separately.

For D=2, if we already have two equal slices, then we don't need any cuts. If no two slices are equal, then we can cut any slice into two equal halves with one cut.

Similarly, for D=3, if we already have three equal slices, then we don't need any cuts. We can also cut any slice into three equal slices with two cuts. The extra case to consider is whether we can do it with a single cut.

If we do only 1 cut we end up with N+1 slices: N-1 original slices and two new slices. Three of them need to be of the same size, so this size has to be equal to the size of at least one uncut slice. We can try all possibilities (up to N) for the target size and all possibilities (up to N) for which slice we cut. If the slice p to be cut is not larger than the target size s, we disregard the case. Otherwise, we cut p into a part of size s and another part of size Ap-s. Then, if there are 3 slices of size s in the set of N+1 slices, it is possible to do it with one cut. If that doesn't happen for any of the considered possibilities, then we definitely need two.

Test Set 2
Let's define a fully-usable slice as a slice which we can use fully to produce the slices we need, either by cutting it into 2 to D equal sized slices, or just using it in its entirety as it is. That is, a fully-usable slice is a slice that will leave no further leftovers.

Here is a key observation: for every slice we will produce, we will need to use one cut, except possibly for one slice cut from each fully-usable slice we use. That is, we will need D-K cuts to produce D equal slices, where K is the number of slices used fully.

For example:

It is always possible to produce D equal slices by cutting any original slice with D-1 cuts (K=1).
The best possible case is when we already have D equal original slices, because we make 0 cuts (K=D).
Also notice that we never have to consider K=0, since K=1 is always possible by cutting one original slice into equal pieces, and we want the maximum possible K.

By the observation above, the final size of our produced slices (hereafter the target size) is going to be one of the original sizes (from one of the slices we fully use) divided by an integer between 1 and D. Therefore, we have to check at most N×D possible target sizes. With any other size, we would have 0 fully-usable slices.

For each such target slice size, we should do the following:

First, we ensure that we can actually use it: if the total number of slices of this size that can be produced by cutting all original slices is less than D, then, obviously, this size is not useful for us. A slice of size Ai can be used to produce up to floor(Ai/s) slices of target size s.
Then, we need to find all the fully-usable slices: their size is evenly divided by the target slice size, with no remainder.
Now, since we need to maximize the number of original slices that are fully-usable, we can use a greedy approach and take those slices one by one in non-decreasing order of size, until we have as many fully-usable original slices as possible (that is, taking the next one would cause us to produce more than D target slices). If we use up all fully-usable original slices, we could use the other non-fully-usable original slices in any order.
As per our prior observation, each fully-usable original slice gives us one "free" target slice; all other target slices will need a cut each to be produced. That is, the total number of cuts for the current target slice size is D minus K, the number of fully-usable original slices we use.
The part above can be done in O(N) time if we sort the Ais once (in non-decreasing order) at the beginning, resulting in an O(D×N2) time complexity for the overall algorithm.

Test Set 3
First of all, notice that we can precompute the largest possible target slice size in O(log(max(Ai))×N) time with a binary search on the target size. Then, we can save some time by not considering any target slice sizes that are greater than the calculated limit.

Now, as in the solution for Test Set 2, we iterate through the Ais (in non-decreasing order) and all numbers of cuts c (1 to D). Instead of doing an additional pass through N original slices as in the solution for Test Set 2, we can just mark this original slice as a fully-usable slice for target size Ai/c. To do that efficiently, we use a dictionary (ideally implemented as a hash table) where the keys are valid target sizes, and the values are tuples containing the number of fully-usable slices found so far for that target size, and the number of target slices produced from them. Notice that the keys are fractions; to ensure that we do not represent the same fraction in multiple ways, we can use an algorithm to find greatest common divisors and ensure that all fractions are reduced.

For each target size Ai/c we add 1 and c to the corresponding dictionary values for key Ai/c (assuming the default value for an unset key is zero). If after this operation the number of produced target slices of size Ai/c would exceed D, we should just not consider that slice as fully-usable for this case.

Then we can choose the maximum possible number M of fully-used slices across all valid target slices, and the result is D-M. This improves the time complexity of the algorithm to O(D×N).

Round 2 2020 - Code Jam 2020

Incremental House of Pancakes (5pts, 14pts)

Test Set 1
Test Set 1 is small enough that we can simulate the process. Since each step removes at least one pancake from a stack, this takes at most L + R operations. One thing we can notice is that because the i-th step removes i pancakes, the first i steps together remove i × (i + 1) / 2 pancakes. This means that the number of steps is actually bounded by 2 × sqrt(L + R), which means the simulation algorithm is also O(sqrt(L + R)). It would work for limits that are much larger than the ones in this test set!

Test Set 2
Unfortunately, O(sqrt(L + R)) is still too slow for the 1018 limits in Test Set 2, especially with 1000 cases to go through!

We can simulate multiple steps at a time by noticing that there are two distinct phases of the process. The first phase uses a single stack: the one that starts out with more pancakes. The second phase begins when that stack has a number of pancakes remaining that is less than or equal to the number in the other stack. Notice that if the left stack is the one we serve from in phase 1, it is possible that it is also the first one to be used in phase 2. It is also possible that no customer is served in either phase.

We may serve a lot of customers in phase 1 depending on the difference in size between the stacks at the beginning. If we serve i pancakes from one stack, we remove i × (i + 1) / 2 pancakes from it, so we can efficiently calculate how many customers we can serve in phase 1 by finding the largest i1 such that i1 × (i1 + 1) / 2 is less than or equal to the difference in number of pancakes between the two stacks at opening time. We can calculate that either by solving a quadratic equation and rounding carefully, or by using binary search.

The second phase is where the magic happens. Let us say that when serving customer i, stack X is used and Y is not, but then we serve customer i + 1 from stack Y. Therefore, stack X lost i pancakes and stack Y lost i + 1 pancakes. Since X was used instead of Y for customer i, X must have had no fewer pancakes than Y had at that time. Since X lost fewer pancakes than Y, X must have had more pancakes than Y after we served customers i and i + 1. This means if we ever use two different stacks in the order (X, Y), we must use X next. And, using the same reasoning with the roles reversed, we have now have used (Y, X) most recently, so we will use Y next, and so on. So, once we have used both piles, we always go on alternating between them.

We can use this observation to efficiently determine what happens in phase 2. After updating the original totals by subtracting the pancakes served in phase 1, we know which stack is used first in phase 2. The first stack will be used to serve customers i1 + 1, i1 + 3, i1 + 5, ... which means that if it is used for c1 customers, a total of (i1 × c1) + (c1)2 pancakes are served from it. At this point, we know the value of i1, so once again, we can calculate c1 by solving a quadratic equation or binary search. The other stack is similar, since it will be used to serve customers i1 + 2, i1 + 4, i1 + 6, ... so if it is used for c2 customers, a total of ((i1 + 1) × c2) + (c2)2 pancakes will be served from it. So the final number of customers served is i1 + c1 + c2. The total numbers of pancakes served from each stack come from the quantities in phase 2, with the quantity from phase 1 added to whichever stack was first.

If we use binary searches, each phase requires O(log(L + R)) time. If we directly solve the quadratic equations, each phase is actually constant time. Either is fast enough for the limits of this problem.

Notice that solving quadratic equations may be harder than usual, since typically that involves computing some square roots. While most languages provide a way to do that, they do it with double precision floating point, which does not have enough precision for this problem and can lead to off-by-one errors. We should either compute square roots directly on integers (by binary searching for the answer, for example) or use the built-in function, and then check the returned value and other values in its vicinity to find the correct rounded result.

Round 2 2020 - Code Jam 2020

Security Update (9pts, 11pts)

Let Ri be the number of computers that receive the update before computer i, and Ti be the time between computer 1 and computer i receiving the update. For each i, the input gives us exactly one of these numbers. We can set R1 = T1 = 0 for convenience.

A simplified problem
Let us assume for now that we have all the Tis. If computers i and j share a direct connection and Ti=Tj, then any path that gets to computer i in time Ti does not go through computer j, and vice versa, because all latencies are positive. Therefore, we can assign any positive latency to all those connections. If computer i has a given Ti, it means that any connection coming from computer j with Tj < Ti needs to have a latency of at least Ti-Tj, or otherwise the update could get to computer i in less than Ti time by getting to computer j in Tj time and then using that connection. In addition, for at least one j, the latency of the connection between i and j has to be exactly Ti-Tj, or otherwise the time for the update to reach computer i would be larger than Ti. One simple way to solve this problem is to make all connections between computers with different T values have a latency of exactly |Ti-Tj|; this takes O(D) time.

Notice that the algorithm above finds a valid assignment for any set of Tis. To solve the actual test sets, we are left with the problem: given some Tis and some other Ris, assign all of the non-given values in a way such that sorting the computers by Ti leaves them sorted by Ri, and vice versa. In particular, computers with equal T values should have equal R values, and vice versa.

Test Set 1
In this test set, we can solve the subproblem from the previous section by setting Ti := Ri.

Test Set 2
For Test Set 2, we again focus on solving the subproblem. We do that by first ordering the computers by what is going to be their final Ti value (or equivalently, by their final Ri value). We can partition the set of computers other than the source computer into two: those for which we know Ri (part R) and those for which we know Ti (part T). We can sort each of those in non-decreasing order of the known value. We now have 2 sets that are in the right relative order, and need to merge them as in the last step of Merge Sort. We assign the source computer first. Then we iterate through the remaining C-1 slots in order. Suppose we have already merged N computers, and let computer k be the last one of those. Let i and j be the first computers remaining in parts R and T, respectively. If Ri ≤ N, we take computer i next and assign Ti := Tk if Ri = Rk, and Ti := Tk+1 otherwise. If Ri > N, we take computer j next and assign Rj := Rk if Tj = Tk and Rj := N otherwise.

We can prove that if the original set of values is consistent with at least one latency assignment (which the statement guarantees), this generates a valid order and assignment of missing values, and moreover, it generates one in which the T value of the last computer in the order is minimal. We do that by induction on the number of computers. For a single computer, this is trivially true. Suppose we have C > 1 computers. By our inductive hypothesis, the first C-1 computers in the order were ordered and assigned values in a consistent way, with a minimal T value for the last computer among all options. Let us say the last computer in the full order is computer i, and the next-to-last computer is computer j. By definition of how we assign missing values, Ri = Rj if and only if Ti = Tj. If indeed Ri = Rj and Ti = Tj, then the condition for the final assignment is equivalent to the inductive hypothesis. If computers i and j come from the same part, then the ordering choice between them was fixed, and the assignment of T values if needed is clearly minimal. So consider further the case in which computer i comes from a different part than computer j, and their R and T values are different. We have two cases: either computer i was in part R, or in part T.

If computer i was in part R, then its assigned T value is by definition the largest among all computers, and it's the smallest possible for it to go after computer j, whose value is minimal by the inductive hypothesis. As for the order, Ri ≤ C-1 per the limits. Since computer j comes from part T and was chosen for position C-1 (when N was C - 2), that means Ri > C - 2. Therefore, Ri=C-1, and the chosen position is correct.

If, on the other hand, computer i was in part T, then its T value is minimal because Ti is fixed. As for the order, notice that all computers have either a T value strictly less than Ti or an R value strictly less than C-1, so none of them could have been last. By the inductive hypothesis, Tj is minimal among all possible orders, which means, by the existence of a full assignment, it has to be Tj < Ti, which implies the consistency of the final order and value assignment.

Round 2 2020 - Code Jam 2020

Wormhole in One (10pts, 16pts)

Test Set 1
As the limits for Test Set 1 are very small, we should be able to apply a brute-force algorithm and check all possible directions, starting points, and links between holes. However, there are infinitely many directions and starting points. Let's find a way to work with a finite number of possibilities.

First, let us observe that in order for the ball to touch more than two holes, some pairs of holes must be on lines parallel to the chosen direction. Otherwise, the best we can do is to link together two holes; the ball will go through them and will not touch any other holes.

So, when choosing an initial hit direction, we only need to consider those that are parallel to lines that connect pairs of holes.

Also, the exact starting point is not important. We can decide which hole we want to enter first, and then, regardless of which hit direction we choose, we can position the ball such that it will enter that hole (before any others). For example, because the holes are always at unique integer coordinates, we can choose a starting distance of 0.1 from the hole, in the direction that is the opposite of our hit direction.

Now, we can try all possible starting holes and linking schemes — with a recursive backtracking algorithm, for example — and choose the combination that touches the largest number of holes. This should be enough to pass Test Set 1.

Test Set 2
Suppose for now that we have chosen the direction of the hit. Let's calculate the maximum possible answer given that decision.

Imagine lines parallel to the chosen direction and going through all of the holes. Notice that each hole is on at most one such line. We will call a line an odd line if it contains an odd number of holes (but more than one), and an even line if it contains an even number of holes (at least two). If there are lines which only contain one hole, we call these holes standalone holes.

Let's also consider the holes along each line to be ordered in the chosen direction.

Note that:

We cannot touch more than two standalone holes: one at the beginning, and one at the very end of the ball's journey.
In the best case, we would touch all non-standalone holes.
Let's say we have Codd total holes on the odd lines, Ceven total holes on the even lines, and C1 standalone holes. Then the answer is not greater than Codd + Ceven + min(2, C1).

To touch two standalone holes, we should touch an even number of holes between them. To understand why, note that first of the standalone holes must be an entry to a wormhole, and the second one must be an exit from another wormhole. All of the holes that the ball touches in between those starting and ending holes must be linked in pairs by wormholes, which means there should be an even number of holes. This also means that if the number of non-standalone holes is odd, then we will not be able to touch two standalone holes; in such a case, the answer will not be greater than Codd + Ceven + min(1, C1).

As we will see, these upper limits can actually be achieved, so the answer is:

Codd + Ceven + min(1, C1), if Codd + Ceven is odd
Codd + Ceven + min(2, C1), if Codd + Ceven is even
Note that the parity of Codd + Ceven is the same as the parity of Codd, as Ceven is always even.

Let's construct the linking scheme for the case when Codd is even and C1 is greater than 1:

Connect one standalone hole to the first hole of any even line.
Connect the other holes on that line pairwise consecutively (the last hole will remain unconnected).
Connect the last hole on that line to the first hole of another even line, and repeat steps 2 and 3 until only odd lines are left untouched.
Connect the last hole of the last even line to the first hole of any odd line.
Connect the second hole of that odd line (A) to the second hole of another odd line (B).
Connect the last hole of line B to the first hole of line B.
Connect the last hole of line A to the first hole of another odd line.
Connect the remaining holes of line A in consecutive pairs.
Connect the remaining holes of line B in consecutive pairs.
Repeat steps 5-9 until all the odd lines are used. When there are no more odd lines, connect the last hole of the last odd line to an unused standalone hole.
This scheme can be easily modified for the other cases, when Codd is odd, and/or C1 is less than 2.

Summarizing this, we can make full use of all of the odd and even lines and up to two standalone holes.

To calculate the number of holes on each line for a given direction, we can iterate through all ordered pairs of the holes and find the equation of the line that connects them in the form y = mx + y0. Now, for each m, we store how many times we see each y0. This number will be equal to the number of pairs of holes on this line, which we can use to calculate the number of holes. Note that we only need to do this once, as we will get the counts for all directions we are interested in.

Now, we can iterate through all the directions and calculate the answer for each one as described earlier by iterating through all the lines parallel to the current direction. Our final answer will be the maximum over the answers for all directions.

Note that even though there are O(N2) directions, and O(N) lines parallel to a direction, the total number of lines for all directions is O(N2), as every line can be parallel to two (opposite) directions. So the total time complexity of this solution implemented optimally is O(N2), but slower implementations might also pass.

Round 2 2020 - Code Jam 2020

Emacs++ (12pts, 23pts)

Test Set 1
TL;DR: Think of the brackets like a tree where a position's parent is the closest pair of brackets that contain that position. Go to the Lowest Common Ancestor, then back down the tree.

With this problem, our first thought may be to write a breadth first search for each query and add up the values. Unfortunately, this is much too slow for the bounds provided. Thankfully, we can make use of the structure of the Lisp++ program and the fact that all movement costs are 1 (going left, going right, or going to the matching bracket—we will call going left or right "walking" and going to the matching bracket "jumping" to make the explanation easier).

Two somewhat simple observations are needed before we can talk about the intended solution. The first observation needed is that we almost always want to jump instead of walking inside of a pair of brackets. If we are at a bracket endpoint, and jumping to the other endpoint doesn't make us "overshoot" the query's destination, then we should take that jump instead of walking there. The second observation needed is that if a pair of brackets contains both our current position and our destination, we should never move outside of that pair of brackets.

Using just these two observations, let's discuss what an optimal path looks like for a specific query. Consider the closest pair of brackets that contains both the start and end of the query:

The optimal path for this query must move "up" to this level:

One we are at this level, we jump along the top level of the siblings until we are at the brackets for the end query, then move "down" to the query answer. Note that we can jump either left or right (one of which will wrap around when we hit the bracket's endpoint) so we should check both and take the minimum.

At this point, we treat the Lisp++ program as a tree with the parents in the tree being the closest pair of brackets that enclose us. Note that one of the parents is strictly closer than the other, so we will always go to that parent first on our path "up" or "down" the tree. The layer at the top is simply the lowest common ancestor in the tree, which can be computed in O(log K) time.

Test Set 2
TL;DR: (1) Find two pairs of brackets that partition the Lisp++ program into 4 disjoint sections. (2) Compute the shortest path from the break points to answer queries that go from one region to another. (3) Recurse on the 4 subregions.

For Test Set 2, some of the properties we used are no longer available to us. In particular, it may now be optimal to venture outside of the LCA described above. There are two main classes of solutions that are used to solve this test set. One is a modification of the LCA algorithm above. We will show the other here because it demonstrates an algorithm that is less traditional. Rather than answering the queries one at a time, we will employ a method where we can solve them in batches.

The key property we will be using is the ability to partition the Lisp++ program into multiple (almost) independent sections. First, let's start with a crucial observation. Consider a pair of matching brackets. The only way to get from inside the brackets to outside the brackets is to cross through the brackets themselves. These brackets that are used to split the input into the different sections will be called the special brackets throughout the explanation.

We can use this fact to answer queries differently. Rather than computing the distance from the starting point to the ending point of the query, we can compute the distance from each of the special brackets to the query's starting point and from the special brackets to the query's ending point. Since we know that any shortest path must go through one of the special brackets, we know that the sum of these two distances is the answer to the query.

At first, it doesn't seem like this helps us. However, note that we can compute the answer to all queries that go from inside the brackets to outside the brackets at once! Just compute the distance from the special brackets to all locations (using Dijkstra's algorithm, for example) and use the method above.

Now what about all of the queries that start and end inside the special brackets (or outside the special brackets)? Well, the shortest path from the start to the end might go through the special brackets we chose, so we should make note of the potential answer if it does. Now, we're only interested in paths that do not use the special brackets.

Thus, we can split our problem up into two sub-problems: the "inside" part and the "outside" part. Split the queries up into their appropriate parts (inside and outside) and recursively solve each of these. Note that the special brackets can be removed completely since we know the answers to any query involving them. There is one issue, though: this isn't necessarily fast enough. ☹ If we choose our special brackets poorly so that the "inside" is always a short string, then this algorithm will need O(K2) time. In order to make this fast, we need to ensure that both subproblems are about half the size of the original problem. If we add in some more heuristics and choose our bracket pair randomly, we can make our code faster on average, but it is not guaranteed to pass the data. There is a slight tweak we can make described below which will save us!

Instead of spliting the string into 2 sections, we will instead split the string into 4 sections. Consider a specific pair of brackets. Those brackets' parents are the closest pair of brackets that enclose them. If we split using a pair of brackets and their parents' brackets, we split our input into 4 sections. Note that it is impossible to get from those inner side pieces to the other side without crossing one of our two pairs of special brackets (since the parents are the closest brackets to our original brackets that would allow us to do that).

This small change looks like we just made things more complicated, but it solves our issue from above! First, let's add a pair of brackets to the outside of our string and set its Li, Ri, and Pi to infinity. This way, all pairs of brackets have a parent, except for this new infinite pair we added.

Let's consider all the bracket pairs whose span (from starting bracket to closing bracket, inclusive) includes the middle bracket (there are two "middle" brackets; we can choose either). Call these brackets the "middle line brackets". The middle line brackets will form a chain in which each bracket pair nests under another middle line bracket, or is the outermost bracket that we added. Our middle line brackets have some nice properties that we can use.

If we consider the middle line brackets from outermost to innermost, we can observe that the spans of the brackets go from containing more than half the characters (the outermost bracket that we added spans the whole string) in our Lisp++ program, to containing at most half of the characters. This is because they are always getting smaller, and the innermost one spans at most half of all characters. Let's consider this "pivot point". It comprises a pair of consecutive middle line brackets in which one spans more than half, and it has a direct child that is also a middle line bracket that spans at most half. If we take these two brackets and cut them out of the string, we will have broken the string into 4 disjoint (possibly empty) parts, none of which contain more than half of the characters in the string.

Why? Let's say our brackets look like this A ( B ( C ) D ) A. We know that since the outer bracket pair spans more than half, the region A contains less then half of the characters. We know that the inner bracket either crosses or touches the middle line, so the regions B and D contain less than half of the characters. Finally, C has at most half (remember that we chose C specifically because it was the first middle line bracket that contained at most half of the characters). Note that we cannot get from one region to another without crossing a special bracket. In particular, we cannot get between B and D because the outer special brackets are the parent of the inner special brackets.

Thus, we can solve the problem by finding the two pairs of special brackets we are going to split on, using Dijkstra's algorithm to answer the queries that go between different regions (as well as compute potential answers for those queries that do not), and then recursing into the 4 sub-problems. Since each recursion cuts the length of the input string in half, we recurse at most O(log K) times. The sum of the strings at any particular depth of the recursion is at most the length of the original string. So the total work we needed to do at each layer is at most O(K log K) to run Dijkstra from our 4 special brackets. Also, each query is looked at at most once on each layer of the recursion, so the total complexity is O(K log2 K + Q log K).

Some Common Issues
Here is a list of common issues that might explain a Wrong Answer or Time Limit Exceeded verdict:

The edges are directed! This means that the distance from A to B is not necessarily the same as the distance from B to A. For the intended solution, this means we need to run Dijkstra's algorithm twice, not just once.
If we place values in our code about "infinity", those values must be large enough, but not so large as to cause overflows.
Picking a random edge and just breaking it into doing inside and outside is in general too slow. Even if we break the left and the right apart if they're not connected, we can run into issues. Consider the following. Let X=((((( ... ))))), be of length sqrt(N). If the input is XXXXX ... XXXX sqrt(N) times, then picking randomly is not very efficient. In order to split the input, we need to get lucky and hit one of the outer points of X in order to really cut down on the size of the input.

Round 3 2020 - Code Jam 2020

Naming Compromise (4pts, 8pts)

Test Set 1
In Test Set 1, the limits are small enough that we might be tempted to just try lots of possible outputs and keep the best one. However, since there is no a priori limit on the output length, and we can use any of the English alphabet letters in the output, we need to think a little bit first. In addition, we have to actually implement the calculation of edit distance, which we can learn about via various online resources, including that Wikipedia link.

The distance between two strings of length up to 6 is at most 6, since we could just replace every character in the longer string (or an arbitrary one, if they are of the same length) to match the other string, and delete any excess. Moreover, the distance between two strings is never less than the absolute difference in length between them, because we would need to add at least that many characters to the shorter string (if one is indeed shorter) just to make it as long as the other string. It follows that the output should never be longer than 12 characters.

In addition, if the output N contains a letter that is not present in either of the input strings, exchanging that letter in N for a letter contained in one of the inputs cannot increase either distance, which can be proven by induction by inspecting the recursive definition of edit distance. Therefore, we can limit our search to strings of length up to 12 over the alphabet {X, Y, Z}. There are less than a million candidates, and calculating the edit distance for such short strings is really efficient, so this is fast enough to solve Test Set 1.

It is possible to prove tighter limits on the output, and this can drastically reduce the number of candidates and make the solution even faster. We leave the details to the reader, but consider whether we ever really need an answer longer than 6 letters. If we have ABCDFG and ACDEFG, for example, then instead of compromising on the seven-letter ABCDEFG by adding E to the first name and B to the second name, we could just as easily delete B from the first name and E from the second name, compromising on the shorter ACDFG instead.

Test Set 2
For Test Set 2 we need to use some additional insights. The most important one is noticing that edit distance is actually a distance, and has typical properties of distances like being reflexive and satisfying the triangle inequality. Let e(s, t) denote the edit distance between two strings s and t. For an output N, we want e(C, N) + e(J, N) = e(C, N) + e(N, J) to be minimal. By the triangle inequality, e(C, J) is a lower bound on this quantity, and an achievable one. For example, we can set N equal to C since e(C, C) = 0.

By the reasoning above, we need to find an N such that e(C, N) + e(N, J) = e(C, J) and |e(C, N) - e(N, J)| is as small as possible. Luckily, the definition of edit distance hints at a way of doing that. If the edit distance between C and J is d, it means that there is a path of d valid operations on C that results in it turning into J. Formally, there are d - 1 intermediate strings S1, S2, ..., Sd - 1 such that e(C, Si) = i and e(Si, J) = d - i. Therefore, it suffices to pick Sd / 2. If d is odd, both rounding up and down work, since for both possibilities, |e(C, N) - e(N, J)| = 1.

To find S1, S2, ..., Sd - 1, we can use a common technique to reconstruct the path that achieves an optimal result as found via dynamic programming (DP).

For example, let Ca be the prefix of length a of C and Jb be the prefix of length b of J. The usual algorithm to compute edit distance is based on a recursive definition of a function f(a, b) that returns e(Ca, Jb) (see the link from the previous section for details). Similarly, we can define g(a, b, k) as a fuction that returns e(Ca, Jb) and also a string S such that e(Ca, S) = k and e(Jb, S) = e(Ca, Jb) - k. The new function g can be defined with a similar recursion as f, and then memoized for efficiency. The details are left as an exercise for the reader.

Round 3 2020 - Code Jam 2020

Thermometers (5pts, 19pts)

Test Set 1
First of all, notice that Xi splits the circle into a sequence of segments, so that throughout each segment, the temperature of all the points is the same. Let's say that the i-th segment is the segment which starts at Xi and goes clockwise. We will use di as the length of the i-th segment. Also note that per the problem statement, there are no adjacent segments with the same temperature, so we can safely ignore the actual temperatures Ti.

We can make the following observations:

We never need more than two thermometers on each segment, as a thermometer put in between two others on the same segment will not cover any points which are not already covered by the original two.
In an optimal answer, we should never have two adjacent segments with 2 thermometers each. If we have two adjacent segments with 2 thermometers in each, we can simply "push" the middle 2 thermometers outwards until at least one of them runs into the other thermometer in their segment, which reduces the total number of thermometers by at least one.
The adjacent thermometers on different adjacent segments need to be equidistant from the common point of those segments. Otherwise, the temperature of some points between these thermometers will be incorrect.
The thermometers in an optimal answer can always be put in integer or half-integer positions. If we have an answer in which that is not the case, we can "push" the thermometers until they reach integer or half-integer positions.
Given these observations, and the low limits in Test Set 1, we can iterate through all possible configurations and choose the one that gives the best answer.

Test Set 2
Note that if we know the position of a thermometer on segment A that is closest to an adjacent segment B, we can calculate the anticipated position of a thermometer on B. To do so, we need to "mirror" the known position over the common point of the segments. If the mirrored position doesn't belong to the segment B, then it is not possible for segment A to have the closest thermometer at this position.

First, let's assume that the answer is greater than N. We can apply the following greedy strategy to find the optimal answer (we will show below that it actually works).

Let's start with some segment A and assume that this is the only segment. We can put a thermometer at any point of A (except the endpoints, per the problem statement) and it will cover the whole segment. So the interval of valid positions on A is the whole segment. Now, let's consider an adjacent segment B and see how we can put the thermometers on both segments, so that only two thermometers are used. We can mirror the interval of valid positions in A over the common point of these segments. The interval of valid positions in B is the intersection of the mirrored segment and B. We will refer to this operation as propagation.

We can continue propagating this interval through as many segments as possible and stop if the propagated interval would be empty. If we propagate the last interval back to A through all the intermediate segments, this will give us valid positions for the thermometers to cover all these segments with one thermometer per segment.

As we cannot propagate the current interval anymore, we can define a new interval of valid positions on the current segment, taking into account the old one, effectively putting two thermometers on this segment, and repeat the process again. Once we reach segment A, we should verify that the last interval begins before the first interval ends, so that we could put two thermometers on segment A.

The answer will be N (as we have to put at least one thermometer on each segment) plus the number of times we had to start with the new interval, effectively putting two thermometers on the corresponding segment. We can try all segments as the starting one and choose the best answer. Again, the proof that this always works is given below.

Now let's see how to handle the case of the answer being equal to N, when we put exactly one thermometer on each segment. If we assume that zi is the position of a thermometer on segment i, measured from the segment's beginning (point Xi), then, we can calculate the other positions as follows:

z2 = d1 - z1

z3 = d2 - z2 = d2 - d1 - z1

...

zN = dN-1 - zN-1 = dN-1 - dN-2 + dN-3 - ... +/- z1

z1 = dN - zN = dN - dN-1 + dN-2 - dN-3 + ... +/- z1

These equations give us a quick test for whether the answer could be N.

In case N is even:

z1 = dN - dN-1 + dN-2 - dN-3 + ... - d1 + z1, or, subtracting z1 from both sides:

0 = dN - dN-1 + dN-2 - dN-3 + ... - d1

As long as the above holds, any z1 will satisfy the equation. The caveat here is that some of the zis may not be inside of the corresponding segments. So we need to verify that we can do the same propagation as we did earlier, and reach the first segment without adding the second thermometer on any segment. If we can do so, then the answer is N.

In case N is odd:

z1 = dN - dN-1 + dN-2 - dN-3 + ... + d1 - z1, or, adding z1 to both sides:

2z1 = dN - dN-1 + dN-2 - dN-3 + ... + d1

In this case, there is exactly one value for z1. As in the previous case, we need to verify that this value produces a valid set of positions, and we can do this by propagating z1 through all the segments. If we can do so, then the answer is N.

Propagating an interval through all segments is O(N), and we do this at most once to check N as an answer, and do this N times if the answer is not N. This gives us O(N2) running time.

Proof of greedy solution

Here is an image which roughly describes the steps in the proof below:

We will concentrate on the case when the answer is greater than N, since for the case of N the solution is constructive.

Let's define a few additional terms.

A chain is a sequence of adjacent segments, where there is some placement of thermometers such that each segment can be covered by one thermometer, except for the first and the last ones, each of which requires two thermometers. Note that it is possible that a chain covers the whole circle, starting in some segment, wrapping around the whole circle, and ending at the same segment.

A maximum chain is a chain that cannot be extended clockwise by adding the next segment because it is not possible to place the thermometers (as per the definition of a chain) to cover such a chain.

Two chains are consecutive if the last segment of one chain is the first segment of the other chain. Note that on the common segment, we will have to use two thermometers.

A valid sequence of chains is a sequence of consecutive chains containing all the given segments.

Note that any valid positioning of the thermometers is a valid sequence of chains, and the number of thermometers there is N plus the number of chains. The optimal answer is achieved in the positioning requiring the smallest number of chains.

Lemma 1. If we have a valid sequence of chains and the first chain is not a maximum chain, we can take some segments from the next chain(s) and attach them to this one until it becomes maximum, while keeping the sequence valid and not increasing the total number of chains.

Proof

Let's add a few more terms to our vocabulary.

The position of a thermometer on the i-th segment is the distance between the thermometer and Xi.

A chain flexibility interval is a set of valid thermometer positions on a segment of a chain, which can be correctly propagated through the whole chain. Note that it is always a subsegment (maybe of length 0) of a segment.

The first and last flexibility interval of a chain are the instances of the chain flexibility interval on the first and last segment of the chain accordingly.

We can now redefine a valid sequence of chains as a sequence of consecutive chains, containing all the segments, such that on every segment connecting two chains, if (xlast, ylast) and (xfirst, yfirst) are the last and first flexibility intervals of these chains, then xlast ≤ yfirst.

Now let's consider the first and second chains in some configuration:

a, b and c are the lengths of the first, second and the third segments (if any) of the second chain. Note that a is also the last segment of the first chain.
(x1, y1) - last flexibility interval of the first chain (on segment a).
(x2, y2) - first flexibility interval of the second chain (also on segment a).
For clarity:

x1 ≤ y2 by the definition of a valid sequence of chains.
(a - y2, a - x2) - second flexibility interval of the second chain (on segment b).
Let's connect segment b to the first chain. Then:

The last flexibility interval of the first chain will be mirrored to segment b and becomes (a - y1, min(b, a - x1)).
The second flexibility interval of the second chain will become the first one, and in the worst case will be (a - y2, a - x2) (in other cases it will grow and will contain this interval fully).
First, let's consider the case where the initial flexibility intervals of the chains are intersecting (x2 ≤ y2). In this case the new configuration is valid, as a - y1 ≤ a - x2, so we can go on to try the next segment if possible.

Otherwise, if the original flexibility intervals were not intersecting (x2 > y1), then the new configuration is not valid. Again, the new flexibility intervals will be (a - y2, a - x2) and (a - y1, min(b, a - x1)), they will not intersect, and the first flexibility interval of the second chain will come before the last flexibility interval of the first chain (this is why the configuration is invalid).

However, we can notice that because (a - y2, a - x2) belongs to the second chain, it could be successfully propagated to the next segment (c) by definition. But, because (a - y1, min(b, a - x1)) is even closer to the segment border, we must be able to propagate it as well. This means that we can also attach the next segment to the first chain:

The first flexibility interval of the second chain becomes (b - (a - x2), b - (a - y2))
The last flexibility interval of the first chain will be (b - min(b, a - x1), min(c, b - (a - y1)))
Now the configuration is valid, because b - min(b, a - x1) ≤ b - (a - y2), and x1 ≤ y2 by definition.

What if we don't have a second segment to attach? Then we won't be able to attach the first one either, since the flexibility intervals were not intersecting, and the flexibility interval of a single segment is the whole segment.

Lemma 2. An optimal answer can always be achieved with a sequence of maximum chains and one potentially non-maximum chain.

Proof

Let's consider the first chain of an optimal answer. Let's attach segments from the next chain while we can (we are allowed to do so by Lemma 1). Repeat with the next chain, and so on. When we reach the last chain, leave it alone. Now all the chains except for the last one are maximum, and the last one can be either maximum or not.

The solution described above is building all possible sequences mentioned in Lemma 2, and by that lemma, some of them will be optimal.

Note that our proof has not dealt with small chains properly—that is, chains of length 1, 2 and 3 may not work properly in the proof above, but they are easy to deal with independently as special cases.

Round 3 2020 - Code Jam 2020

Pen Testing (6pts, 11pts, 15pts)

How to approach this problem?
At first sight, it might seem impossible to achieve the number of correct guesses required in this problem. If we just pick two pens at random without writing anything, the probability of success will be 46.666...%. And whenever we write anything, the amount of remaining ink only decreases, seemingly just making the goal harder to achieve. And yet we are asked to succeed in 63.6% of test cases in Test Set 3. How can we start moving in that direction?

Broadly speaking, there are three different avenues. The first avenue is solving this problem in one's head or on paper, in the same way most algorithmic problems are solved: by trying to come up with smaller sub-problems that can be solved, then trying to generalize the solutions to those sub-problems to the entire problem.

The second avenue is more experimental: given that the local testing tool covers the entire problem (i.e., there is no hidden input to it), we can start trying our ideas using the local testing tool. This way we can quickly identify the more promising ones, and also fine-tune them to achieve optimal performance. Note that one could also edit the local testing tool to make it more convenient for experimentation; for example, one could modify the interaction format so that only one test case is judged at a time.

The third avenue starts with implementing a dynamic programming solution that can be viewed as the exhaustive search with memoization: in each test case, our state is entirely determined by how much ink have we spent in each pen, and by which pens are known to have no ink left. Our transitions correspond to the basic operation available: trying to spend one unit of ink from a pen. Moreover, we can collapse the states that differ only by a permutation of pens into one, since they will have the same probability of success in an optimal strategy. Finally, we need to be able to compute the probability of success whenever we decide to stop writing and pick two pens; we can do this by considering all permutations, or by using some combinatorics (which is a bit faster). This approach is hopelessly slow for N=15. However, we can run it for smaller values of N and then use it as inspiration: we can print out sequences of interactions that happen for various input permutations, and then generalize what happens into an algorithm that will work for higher values of N.

Test Set 1
How can we find a small sub-problem in which writing actually helps improve the probability of success? Suppose we have only three pens, with 1, 9 and 10 units of ink remaining. If we just pick two pens at random, we will have two pens with at least 15 units of ink between them only with probability 1/3: 9+10≥15, but 1+9<15 and 1+10<15. However, if we first write 2 units of ink with each pen, we will know which pen had 1 unit of ink since we will fail to write 2 units with it, and will therefore know that the other two pens have 7 and 8 units of ink remaining, so we can pick them and succeed with probability 1!

How can we generalize this approach to the original problem? We can pick a number K and write K units of ink with each pen. Then, we will know the identities of pens 0, 1, ..., K-1 so that we can avoid taking them to the South Pole. However, the other pens will have 0, 1, ..., N-K units of ink remaining, which seems to be strictly worse than the initial state. But we can include a small optimization: we can stop writing as soon as we find all pens from the set 0, 1, ..., K-1. For example, if the two last pens both have at least K units initially, we will stop before we reach them, so by the time we identify all pens from the set 0, 1, ..., K-1 and stop writing, we will not have touched those two pens at all. Therefore we will know that they both have some amount of ink from the set K, K+1, ..., N, and picking them gives us a much higher probability of success. Of course, this will not always be the case, and sometimes we will have only one or zero untouched pens. In this case we should pick the untouched pen if we have it, and add a random pen from those that have written K units successfully.

By running this strategy against the local testing tool with various values of K, we can learn that it succeeds in about 56.5% of all cases when K=3, and is good enough to pass Test Set 1. We will call this strategy fixed-K.

There are certainly many other approaches that pass Test Set 1 — for example, less accurate implementations of the solutions for Test Set 2 and Test Set 3 mentioned below.

Test Set 2
How do we make further improvements? Our current approach suffers from two inefficiencies. The first inefficiency has to do with the fact that we keep writing K units of ink with each pen even if we have already found the pen which had K-1 units of ink initially. Since we are now searching for 0, 1, ..., K-2, it suffices to write only K-1 units of ink with each subsequent pen at this point. More generally, if X is the most full pen from the set 0, 1, ..., K-1 that we have not yet identified, we only need to write X+1 units of ink with the next pen. Having identified all pens from the set 0, 1, ..., K-1, we stop writing and take the two pens not from this set that have written the least units to the South Pole. We will call this improvement careful writing.

It turns out that careful writing with K=4 increases our chances substantially to 61.9%, enough to pass Test Set 2!

The second inefficiency is that out of the pens that are determined to have at least K units, we take two to the South Pole, but the rest are useless. For example, we never end up taking any pen except the last K+2 pens to the South Pole! Therefore we could start by writing with the first N-K-2 pens until they are used up and have exactly the same success rate. We can improve the success rate if we use the information that we get from those pens to make potentially better decisions!

More specifically, we can do the following: initially, write with the first pen until it is used up (and therefore we know how much ink was in it). Then, write with the second pen until it is used up, and so on. In this manner, we will always know exactly which pens are remaining. At some point we should decide to stop gathering information and switch to the fixed-K strategy. The information we gathered can be used to pick the optimal value of K, which is potentially different in different branches.

This gives rise to a dynamic programming approach with 2N states: a state is defined by the set of pens remaining after we have used up some pens. For each state we consider either writing with the next pen until it is used up, or trying the fixed-K strategy with each value of K. Note that we only need to run the dynamic programming once before the interaction starts, and then we can use its results to solve all test cases.

We will call this improvement to the fixed-K strategy pen exploration. It turns out that pen exploration allows us to succeed in about 62.7% of all cases, which is also enough to pass Test Set 2. We expect that there are many additional ways to pass it.

Test Set 3
What about Test Set 3? You might have guessed it: we can actually combine careful writing and pen exploration! This yields a solution that succeeds in about 63.7% of all cases, which is close to the boundary of Test Set 3, so while it might not pass from the first attempt because of bad luck, it will definitely pass after a few attempts.

This is not the only way to solve Test Set 3, though. If we take a closer look at the careful writing solution, we can notice that we can achieve exactly the same outcome using a bottom-up approach instead of a left-to-right approach. More specifically, we start by writing one unit of ink using each pen in the order from left to right, until we find a pen that cannot write; we can conclude that it is the pen that started with 0 units of ink. Then, we can go from left to right again and make sure that each subsequent pen has written two units of ink (which will entail writing one more unit of ink from pens that have already written one, or writing two units of ink for pens that were untouched in the last round), until we find the pen that had 1 unit of ink in the beginning. We continue in the same manner until we have found pens 0, 1, ..., K-1, just as before. The amount written with each pen at this point will be exactly the same as the amount written with each pen in the original careful writing approach.

However, this formulation allows us to improve this approach: we no longer need to pick K in advance! We can instead make a decision after finding each pen X: either we continue by searching for the pen X+1 in the above manner, or we stop and return the two pens that have written the least so far. We will call this improvement early stopping. In order to make the decision of whether to continue or to return, we can either implement some heuristics, or actually compute the probability of success using a dynamic programming approach where the state is the amount written by each pen at the time we have found pens 0, 1, ..., X. This dynamic programming has just 32301 states for N=15, so it can be computed quickly.

Careful writing with early stopping works in about 64.2% of all cases if one makes the optimal decisions computed by the aforementioned dynamic programming, allowing one to pass Test Set 3 with a more comfortable margin. Once again, we expect that there are many additional approaches available for Test Set 3.

Ultimate solution
All three improvements — careful writing, pen exploration and early stopping — can be combined in one solution. It is a dynamic programming solution in which the state is once again the amount written with each pen, and we consider three options for every state:
1. Write with the leftmost remaining (=not known to be used up) pen until it is used up.
2. Write with all pens from left to right to find the smallest remaining pen.
3. Return the two rightmost pens that have not been used up yet.
Note that computing the probability of success for the third option is easy in this solution, as each of the remaining pens can be at any of the available positions. This means that any pair of remaining pens is equally likely to appear as the two pens we are taking to the South Pole. This means, in turn, that if the pens we are taking to the South Pole have written X and Y units, then the probability of success is simply the number of pairs of remaining pens with initial amounts A and B such that A+B>=N+X+Y, divided by the total number of pairs of remaining pens.

This dynamic programming has 1343425 states for N=15; therefore, it can run in time even in PyPy if implemented carefully. Its probability of success is around 64.4%, which is more than enough for Test Set 3.

Moreover, we have compared the probability of success for this approach with the optimal one (computed by the aforementioned exhaustive search with memoization, after a lot of waiting), and it turns out that for all N≤15 this solution is in fact optimal. Therefore, we suspect that it is optimal for higher values of N as well. We do not have a proof of this fact, but we would be very interested to hear it if you have one! Of course, neither this solution nor its proof are necessary to pass all test sets in this problem.

Constraints and judge
Finally, let us share some considerations that went into preparing the constraints and the judge for this problem. Given its random nature, the fraction of successfully solved test cases will vary for every solution. More specifically, thanks to the Central Limit Theorem we know that for a solution with probability of success P that is run on T test cases, the fraction of successfully solved test cases will be approximately normally distributed with mean P and standard deviation sqrt(P×(1-P)/T). In Test Sets 1 and 2 the standard deviation is therefore around 0.35%, and in Test Set 3 it is around 0.15%.

Considering the probabilities that a sample from a normal distribution deviates from its mean by a certain multiple of its standard deviation, we can see that for practical purposes we can expect to never land more than, say, five standard deviations away from the mean. This creates a window of about ±1.75% in Test sets 1 and 2, and a window of about ±0.75% in Test Set 3 such that whenever a solution is outside this window, it will always pass or always fail, but within this window the outcome depends on luck to some degree.

This luck window is inevitable and its size almost does not depend on the required success rate, only on the number T of test cases. Therefore it is impossible to completely avoid the situation when the success rate of a solution falls into the luck window, and therefore one might need to submit the same solution multiple times to get it accepted, with the number of required attempts depending on luck. Increasing the number T reduces the luck window, but increasing it too much requires the solutions to be extremely efficient and can rule out some approaches. Therefore, we decided that the best tradeoff lies around T=20000 for Test Sets 1 and 2, allowing less efficient approaches to still pass there. For Test Set 3, we felt the best tradeoff lies around T=100000 to better separate the solutions that are optimal or close to optimal from the rest and to reduce the luck window, while still not pushing the time limit to impractical amounts.

We have picked the required success rates in the following manner:
- For Test Set 1, we picked it such that the fixed-K strategy is outside the luck window and always passes.
- For Test Set 2, we picked it such that both careful writing and pen exploration are outside the luck window and always pass.
- For Test Set 3, we picked it such that the ultimate solution with all three improvements is outside the luck window and always passes, but solutions with careful writing and one of early stopping or pen exploration are within the luck window and therefore might require multiple submissions, but not too many. Lowering the threshold further in this test set would bring pen exploration without careful writing within the luck window. More generally, there is a continuum of solutions here if one uses inexact heuristics instead of exact dynamic programming to make the early stopping decision, and some of those solutions would inevitably land in the luck window.
Another peculiar property of this problem is that the judge is not deterministic, whereas typically in interactive problems, the randomness in each test set is fixed in advance, and submitting the same deterministic code always leads to the same outcome. This is necessary to avoid approaches in which one first uses a few submissions to learn some information about the test sets, for example by making the solution fail with Wrong Answer or Runtime Error or Time Limit or Memory Limit, to pass two bits of infomation back. This information can then be used to achieve a higher probability of success.

In order to see how this small amount of information can help significantly, consider a solution that has a probability of success that is five standard deviations smaller than the required one. Using the simple "submit the same code again" approach, one would need more than 3 million attempts on average to get it accepted, which is clearly not practical. However, we can do the following instead: we can submit a solution that just writes with all pens until they are used up, therefore gaining full knowledge of the test set. Then, it runs the suboptimal solution several million times with different random seeds, which by the above argument will find a random seed where the suboptimal solution in fact achieves the required success rate. Now the solution just needs to communicate back 20-30 bits comprising this random seed, which even at the two bits per submission rate requires just 10-15 submissions, which can be made within the space of a round. Finally, one could just submit the suboptimal solution with this random seed hardcoded, and pass.

We hope that this provides some insight into why this problem is designed the way it is, and we are sorry that some potential remained for contestants to be negatively impacted by the luck window or by the judge's non-determinism.

Round 3 2020 - Code Jam 2020

Recalculating (6pts, 11pts, 15pts)

A quarter pi helps the math go down
As can be seen in the pictures in the statement, lines dividing distinguishable and non-distinguishable areas are always 45 degree diagonals. The reason is that the distinguishability can only change when the retrievability of some repair center changes, and those only change when crossing diagonals because of the sum in the L1 distance definition. Since horizontals and verticals are much easier to deal with than diagonals, we can rotate the whole problem by π/4 = 45 degrees. If we do that directly (for example, by multiplying all points by the corresponding rotation matrix) we will find ourselves dealing with points at non-integer coordinates, which has problems in itself. Notice that rotating is equivalent to projecting into new axes of coordinates. In this case, the directions of those new axes are the rows of the rotation matrix, or (2-2, 2-2) and (2-2, -2-2). The vectors (1, 1) and (1, -1) have the same directions but do not have length 1. We can still project onto them and end up with a rotated and re-scaled version of the input. Luckily, neither rotation nor re-scaling affects the ultimate result. So, a convenient transformation is to map each point (x, y) in the input to (x+y, x-y). Notice that in this rotated and scaled world, L1 distance changes to L∞ distance, which is just a fancy way of saying that those diagonals turn into horizontals and verticals that are exactly D meters away from the point in question. Although we will not explicitly mention it, this transformation is applied as the first step of all solutions presented here.

Test Set 1
We can write a solution for Test Set 1 by examining a few cases and finding a formula for each one. The set of points from which a repair center can be seen is an axis-aligned square of side 2D with the repair center at its center. Let us call that the r-square of the point.

There are 3 possible situations:

I: The r-squares do not intersect.
II: The r-squares intersect and the repair centers are outside each other's r-squares.
III: The repair centers are inside each other's r-squares.
Situation I is the easiest to handle, because the answer is always 0, as Sample Case #2 illustrates.

Situation II is illustrated by Sample Case #1. As suggested in the statement, we can find the red area as 3 times the area of the intersection of both r-squares (notice that the intersection is not necessarily a square) and the blue area as the sum of both r-squares, 2×(2D)2, minus the area of the intersection.

In Situation III, the total area in which Principia could be deployed can be calculated in the same way as before. The distinguishable area, however, is slightly different. It can be simpler to calculate the non-distinguishable area (highlighted below), which consists of four copies of the same region, and then complement the result.

Test Set 2
Recall that Info(p) is the set of relative locations of repair centers that can be retrieved from a point p. Notice that when two points p and p' are very close, Info(p) and Info(p') will look similar. If the repair centers that can be retrieved from both are the same (which is true most of the time for points that are close to each other), then Info(p') is equal to Info(p) shifted by the shift between p and p'. However, if at least one repair center is retrieved from one of these points and not from the other, that is not true. In particular, Info(p) and Info(p') could be sets of different numbers of points.

First we deal with the changes in which repair centers can be retrieved by splitting the interesting area into parts. Within each part, the set of repair centers that can be retrieved is constant. Consider all the horizontal lines y=X+D and y=X-D for each x-coordinate X of a point in the input, and all vertical lines x=Y+D and x=Y-D for each y-coordinate Y of a point in the input. The points that are not surrounded by 4 of these lines (for example, the points above the highest horizontal line) are too far from all of the repair centers to be able to retrieve any of them, so we can disregard them for the rest of the analysis. These up to 4N lines divide the remaining points into up to 4N2 - 4N - 1 rectangular regions. Since all sides of all r-squares fully overlap with these lines, the set of repair centers that are retrievable from any point strictly within one of those regions is the same. The set of repair centers that can be retrieved from points on the lines might be different from those retrieved from any of its adjacent regions. However, since the area of each line is 0, the probability of Principia being deployed there is 0, so we simply ignore them and work with points strictly within regions. For each region R, we calculate the total area A(R) of distinguishable points in the region and the total area B(R) of points where Principia can be deployed. The answer is then the sum of A(R) over all regions divided by the sum of B(R) over all regions.

Fix a current region C. Going back to the first paragraph, Info(p) and Info(p') are shifts of each other for all p and p' from the same region. Calculating B(C) is easiest: it is either the area of C if Info(p) is non-empty for any point p in the region, and 0 otherwise. To calculate A(C), we can use an analysis generalizing our reasoning in Test Set 1. We need to find other regions R where Info(q) is a shift of Info(p) for a point q in R. In Test Set 1, this happened for the regions where one repair center can be seen, because the sets of a single point are always shifts of each other. We can check whether Info(p) and Info(q) are shifts of each other and find the appropriate shift if they are. First we check that they have the same number of points, and then we sort the points in an order that is invariant by shift (for example, sorting by x-coordinate and breaking ties by y-coordinate). In this way, we can fix the shift as the one between the first point of Info(p) and the first point of Info(q). Finally, we check that that shift works for all pairs of i-th points. If true, we can shift R by the found amount to obtain R', and the intersection between R' and C is a rectangle in which the points are non-distinguishable. If we do that over all regions R, the union of all those intersections is exactly the area of non-distinguishable points in C, and we can subtract it from the area of C to obtain B(C). There are many algorithms (with different levels of efficiency) to find the area of the union of rectangles aligned with the axes. Given the low limits of Test Set 2, it suffices to use a technique like the above, in which we extend the rectangle sides to lines and divide into regions, checking each region individually.

For each region C, the algorithm needs to find Info(p) for a point p in C, which takes time O(N), then iterate over all other O(N2) regions R and find Info(q) for a point there, check Info(q) against Info(p) for a shift, and possibly produce an intersection. That takes O(N) time per R, or O(N3) overall for the fixed C. Then, we need to take the union of up to O(N2) rectangles, which, with the simple algorithm above, can take between O(N4) and O(N6) time, depending on implementation details. This means the overall time, summing over all possible C, can be up to O(N8). Most implementations should be OK in most languages though. As we will see, there are many optimizations needed for Test Set 3, and even just doing the simplest of them would be enough to pass Test Set 2. Alternatively, there is a known algorithm to find the area of the union of K rectangles in O(K log K) time, and multiple references to it can be found online. Using it off the shelf would yield an overall time complexity of O(N4 log N), which is small enough to handle much larger limits than Test Set 2's.

Test Set 3
To solve Test Set 3, we have a lot of optimization to do. The first step is to avoid calculating Info for each region more than once. That alone does not change the ultimate time complexity of the algorithm from the previous section, but it is a necessary step for all optimizations that follow.

We divide the work into two phases. In the first phase, we group all regions that have equivalent Info sets. For each region C, we calculate S := Info(p) for an arbitrary point p in C as before, discard it if S is empty. Otherwise, we sort S, and then shift both C and the sorted result by the first point such that the shifted S' has the origin as its first point. In this way, S' is a normalized pattern for C, and two regions with Info sets that are shifts of each other end up with the same S. After doing this, we can accumulate all shifted regions for each S that appears, and process them together.

Notice that we can sort the input points at the very beginning and then always process them in order such that every calculated S is already sorted, to avoid an extra log N factor in the time complexity. A rough implementation of this phase takes O(N3) time if we use a dictionary over a hash table to aggregate all regions for each set S. We optimize this further below.

For the second phase, we have to process the set of shifted regions for each shifted S'. Since the regions are already shifted in a normalized way, we can process them all together. That is, instead of calculating A(C) for each individual C, we calculate A(S') := the sum of A(C) over all C in S'.

The picture below shows an example of the input we need to process for a fixed S'. There are multiple rectangular regions that have been shifted, so some may overlap now. We need the area of the part where no intersections happen (highlighted in the picture). If we do this by extending sides and processing each resulting region individually, we end up with an algorithm that takes between O(K2) and O(K3) time again, where K is the number of rectangles. However, the sum of the number of rectangles over all S' is O(N2), because each original region appears in at most one group. Therefore, the overall cost of the second phase implemented like this over all S' would be between O(N4) and O(N6).

We now have an algorithm with a first phase that takes O(N3) time and a second phase that takes between O(N4) and O(N6) time overall. We need to optimize further.

For the first phase, if we want to keep O(N2) regions and go significantly below O(N3), we need to make the processing of each region not require a full pass over the input points. Consider a fixed row of regions between the same two horizontal lines. Notice that each repair center can be retrieved from a contiguous set of those regions, and they become both retrievable and non-retrievable in sorted order of x-coordinate. Therefore, we can maintain the list of points that represent S in amortized constant time by simply pushing repair centers that become retrievable to the back of the list and popping repair centers that become non-retrievable from the front. This technique is sometimes called "chasing pointers".

Unfortunately, this is not enough, as we need to shift each S by a different amount, and shifting S requires time linear in the size of S. It is entirely possible for S to contain a significant percentage of points for a significant percentage of regions. We can do better by using a rolling hash of S. That would get us a hash of each S without any additional complexity. Unfortunately, we cannot shift the resulting hash. The last trick is, instead of hashing the actual points, hash the shift between each point and the last considered (adding a virtual initial point with any value). Those internal shifts are invariant to our overall shift of S, and since the first point of S' is always the origin, we can simply remove that one from the hash. The result is something that uniquely (up to hash collisions) represents the shifted S'. This change optimizes the first phase to run in O(N2) time.

To optimize the second phase — specifically the calculation of the values A(C) — we can use an algorithm similar to the one mentioned at the end of the previous section to calculate the union of the area of all rectangles. Consider a sweep line algorithm that processes the start and end of each rectangle in order of x-coordinate. We can maintain a data structure that knows, for each y-coordinate, how many rectangles overlap the sweep line at coordinate y. We need to be able to insert a new interval of y-coordinates each time a rectangle starts, remove one each time a rectangle ends, and query the total length of y-coordinate covered by exactly one rectangle. Multiplying that by the difference in x-coordinate between stops of the sweep line, we can calculate how much area to add to A(S') at each stop of the sweep line.

We can use a segment tree to efficiently represent that. At each node of the segment tree we need to record:

(1) How many rectangles processed so far fully cover the interval represented by the node and do not fully cover its parent.
(2) The total length covered by 1 or more rectangles within the interval represented by the node.
(3) The total length covered by 2 or more rectangles within the interval represented by the node.
(1) and (2) are exactly what need to be recorded for the algorithm to calculate just the area of the union of the rectangles. When inserting an interval I at a node representing interval J, if I and J do not overlap, we do nothing. If J is contained in I, we simply increment (1) and do nothing recursively. For any other case, we insert recursively into the node's children. Removal is similar: since in our use case removing J means that we previously inserted J, we are guaranteed that in the case where J is contained in I, (1) is positive, and we just decrement it. After inserting or removing as above, we recompute values (2) and (3). That recomputation can be done in constant time based on (1) and the (2) and (3) values of the children, if any, with a case analysis on whether (1) is 0, 1, or 2 or more. The details are left as an exercise to the reader. The total length covered by exactly 1 rectangle is exactly the value (2)-(3) on the root of the tree.
Since each insertion and removal requires going through at most O(log K) nodes, and the queries are resolved in constant time, using this sweep line algorithm with the described segment tree structure results in an algorithm that processes a set of K rectangles for the second phase in O(K log K) time. This results in O(N2 log N) time overall for the phase (and the algorithm), since, as we argued before, the sum of all K over all S' is O(N2).

Virtual World Finals 2020 - Code Jam 2020

Pack the Slopes (10pts, 22pts)

Test Set 1
A translation of the problem statement is that we wish to find the minimum cost maximum flow on a directed tree. The rest points can be represented by nodes, and the ski slopes can be represented by edges. A commonly used algorithm to solve minimum cost maximum flow problems is successive shortest paths. In the case of our directed tree, a simplified successive shortest paths approach can be applied.

Observe that we can add skiers greedily. Of all the nodes that can be reached from the top of the mountain (the root of the tree), we should always send a skier to a node with a minimum cost path using only edges that still have capacity remaining. Since we are working with a tree, there is only one path from the root to each node. With this in mind, we can find the path costs to all the nodes using any tree traversal from the root while calculating the cost to each node as the cost of its parent (which we calculated first) plus the cost of the edge connecting them. Then, we can sort the nodes by their path cost in ascending order. We should always use the first node in the list that still has some capacity along its path.

Let us maintain the the number of skiers sent through each edge. We can work through our sorted list of nodes in order. We check if the current node in the list still has some capacity by checking the edges in the path, and either update the capacities if we can push another skier, or move on to the next node in the list if we cannot. Doing this for a single skier takes O(N) time. We know we will keep using the same path until some edge reaches capacity, so we can compute the minimum capacity left in the path with a linear pass over it, and simulate sending that many skiers at once. Each such a step takes O(N) time overall, and since we have to try up to O(N) possible destinations, the overall algorithm takes O(N2) time, which is efficient enough to solve Test Set 1.

Test Set 2
For the large input, we need to do more. We can use a heavy light decomposition of the tree to manage the remaining capacities of the edges in the tree. The decomposition will need to support updates and queries on paths from the root to an internal node. The updates involve subtracting an amount from the capacity of each edge. The queries involve finding the minimum capacity of all the edges on a path.

A heavy light decomposition breaks a tree up into a collection of disjoint paths such that the number of decomposition paths from any node to the root is at most O(log N). If we keep a segment tree for each decomposition path, then we can support both the query and update operations in O(log2 N). Note that the segment tree we use must support lazy range updates. This improves the running time of each step of the algorithm for Test Set 2 to take only O(log2 N) time instead of O(N) time. This makes the algorithm run in O(N log2 N) time in total, which is sufficient to solve the Test Set 2.

Virtual World Finals 2020 - Code Jam 2020

Adjacent and Consecutive (10pts, 32pts)

As is the case in many problems about games, this problem revolves around evaluating a given state of the game and deciding whether it is winning or losing. The problem defines mistakes as transitions from winning to losing states, so once we have determined the type of each state, we can easily count up the mistakes.

Test Set 1
In all zero-sum finite two person games with no ties, the basic framework to decide whether a state is winning is the same. Let us call a state A-winning (or equivalently, B-losing) if player A can ensure a win from it, or B-winning/A-losing otherwise. For a given state, if the game is over, check the winning condition. Otherwise, if it is player X's turn, try every possible play. If any of them leads to an X-winning state, then the current state is also X-winning. Otherwise, the current state is X-losing. We can implement this as a recursive function, but given that there can be O(N2) possible plays on any given turn, this will only work quickly enough for small values of N.

We can see immediately that there can be a lot of repeated work in that recursive evaluation. For example, consider a state S and another state S' such that S' is the result of making P plays on S. Since there are P! ways to make those plays, state S' potentially needs to be evaluated P! times to evaluate S once. Memoizing the function or turning it into a dynamic programming implementation would thus save a lot of recursive calls.

In addition, there are many states that can be evaluated without the need for a recursive call. Some simple cases include states in which there already are two adjacent and consecutive tiles placed (the "already won" condition for player A). We can prune the recursive calls tree by simply returning those states as A-winning. We can prune it further: if there is a play that creates such a pair during A's turn (the "can win immediately" condition), we just do that instead of trying all possible plays for A. We can also restrict B to only try plays that do not leave a known A-winning condition. There are ways to prune the tree further, some of which we explore in the Test Set 2 section below.

We can pass Test Set 1 either with a combination of simple pruning and memoization/dynamic programming, or a lot of pruning.

Test Set 2
The number of possible states in Test Set 2 is just too big to even fit in memory, even after careful pruning. However, pruning enough can help us find sets of equivalent states, that is, groups of states that we can prove have the same winner. By doing that, we can dramatically reduce the number of memoized states.

Consider only A's turns first. We have already mentioned the "already won" and "can win immediately" conditions above. On top of those, we can notice that if there is some set of three unplayed tiles available with three consecutive numbers, and there is some block of three consecutive cells available, A can win by playing the middle number in the middle cell (we call this the "can win in 2 moves" condition). Whatever B plays next, A's following turn will fulfill either the "already won" condition or the "can win immediately" condition.

The conditions above allow us to find some states which are definitely A-winning. Notice that in any other state where it is A's turn, it's impossible for any of the already-placed tiles to end the game as part of an adjacent and consecutive pair. Therefore, they could be replaced by "unavailable" cells without a specific number on them. So, the remaining cells can be represented as the multiset of sizes of the groups of consecutive cells that are left; call it LC. The exact locations of those groups on the board are not relevant to the final outcome. Since each tile can only form adjacent and consecutive pairs with other leftover tiles, we can similarly represent the remaining tiles with a multiset of lengths of consecutive runs of tiles; call it LT. For example, the state

7 2 6 _ _ 3 _ 4 _ _ 5
has the leftover tiles 1, 8, 9, 10, 11, so its leftover cells can be represented by the multiset LC = {1, 2, 2} and its leftover tiles by the multiset LT = {1, 4}.

In addition, since the "can win in 2 moves" condition has already been checked, we know that at least one of LC and LT does not contain any integer greater than or equal to 3. To simplify the algorithm, notice that the game in which we swap tile numbers and cell numbers is equivalent (because the winning condition is symmetric). Therefore, states with LC and LT swapped are equivalent. We can therefore assume it is always LC that has only integers 1 and 2.

After this compression, the number of states is dramatically reduced. Notice that because we know that there are no three consecutive leftover numbers, at least N/3 numbers have been played already. That means that the sum of the integers in both LC and LT (which is the same for both multisets) is at most 2×N/3. Therefore, the total number of possible multisets LT under those conditions is bounded by the sum of partitions(K) for every K between 0 and 2×N/3, which is partitions(2×N/3+1). Given such an LT multiset, the number of possible multisets LC of only 1s and 2s such that the sum of LC and LT is the same is bounded by N/3 (which is the maximum number of 2s). So, the number of states is no greater than (N/3)×partitions(2×N/3+1), which is a fairly small number for the given limits for N. Moreover, we need only one memoization or dynamic programming table for all test cases.

There are multiple sufficiently fast ways to implement the second player's turns (which are the slowest ones), and there are further restrictions that can be placed on plays to optimize our strategy even more. Some of those options result in an algorithm whose complexity makes it clear that it runs within the time limit. Some other options have a theoretical complexity that is either too large or hard to estimate tightly enough to be convinced it's fast enough. Fortunately, it's possible to compute the recursive function for all possible states without reading any data, and be sure that it runs in time before submitting.

The Test Set 3 that wasn't
We considered having a third test set for this problem requiring a polynomial solution. We progressively found solutions taking O(N3) time, O(N2) time and even one requiring only O(N log N) time. We ultimately decided against adding the test set because the possibility of solving it by guessing the right theorem without proof was a significant concern. The benefit of the extra challenge without making contestants have to read a full extra statement is of course a significant benefit, but it was dampened by our estimation that the likelihood of it being solved legitimately was small. If we had made it worth a small number of points, it would not have been worth the relative effort to solve it. If we had made it worth a lot, though, that would have diminished the value of the work needed for our favorite part of the problem, which is solving Test Set 2.

If you are up to try an extra challenge without spoilers, stop here. If you want some hints on how those solutions go, read ahead!

The first theorem further compresses the states considered in the Test Set 2 solution on A's turns. Consider a state that is not under the "already won", "can win immediately" or "can win in 2 moves" conditions represented by some multisets LC and LT, where only LT can contain integers greater than 2. That state is A-winning if and only if the state represented by multisets LC and L'T is A-winning, where L'T is obtained from LT by replacing each integer X in LT with floor(X / 2) copies of a 2 and a 1 if X is odd. This reduces the number of states of the dynamic programming to O(N3), and we can process each of those states in constant time because there is only a bounded number of effectively different plays to make.

If we pursue that line of reasoning further, we can find that we can check A-winningness with a small number of cases. Let LCi be the number of times i is in LC, and LTi be the number of times i is in LT. Because we already assumed that LC had no integers greater than 2, LCi = 0 for all i ≥ 3. Let K = LC1+2×LC2 = sum of i×LTi over all i be the number of turns left and Z = (LT2+LT3) + 2×(LT4+LT5) + 3×(LT6+LT7) + ... be the number of times 2 appears in L'T as described in the previous paragraph. Then, the second theorem we can prove is that:

If K = 2, then the state is A-winning if and only if LC2 = LT2 = 1.
If K > 2, then the state is A-winning if and only if K is odd and 2×(LC2+Z) > K.
From the need to avoid the "already won", "can win immediately" and "can win in 2 moves" conditions, it is possible to reduce the number of plays on B's turn to a set of a bounded number of options that always contains a winning move if there is one. This, together with the fact that the conditions above can be checked in constant time, yields an algorithm that decides for any state whether it is winning or losing in O(N) time, making the overall process of a test case take O(N2) time. With some technical effort, we can represent the board in a way that we can update and use in O(log N) time to check all the necessary conditions, yielding an algorithm that requires only O(N log N) time to process a full test case.

The proofs for the theorems on the paragraphs above are left as an exercise to the reader. As a hint, it is easier to prove the second more general theorem, which is already nicely framed, and then obtain the first theorem as a corollary.

Collapse right panel

Virtual World Finals 2020 - Code Jam 2020

Hexacoin Jam (10pts, 10pts, 22pts)

The main task in this problem is to count how many ways there are to pick a permutation and a pair of numbers from the list such that their (possibly truncated) sum falls within the given interval. This is the numerator of a fraction representing the desired probability. The number of total picks (our denominator) is simply 16! × N × (N - 1) / 2. Then, we can use a known algorithm (like dividing numerator and denominator by the greatest common divisor) to reduce the fraction to the desired form. In the rest of the analysis, we focus only on calculating the non-reduced numerator.

One common denominator among all solutions for all test sets is that the actual values of the digits in the list do not matter. Once we fix a pair of numbers X and Y from the list, only the "digit structure" of the pair (X, Y) matters. The digit structure of a pair of numbers is a unique way to see their coupling as digits. We define it as the lexicographically smallest pair (P[X], P[Y]) across all possible digit permutations P, where P[X] is the result of replacing each digit of X with the value assigned to it by P. Notice that digit structures have length 2D.

The total number of digit structures grows rapidly when D increases, but depends only on D, which has small limits. The total number is 15 for D=2, 203 for D=3, 4140 for D=4, and 115975 for D=5.

Test Set 1
In Test Set 1, the number of different digit structures is really small. In addition, for a fixed structure, we only care about the values assigned by the chosen permutation to up to 2D digits. For each digit structure with d unique digits, we can compute the value of the truncated sum for a valid assignment of those d digits, noting that there are (16 - d)! ways to generate each assignment.

The number of valid assignments can be somewhat large at 16! / 10! or about 6 million for 6 different digits, but there is only one digit structure within Test Set 1 that has that many different digits. There are a handful that have 5, for which there are around half a million assignments each, and most structures have 4 or fewer different digits, which have fewer than 50 thousand different assignments each. Moreover, this computation only depends on D and not on the rest of the input, so we have to do it only once for each possible structure for D=2 and D=3.

Given the precomputation above, we can iterate over the pairs of integers from the list, compute their digit structures, and use two binary searches to see how many of the numbers in the list are in range. Then, we compute the number of different digits in the structure d and multiply the total by (16 - d)!, which gives us the number of sums in range that can be produced for the given pair. Summing that result over all possible pairs gives us the answer we need.

Test Set 2
The precomputation above can be slow in Test Set 2. Not only do we have 4140 additional digit structures to process, but most importantly, a few of those have 7 and 8 unique digits. Each additional digit means an order of magnitude extra possible assignments. There are several ways to handle this, and we need only a few of the tricks below to make it work.

The first issue is that the lists we need to store are now too long to fit in memory. Since there are only up to 164 different results, many of those results would be repeated, so we can compress them based on those repetitions. This is still tough to pull off, so the best thing to do is to just not store the full list. We know we only care about how many items on the list are in range for up to T different ranges. So, we can read all cases before starting the computation, split the full range of sum results at every A and B that we read into up to 2T+1 minimal ranges, and then compress together all numbers that are within the same range. This definitely fits in memory.

We can also choose to not memoize between different test cases, and treat them one at a time. If we do that, we have a fixed A and B, so there is no need for lists, just a counter. We can either treat each pair of numbers individually as well (speeding up the process of assignments — see below) or try to memoize digit structures if any are repeated within the same test case. It is possible that almost every pair has a different digit structure, of course, but there are few digit structures with the maximum number of unique digits. This means the memoization reduces the total runtime in what was our previously worst case, and the new worst case (all different structures) is not as bad because many of those structures will have fewer different digits.

We can reduce the number of digit structures further by realizing that digit structures like (011, 022) and (012, 021) are equivalent, in the sense that for any assignment, their sum is the same: both are 11 × (P[2] + P[3]) + 100 × P[0], where 11 and 100 are in base 16. This only works in conjunction with some form of memoization.

Once we have either a range [A, B] or a small list of ranges fixed at the moment of processing the assignments, we can use it to do some pruning. Suppose we assign values to the digits in most significant positions first. When we have done a partial assignment, we can compute or estimate a range of possible values that the sum may have when we finish. As more highly significant digits get assigned, that range shrinks. If that range is completely inside our target range (or one of our target ranges) we can stop and know that all further assignments work, counting them using a simple multiplication. If the range is completely outside of the target range (or all target ranges), we can also stop and count nothing further.

As mentioned above, we need only some of the optimizations above to manage to pass Test Set 2. Of course, the more of them we find and implement, the more confident we can be about the speed of our solution.

Test Set 3
To simplify the problem, we can use a common technique when dealing with counting numbers in closed intervals [A, B]. We write a function f(U) that only calculates the value for closed intervals [0, U-1]. Then, the result for an interval [A, B] is f(B + 1) - f(A). In this case, we can use this to take care of the overflow as well, by simply ignoring the overflow and counting the number of hits in the interval [A, B] plus the number of hits in the interval [16D + A, 16D + B], which are the only possible sums whose truncation would yield a result in [A, B]. After we write our function f, this translates to a result of f(B + 1) + f(16D + B + 1) - f(A) - f(16D + A). We focus now on calculating f(U), that is, the number of picks of a permutation and a pair of numbers that yield a (non-truncated) sum of strictly less than U.

Both the number of possible values a sum can have and the possible number of pairs are small (for computers). We can use an asymmetric "meet in the middle" approach to take advantage of both those facts. We do this in a similar way as what we did for Test Set 1 and possibly for Test Set 2, by keeping a count on each digit structure, and then iterating over the pairs of numbers from the list to see how much we have to use each of those counts.

First, let's consider all the ways to add up to something less than U. Let us fix the first number to have a value of x, so the second number can have any value less than U-x. A number y is less than U-x if and only if it has a smaller digit than U-x at the first position at which they differ. We can represent this set of ys by saying they are all the ys that start with the first i digits of U-x, then continue with a digit d smaller than the (i+1)-th digit of U-x, and any remaining digits can be any digit.

For example, if U=2345 and x=1122, then U-x=1223 and y can be of the form 0***, 10**, 11**, 120*, 121*, 1220, 1221, 1222, where * represents any digit.

For each pair of an x and a prefix for y, we can represent all the pairs of numbers from the list that match by matching that with a digit structure that allows for *s at the end. In this way, a pair of numbers from the list X and Y can be mapped to x and y by a permutation if they have the same digit structure, disregarding the actual digit values. To represent this, we normalize the digit structure as we did before: no digit appears for the first time before another smaller digit. Therefore, a structure like x=1122 and y=10** is represented as x=0011 and y=02**. As before, the number of permutations that can match a pair of numbers to this digit structure is (16 - d)!, where d is the number of unique digits that appear in the structure.

Notice that different values of x can yield the same structure. For example, x=1133 yields U-x=1212, and for y=10** the structure is the same as for x=1122 and y=10**.

For each digit structure of 2D total digits that have between 0 and D-1 asterisks at the right end, we count the number of permutations that make its parts add up to something less than U.

We now process the pairs of numbers from the list. For each pair, we build its normalized digit structure as before, and add the count for that digit structure to our running total. We also add the count of the structures that result in replacing up to D-1 rightmost digits with *s.

We can express the complexity of this algorithm in terms of the base B, the number of digits of each number D, and N, the size of the list of numbers. The first stage iterates through up to O(BD) possible values for x, and for each one, considers up to O(B×D) digit structures for y. If factorials up to B are precomputed, and we store the results in a hash table or an array with a clever hashing for digit structures that keeps hashes unique and small, this requires only constant time per pair, so O(BD+1×D) time overall. The second stage requires processing up to O(D) digit structures per pair, so it takes O(N2×D) time in total. The sum of both stages gives us the overall time to compute f, which is O(BD+1×D + N2×D). Since we need only a constant number of calls to f, that is also the overall time complexity of our algorithm.

Another solution is to to use all of our Test Set 2 tricks in an efficient way. Consider especially the last one: when considering the pair of i-th most significant digits, only the pairs whose sum is close to either that value at A or that value at B produce a range of possible sums for the pair that is neither fully inside [A, B] nor fully outside of it. That means that for the i-th digits, there are only a linear number of pairs of digits that can be assigned at that position that would make the process not stop immediately, instead of a quadratic number. This shrinks the number of assignments to approximately the square root of its previous value, behaving more as exponential on D than as an exponential on 2D. This gives us a time complexity comparable to that of the other solution presented above. While the overall time complexity is still higher, these two solutions can perform pretty similarly for the limits we have: the bound on the number of assignments is actually smaller than BD because it behaves more like B! / (B - D)!. Those savings, plus those from not needing extra D terms, make up for the fact that the backtracking solution's time complexity has a term with behavior similar to BD and a term with behavior similar to N2 multiplied together instead of added.

Notice that the basic insights of both solutions are closely related. The pruning provides such a large speedup for the same reason that we can use the "meet in the middle" approach based on digit structures.

Virtual World Finals 2020 - Code Jam 2020

Musical Cords (15pts, 27pts)

Test Set 1
In this problem, we are given a circle, a collection of points around the perimeter of the circle, and the attachment values for each of those points (Li). We want the top K pairs of points that maximizes the Euclidean distance between the points plus their attachment values. For Test Set 1, we know that the point values are picked randomly. We will need to use this property somehow. We also know that K=1, so we are looking for the top pair of points.

Imagine considering the points around the circle in the clockwise order. Let us say that we are at a point p. Also, without loss of generality, let us only consider points that are at most 180 degrees clockwise from p. That is, we only consider half of the circle starting at p. If we find the maximum pairing (maximum distance plus attachment values) for every possible p, the maximum over all of those pairs will be the answer. This is because considering each point, (and points that are up to 180 degrees clockwise from it) will consider every possible pair of points at least once.

So, we are considering all points p in the clockwise order around the circle and trying to find the maximum pairing for each of these. If we consider the possible other points we could pair with p in the counterclockwise order starting from the location 180 degrees clockwise of p (the "other side" of the circle from p), then we can observe some useful properties. Once we have considered some pairing point x, then some further counterclockwise point y will be closer to p. Thus, for y to be the maximum pairing, its attachment value must be greater than x's attachment value. So, we only need to consider points that increase in their attachment values.

We know that the attachment values are randomly selected. In a sequence of N integers randomly selected from a uniform range, we expect the running maximum to change only log N times. To see why, observe that first element has a 1/1 chance of being the new maximum, the second element has a 1/2 chance, the third element has a 1/3 chance, and so on. This is the harmonic series. The sum of its first N terms is bounded by O(log N). Thus, we expect to see only O(log N) changes of maximum. This means the explained solution has an expected time complexity of O(N log N), which should be fast enough to pass. The worst case among all possible inputs takes O(N2) time. However, as we showed, this is extremely unlikely with random attachment values.

Now, when we are considering a point x, we need an efficient way to find the first point in the counterclockwise order from x whose attachment value is greater than x. This is actually equivalent to a known problem: the next greater element problem. The twist is that our values are on a circle, so we have wraparound. One way to deal with this is to make an array of attachment values, append it to itself, and then apply a fast algorithm for the next greater element problem to the resulting array.

There are other approaches that also take O(N2) time in the worst case, because they might compare a high percentage of all pairs, but use other clever speedups like jumping to the next greater value, to have a high chance to cut the number of comparisons way down.

Test Set 2
The attachment values of points are not randomly generated in this test set, so we will need to find a different approach. Also, K=10, so we also need to find more than one pair of points. Instead of trying to find the top K pairs directly, we can find the best pairing for each point, and extend this solution to find the top K pairs overall. Let's first figure out how to find the best pairing for each point, then explain how to extend it to find the top K pairs.

We start by finding, for each input point P, which other input point Q is its best pairing. That is, we find for which other input point Q the function LP + LQ + distance(P, Q) is maximized.

Now consider points P not necessarily in the input. We define a value function VP(Q) = distance(P, Q) + LQ. Notice that if P is a point in the input, VP(Q) + LP is the amount of cord required to connect P and Q.

Let us approach the problem visually. Imagine moving P around the circle in the clockwise order and computing the value function VP(Q) for all the input points Q that are up to 180 degrees clockwise of P. As we move P, how does the distance function to some other point change?

The Euclidean distance part of the distance function is equivalent to computing the length of a circular chord. That is, VP(Q)=2×R×sin(angle(P, Q)/2)+LQ. Notice here than the angle function is in the clockwise direction, and it's always less than or equal to 180 degrees. Consider how this function changes when P changes. That is, consider the function WQ(P)=VP(Q). As the domain for VP is the Qs that are up to 180 degrees in the clockwise direction, the domain for WQ is the Ps that are up to 180 degrees in the counterclockwise direction.

For each Q, WQ has a constant term LQ, and then a term that looks like half a sine wave. Notice that the graph of the function is the same for any Q, but translated: the change in angle(P, Q) translates it horizontally, and the change in LQ translates it vertically. An example of how these graphs might look follows. Each black curve corresponds to WQ for a different point Q.

Because all of the distance functions have the same shape, we can see that WQ has maximum values compared to all other W functions for a continuous range of points P. So, can we efficiently compute these ranges for each point? If so, we can use these ranges to find the maximum for each possible point by finding which maximum's range it falls into.

We can analyze this graph by sweeping across it from left to right—that is, increasingly along the x-axis. Since the x-axis represents points on a circle, it may be easier to visualize if we instead represent each point by its angle with respect to the positive half of the x-axis, as usual. For a fixed angle A, let us consider only the curves whose domain includes at least one point in the range [0, A] and call that set CA. As we consider increasing values for A, at some point we consider a curve for the first time, with domain [A, B]. The curve is definitely maximum among all curves in CA at B, because it is the only curve at that point. So, the range for which it is the maximum must end there. Since we know there is only one such range, we can binary search to find the beginning of the range.

We keep a sorted list of non-overlapping ranges as we go, and add new ranges to the end. Note that we may need to pop off some ranges from the end of the list if we find a new range that covers them completely, or shorten them if the new range covers them only partially. There can be zero or more left-most ranges where the new curve is fully under the maximum curve in that range, zero or more right-most ranges where the new curve is fully over the maximum curve in that range, and zero or exactly one range where the new curve and the curve from the range cross. We binary search within the range for the exact crossing point.

One issue we still need to deal with is that the graph is actually cyclic. It represents the distance function to points as we go around a circle. So, the distance functions we plot can "wrap around". We can deal with this via a method similar to what we used in Test Set 1: append a copy of the input to itself, and go around the input twice.

The final list of ranges can be used to get the maximum pairing for each point on the circle. For each input point P, we find the Q for which WQ is the maximum at P. Since we only need to check input points P, we can restrict the domains of all functions and intervals discussed so far to input points without affecting the correctness. This allows us to binary search over an array of O(N) values as opposed to over a range of real numbers.

The time complexity of constructing the list of ranges is therefore O(N log N). Then, we go through the list iterating over the list of possible Ps for each Q. While a particular Q can have a long range, overall, we iterate over each input P at most once (or twice for the duplicated input). This makes the final step take linear time.

An O(N) algorithm for the above problem does exist. This is left as an exercise for the reader.

We have a way to find the best pairing for each point. How do we extend this solution to find the top K unordered pairings? It is possible that a pair in the top K is not in our list of best pairings for each point. However, we can still use our list. Consider sorting the list by the Euclidean distance plus attachment values of the pair in descending order. The first entry in the list will definitely be the top pair overall. The second top pair will either be the next entry in our list, or it will be paired with one of the points in our top pair. We can look at all pairings involving the points in our top pair, and now we know the second best pair. We can repeat this process to find the top K. In each step, the pair we choose is either the next best pair in our list that we haven't seen, or it is a pairing with a point in the previous pairs in our list. In each of these K steps, we take O(N) time. Sorting our list initially takes O(N log N) time, so our algorithm requires O(N log N + N × K) time overall, which is fast enough to pass.

Virtual World Finals 2020 - Code Jam 2020

Replace All (15pts, 27pts)

The first thing we can notice is that since we replace every occurrence of a character, and at the end we count unique characters, we can regard the input S as a set. That is, multiple occurrences of the same character can be ignored, and the order of the characters is unimportant. We write "x → y" to represent the replacement of a character x by a character y. Here, and in the rest of the analysis, we use both lowercase and uppercase letters to represent variables, not actual letters that can happen in the input.

To simplify the explanations, we can represent the list of replacements as a directed graph G where nodes represent characters, and there is an edge from x to y if and only if there exists a replacement x → y in the input. Notice that each weakly connected component of G defines a problem that can be solved independently, and combining the results corresponds to adding up the number of distinct characters that can occur in the final text from each group. In particular, notice that a character c that does not appear in any replacement forms a single-node connected component, and the result for each such character is either 1 if c is in S, and 0 otherwise.

Let us define diversity as the number of unique characters in a text. Diversity of the last text is exactly what we are trying to maximize. If both x and y are in U, the diversity after performing x → y on U is 1 less than before. On the other hand, if at least one of x and y is not in U, then the diversity before and after performing x → y is the same. That is, the replacement can either cause a diversity loss of 1, or not cause any loss. We can then work on minimizing the diversity loss, and the final answer will be the original diversity minus the loss.

Test Set 1
In Test Set 1 the in-degree of each node of G is limited to at most 1. This restricts the types of weakly connected components the graph can have. If there is a node in the component with in-degree equal to 0, then the component is a directed tree and the node with in-degree equal to 0 is the root. If all nodes in the component have in-degree equal to 1, then there is a cycle (which we could find by starting at any node and following edges backwards until we see a node for the second time). Each node in the cycle only has its incoming edge coming from another node in the cycle, but it can have edges pointing out to non-cycle nodes. This means each cycle-node can be the root of a directed tree. These are the connected components of pseudoforests.

We can break down this test set by component type. Let's consider the simplest possible tree first: a simple path c1 → c2 → ... → ck. If both ck-1 and ck are in the initial text S, then we cannot perform ck-1 → ck without losing diversity. No other replacement in the text can make ck-1 or ck disappear from the text, so they will both be present until ck-1 → ck is performed for the first time. In addition, after we perform ci-1 → ci for any i, we can immediately perform ci-2 → ci-1 without loss of diversity, since ci-1 will definitely not be in the text then. Therefore, performing every replacement in the path in reverse order works optimally.

With some care, we can extend this to any tree. Consider a leaf node y that is maximally far from the root. The replacement x → y that goes into it is in a similar position as the last replacement in a path, but there could be other replacements that remove x from the text before performing x → y, which would prevent x → y from causing diversity loss. Since y is maximally far from the root, however, all other replacements that can help also lead to leaf nodes. If x and the right side of all replacements that go out of x are in S, there is no way to avoid losing diversity. However, only the first one we perform will cause diversity loss, whereas all the others can be done when x is no longer in the text, preventing further loss. We can generalize this by noticing that we can "push" characters down the tree without loss of diversity as long as there is at least one descendant node that is not in the current text. This is done by considering the path from a node x to the descendant that is not in the text y, and processing the path between x and y as described earlier.

From the previous paragraph, we can see that any edge x → y that does not go into a leaf node can be used without diversity loss. Even if there are no descendants of x in the tree that are not in the initial text S, we can process some unsalvageable leaf node first, and then we will have room to save x → y. Any edge z → w that goes into leaf nodes may be salvageable as well, as long as z has other descendants. Putting it all together, we can process the nodes in reverse topological order. At the time when we process a node x, if there is any descendant of x that is not in the current text, then we can push x down and then process all of x's outgoing edges without any diversity loss. If we process a node x and its subtree consists only of nodes representing letters currently in the text, that means that we will lose 1 diversity with the first edge going out of x that we process (it doesn't matter which one we process first), but that loss is unpreventable.

To handle cycles, consider first a component that is just a simple cycle. If there is at least one node representing a character x not in S, then we can process it similarly to paths, without diversity loss: we replace y → x, then z → y, etc, effectively shifting which characters appear on the text, but keeping diversity constant. If all characters in the cycle are in S, then the first replacement we perform will definitely lose diversity, and after that we can prevent diversity loss by using the character that disappeared as x in the process from the first case.

If a component is a cycle with one or more trees hanging from it, we can first process each tree independently. Then, each of those trees will have at least one node representing a character not in the current text: either there was such a node from the start, or it was created as part of processing the tree. So, we can push down the root of one of those trees (which is a node in the cycle) without losing diversity. Then, there will be at least one node in the cycle whose represented character is not in the text, so we can process it without diversity loss.

Test Set 2
In Test Set 2, the graph G can be anything, so we cannot do a component type case analysis like we did for Test Set 1. However, we can still make use of the idea of processing paths and pushing characters to avoid diversity loss following our first replacement.

We tackle this by morphing the problem into equivalent problems. We start with the problem of finding an order of the edges of G that uses each edge at least once while minimizing steps in which we cause diversity loss.

First, notice that once we perform x → y, we can immediately perform any other x → z without diversity loss. So, if we have a procedure that uses at least one replacement of the form x → y for each x for which there is at least one, then we can insert the remaining replacements without altering the result. That is, instead of considering having to use every edge at least once, we can simply find a way to use every node that has out-degree at least 1. Furthermore, any replacement x → y such that x is not in S can be performed at the beginning without changing anything, so we do not need to worry about using them. Notice that we are only removing the requirement of using particular edges, not the possibility. Therefore, we can make the requirements even less strict: now we only need to use every node with out-degree at least 1 that represents a character present in S.

Now that we have a problem in which we want to cover nodes, and we know from our work in Test Set 1 that we can process simple subgraphs like paths and cycles in a way that limits diversity loss, we can define a generalized version of the minimum path cover problem that is equivalent.

Given a list of paths L, let us define the weight of L as the number of paths in L minus the number of characters c not in S such that at least one path in L ends with c. That is, for each character c not in S, we can include just one path ending in c in L without adding to its weight. Let us say that a list of paths L covers G if every node in G with out-degree at least 1 that represents a character in S is present in some path in L in a place other than at the end. That is, at least one edge going out of the node was used in a path in L. We claim that the minimum weight of a cover of G is equal to the minimum number of steps that cause diversity loss in the relaxed problem.

To prove the equivalency, we first prove that if we have a valid solution to the problem that causes D diversity loss, we can find a cover that has weight at most D. Afterwards we prove that if we have a cover with weight D, we can find a solution to the problem that causes a diversity loss of at most D.

Let r1, r2, ..., rn be an ordered list of replacements that satisfies the relaxed conditions of the problems and has D steps that cause diversity loss. We build a list of paths L iteratively, starting with the empty list. When considering replacement ri = x → y:

If the text before and after ri are the same, we do not change L.
If there is a path starting with y in L, we append x at the start of it.
Otherwise, we add ri as a new path in L.
Notice that the first time we perform a replacement x → y where x is a node that we need to cover, we cannot be in case 1: the xs that were originally in S have not been replaced yet, so ri will replace them and change the text. Cases 2 and 3 add x as a non-final part of a path. Further processing can only append things to the start of the path, so x will remain a non-final member of it. This proves that L is a cover.

Now, suppose step ri = x → y falls into case 3 and adds a path to L that increases its weight. Because we are not in case 1, x is in the text at the time we begin to perform ri. Since we are in case 3, there wasn't a path in L starting with y. That means that for any previous replacement of the form y → z that erased ys from the text, there was another replacement of the form w → y that that reintroduced y and caused it to no longer be a starting element of the path in L in which y → z was added. Moreover, since we are assuming this step increases L's weight, either y was in S or there is a path in L ending with v → y, which introduced y into the text. In either case, y is in the text before performing ri, meaning that ri causes diversity loss. Since every step that adds weight to L is a step that causes diversity loss, the weight of L is at most D.

Now let us prove that given a cover L of weight D, we can find a solution to the problem with diversity loss D. For that, we need to process paths in a manner that is not as simple as the one we used for Test Set 1. The way in which we process paths in Test Set 1 could only result in diversity loss on the first replacement made, but it also produced other changes in the text. Those changes could hurt us now that we have to process multiple paths within the same connected component.

To process a path P, we consider the status of the text U right before we start. We call a node strictly internal to a path if it is a node in the path that is neither the first nor the last node in the path. We split P into ordered subpaths P1, P2, ..., Pn such that the last node of Pi is the same as the first node of Pi+1 (the edges in the subpaths are a partition of the edges in P). We split in such a way that a strictly internal node of P is also a strictly internal node of the Pi where it lands if and only if the character it represents is in U. Then, we process the subpaths in reverse order Pn, then Pn-1, ..., then P1. Within a subpath, we perform the replacements in the path's order (unlike in Test Set 1). Since the intermediate characters within a subpath do not appear in S, performing all the replacements of a subpath that starts with x and ends with y has the net effect of replacing x by y. In the case of P1, if x is not in U, then there is no effect. After processing subpath Pi+1 that starts with xi+1, xi+1 is not in U, and after processing subpath Pi, xi is not in U and xi+1 is restored. The net effect is that processing the path in this way effectively replaces the first character of the path that is in U by the last character of the path, and doesn't change any other character.

Let L' be a sublist of L consisting of exactly one path that ends in c for each character c not in S. L - L' contains exactly D paths. We process the paths in L' first, and then the ones in L - L'. When we process a path in L', we change the text by introducing a new character to it. However, that character is not in the original S, and it is always a new one, so those changes cause no diversity loss. When processing all other paths, since the net effect is that of a single replacement, we can cause at most 1 diversity loss each time, which means at most D diversity loss overall.

Notice that if we have a path that touches a node x that represents a character in S, we can replace that node in the path with a cycle that starts and ends at x and visits its entire strongly connected component. A replacement like this one on a path in a list does not change its weight. Therefore, we can relax the condition to require covering just one node of out-degree at least 1 and with a represented character in S per strongly connected component.

We can turn this into a maximum matching problem on a bipartite graph similarly to how we solve the minimum path cover problem in directed acyclic graphs. Consider a matching M between the set of strongly connected components C (restricted to nodes representing characters in S) that need to be covered, and the set D equal to C and all remaining nodes representing characters not in S. An element c from C can be matched to an element d from D if there is a non-empty path from c to d (it cannot be matched with itself). This matching represents a "next" relationship assignment, and unmatched elements represent "ending" nodes in paths. More formally, let f(c) be the function that maps a member of C to the member of D that corresponds to the same strongly connected component. We can create a cover with weight exactly the size of the unmatched elements from C by creating a path that adds weight for each unmatched element from C, and adding paths that do not add weight for each element in D - C.

For each unmatched element c from C, add it to a new path, then append to its left its matched element M(f(c)), then M(f(M(f(c)))), etc. Then, for each matched element c in D - C, which are the characters not in S, do the same. This yields a set of paths that touches every element of C, and the ending elements are either unmatched elements from C or characters not in S which do not add weight. Notice that some unmatched elements could yield single node paths, which are paths not really considered above. However, since such a path always counts toward the weight, we can append any replacement to its end to make it not empty without increasing the weight.

Therefore, the size of the unmatched elements from C in a maximum matching of the defined relation, which we can calculate efficiently by adapting a maximum flow algorithm like Ford-Fulkerson, is equal to the minimum weight of a cover, which is what we needed to calculate.

Despite the long proofs, the algorithm is relatively straightforward to implement. The graph can be built in time linear in the size of the input, while the strongly connected components and their transitive closure, which are required to build the relation for the matching, can be found in quadratic time in the size of the alphabet A (the relation itself can have size quadratic in A, so it cannot be done faster). The maximum matching itself takes time cubic in A, because it needs to find up to A augmenting paths, each of which can take up to O(A2) time (linear in the size of the relation graph). This leads to an overall complexity of O(A3), which is fast enough for the small alphabet size of this problem. Moreover, we could use something simpler and technically slower for the strongly connected components and their transitive closure like Floyd-Warshall, simplifying the algorithm without affecting the overall running time.

Qualification Round 2021 - Code Jam 2021

Reversort (7pts)

The pseudocode of the solution is almost given in the statement. We just need to give it the formal shape in the programming language of our choice. There are two non-straightforward statements in the provided pseudocode. We assume we store L in an array, so we have quick access to any index within it.

The first one is to figure out the index of the minimum element in a contiguous subarray. There may be some library functions to perform this task. For example, in C++ we can use min_element, in Python we can use index and min to figure out the minimum element. We can also run another loop to find out the minimum element in the subarray. Please note, the input numbers are all different so the minimum in each iteration is unique.

The second one is to reverse a subarray. Again, there may be some library functions to perform this task. For example, we can use the reverse STL library function for C++, reversed or reverse or simply slicing in Python. We can also run a loop to reverse the subarray.

The length of the subarray we are reversing in the second step above is the cost of the reversal. Accumulating these costs will give our final answer.

This solution has the time complexity of O(N2)
. We are running an outer loop from 1 to N−1
. Inside the loop we are performing two steps that take linear time each: minimum finding and array reversal. Hence the time complexity is O(N2)
.

One final note, there are solutions to this problem that run in O(NlogN)
 time, but they are a lot more complex. Do you want to try to find one?

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2021 - Code Jam 2021

Moons and Umbrellas (5pts, 11pts, 1pts)

Test Set 1
In Test Set 1, the size of the mural is small enough that we can test every possible final mural. Let ℓ
 be the length of S
. We can either check for each string of length ℓ
 consisting only on Cs and Js whether it agrees with S
 in the non-? spots, or brute-force only the ?s directly. For each valid way of finishing the mural, we can calculate the total cost to Cody-Jamal, and keep a running minimum to output at the end. Since we check at most 2ℓ
 final murals, and for each we need only an additional linear pass to check the score, this algorithm takes O(2ℓ⋅ℓ)
 time, which is fast enough for Test Set 1.

Test Set 2
In Test Set 2, ℓ
 can be up to 1000
, so an exponential algorithm will not do. However, a simple observation can yield a much faster algorithm: since X
 > 0 and Y
 > 0, we would like to avoid inserting CJ or JC into the string. So if all letters surround a consecutive substring of ?s are the same letter a
, then making all those ?s into a
 adds no CJs or JCs. If the substring is surrounded by a
 on the left and b
 on the right, with a≠b
, on the other hand, at some point there will be a change from a
 to b
, so we will be forced into an occurrence of ab
. Making all the ?s into a
s (or b
s) makes sure that we only add that one forced occurrence and nothing else. This yields a greedy algorithm that can be implemented in multiple ways, some taking as little as O(ℓ)
 time. Notice that the cost is the same on the resulting string than on the string where ?s are removed instead of replaced, which leads to a 1-line implementation:

S.replace('?', '').count('CJ') * X + S.replace('?', '').count('JC') * Y
Extra credit: Test Set 3
In the solution for Test Set 2 we assume that minimizing the number of occurrences of CJ and/or JC is best, which is true only when X
 and Y
 are non-negative. This means that Test Set 3 needs a completely different solution. In this case, a technique that would not normally show up in the second problem in a Qualification Round: dynamic programming.

We can take the function f(s)
 that we need to compute and define it recursively as follows:

f(?s)=min(f(Cs),f(Js))
f(a?s)=min(f(aCs),f(aJs))
 for a∈{C,J}
f(aas)=f(as)
 for a∈{C,J}
f(CJs)=X+f(Js)
f(JCs)=Y+f(Cs)
f(s)=0
 if the length of s≤1
You can verify that the cases above form a partition and properly define the recursion. We have a constant number of cases, each of which can be computed in constant time not counting the recursion calls, so the time complexity of the solution is O(D)
 where D
 is the size of the domain of the function. You can check that every value of f
 that is ever required to calculate f(S)
 can be defined by a suffix of the input S
 and at most two extra characters at the beginning. This means D
 is linear in the length of S
, and so is the time complexity of the algorithm. Notice that we need to represent each element in the domain in constant space for this to work out exactly as mentioned, for example representing suffixes of S
 by just their length or an index into S
. However, for the low limits of this problem, even a larger representation using copies of the full suffix is fast enough.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2021 - Code Jam 2021

Reversort Engineering (7pts, 11pts)

Test Set 1
The solution to the problem is a permutation of the numbers from 1 to N
. The cost of each permutation can be calculated by simulating the Reversort algorithm as described in the analysis of the Reversort problem in O(N2)
 time complexity. There are N!
 distinct permutations of size N
, containing the numbers from 1 to N
 exactly once each. The cost of each permutation can be calculated and the answer is any permutation that has a cost equal to C
. If there is no such permutation, output IMPOSSIBLE. The time complexity of the overall solution is O(N!⋅N2)
.

Test set 2
As N
 is large for Test Set 2, we cannot generate every possible permutation. The major observation here is that the range of valid costs for a given N
 lies between N−1
 (when the cost of each reverse operation is the minimum possible, which is 1
) and N⋅(N+1)2−1
 (when the cost of each reverse operation is the maximum possible, which is N−i
. Cost = N−1
 when the array is already sorted.

All costs in between those two limits are possible, as we shall see. Hence, if C
 is not in the valid range for given N
, output IMPOSSIBLE. Otherwise, we perform the following construction by recursion, which also serves as proof that the costs in range are indeed possible. The first iteration costs between 1 and N
, so we should choose a cost x
 for it such that C−x
, fits in the possible range for a permutation of size N−1
. You can check that this is always possible, and even compute the full range of x
 values that work by solving the system of inequalities.

Now, recursively generate a permutation P
 of size N−1
 and cost C−x
. Then, add 1
 to all integers in P
 and insert 1
 at its left end, getting a new permutation of integers between 1
 and N
. Then, reverse the prefix of P
 of length x
 as the cost of the initial iteration should be x
. The non-recursive steps take O(N)
 to adjust P
. Since we perform those for each index, the overall complexity of the solution is O(N2)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2021 - Code Jam 2021

Median Sort (7pts, 11pts, 10pts)

This problem is about information theory. There are N!/2
 essentially different outputs, which means we need at least log2(N!/2)
 bits of information. For N=50
 that is slightly over 213
. That means that for Test Set 2, extracting less than a bit per query (on average) can work, but for Test Set 3 it will not. In Test Set 1, on the other hand, this informational analysis is not necessary.

Test Set 1
In this test set we have 300 queries per case, which is more than the number of different triples (N3)=120
. This means that we can query every possible subset of 3
 elements and memoize the results. Freed from the limitations of the number of queries, any solution to sort based on medians works. One simple way is to notice that the only elements that are never the median are the first and last elements, so we can identify those, and arbitrarily choose which one is the first and which one is the last. Then, we can eliminate those two and find the candidates for second and next-to-last as the ones that are never median in queries with only remaining elements. To know which one to assign second, we can check an additional query between the element we chose to go first and the two newfound elements: the one who is the median of that subset should be second, and the other one should be next-to-last. We can iterate this to find and place pairs of elements moving inwards until we place them all.

See below for other sorting algorithms that work on medians. While the other test sets have the additional burden of having to make them work in an online way, we can use the pre-query tactic for this one to make our lives easier, even if we implement the same basic algorithm. It also allows for simpler implementations of some of the algorithms that use extra queries.

Test Set 2
Since just 1
 bit of information per query is enough for this test set, we can use one of our known comparison-based optimal sorting algorithms like Merge Sort or Heap Sort. Even Insertion Sort can work if we use binary search to look for the insertion point. While Insertion Sort + Binary Search requires a suboptimal O(N2)
 number of operations, it uses an optimal O(NlogN)
 number of comparisons.

For any of these algorithms we need a way to simulate a binary "less than" operation between two arbitrary items i
 and j
. One way to do that is to ask for the median of i
, j
 and k
 while knowing k
 is not the median. So, we could try to find an overall minimum or maximum and then use that as k
 for every other query. Notice that there are 2
 elements that can be minimum and/or maximum and neither would be the median of any query. So, we can do a query for the median of the first three elements. We can discard the median of those and keep the other 2
 as candidates. We add any element we have not checked and do another median query, discarding the median and keeping 2
 candidates again. After N−2
 queries, we have discarded N−2
 elements, and the 2
 that remain are the minimum and maximum.

Test Set 3
To solve Test Set 3 we can observe that our implementation of Insertion Sort can actually extract more than 1
 bit per query and have a smaller constant. Instead of looking for the minimum/maximum first, we simply use the minimum of our current range in the median query as k
. This means it is not guaranteed to never be the median. However, if the "current minimum" is the median of our query, we know exactly where to insert and can stop searching. This effectively makes our query a binary comparison with a little bit of extra information, and that little bit (plus being careful about never overspending) can be enough to pass Test Set 3. More importantly, it enables us to not find for the minimum first, which is a significant overexpenditure.

We can also find solutions that have more wiggle room by extracting a lot more than 1
 bit per query. Specifically, we want to extract something closer to the optimal log23≈1.58
 bits per query. That means considering all 3
 possible outcomes of the query, and have them happen with approximately equal probability (see the information theory article for details on the link between the probability distribution of outcomes and the information content of the query response).

We can further refine the Insertion Sort implementation to use a ternary search (in this case, this means a search similar to binary search that splits into three parts instead of two) to extract the full potential out of the median query. At each step, we query the two elements that are 1/3
 and 2/3
 of the way into our current range, together with the element to be inserted. The result narrows down the search to one of three ranges (roughly first, middle or last third). Notice that any query that we do to handle border cases (like inserting at the very beginning or very end) does not get the optimal 1.58
 bits of information, so we want to be careful not to use those, or to use them infrequently.

A different option is to do a two-pivot randomized quicksort. By using two pivots, we make full use of each median query involving them and each other element, as there are three buckets where the other elements can fall, each corresponding to a possible response to the query. The only issue with this approach (and any other approach based on a divide and conquer sorting algorithm) is that there are two ways to orient each recursive result if they have more than one element. If we use a query to decide which way is consistent with our decision on how to order the pivots, that query gives us only 1
 bit of information. Luckily, this is rather infrequent as it only happens proportional to the number of branches in the recursion tree that contain more than one element, which is a small number.

Qualification Round 2021 - Code Jam 2021

Cheating Detection (11pts, 20pts)

Solutions for this problem are based on the fact that the cheater's advantage pumps up their numbers independently of the difficulty. Judged by their total number of correct answers, a cheater with base skill level B
 is going to look like a player with skill B+Δ
 for some significant Δ
 (Δ
 depends on B
). However, this cheater does worse on easy problems than a player of actual B+Δ
 skill level, and better on hard problems, because the correct answers that are coming from cheating happen uniformly instead of more heavily on easier problems like skill-based improvements.

Test Set 1
There are multiple ways to get to 10%
 accuracy to pass Test Set 1. One such way is to estimate each question's difficulty as its number of correct answers, then sort by that difficulty, and check how uniform the distribution of corrects for each candidate is. The closer to uniform a candidate looks, the more they look like a cheater. One possible metric is the number of pairs of questions with different answers such that the incorrect answer is estimated to be for an easier question than the correct one. Using this metric is just enough to pass Test Set 1.

Test Set 2
The issue with counting inversions as the suggested metric for Test Set 1 is that it is a metric that is very susceptible to the contestant's strength. More concretely, a list that has few corrects or few incorrects has fewer opportunities to have inversions than one that is fairly evenly split. We can solve this by dividing the number of inversions by the expected number of inversions in a randomly arranged one. This reduces the noise enough to pass Test Set 2.

Another way to increase accuracy, of invesions or any other metric, is to check super-easy and super-hard questions, because the difference between a uniform distribution of correct answers and a heavily biased distribution is more pronounced in those. Exactly how much depends on the metric, but around the easiest and hardest 5%
 of questions seems like the right number experimentally for our solutions.

Other metrics are more accurate than inversions and also help solve the problem. We found two techniques that work well enough: sort the players by estimated skill level (i.e., by total number of correct answers) and compare the number of corrects only in the "extreme" questions with the number of corrects of other players with similar estimated skill level among those. Then, assume that the cheater will have the greatest difference with its neighbors. Another technique could be to estimate the actual skill level (by computing the inverse sigmoid of the accuracy of each player) and the actual difficulty of questions (again, the inverse sigmoid of its proportion of corrects). Then, using those two, estimate the expected number of correct answers for each player in the extreme questions. The player with the largest difference between that estimation and the real value is the cheater. Using this latest technique can get a solution above 90%
 accuracy.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1A 2021 - Code Jam 2021

Append Sort (12pts, 14pts)

Test Set 1
First of all, note that it is never optimal to append anything to the first number. Now, with the low limits we can iterate through all the ways to append digits to the second and third numbers while achieving the problem goal. We should stop once we understand that we cannot do better, for example, when both numbers become longer than 4 digits.

Test Set 2
We can use a greedy approach to solve the problem: for every number starting with the second one, make it larger than the previous one but as small as possible. Note that this ensures we use as few digits as possible for the current number and at the same time makes it as easy as possible for the next number. This means the greedy choice is optimal.

So the problem now is how do we compute this efficiently. More formally: given integers A
 and B
, find the minimum B′
 such that B′>A
 and B
 is a prefix of B′
. Let us denote the number of digits in an integer Z
 as len(Z)
.

First of all, if A<B
, we can set B′=B
.

Now let's consider the case when A≥B
 and len(A)=len(B)
. Note that appending any digit to B
 will make it larger than A
. To make it as small as possible we can just append a zero.

The only case that is not considered now is when B
 has fewer digits than A
. Let k=len(A)−len(B)
.

First, we can try making B′
 equal to B×10k
, that is, append k
 zeroes to the right of B
. If such a B′
 is larger than A
, then this is the optimal solution.

Now, we check if it is possible that B′
 is the same length as A
. If appending k
 nines to B
 doesn't result in something larger than A
, then B′
 will need to have more digits than A
. In such a case, we can just make B
 exactly 1
 digit longer A
 and as small as possible by appending k+1
 zeroes to B
.

If appending k
 zeroes to B
 is too small but appending k
 nines makes it larger than A
, then B
 is actually a prefix of A
. In such a case B′=A+1
 is the optimal answer.

All the checks performed above are linear in the length of A
 and B
 and each number in the input is processed at most once as A
 and at most once as B
. However, numbers may become longer after each operation, but only by 1
 digit. Therefore, the complexity of the overall algorithm is quadratic in the total number of digits in the input, and linear in the number of digits of the output. As an example, an input of N
 strictly decreasing integers of the same length yields an output where the i
-th integer consists of exactly one more digit than the previous, which needs (N⋅(N−1))/2
 operations in total.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1A 2021 - Code Jam 2021

Prime Time (7pts, 13pts, 15pts)

Test Set 1
In Test Set 1, the total number of cards in the deck is at most 10
. A small number like 10
 suggests we can brute force and simulate all possible game scenarios. For each card we can decide whether it belongs to the first group or the second group. Next, we sum up the numbers on the first group cards and compare with the product of the numbers on the second group cards. If they are equal, current sum or product is one of the possible candidate scores. After checking all possible partitions, we output the maximum score or output the score 0
 if there is no feasible way to partition the cards.

Two things to watch out for. First, each of the groups has to be non-empty. This is easy to handle. Second, the product of all the numbers may overflow the integer type of our choice (in the worst case there may be 10
 cards, each with 499
). There are many ways one can avoid this problem. One way can be to first find the sum of the first group numbers and then try to divide it by the second group numbers, one by one. If at some point we are unable to perform the division or if after all the divisions the result is not 1
, we know current partition candidate can not be good. Another way would be to notice that the sum of the first group can not be very high. A naive upper bound is 4990
 (all the 10
 numbers are 499
 and they are in the first group). Hence we can find the product of the second group numbers in floating point and compare with the sum of the first group numbers. Since the upper bound of the sum of the first group numbers is very low, precision error should not cause any trouble.

There are 2N
 ways to partition the cards where N
 is the number of cards. After each partition we can find the sum and the product of the groups with a linear loop. Hence the total run time is O(2NN)
. We could also get rid of the linear loop but such optimization was not necessary.

Test Set 2
In Test Set 2, the total number of cards is at most 100
. Performing all possible partitions would surely timeout. But notice that we cannot put many numbers into the second group because the product of positive numbers greater than 1
 quickly increases. This time the upper limit for the sum of the first group numbers is 49900
. That means, we can not have more than 15
 numbers in the second group (216>49900
). We can use this observation to optimize our brute force solution.

Let S
 be the set of all possible partitions using the first i
 cards for some arbitrary ordering of the cards. Instead of keeping explicit partitions, we will represent the partition as (x,y)
 where x
 is the sum of the numbers in the first group and y
 is the product of the numbers in the second group. This helps us to reduce the state space. Once we partition the first i
 cards, it does not matter exactly which card went to which group. We just care about the sum and the product of the respective groups. Now let b
 be the value of the i+1
'th card. Hence for each partition (x,y)
 in S
 we will have two choices: (x+b,y)
 and (x,y×b)
. If this makes the product of the second group numbers larger than 49900
, we prune this state. After considering all N
 cards, we check if there is some partition where the sum of the first group numbers is equal to the product of the second group numbers (i.e., a tuple in S
 where both values are equal). We output the maximum of these sum or product, or 0
 if there is no such partition.

We can have at most 100
 numbers in the input. So it might be tempting to think that there may be (10015)
 possible valid partitions. But the product of the numbers in the second group can be at most 49900
 and each of these 49900
 numbers can be uniquely represented as the product of primes. So the remaining numbers will go to the first group. Hence after considering each of the input numbers there can be at most 49900
 possible partitions (49900
 possible second groups and for each of these there is a unique first group). So this naive calculation gives us a limit of 49900×100≈5×106
 operations.

So far we have used the fact that each score can be uniquely represented as a product to prove that our solution is fast enough, but it can also be used to obtain an alternative approach for Test Set 2: we have only 49900
 candidates for the final score, so we can iterate over them one by one. For each candidate score, there is exactly one way to represent it as a product of primes. Assuming we know the prime factorization of the candidate how can we determine if the candidate is valid? Suppose the prime p
 appears in the prime factorization q
 times. Then, the prime p
 has to be in the input at least q
 times. This condition guarantees us that the second group is achievable. Next we need to come up with a condition to check the existence of the first group. If we know the sum of the input numbers and subtract the sum of the numbers in the second group we can get the sum of the first group which has to be equal to our candidate. This guarantees the first group existence. Both of these conditions can be checked very easily given the prime factorization of the candidate score.

It turns out that this approach generalizes very well to Test Set 3.

Test Set 3
In Test Set 3, N
 can be up to 1015
, which means the sum of the first group can be as high as 4.99×1017
. But the number of cards in the second group can not be very high. Since 260>4.99×1017
, we can consider 60 as the upper bound of number of cards in the second group. Although the product of at most 60 cards in the second group can range from 2
 to 260
, the sum of the second group numbers can be only up to 60×499=29940
 (the actual maximum possible sum of the second group numbers is 3025
 under the problem's constraint but even a crude estimate of 29940
 is enough to get a working solution). Let X
 be the sum of all the cards in the input. Then, the sum of the first group numbers must be between X−29940
 and X
, inclusive. This means we have only 29941
 candidates for the final score, so we could apply the second approach from Test Set 2 if only we could factorize those candidates.

Unfortunately factorizing a number as big as 1017
 is not an easy task. A naive way may be to run a loop up to 1017−−−−√
. However, in this problem we do not need to care about the primes higher than 499
. To put it another way, if the candidate has a prime factor other than the input prime numbers, we cannot achieve this score in the second group. Hence, it is enough to try to factorize the 29940
 candidates with only primes from 2
 to 499
. It takes about 29940×(95+60)≈4.6×106
 operations to factorize the 29940
 candidates (there are 95
 primes between 2
 and 499
 and in total there can be at most 60
 prime factors). We could also do sieve-like factorization reducing the number of operations to about 29940loglog499≈105
 but this is not necessary. After the factorization, we can run a loop to check if the number of each prime exceeds the input count and also to sum these primes. This takes 29940×95≈3×106
 operations.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1A 2021 - Code Jam 2021

Hacked Exam (8pts, 6pts, 25pts)

In this problem we want to maximize our expected score. Let pq be the probability of question q's answer being T, and let QT and QF be the sets of questions we answer with T and F, respectively. By linearity of expectation, the expected score to maximize can be written as
( 
∑
q∈QT
 pq)+ 
∑
q∈QF
 (1−pq).
Notice that linearity of expectation can be used regardless of any conditional probability between different questions. In this case, those conditional probabilities can be quite complicated, so being able to ignore them and treat each question independently despite them not being independent in the probability theory sense simplifies the solution process significantly. This is a trick that is important in many problems about expected values.

We operate with fractions throughout most of the proposed solutions. As noted in the sample, the needed numbers may exceed 64 bits for Test Set 3, and potentially for Test Set 2 as well, depending on the exact implementation of the algorithm and the fraction operations. It is absolutely possible to solve Test Set 2 with simply 64-bit integers and Test Set 3 with just 128-bit integers. Our most popular languages all support this in some way: C and C++ have __int128 support, Java and C# have BigInteger, JavaScript has BigInt, Bash has bc, and Python, Ruby and Haskell have native long integer support. We usually strive to make most of our problems solvable with just 64-bit integer arithmetic, but in this case, limiting the number of questions so much would allow suboptimal solutions in fast languages to pass Test Set 3.

Notice that we can have information of 1 or 2 other students in the first two test sets, and also 3 in the last test set. We could solve each number of students independently, but there is no need for that. Adding a student with the same answers and score as a student in the input results in a completely equivalent case, so we can always assume that there are a maximum number of students by copying any student the needed number of times.

Test Set 1
In Test Set 1, the number of questions Q is small enough that we can simply enumerate all possible 2Q sequences of answers, and filter out those who are inconsistent with the input (i.e., those for which one of the students would obtain a different score than they actually got). From the consistent ones, we can estimate the pqs above as the ratio between the number of sequences that answer T to question q, over the total. Then, we can simply choose to answer T to those questions with pq>
1
2
 
 and F to those with pq<
1
2
 
. We can answer the questions with pq=
1
2
 
 either way.

Test Set 2
In Test Set 2 Q is large, so we cannot enumerate the sequences of answers. We can, on the other hand, figure out the probabilities pq in a different way, and then proceed as before with choosing the answers by comparing those values to 
1
2
 
.

An insight-based solution
One way to solve Test Set 2 is by splitting the questions into types. If two questions q1 and q2 received the same answer from each student, then by symmetry pq1=pq2. Then, let pab be equal to the probability of a question's answer being T given that the first student answered a and the second student answered b to it. By the first observation, each pq is equal to one of the 4 values pTT, pTF, pFT or pFF. Moreover, by the symmetry of complementing answers, pTT=1−pFF and pTF=1−pFT. Therefore, we can express every pq as a linear function of up to two variables pTT and pTF. That means we can express the expected score of both students as linear functions on those two variables too. Given that their expected score should match their actual score, that gives us two equations with two unknowns. We can derive the real values of pTT and pTF from that system of equations. With every pq calculated, we can simply choose, for each question, an answer that has maximum probability, as in the solution for Test Set 1.

Notice that there are 4 possible cases depending on how our two variables compare with 
1
2
 
 (if one is exactly 
1
2
 
 that means two cases are equivalent and if both are 
1
2
 
 then all cases are equivalent). The 4 cases exactly match with either answering the same as one of the students, or answering the opposite of one of the students. We can use this observation to greatly simplify the implementation as the score of a sequence given by a student is given to us, and the score of the complement of the answers of student i is Q−Si, so we can easily obtain the scores of the 4 options and pick a highest one.

A more competitive-programming-standard solution
In case of having 2 students, we can use dynamic programming to calculate the probability of each question having a particular answer. We compute the recursive function f(s1,s2,q) defined as "how many ways are there to answer questions q,q+1,q+2,…,Q such that student 1 gets exactly s1 of them right and student 2 gets exactly s2 of them right?" We can define that function recursively as
f(s1,s2,q)=f(s1−I1(q,T),s2−I2(q,T),q+1)+f(s1−I1(q,F),s2−I2(q,F),q+1)
where Ii(q,c) is 1 if student i answered c to question q and 0 otherwise. The base cases are f(0,0,Q+1)=1 and f(s1,s2,Q+1)=0 whenever one of s1 or s2 is not 0. To simplify memoization, we may want to add a case to simply answer 0 whenever s1 or s2 are negative, but the recursive equation holds as written for those cases.

By memoizing that function, we can compute it in time O(Q3). We can calculate the probability of question 1's answer being T as
f(S1−I1(1,T),S2−I2(1,T),2)
f(S1,S2,1)
 
.
Then, by symmetry, we can reorder the questions to make any question the first one and re-run to compute the probability for any question. After having all probabilities, we simply answer the most likely answer for each question and sum its probability to our expected score. Since we need to run the probability computation O(Q) times (once per question), the overall algorithm takes O(Q4) time. This can be a little too slow.

If we add only the first observation of the insight-based solution, we can notice that two questions that were answered the same by both students have identical probabilities. Then, we only need to calculate the probability of up to 4 question types (in the notation of the previous solution, we use dynamic programming to calculate all the pabs). This improves the overall running time to O(Q3), which fits better within the time limit. An observation about complement could further reduce this to only 2 question types, but that does not change the time complexity and it is not needed to pass this test set.

Test Set 3
The dynamic programming solution for Test Set 2 can be generalized to Test Set 3 by adding an additional score as another parameter to the recursive function. However, the additional dimension and larger limit for Q can easily make such solutions too slow.

Combining the full insights of the first solution to Test Set 2 with the solution to Test Set 1 works, though: there are 8 probability variables pabc, and pairs of complementary variables have complementary probabilities, so we only care about 4 different ones.

Let us call the subindex of the variables (the abc part) the "type" of a question. Let us number the types 1 through 4 in any order. If there are qj questions of type j, we can use quadruples (t1,t2,t3,t4) with 0≤tj≤qj for all j to represent sequences of answers that answer T to exactly tj questions of type j. We know that there are
(
q1
t1
 
)⋅(
q2
t2
 
)⋅(
q3
t3
 
)⋅(
q4
t4
 
)
sequences of answers represented by this particular quadruple. If we filter the quadruples by the ones that give each student their actual score, we are effectively enumerating answers that are consistent with the input. This is what we did for Test Set 1! In this way, we can count which amount tj of questions of type j has the largest probability and choose that one, for each j.

There are at most (Q/4)4 quadruples to check. This makes the time complexity of the algorithm O(Q4), but the 1/256 constant is pretty significant, and a good implementation runs comfortably in time.

But wait! We can refine this solution even more by using the solution for Test Set 2 that ends with solving the system of equations! We can express the score of each student as a linear function of t1,t2,t3 and t4. That gives us a system of 3 equations and 4 unknowns. That means that we only need to try all possible values for one of the tj and then simply solve the system to find unique values for the other three. That refines the solution above to requiring only O(Q) time.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1B 2021 - Code Jam 2021

Broken Clock (5pts, 6pts, 19pts)

Test Set 1
In Test Set 1, our only unknown is which hand is which. Since there are only 3!
 possibilities for the assignment of angles in the input to a specific hand, we can just try them all. After we have angles assigned to specific hands, we can just read the time. The easiest way is to just check the hours hand, as the statement tells us it moves one tick per nanosecond, so the current number of nanoseconds after midnight is equal to the angle in ticks of the hours hand. Notice that we still need to check that the time in the other two hands is consistent with this! We can do this by reading the number of minutes and seconds from their hands using the definitions in the statements, and see if it is consistent with the current number of minutes and seconds as computed from the nanoseconds we got from the hours hand. Alternatively, we can do the reverse process of getting from current time to how the clock should look. We do that in more detail below, as it is needed to solve the other test sets.

The solution to Test Sets 2 and 3 is based on first solving the inverse problem: given a time of the day, find a canonical set of 3
 angles that would produce it. Luckily, the way to solve it is explained on the statement: we know the speeds of each hand so we can translate a time of the day into a number of ticks since midnight, and then take it modulo the number of ticks in a full circle (12×1010×360
). Once we get three angles, we need to consider the potential rotations. We can say that canonical sets of angles always have at least one 0
, but there can be up to 3
 ways to do that (choosing each hand to be the 0
 one): simply choosing the lexicographically least of them (after sorting them) suffices.

Test Set 2
In Test Set 2, there are only 12×60×60
 possible times of the day to consider. We can simply build the canonical representation a
 for each time t
 and store it in a reverse dictionary from representations to times (which one we save if there are multiples does not matter) as a first step. Then, for each input, we get its canonical representation and find a corresponding time in the dictionary to output.

Test Set 3
In Test Set 3, the number of different times of the day is too large to check them all individually. A possible approach is to restrict the number of times of the day that can correspond to a specific input set of angles a
 and then check each of those to see if anyone fully matches by using the reverse conversion.

We can try every possible assignment of each angle to a specific hand. There are only 3!
 possibilities for that. Let us say that we now have an angle of ah
 for the hours hand, am
 of the minutes hand and as
 for the seconds hand.

Let us write the real time as h
 full hours plus n
 nanoseconds, that is, h⋅3600⋅109+n
 nanoseconds since midnight, with h
 between 0
 and 11
. The angle (in ticks) of the minute hand is 12⋅n
 and the angle of the hour hand is h⋅3600⋅109+n
. The difference between those two numbers h⋅3600⋅109−11⋅n
 needs to be equal to the difference between the angles we read from the input ah−am
. Note that this works because the difference between angles is invariant through rotations. If we try every possible value for h
 between 0
 and 11
, every variable except n
 in the last equality has an actual value, and we can solve for n
. If the value for n
 happens to be an integer, it gives us a real time candidate.

Finally, for each of the up to 3!⋅12
 real time candidates, we do the reverse check as we did in the previous solution.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1B 2021 - Code Jam 2021

Subtransmutation (13pts, 18pts)

To solve this problem, we can first tackle a simplified version: given a unit of metal x
, can it be used to produce all the metals required by the input?

To solve this simplified problem, we can use a greedy strategy. Keep track of the multiset H
 of units of metal we have (starting with {x}
) and the multiset D
 of units of metal we owe (starting with the ones given as input). Iterate the following steps:

Remove the intersection H∩D
 from both H
 and D
.
If D
 is empty, the answer is yes.
If H
 is empty and D
 is not, the answer is no.
Take all c
 units of the metal i
 with the largest number still in H
. Remove all c
 units of i
 from H
 and insert c
 units of i−A
 and c
 units of i−B
 into H
 (if either new metal number is invalid, skip those).
This procedure works because those units that we greedily transform are not something that we owe, so we have nothing better to do with them (it may be futile to convert them, but it does not hurt). And since we have to produce the units that we owe, we might as well just pay them back as soon as possible, as anything we could do with the current unit we could also do with a future unit that we would use to pay the debt instead.

Test Set 1
In Test Set 1, notice that if it is possible to find an answer m
 for an input where N
 and all Ui
 have maximum value, that procedure also produces enough units for any other possible input. If we implement the greedy strategy above and try it for this particular input on increasingly large values for m
, we can quickly arrive at the realization that m=29
 solves it. This means that all inputs can be generated with a metal with number no greater than 29
. So, using the same procedure of checking increasingly large values of m
 for the given input is a valid solution for the full Test Set.

It is also possible to notice and/or prove theoretically that there are no impossible cases and that the answers are small. A quick argument is that numbers grow exponentially: from a single unit of number m
 we can create 2 units of number m−2
 (with leftovers). This means we can create 2i
 units of element m−2i
 from a single unit of element m
. We can distribute these among the elements m−2i−N+1
 through m−2i
 (with a lot of leftovers), getting 2i/N
 units of each of N
 consecutive elements. These can be further moved to elements with lower numbers if needed. This shows that an m
 such that 2m−2×20/20>20
 ought to be enough, and m=49
 fulfills such a requirement.

Test Set 2
As the samples show, impossible cases can happen in Test Set 2. This means running our Test Set 1 solution as is will result in the program not finishing and getting a Time Limit Exceeded error. A simple solution, however, is to notice that there is also a somewhat small maximum answer for all possible cases, and then simply stop once we are past it and return impossible.

Knowing what that maximum answer is, and proving it, is harder as we cannot just rely on experiments. We have to lean more on the theory of the problem.

We can first notice that starting from metal m
, every metal we can possibly produce has the same remainder as m
 modulo the greatest common divisor between A
 and B
 (let us call that g
). This is an application of Diophantine equations, but while knowing that specific theory could help tackle the problem, it is not absolutely necessary. Now, consider the remainder modulo g
 of every i
 such that Ui>0
. If they are all the same value k
, then m
 modulo g
 needs to be k
 as well. If they are not all the same, then the case is impossible.

At this point, we can express all metal numbers that matter as g×x+k
 for some x
. To show that all of remaining cases are possible, we can generalize the proof we did for Test Set 1 and use Bézout's identity to build them, or we can simply iterate all relatively prime values for A/g
 and B/g
 and solve the largest case experimentally. If we do that, we can arrive at the conclusion that the maximum possible answer for a possible Test Set 2 case is 402
. Therefore, we can do the same solution as in Test Set 1, exploring values for m
 up to 402
, and returning impossible if we did not find an answer.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1B 2021 - Code Jam 2021

Digit Blocks (16pts, 23pts)

Let Di,j
 be the digit on the block that has exactly j
 blocks under it in the i
-th tower. The number written on the i
-th tower is then
∑j=0B−110j⋅Di,j
and the total score is
∑i=1N∑j=0B−110j⋅Di,j.
Notice that each term in the last sum is independent from others, that is, each position in a tower has a particular value that gets multiplied by the digits, and exchanging blocks at the same heights that go on different towers does not change the final outcome. Our job to maximize the score is to get the largest digits paired up with the largest values. That means, putting large digits near the top of the towers. Moreover, the value of a position is more than 9
 times the value of all the positions below it combined. Therefore, delaying the use of a position until we can get the maximum possible digit can be worth sacrificing the ability to decide on the order of all positions below it for up to 9
 towers, and possibly a lot more (as even a random order gets some value).

All the approaches we present are based on trying to reserve high-valued positions for high-valued digits, sacrificing other positions.

Greedy strategies
The simplest greedy strategy is: try to place 9
⁠s in the top position of every tower, and sacrifice everything else. That means, when a block comes, if it is a 9
, place it in the highest non-finished tower we have. If it is not, place it in the highest tower we have that has fewer than B−1
 blocks on it. This reserves the top spots for 9
⁠s and accelerates the availability of newer top spots by trying to build towers up as soon as possible.

There are other similar strategies, like doing the same but trying to place 9
⁠s in the top two spots of every tower, possibly pivoting to just the top spot when we are running out of time. Similarly, we can pivot to accepting lower digits for second-from-the-top spots or for top spots when there are few blocks left and the likelihood of enough 9
⁠s coming becomes low.

These simple strategies oscillate between getting a score of 87%
 and 91%
 of S
, which means many of them are enough to pass Test Set 1, but none seems to be close to pass Test Set 2. More sophisticated heuristics can pass Test Set 2, but they need to consider at least the top 2
 positions and also deal with tactically placing not just 9
⁠s but also 8
⁠s. These type of solution need careful tweaking and also comes with some uncertainty that it will eventually work out. The solutions in the next section address both those issues, ensuring the points and quite possibly saving us time.

Dynamic programming strategies
An alternative way is to try to maximize the expected score. This does not exactly maximize the probability of getting above a specific threshold (being more aggressive or conservative near the end depending on how close you actually are to the threshold might be better), but it's really close and much simpler.

The optimal such solution (the one that gets S
 as its expected score) is too slow, but it is worthwhile to consider it as a starting point. By linearity of expectation, when considering where to put a block, only the heights of all current towers matters, but not the actual values that were put there (since that is the score we will get regardless of all future decisions). We can therefore consider the multiset of tower heights the status, and optimize a function f(status, next_digit) that checks every possible placement for each possible next digit. The problem with this idea is, of course, the status space is too big for the time limit. This is what we did to calculate S
, but we ran it for a lot longer than the time limit.

As we mentioned before, we can improve on the time limit by not caring too much about what happens with the low-value positions. That is, consider only a subset of the statuses. One example is this: for each digit we consider only the options of the first greedy strategy. We either put it in the top-most position of some tower, or we put it in the highest tower that has fewer than B−1
 blocks on it. This severely reduces the number of possible statuses, as there is only one tower that can have a height other than 0
, B−1
 or B
. Instead of making the decision greedily, we leave the decision of which option to choose to the dynamic programming part. This solution is fast enough and gets about 96%
 of S
 as its score. Not enough for Test Set 2, but getting closer. Doing the same but leaving the top 2
 positions up for optimization, on the other hand, gets over 99.5%
 of S
 and passes Test Set 2.

Since the top position represents close to 90%
 of the value, it makes sense that optimizing it gets 90%
 of the score. Similarly, the top two positions represent close to 99%
 of the value. More formally, consider two modified problems in which the top position or top two positions retain their value, but all other positions are valued as 0
. Our proposed solutions get the maximum expected scores S1
 and S2
 for those modified problems, and we know that S1>0.9⋅S
 and S2>0.99⋅S
. Therefore, our solutions' expected score is guaranteed to be above each respective threshold, and the extra score we get from the lower positions gives us an additional gap to make the probability of success extremely high: our estimates are that each solution has less than 10−10
 probability of failing the respective test set.

Round 1C 2021 - Code Jam 2021

Closest Pick (9pts, 16pts)

Test Set 1
In Test Set 1, the limits are small enough that we can try every one of the K2
 possible pairs of integers to pick. For each of those, we can calculate the probability of winning with each possible draw. There are K
 possible values for c
. If for each one, we check the distance to each Pi
 and to our own two picks, we end up with an overall running time of O(K3⋅N)
. This is fast enough to pass.

There are several possible optimizations. A simple one is to search for the "closest picked integer" using a sorted structure, reducing that phase to O(logN)
 and the overall running time to O(K2⋅NlogN)
. It is also possible to search for the distance from each c
 to each Pi
 only once, independently of our choices, which reduces the overall running time further to O(K2)
.

Test Set 2
For Test Set 2, we can optimize all phases from the previous solution. If every integer is already picked at least once, then the answer is always 0
 as the samples demonstrate. Otherwise, we can divide the non-purchased integers from 1,2,...,K
 into intervals between Pi
s. For example, if the Pi
 values are 3,4,8,3
 and K=8
, the intervals would be [1,2],[5,7]
.

If we make only one of our picks within one of those intervals, we can get closest to all of it if the interval contains 1
 or K
, by picking ours next to the only delimiting Pi
 (picking 2
 from [1,2]
 in the example above). If the interval is surrounded by two Pi
s, we can get closest to at most half of the integers in the interval (rounding up), which is achievable by picking either end (picking either 5
 or 7
 from [5,7]
). If we make both of our picks within one of those intervals, we can get closest to all integers in the interval by picking both ends (5
 and 7
 from [5,7]
).

This shows that only integers that are next to an already purchased integer are worth picking for ourselves. At this point, we can go back to our last Test Set 1 solution and do the same but restricting our picks to integers next to a Pi
, which makes the K
s in the running time be O(N)
, making it fast enough with the right implementation.

A few extra reasoning steps lead to a much more efficient solution. There are only O(N)
 ways to make both picks within the same interval, and we can check each of them. If we choose two different intervals, it is always optimal to choose the two most valuable ones, so we do not need to check each combination separately. This leads to a single additional case that requires O(N)
 time to compute to find the top two intervals. All in all, this solution takes only linear time (after sorting the array to find the intervals). This solution requires some care about corner cases, specifically the cases when no or just one integer from [1,K]
 is not a Pi
, and the case of the intervals list containing a single interval.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1C 2021 - Code Jam 2021

Roaring Years (15pts, 20pts)

Test Set 1
A straightforward approach
Notice that 1234567
 is a roaring year that happens after every possible input. That means that if we simply check every year after Y
 to see if it is roaring or not, we need to check no more than 1234567
 years per case. It is a lot less in practice, but that is also harder to prove.

To check whether a year y
 is roaring, we can check for each prefix p
 of y
 whether "completing it" by appending p+1
, p+2
, etc., would work. Since only prefixes that are at most half the length of y
 can work, this is a small number of things to check. If we are unconvinced about the running time, we can simply run a check of roaringness for every integer up to 1234567
 once initially and remember it, after which each test case is simply looking for the next "true" in a rather small table, making the running time mostly independent of the actual input, so a test run can help convince us that it will run in time without risking penalty with a real submission.

An approach with a more clear time complexity
Another way to go is to first use the observation at the end of the solution above: the length of the first number in the concatenation is small — at most 3 digits. We can try each of the 999
 possibilities and concatenate numbers to it until it goes above the 1234567
 threshold. Notice that this is at most 6
. This builds a set of all possible roaring years that matter in very little time. After that, we can use linear search in this very small list (less than 6000
 elements) to find the smallest roaring year that is larger than Y
 for each test case. Of course, we could use binary search to make each test case run even faster, but that is not required.

Test Set 2
None of the approaches above work for Test Set 2. The gap between two consecutive roaring years can easily be large (for example, you can prove as an exercise that there are no other roaring years between 100000000100000001
 and 100000001100000002
) and checking an individual one, while quick, takes a bit more time than before. For the second approach, there are now 109−1
 candidates for the first number in the concatenation, and the amount of concatenated numbers can also be much larger for some of those, making the full set of roaring years too big to even fit in memory, not to mention the computation time.

A case-based approach
The last sentence of the previous paragraph hints at an idea. While the full set of roaring years in range is too big, the full set of roaring years that are the concatenation of 3
 or more numbers is not: for that case, the maximum possible starting number has 6
 digits. We can use the second Test Set 2 solution with this change to solve for those. We still need to check roaring years that are the concatenation of exactly two consecutive numbers. Notice that f(x)=
 the concatenation of x
 and x+1
 is an increasing function. Therefore, we can use the bisection method to efficiently find the minimum of that subset of roaring years that is above Y
. After we found the best candidate from each subset, we simply return the smallest of them.

A general approach
We can also generalize the second case of our first approach for Test Set 2. The family of functions fn(x)=
 concatenation of x,x+1,…,x+n−1
 are all increasing. We can therefore use bisection to find the best candidate for each possible amount of consecutive numbers n
, and then answer the best of those. Since n
 is at most logY
 this yields a method that runs in O(log2Y)
 time, which means it can work for really large bounds.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1C 2021 - Code Jam 2021

Double or NOTing (14pts, 26pts)

Test Set 1
Let's construct a graph where the nodes are numbers and the edges are the operations described in the statement. The problem asks us to find the shortest path from node S
 to node E
 in this graph. The graph is infinite, but we can inspect only a limited set of nodes to find the answer.

When thinking about binary representations, the double operation can be described as "add a trailing zero". Note that it is never optimal to add more zeroes than we have bits in E
: because the NOT operation can only drop leading digits, any additional zeroes will ultimately have to be dropped anyway.

We can use this fact to limit the maximal length to the sum of the lengths of S
 and E
, and then use a Breadth-first search algorithm to find the shortest path.

Test Set 2
Let's define a bit group as a maximally long group of consecutive zeroes or consecutive ones (that is, a group of all bits b
 that does not have a b
 adjacent to it). Let's assume that there are K
 bit groups in S
 and L
 bit groups in E
.

The double operation appends a bit to the right end of the binary string, and the NOT operation complements and possibly drops bits from the left end. Therefore, the result of applying operations to S
 comes from some (possibly empty) suffix of S
 (possibly complemented) and then some extra bits that were created using double. Let us call the bits that come from a bit in S
 reused.

First, let's consider the case when we do not reuse any bits from S
. In such a case we need to construct E
 naively using only the bits added by double operations and removing all the bits that came from S
 originally. We can do this by adding zeroes with double operations and applying a NOT operation whenever we need to start a new bit group. We also need to apply additional NOT operations to eliminate all bits that came from S
 originally. If we end up with E
, then let's count this as a possible answer. For example, to get from 1012 to 111002 we would need to do perform the following sequence of operations (we use a space to separate bits originated from S
 from the extra bits added with double operations):

Operation	Result
double (to add an extra bit)	101 0
double (to add an extra bit)	101 00
double (to add an extra bit)	101 000
NOT (because the length of first bit group in E
 is 3, and we have added 3 zeros already, now we need to start a new bit group)	10 111
double (to add an extra bit)	10 1110
double (to add an extra bit)	10 11100
NOT (because we already have the desired E
 as suffix, now we just need to remove extra bits from S
)	1 00011
NOT (to remove the leftover bit of S
)	11100
If S
 ends in 0
, we need to perform an extra NOT operation at the beginning so that our first double operation creates a brand new bit group. After we finish removing all bits from S
, we might need another NOT operation, if the removal process left us with the complement of E
.

Now, let's try to reuse some bits from S
.

The NOT operation removes one bit group from the left end of the binary representation and complements the rest. It is never optimal to apply a NOT operation more than K+1
 times, as in such a case we will either be looping between 0
 and 1
, or will be removing bits which were added with the double operation (which we could have just not added in the first place if they were not useful).

Let's assume that the answer will have NOT operations applied exactly X
 times. Let's apply X
 NOT operations to S
 to obtain S′
. If S′
 is not a prefix of E
, then it is not possible to get to E
 from S
 using X
 NOT operations, as the double operation will not change the prefix of the string.

If S′
 is a prefix of E
, let's see whether we can construct the suffix. We will need exactly Y=len(E)−len(S′)
 double operations. Let's say there are M
 bit groups in the suffix we need to create, then we will also need to apply M
 or M+1
 NOT operations, depending on the parity of M
 and the first bit of the prefix. Let's denote the number as Z
. If Z
 is greater than X
, then this case does not work, as we would apply more than X
 NOT operations, violating the assumption. Otherwise, the answer is X+Y
, as even if X
 is greater than Z
, extra NOT operations won't affect the suffix (we can perform them before adding any extra bits with a double operation).

We can now iterate through all possible values of X
 from 0
 to K+1
, inclusive, and choose the minimum answer between all of those and the non-reuse case we considered first. If none of the cases work, we output IMPOSSIBLE.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2021 - Code Jam 2021

Minimum Sort (15pts)

In this problem, we are given only one tool to get information about our input: the minimum query. Moreover, the query becomes cheaper when done over long distances. This means a sorting algorithm that normally spends running time finding minimums would align well with our needs. One well known such algorithm is Selection sort.

In Selection sort, we search for the minimum of the full list, put that in its correct position (the beginning), and continue solving recursively searching for the minimum of a suffix of the list, and moving that minimum to the beginning. This can be implemented with N−1
 pairs of the minimum query and (possibly) a swap operation as follows:

for i := 1 to N-1
  j = minimum_index_of(i, i+1, ..., N)
  if i != j:
    swap(i, j)
The minimum queries above are over ranges of sizes N,N−1,...,2
. This means the total cost is exactly
∑i=2N⌈108i⌉.
For N=100
, this is 418737795
, which is less than the limit of 6×108
.

Round 2 2021 - Code Jam 2021

Matrygons (7pts, 13pts)

Test Set 1
The statement of this problem requires calculating a function from integers to integers. After we select a polygon to use, the remainder of the work is similar to the overall: select more polygons. This almost screams: write a recursive solution!

One way of doing that is with a top-down approach: start by picking the largest polygon, and then pick the rest. For all polygons except the largest, there is the additional restriction of "fitting" into the previous polygon. That is, the number of sides of the new polygon must be a proper divisor of the number of sides of the last one.

We can code that into a simple function that takes a number of sides left t
 and the size of the last polygon used p
: f(t,p)=1+maxq:q|pf(t−q,q)
 (one plus the maximum f(t−q,q)
 over all q
s that are divisors of p
) adding f(0,p)=0
 as base case, and appropriate conditions that q
 is indeed a polygon. The answer is then maxpf(N,p)
. This recursion is fast enough for the small bounds of Test Set 1 even if we iterate all possible 3≤q≤min(p−1,t)
. Of course, we can avoid that and find the divisors faster by iterating only up to p–√
 and checking both q
 and p/q
, but the optimization is not necessary for Test Set 1, and not sufficient for Test Set 2.

Test Set 2
While memoization is the first instinct to speed up a recursive function, the function described has too large of a domain for that. A bottom-up approach, on the other hand, is much more suitable.

Consider starting from the smallest polygon. This can be done by using a function that is described by a similar expression, but swapping which of p
 and q
 is a parameter and which is iterated over: f(t,q)=1+maxp:q|pf(t−p,p)
. This switch allows for a significant speed up: when checking all possible p
 we can simply jump by q
, dividing the total size of the iteration by q
. This is enough to pass Test Set 2 if implemented carefully, but it may be hard to be confident that it is.

We can do better and improve our confidence with the following observation: once we pick a polygon of size q
, all future polygons will have sizes multiples of q
. Therefore, we can divide the whole problem by q
 (and allow pseudo-polygons of size 2
) by setting f(t,q)=f(t/q,1)
. If we are careful about not allowing size 2
 for the very first polygon, this leaves us having to calculate only g(t)=f(t,1)
, which has a domain of memoizable size. If we add memoization, we have a faster solution, and more importantly, one for which we can pre-compute the entire recursive function and be sure about the running time irrespective of the input.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2021 - Code Jam 2021

Hidden Pancakes (10pts, 21pts)

Test Set 1
For Test Set 1, the number of pancakes is small enough to do something exponential. One possible way would be to go through every possible order of pancakes. That would be too slow, but we can identify some repetition: any hidden pancakes at any point could be hidden in multiple ways, and that does not affect how we continue the process. Removing the cost of calculating these overlapping cases leads to a dynamic programming solution.

We can formalize the idea above as follows: instead of remembering exactly the current stack of pancakes as the current state, we can store one of 3
 states for each pancake: unused, visible, or hidden. The visible pancakes are always stacked in decreasing order of radius, and the position in the stack of hidden pancakes, as argued above, does not affect the process. Thus, we can define a function f
 recursively that takes a state description and returns the number of valid ways to finish a pancake stack from that state. Applying f
 to the state in which all pancakes are visible gives the answer of the problem.

Let's define f(s)
 recursively, where s
 is a state. If no pancakes are unused in s
, then the number of ways to finish it is simply 1
. This is our base case. If there are unused pancakes, we can go through all of them to choose which is the next pancake that should be cooked. If si
 is the state that results from cooking the pancake of radius i
 centimeters when the state was s
, then f(s)=∑if(si)
 where the summation is over pancakes that are unused in s
.

Since each pancake can independently be in one of 3
 states, the domain of f
 has 3N
 overall states. The non-recursive cost of computing f
 is a low-degree polynomial on N
. The exact degree depends on the implementation, but it is not too hard to do it in linear time, yielding an overall algorithm that runs in O(3N⋅N)
.

We can also use backtracking to solve the problem. We start building the permutations one element at a time, checking that the Vi
s for this partitial permutation match the ones needed for this test case. It can be proven that the complexity of such a solution is bounded by O(2NN3)
, which is significantly better than O(N!)
 of the simplest brute force algorithm, and is enough to solve this test set.

Test Set 2
From the input, we can keep track of the current list of visible pancakes. To make things easier, when a pancake gets covered up in the stack, we will view this as the pancake being "removed" from the list of visible pancakes. We can create a list of inequalities relating the sizes of the pancakes. Let Pi
 be the radius of the i
-th pancake added.

First, it's worth noting that each time we add a pancake to the stack, the size of the list of visible pancakes will either increase by 1
, stay the same, or decrease. If the size of the list ever increases by more than one, then this case is impossible so our answer is 0
.

Let's consider the possible scenarios when we are adding the i
-th When the size of the list increases by 1
 (Vi=Vi−1+1
), we know that the new pancake is smaller than all of the pancakes in our list. We can add the inequality Px>Pi
 where x
 is the pancake that was previously at the end of the list.

If the size of the list stays the same or decreases (Vi≤Vi−1
), then we know that the new pancake is larger than the last Vi−Vi−1+1
 pancakes on the list and each of these pancakes gets removed from our list. We can add the inequality Pi>Px
 where x
 is the last pancake we removed from the list. Also, if Vi>1
, then the new pancake is strictly smaller than the rest of the pancakes in the list. In this case, we can add the inequality Py>Pi
 where y
 is the first pancake we did not "remove".

Note that in the latter case, we would already have added an inequality saying that Py>Px
. But, this inequality is redundant now that we have the extra inequalities Py>Pi
 and Pi>Px
. Therefore, we can remove the inequality Py>Px
 to prevent us from having any redundant inequalities that can cause us issues later.

After removing the redundant edges, each pancake is on the right side of at most one inequality in the form of A>B
. Because of this, the inequalities can be modeled as a tree where we have an edge from A
 to B
 if A>B
. We can then solve for the number of valid cooking orders of N
 pancakes recursively starting at the root of the tree which is the largest pancake. Note: the largest pancake is whichever pancake was at the beginning of our list at the end.

Let s(i)
 be the number of pancakes in the subtree rooted at i
. Also, let f(i)
 be equal to the number of valid permutations of the pancake sizes (ranging from 1
 to s(i)
) in the subtree starting at i
.

To solve for f(i)
 we need to count the number of ways we can assign pancake sizes to the subtrees belonging to our direct children. Then, each child subtree can permute itself in f(j)
 ways (where j
 is a direct child of i
). So, f(i)
 is the product of all f(j)
's and the number of ways we can assign pancake sizes to our subtrees.

Since i
 is the largest pancake in our subtree, it must be given the largest size. The other s(i)−1
 pancake sizes for our subtree can be assigned to any subtree. The number of ways to do this can be counted using Multinomial Coefficients.

If we precompute the factorials and inverse factorials (to allow for division under mod), we can count the number of valid cooking orders in O(N)
. Building the tree of inequalities also can be done in linear time. This gives us a final time complexity of O(N)
 (assuming we precompute factorials and inverse factorials in linear time).

An alternate solution
There is a different solution to the problem that requires two observations: (1) the largest pancake (the one with a radius of N
 cm) can only be placed at position k
, where k
 is the largest integer such that Vk=1
 and (2) since the pancake with a radius of N
 covers all other pancakes, the order of the pancakes before the largest pancake does not impact the Vi
s for the pancakes after the largest pancake. This allows us to split the problem into two independent parts. We solve the left part and the right part separately. For the right part, the largest pancake will always be visible, so we subtract 1
 from all Vi
 in the right part to account for it (note we do not actually do the subtraction, because that would be too slow, but we just implicitly do it). We must also choose which pancakes were in the left and right parts (there are (N−1k−1)
 ways of doing this where the largest pancake was at index k
). As base cases: If the range is empty, then there is 1
 valid ordering, and if there is no Vi=1
, then there are 0
 valid orderings. Otherwise, the product of the left part's answer, the right part's answer, and the binomial coefficient is the total number of orderings.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2021 - Code Jam 2021

Retiling (11pts, 23pts)

The first simple observation we need to solve this problem is that we never swap two tiles that are showing the same color.

The main not-that-simple observation is that any given tile should either be flipped or swapped (or neither), but never both, regardless of costs. This can be proved by considering all operations that affect tile a
: if there is a swap of a
 and b
 followed by a flip of a
 or vice versa, the same effect can be achieved for less cost by simply flipping b
 (which may have moved by its own swaps in between).

A small addition to the above is that we never want to flip the same tile more than once. Since each tile is only affected by one type of operation, we can assume that all swaps happen before all flips. Thus, after all swaps are done, the flips are fixed: any tile that has a different color than the target state must be flipped, and the other tiles must not.

Test Set 1
In Test Set 1, we can make the additional observation that no tile is to be swapped twice. Since we should not swap tiles when they are showing the same color, swapping a
 and b
 and then b
 and c
 is equivalent to flipping a
 and c
, which costs the same. Because we swap each tile only once, we must only swap two tiles if the swap fixes both of their colors (the samples already hint at this fact). Since swaps fix colors for two tiles and flips for one tile, we want to do as many swaps as we can within this restrictions, and then flip the rest.

At this point, the problem becomes a classic one: some cells of the matrix need to be changed, and we want to match as many of those as possible with an adjacent cell that also needs to be changed, but to the other color. Since the graph of cells and adjacencies is bipartite, this can be solved with a maximum matching algorithm on a bipartite graph. The solution is then the number of cells that need to change minus the size of such a maximum matching.

Test Set 2
As the additional sample shows, in Test Set 2 it is very possible that we need to swap the same tile multiple times. Luckily, we can generalize the solution explained for Test Set 1 for this scenario.

When a tile participates in multiple swaps, it effectively executes a swap with another tile that is not necessarily adjacent. While the other tiles are displaced by these operations, they are all of the same color, so that displacement has no effect. To put it in a different way, we can look at a swap as moving an M to an adjacent cell, and consecutive swaps are just consecutive moves.

Assume without loss of generality that the number of Ms increases between the starting state and the target state (otherwise, use Gs instead). Since we said we do all swaps first, that means that we move some of the Ms that need to switch to Gs to positions that have Gs that need to be Ms. We can do that by matching those two types of cell (regardless of adjacency). Any unmatched G into M position needs to be flipped (the number of these is fixed by the input).

For each matched pair, the cost of fixing both positions is either the minimum orthogonal distance between the two positions multiplied by S
 or 2F
, since we can also do two flips.

The reasoning above basically built a weighted bipartite matching between cells that start at M and need to be turned into G and cells that start at G and need to be turned into M. The total cost is the cost of the maximum matching of minimum weight plus F
 times the number of unmatched cells. We can apply any minimum cost maximum matching algorithm for bipartite graphs like the Hungarian algorithm.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2021 - Code Jam 2021

Build-A-Pair (3pts, 12pts)

Test Set 1
With the total number of digits up to 8 we can iterate through all possible pairs of numbers we can construct from these digits and choose the optimal one. One way to do this is to iterate through all possible permutations of the given digits and then split every permutation into two in all possible ways as if it were a string. For every such a split we can construct the numbers and calculate the answer. Be careful though, as numbers should not start with 0, every split where 0 is the first digit in a number is not valid.

If we denote the length of D
 as N
, then this takes O(N!×N2)
 time.

Test Set 2
First we can calculate the lengths of each resulting number. These should be as close as possible, which means in case N
 is even, each number must have exactly N/2
 digits, and if N
 is odd, then the larger number must have ⌈N/2⌉
 digits and the smaller number must have ⌊N/2⌋
 digits.

Further we will refer to the larger number as A
 and to the smaller number as B
.

Now, let's deal with the case when N
 is odd. In such a case, we can iterate through all non-zero digits and try them as the first digit of A
 and B
. Now we would want to make A
 as small as possible (as it is already larger than B
), and B
 as large as possible. Luckily, these two goals are complementary: we can use the ⌊N/2⌋
 smallest remaining digits to construct the rest of A
 (just take them in non-decreasing order), and use the rest to construct B
 (by taking them in non-increasing order). Notice that choosing the leading digits greedily is also possible, but care must be taken to not choose zero. This way of simply trying everything only takes O(b2×N)
 time anyway, where b
 is the base (10
), which is fast enough for the limits of the problem.

The case when N
 is even is trickier, as just choosing the first digits does not give us a unique way to construct the rest of A
 and B
, because there is no guarantee that A
 will be larger. However, we can use similar technique as part of the solution.

Note that if we have a prefix of length i
 A1A2…Ai
 of A
 and prefix B1B2…Bi
 of B
, we can guarantee that A
 will be larger than B
 no matter what digits we use further if and only if there is a k≤i
 such that Aj=Bj
 for all 1≤j<k
 and Ak>Bk
. This gives us the following algorithm: choose all possible ways to construct A1A2…Ak−1
 and B1B2…Bk−1
, then iterate through all possible Ak
 and Bk
, then construct the rest of A
 and B
 so that the former is as small as possible and the latter is as large as possible, using the same technique as above. For each of these possibilities, update the answer with the difference between the constructed numbers. Note, as before, that we need to make sure that the numbers we construct do not start with 0.

The time complexity of such an algorithm is O(2N/2)
 for selecting A1A2…Ak−1
 and B1B2…Bk−1
 (these are pairs of equal digits, so we cannot have more than N/2
 of them, and the order among them is irrelevant, as long as we do not start with zero), O(b2)
 for selecting Ak
 and Bk
, and O(N)
 for constructing the numbers once we got the prefixes. Overall this gives us O(2N/2×b2×N
, which is really fast in practice with a good implementation.

The algorithm can be optimized further, but it is not required for the given constraints. There are ways to use the fact that number of digits is limited to reduce 2N/2
 to something with b
 (the base) instead of N
 in the exponent. There is also a way to greedily solve the full problem in linear time. Moreover, if the input and output are given in run-length encoding, it can be solved in linear time in that encoding of the input, which is O(logN)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2021 - Code Jam 2021

Square Free (7pts, 13pts)

Test Set 1
In general, we will build all grids with the correct row and column sums, then check each one to see if there are squares in them. In Test Set 1, the size of the input is at most 6×6
. This means that there are 236
 different possible grids, which is too many to generate, so we must make use of the row sum constraints. We do this by building the grid one cell at a time. As you fill in each cell, ensure that corresponding row and column sums are still possible. For example, if the row sum should be 3, but there are already 3 /s in this row, you cannot put another /. Similarly, if the desired row sum is 3, but there are already R−3
 \s in the row, you cannot put another \. Once we are done filling out the grid, we check if there are any squares in the grid that we have made. If there are no squares, we are done! Otherwise, we move on to the next possible grid. If we finish searching all possible grids and we have not found any, then it is impossible.

Are we sure this is fast enough? Each row either needs 0, 1, 2, 3, 4, 5, or 6 /s in it. There are (60)=1
 choices with 0 /s, (61)=6
 choices with 1 /s, (62)=15
 choices with 2 /s, (63)=20
 choices with 3 /s, (64)=15
 choices with 4 /s, (65)=6
 choices with 5 /s, and (66)=1
 choices with 6 /s. Thus, each row has at most 20 different valid choices. So this algorithm will explore at most 206
 different grid. In practice, we will get nowhere near this bound. In particular, there is at most one valid row for the bottom row (since we must satisfy the column sum constraints). This alone reduces the search space to at most 205
, but this, too, is an overestimate since there will be a lot of pruning throughout the search.

Checking for squares can be done in several ways. The easiest is to iterate over all possible places that the top-most row of the square can be (and which consecutive columns the /\ are in). Then, for each possible size of square (1, 2, 3), just check the corresponding grid entries on the four sides of the square.

Test Set 2
The bounds for Test Set 2 are much too large to exhaustively search all grids, so we will need an insight. We call a grid that satisfies the row and column constraints a configuration. First, let's discuss how to find some configuration (it may or may not be square free). To do this, we will set this up as a graph and run maximum flow on it. There are R
 vertices that represent the rows and C
 vertices that represent the columns. We will put an edge with capacity 1 between every pair of (row, column) vertices. We will then connect the i
⁠-⁠th row vertex to a super-row vertex with capacity Si
 and connect the i
⁠-⁠th column vertex to a super-column vertex with capacity Di
.

We now run flow through the network using the super-row vertex as the source and the super-column vertex as the sink. If the network is saturated (that is, every edge leaving the source has flow = capacity), then we have a solution. If there is flow in the edge between row r
 and column c
, then the corresponding entry in the grid is a /, otherwise it is a \. If the flow is not saturated, then it is impossible to make a grid with the appropriate row and column sums. We leave a formal proof of the bijection between configurations and valid flows on the described network as an exercise.

At this point, we have some configuration, but it may have squares in it. We will discuss three different ways to produce a grid that is square free.

Lexicographically Smallest Configuration
In this solution, we notice that the lexicographically smallest configuration (treating \ as smaller than / reading in row-major order) is square-free! At first this is not obvious. However, think about two rows (ri<rj
) and two columns (ck<cℓ
). If (ri,ck)=/,(ri,cℓ)=∖,(rj,ck)=∖,(rj,cℓ)=/
, then this grid is not the lexicographically smallest configuration. Why? Because we can swap all four of those without breaking the row or column constraints, while giving us a smaller configuration ((ri,ck)=∖,(ri,cℓ)=/,(rj,ck)=/,(rj,cℓ)=∖
). This means that a lexicographically smallest configuration has no squares in the grid, because the top-most row of a square must contain /\ in the same columns in which the bottom-most row of the square has \/.

How do we find the lexicographically smallest configuration? There are two ways: (1) give the edge between row r
 and column c
 a cost of 2r−1+(c−1)R
 and run minimum-cost maximum-flow. This will guaranteed find the lexicographically smallest, but the edge-costs will be huge (up to 2RC−1
). (2) We will run maximum flow iteratively. Say we have run maximum flow. Go through the edges in row-major order of their corresponding cell. If there is no flow going through an edge, then it is a \. Great! This is the smallest this can be, so remove this edge from the graph to lock that in. If there is flow going through the edge, then we "unpush" the flow from that edge (decrease the flow from the sink to the column vertex to the row vertex to the source by 1) and temporarily change that edge's capacity to 0. We then run flow again. If the flow is still saturated, then we know there exists a configuration where this entry in the grid is a \, so we can permanently remove this edge from the graph. If it is not saturated, then we must put that edge back into the graph, and this entry is forced to be a /.

Complexity-wise, the first run of maximum flow takes O((RC)2)
 time. Then, for each flow we run, we must only push one unit of flow through, which only takes a single augmenting path, so O(RC)
 time per edge. Thus, in total, this takes O((RC)2)
 time. Note that you can also fully re-run maximum flow for each edge instead of only pushing one unit of flow with a sufficiently optimized flow implementation.

Minimum-Cost Maximum-Flow
In the Lexicographically Smallest Configuration solution, we described how you can use minimum-cost maximum flow to solve this problem with exponential edge costs. Here, we will solve the problem using only polynomial sized edge costs. This solution makes use of the same idea as the previous one: we want to avoid (ri,ck)=/,(ri,cℓ)=∖,(rj,ck)=∖,(rj,cℓ)=/
, but other than that, we do not need a lexicographically smallest configuration. If we set the edge cost between row r
 and column c
 to be r×c
, then we will not get this configuration, since swapping all of these symbols does not affect the row/column sums and strictly decreases the total cost. The total cost is reduced by i×k+j×ℓ
 and increased by i×ℓ+j×k
, which is a net decrease of (i−j)(k−ℓ)>0
 since i<j
 and k<ℓ
.

Flip-Flop!
Another solution is to simply find some configuration, then check it for squares. If there are no squares, then we are done! If there is a square, consider the top-most and bottom-most row in the square. We will swap the top /\ with the bottom \/. This does not affect the row/column sums. In doing this, we have broken the current square, but may have created another square. We continue breaking squares until we do not find any. This process must eventually finish since at each swap, we are always making our grid lexicographically smaller. You can never do more than O((RC)2)
 swaps of this form.

Common Mistake
Be careful! Just because the sum of the Si
⁠s is equal to the sum of the Di
⁠s does not mean that a configuration exists! One example is the following input, where those sums coincide, yet there are no grids that meet all the per-row and per-column requirements.

4 6
2 0 6 6
4 2 2 2 2 2
Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2021 - Code Jam 2021

Fence Design (11pts, 19pts)

This problem asks us to find a triangulation of the given set of points that uses two particular edges. Many of the arguments that follow are referenced in the linked article, so you can use it as an introduction.

The first step to solve this problem is to notice some properties of the finished job. Let P
 be the input set of poles and F
 be a maximum set of fences with endpoints in P
 that do not intersect other than at endpoints.

1. The fences that are the edges of the convex hull of P
 are in F
.
Proof: By definition, the edges of the convex hull do not intersect with any other possible fence. Therefore, they can be added to any set of fences that do not contain them without generating any invalid intersections, so any maximum set contains them all.

2. The size of F
 is at most 3N−3−c
 where c
 is the size of the convex hull of P
.
Proof: Consider the graph where poles are nodes and fences in F
 are the edges. The graph is planar, so the generalized form of Euler's formula applies. The formula states that N+A=K+|F|
, where A
 is the number of internal areas (not counting the outside) and K
 the number of connected components of the graph.

Notice that each edge on the convex hull is adjacent to exactly one internal area, and other edges are adjacent to at most two internal areas. Therefore, the sum of the number of sides of all internal areas is at least 3A
 and that is counting each convex hull edge once and each other edge at most twice, so 3A≤c+2(|F|−c)=2|F|−c
, which means A≤(2|F|−c)/3
. Replacing that in Euler's formula we obtain N+(2|F|−c)/3≥K+|F|
. It follows that 3N+2|F|−c≥3K+3|F|
, and then 3N−c−3K≥|F|
. Since K≥1
, we obtain |F|≤3N−3−c
.

3. The size of F
 is at least 3N−3−c
.
Proof: By induction. The base case is when all points in P
 are on the convex hull. For that case, consider a set of all edges in the convex hull of P
 plus any triangulation of P
. This set has exactly 2N−3
 edges, which is equal to 3N−3−c
 when N=c
, proving that there exists a set of at least that size without any invalid intersections.

If there exists a point p
 not on the convex hull of P
, consider an optimal set of edges for P−p
, which by inductive hypotheses has size 3(N−1)−3−c
. Because |F|=3(N−1)−c−3
, and 3(N−1)−c−3K≥|F|
 from (2), K
, the number of connected components, must be exactly one. Similarly, every internal area must be a triangle, with all edges in the convex hull being adjacent to one of them and all edges not in the convex hull being adjacent to two of them. By definition, p
 is not on the convex hull. By the fact that there are no collinear triples, p
 is not in an existing edge. Thus, p
 is strictly contained in one of these triangles. Therefore, we can add edges from p
 to each of the 3
 vertices of the triangle that contains p
 to get a solution for our problem of size 3(N−1)−3−c+3=3N−3−c
.

From (2) and (3) above we know the exact number of fences we need to build (given the size of the convex hull of the set of poles). Moreover, we know that such an answer contains the convex hull of P
 and every internal area delimited by fences in the output is a triangle. We can use this to devise algorithms to generate optimal sets.

Test Set 1
There are many possible solutions for Test Set 1. For example, the procedure in the proof of point (3) above shows how to solve the problem with no pre-placed fences. There are ad-hoc ways to get around the problems with pre-placed fences, but they take a lot of work, and there is something simpler.

The proofs above show that any maximal set of fences is also maximum (notice that we only used maximality in our reasoning). Therefore, we can simply add fences to a set as long as they do not intersect with any previously added fence. This algorithm can accommodate pre-placed fences quite easily: simply start with them. There are O(N2)
 potential fences to consider, and for each one we need to check whether it intersects any of the fences we already have. Since the solution overall is of size O(N)
 this means checking O(N3)
 intersections. Checking a pair of line segments to see if they intersect can be done in constant time, which means this algorithm takes O(N3)
 time overall.

Test Set 2
As in Test Set 1, there are lots of algorithms that solve this problem without considering the pre-placed fences, but only some of them are easy enough to adapt to them. For example, the procedure from the proof of (3) can be implemented efficiently: if the order in which we process points is randomized and we keep the current triangles in a tree-like structure to perform the search for a triangle efficiently, we can get an expected O(NlogN)
 time complexity. This leads to an algorithm similar to the incremental algorithm to compute a Delauney triangulation.

Another option is to modify the Graham Scan algorithm to efficiently find the convex hull to keep not only a convex hull of the visited points, but also all the triangles for the points inside it.

Unfortunately, while the algorithms above can work with a lot of ad-hoc code to accommodate the pre-placed fences, they become really cumbersome. Below we present some better alternatives.

Let x
 be the intersection of both lines that are the infinite extension of a pre-placed fence. Since the fences don't intersect, x
 can occur on one of them, but not on both. Let us call any pre-placed fence that does not contain x
 f1
, and the other fence f2
. By their definitions, all of f2
 is on the same side of the line that extends f1
. We can recognize which fence can be f1
 by comparing whether the orientation of the two endpoints of a potential f1
 and each endpoint of f2
 is the same (that is, checking whether both endpoints of the candidate f2
 lie on the same side of the line that goes through f1
).

Sweep-line
We can solve the problem without pre-placed fences with a sweep-line that considers the points in order of x-coordinate and maintains a convex hull of all the already seen points, as in the monotone chain convex hull algorithm. When considering a new point p
, we simply connect it to all points from the already-seen set that do not cause intersections from p
. Notice that those points are a continuous range of the right side of the convex hull of the already-seen set. Therefore, we can find those points efficiently with ternary searches on its right side.

To accommodate pre-placed fences, we first rotate the plane to make f2
 vertical (possibly scaling everything to use only integers) and then run the sweep line algorithm only on points that are on the same side of the line that goes through f1
 as f2
 (including both endpoints of f2
 and neither endpoint of f1
). Since f2
 is now vertical, it will be included by the algorithm in the set when its second endpoint is processed. After that, we rotate everything again to make f1
 vertical, and start the algorithm from the set and convex hull we already have (the endpoints of f1
 will be the first two points that are processed in this second pass). As before, f1
 will be added naturally by the algorithm.

Notice the accommodation of the pre-placed fences only requires linear time work, so the overall algorithm, just as the version without pre-placed fences, requires O(NlogN)
 time overall.

Divide and conquer
This divide and conquer algorithm also has a correlate to computing the convex hull. The idea is simple: divide the set of points with a line that goes through 2
 points, compute the result of each side (both of which include those 2
 points), and then combine.

Let us call the two recursive results P
 and Q
. The convex hulls of both are convex polygons with a shared side. Then, we keep two current poles p
 and q
. Initially, both p
 and q
 are on the same endpoint of the shared side. Both move away from that shared side: p
 through consecutive vertices of P
 and q
 through consecutive vertices of Q
. Initially we move them both together. After that, let p0
 and q0
 be the previous pole where p
 and q
 were, respectively, and p1
 and q1
 be the next value for each (that is, p0p
 and pp1
 are adjacent sides of the convex hull of P
 and q0q
 and qq1
 are adjacent sides of the convex hull of Q
). Then,

If p1q
 does not intersect p0p
, we set p=p1
.
If q1p
 does not intersect q0q
, we set q=q1
.
Otherwise, we stop.
Each time we move one, we add the fence connecting the current p
 and q
 to the result. When we are done, we do the same starting from the other endpoint of the shared side.
To divide evenly, we can pick any point x
, sort the other points by angle, and pick the median as y
, dividing by the line that goes through x
 and y
. Alternatively, we can pick points randomly. On average, the split of points would be about even (as we did in the proposed solution for Juggle Struggle: Part 1).

The work done to combine takes linear time and the work done to split takes either O(NlogN)
 time for the sorting version or linear time for the randomized version. Using the Master theorem we can then see that using the randomized version, the overall algorithm takes O(NlogN)
 time, and using the sorting version, the overall running time is O(Nlog2N)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2021 - Code Jam 2021

Binary Search Game (9pts, 26pts)

Test Set 1
Let f(k)
 be the number of ways in which the game ends with a score ≥k
. Then, the number of ways in which the game ends with a score of exactly k
 is f(k)−f(k+1)
. Therefore, the answer is
∑k=1Mk(f(k)−f(k+1))=∑k=1Mkf(k)−∑k=1Mkf(k+1)=∑k=1Mkf(k)−∑k=2M+1(k−1)f(k)=f(1)−Mf(M+1)+∑k=2M(k−(k−1))f(k)=∑k=1Mf(k).
Notice in the last step that f(M+1)=0
 by definition.

Let us now focus on calculating f(k)
. For each card, we only care about whether its value is ≥k
 or not. Let S
 be a fixed subset of cards which have value ≥k
 (we will go through all 2N
 such subsets). We can assign a score of 0
 (for cards outside of S
) or 1
 (for cards in S
) to each consecutive subsequence of cells on the board that can be presented to players, in increasing order of length. Notice that only subsequences of cells whose length is a power of 2
 and who are "aligned" can be presented to a player. That is, sequences containing cells of the form i2j+1,i2j+2,…,i2j+2j
 for each j
 between 0
 and L
, inclusive, and each i
 between 0
 and 2L−j−1
, inclusive. There are 2L+2L−1+⋯+2+1=2L+1−1
 valid combinations to go through. If we know the score for sequences of shorter lengths, we can decide the score for a sequence by checking whose turn it is (it is Alice's turn if and only if j
 and L
 have the same parity) and picking either the maximum or the minimum score of its two halves. If the score assigned by this to the full sequence is 0
, then there is no way to finish with a score ≥k
 with combination S
. Otherwise, we need to compute the number of ways in which this can happen. This is simply the number of original card values that yield S
, which is (M−k+1)t(k−1)N−t
 where t
 is the number of cards that are ≥k
 in S
.

Overall, this solution takes O(M×2N×2L)
 time, which is fast enough to pass Test Set 1.

Test Set 2
Note that the expression (M−k+1)t(k−1)N−t
 from the previous solution, if we consider k
 the only variable, expands into a polynomial of degree at most N
. Consequently f(k)
 is also a polynomial in k
 of degree at most N
 (as it is the sum of such polynomials).

If f(k)
 is a polynomial of degree at most N
, then g(x)=∑xk=1f(k)
 is a polynomial in x
 of degree at most N+1
. This is a consequence of Faulhaber's formula.

Thus, the polynomial g(x)
 can be fully determined by evaluating it at N+2
 different points, say g(0),g(1),…,g(N+1)
. We can evaluate those values in the same way as Test Set 1 and then use interpolation to find g(M)
.

This yields an algorithm that takes time O(N×2N×2L)
 to evaluate N+2
 points of f(k)
, plus the time complexity of evaluating g(M)
 using interpolation. The latter can be done by evaluating the Lagrange polynomial in a straightforward way in O(N2)
, and there are faster methods too, but this does not impact the overall runtime.

To speed up the calculation of g(0),g(1),…,g(N+1)
, we find a way to reduce the 2N
 factor. Note that if a card's index appears only once (or not at all) in the board, we can do without fixing its value in advance (as being ≥k
 or not). We can fix the values that are repeated in the board and then run the greedy algorithm but instead of scores 0
 and 1
 we compute, for each subsequence, the number of ways that it can be made 1
 by assigning the values of cards whose indices are not repeated in the board. The decisions can still be made greedily, but instead of doing all the combinatorics at the end, we do them at each decision point.

With this optimization, we only need to run the greedy algorithm 2X
 times per each value of g
 that we need, where X
 is the number of cards whose indices appear more than once in the board. There can be at most 2L−1
 of those cards, so we have reduced the time complexity of the first step to O(N×22L−1×2L)
. This is finally enough to pass Test Set 2.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2021 - Code Jam 2021

Cutting Cake (20pts)

View problem and solution walkthrough video
A Single Icing Patch
Let's first consider a case where we have only one icing patch with a vertical edge on the right having enjoyment values A
 and B
. If our vertical cut line is all the way to the left of the triangle, we would get 0
 enjoyment and our twin would get B
 enjoyment. If our cut line is to the right of the triangle, we would get A
 enjoyment and our twin would get 0
 enjoyment.

Cake with one icing patch and cut line going through the middle of it.

Otherwise, our cut is somewhere in the middle of the triangle. In this case, we would get A⋅P(X)
 enjoyment and our twin would get B⋅(1−P(X))
 enjoyment where P(X)
 is a value between 0 and 1 representing the proportion of the triangle that is to the left of a cut line at position X
.

We can calculate P(X)
 by considering similar triangles. Let TX
 be the triangle that is to the left of our cut line when cutting at X
. We can notice that TX
 and our full triangle are similar triangles. This means that the ratio between the width and height of TX
 is the same as the ratio for the full triangle. So, given a cut location X
, the width and height of TX
 are both X−leftwidth
 of the full triangle's width and height respectively. Therefore, we can calculate P(X)
 using the following formula:

P(X)=(X−leftwidth)2
Notice that P(X)
 is a quadratic (polynomial of degree 2). With this, we can write the formulas for our enjoyment and our twin's enjoyment as polynomials in terms of X
:

Our enjoyment=A⋅(X−leftwidth)2
Twin's enjoyment=B⋅(1−(X−leftwidth)2)
The value we care about is the difference between these enjoyments. So, we can take the difference between enjoyments and solve for the minimum absolute value within a given range of our cut X-coordinate (from the left of the triangle to the right of the triangle). We can do this by checking the following:

The value at the extreme point of the quadratic, if that's within the interval.
The values at each endpoint of the interval.
Whether the quadratic crosses 0 within the interval.
We can know whether the function crosses 0
 by checking whether the sign at the two endpoints is different. In case the extreme value is within the range, we need to check whether that sign is different from endpoints as well.

Note that our triangle mold might not have a vertical edge. In this case, we can split the triangle in two parts and solve for the left and right sides separately.

Triangle without vertical edge cut in half so each half has a vertical edge.

The formula for the right side is slightly different but very similar to that used for the left. So, we can calculate the proportion, P(X)
, for a triangle with a vertical edge on the left as follows:

P(X)=1−(left+width−Xwidth)2
Many Icing Patches
For the full problem, we have many icing patches. If we look at all unique X-coordinates of the vertices representing the triangular icing patches, we can notice that if we cut somewhere between two adjacent X-coordinates, each triangle is either always:

Entirely to the left of our cut line (contributing to our enjoyment).
Entirely to the right of our cut line (contributing to our twin's enjoyment).
Cut in two pieces by the cut line (contributing to both our and our twin's enjoyment).
A cake with three icing patches and vertical dotted lines showing all unique X-coordinates.

Therefore, for every pair of adjacent X-coordinates, we can add the constant enjoyments from the triangles that are always either to the left or to the right of the cut line to the quadratic functions from the triangles that are being cut. This means that the function D(X)
 that represents the difference between enjoyments if we cut at X
 is a piece-wise polynomial of degree no more than two. Between every pair of adjacent X-coordinates, we can solve for the minimum absolute value of the difference between our and our twin's enjoyment. Our final answer is then the minimum across all X-coordinate ranges.

Using a sweep-line technique, we can maintain the polynomials representing our and our twin's enjoyments. Because we need to sort the points by X-coordinate, our overall solution requires O(NlogN)
 operations for sorting and O(N)
 operations on fractions. Keep in mind that the numbers for the numerator and denominator might not fit into 64 bit integers. Notice that the size of those numbers grows logarithmically in the size of the input, so O(NlogN)
 operations is a reasonable approximation of the overall time complexity of the algorithm.

Some behind the scenes trivia
Because of the O(NlogN)
 complexity mentioned in the previous paragraph, this problem started out as the same statement but intended to be solved using floating point. However, as illustrated by the fact that it needs fractions with integers that do not fit in 128 bits, the precision was a big problem. The amount of restriction all variables needed to keep the precision issues workable would have made a solution that simply iterates through all triangles for every range and does a ternary search usable. Such a solution can be guessed without understanding the polynomials we explained. Using fractions was a way to require solutions to understand those polynomials. Setting a small value for N
 allows using unbounded integers without worrying about the additional running time. Making all the triangles the same shape also helps with this: it keeps the size of the fractions from getting too big.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2021 - Code Jam 2021

Slide Circuits (10pts, 20pts)

View problem and solution walkthrough video
To simplify the notation, we can reframe this problem in terms of graphs. We represent the input with a graph G
 that has one node per building and one directed edge per slide. The enabled/disabled states we can represent by subgraphs G1,G2,...,GN
 where Gi
 is a subgraph of G
 containing only the edges representing slides that are enabled after the first i
 operations.

A graph is fun if every node belongs to exactly one cycle. The question is, for each i
, to identify an edge (v,w)∈G−Gi
 such that Gi∪{(v,w)}
 is fun.

Test Set 1
The first step to solve the problem is to describe fun graphs more directly: A graph is fun if the in-degree and out-degree of every node is 1
. Therefore, the edge (v,w)
 that we need in step i
 must be such that the out-degree of v
 in Gi
 is 0
, the in-degree of w
 in Gi
 is 0
 and all other in-degrees and out-degrees in Gi
 are 1
. This implies that, given Gi
, we can simply check the degrees and find the only possible candidate for v
 and w
, if any. If (v,w)∈G
, we found a answer. If there is no candidate for either, or (v,w)∉G
, then there is no answer for step i
.

In Test Set 1, we can maintain the in-degree and out-degree of each node in the current Gi
. When there is an enable operation, for each affected edge (v,w)
 we need to increase the out-degree of v
 and the in-degree of w
 by 1
. For disable operations, we do the same but decreasing by 1
. Then, we can do a linear pass to find candidates v
 and w
. If there are unique candidates for both, we check if (v,w)
 is in G
 and give the appropriate output. This can be done in linear time in the size of the graph per step, which is O(B+S)
 time, or O(N(B+S))
 time overall, which is fast enough to pass Test Set 1.

Test Set 2
Our solution for Test Set 2 is an optimized version of the solution presented for Test Set 1. Consider multisets of nodes Ii
 and Oi
. The number of occurrences of v
 in Ii
 is equal to the out-degree of v
 in Gi
, and the number of occurrences of v
 in Oi
 is equal to the out-degree of v
 in Gi
. Let I′e
 and O′e
 be the way the multisets Ii
 and Oi
 should look for e
 to be the answer after step i
.

If we keep hashes of the Ii
 and Oi
 that are efficient to update and check a dictionary from the pairs of hashes of (I′e,O′e)→e
, we can solve the problem. There are many options that work with different trade-offs in reliability, ease of implementation and ease of proof.

Sum of random values
Let us start by assigning a random integer xv
 to each vertex v
, that we keep throughout a test case. The hash of a multiset in this case is the sum of the values over all the vertices it contains (if it contains a vertex multiple times, its value is summed that many times), modulo some large number. For extra randomness we could use separate values for the I
 and O
 hashes.

Let t=∑vxv
 be the sum of all those random values. Then, the hash of I′(v,w)
 is simply t−xw
 and the hash of O′(v,w)
 is t−xw
. To get the hashes of I′i+1
 and O′i+1
 we can add or subtract to the hashes of I′i
 and O′i
 (for simplicity, I′0=O′0=0
). The amount to add or subtract is the sum of the values of the starting/ending points of all edges that the operation is changing. We can find that efficiently by building two arrays (one for starting points and one for ending points) of sums over the first i
 multiples of M
 for each i
 and each M
. Then, the sum between the i
-th and j
-th multiples of M
 is just the difference between the values for M,j
 and M,i−1
. The array for a specific value of M
 contains ⌊S/M⌋
 values, and ∑M≤S⌊S/M⌋≤∑M≤SS/M=S∑M≤S1/M=O(SlogS)
, so this is efficient enough.

It has hard to prove formally that the sums work well as hashes. We present next a closely related variant for which is much easier to be convinced that the probability of collisions is really small.

XOR of random values
The idea in this case is to use XOR instead of sum. The whole implementation can be done in the same way as in the previous case. However, because XOR is its own inverse, two multisets with the same parity in the number of occurrences for all vertices have the same hash. We can solve this by adding a count of the number of edges in Gi
. This can still cause collisions, but since all numbers of occurrences in I′
 and O′
 are either 0
 or 1
, having both the correct parities and the correct total guarantees we are looking at the same multiset. This does require maintaining that total number of edges update, but that can be done in constant time per step.

In the case of XORs, each bit in the result is independent from all other bits. Since the values xv
 are randomized, the probability of the bit being equal in two multisets with different parities is 1/2
. Therefore, the probability of 64
 bits coinciding by chance is 2−64
, which is vanishingly small.

Polynomial hashes
Finally, in the XOR version above, we are actually hashing sets of nodes depending on the parity of their in- or out-degree, in a way. We know polynomial hashes are good for hashing sets, so we can simply use one of them. This is a third way of solving the problem.

Notice that a polynomial hash is actually equivalent to the sum version except the xv
 values are powers of a prime instead of randomly chosen. Random choices are more resilient to adversary data and provide similar properties of uniformity of distribution. This is an informal argument that justifies the sum of random values being a good hashing for this problem.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2021 - Code Jam 2021

Ropes (15pts, 10pts, 15pts)

View problem and solution walkthrough video
This problem can be solved in many different ways! Below is a collection of different solutions we came across. If yours is not here, we would love to hear it! Head over to our Code Jam group and share your favorite idea.

For consistency, here are a few terms we will use throughout:

"Playing (x,y)
" means connecting tree x
 on the North to tree y
 on the South.
"Playing greedily" means looking at all possible options and choosing a move that maximizes your score on this turn. Sometimes we will choose randomly, sometimes we will pick a specific one of these. Note that the judge plays greedily (and chooses their move randomly).
The percentages given below are approximate (found via experimentation) and rounded to the closest integer value.

Playing Greedily
... by picking randomly (28%
)

We just pick our move randomly like the opponent is going to. This is not a great strategy!

... by picking non-randomly (55%
)

This strategy works surprisingly well for how simple it is. Simply play the same way as your opponent, except instead of picking randomly, choose the one that comes first lexicographically—that is, choose (x,y)
 that minimizes x
, then minimizes y
 if there is a tie. To understand why it works well, read the "Push Them East!" strategy, as they work for similar reasons.

... by picking non-randomly and starting in a better place

This the same as the previous strategy, but instead of starting at (1,1)
, we pick some other spot. The result varies depending on the exact starting spot, but (1,3)
 already gets us 75%
 probability of winning. Read below to understand why this has such a big impact in the overall probability.

Mirroring
First attempt (64%
)

This strategy is based on a simple idea: if the opposing team plays (x,y)
, then we will play (y,x)
. Why is this a good idea? Because if they just played (x,y)
 and scored Δ
 points, then we will score Δ+1
 points on our turn because we will also cross their newly placed rope as well as all other ropes the opponent just crossed.

There are two issues, though. The first issue is that we are going first, so we cannot mirror them. We will solve this by playing (1,1)
 as our first move. This move does not impact the remainder of the game as no other rope can cross this one. The second issue is what to do when they play (x,x)
? In this case, we will just play the highest scoring (z,z)
 that is available.

Now with a better start (82%
)

This strategy is the same as the previous section, except our first move is (10,10)
 instead of (1,1)
. Why is this better? Think about when we did well in the previous version. If x≠y
, then we gain one point on them. The only time they can gain on us is if x=y
 (or it is their last turn). By playing (10,10)
, we are encouraging them to play diagonal moves early on in the game, which gains us a reasonably good lead. 10 was found experimentally to be the best value. Starting with (2,2)
 gives 72%
 and starting with (25,25)
 gives 64%
.

Push It East!
First attempt (50%
)

It will help to think of this in three phases: Phase 1
 sets up the ropes in a reasonable way (and hopefully gains us some points). Phase 2
 plays greedily (and hopefully maintains the lead from Phase 1
). Phase 3
 is the final turn for our opponent (hopefully their score from this turn is small).

This strategy is called "Push It East!" because our main goal is to make Phase 3
 as bad as possible for our opponent, and how we do that is ensure that their final move is as far East as possible. If their final move is (x,y)
, then the maximum they can possibly score is (100−x)+(100−y)
. In particular, if we can push them all the way to the East, they will score 0.

For our first attempt, Phase 1
 will simply "give up" our first turn by playing (1,1)
. We enter Phase 2
 now (note that we are currently tied). Phase 2
 will play greedily, breaking ties by taking more western trees as often as possible (say by choosing (x,y)
 that minimizes x+y
). With this setup, Phase 3
 will typically take a tree that is quite far East. (Yay!)

A better start (73%
)

If we start Phase 1
 with (2,2)
, then the opponents will play (1,x)
 (or (x,1)
), scoring one point. We then play (1,3)
 (or (3,1)
), scoring two points. At this point, we enter Phase 2
 ahead by one point and the opponent can only score at most one point on the next turn. This dramatically improves our win probability.

A great start (92%
)

Let z
 be an integer near 10 (say 9, 10, or 11). Let's start Phase 1
 with (z,z)
. For the next few moves, the opponent's moves will typically be (x,y)
, where one of x
 and y
 is "small" (<z
) and the other is "large" (>z
). We can continue encouraging the opponent to make these "small"/"large" moves by choosing moves that are "short" ((x,y)
 or (y,x)
 where x<10
, but as large as possible, and y>10
, but as small as possible). We will continue making "short" moves which further encourages the opponent making "small"/"large" moves.

Say z=10
. After we have made 10
 moves (and the opponent has made 9
), we should now be ahead by 5
 to 10
 points (since we score approximately 0+2+⋯+18
 and they score approximately 1+3+5+⋯+17
). Phase 2
 contains 90 greedy moves for each of us where we typically maintain the 5−10
 point lead. Then, in the opponent's final turn, we hope they score fewer than 5 points since we have pushed them so far East.

Notice that just choosing every move (except the first) to be (x,y)
 that first maximizes the score, then minimizes x+y
 as above also works quite well because maximum score moves will usually cross our (z,z)
 rope. This gets win percentages around 92%
 as well.

World Finals 2021 - Code Jam 2021

Divisible Divisions (10pts, 35pts)

View problem and solution walkthrough video
Test Set 1
In Test Set 1, S is reasonably small. This allows us to compute the answer for every prefix of the input using dynamic programming. We compute two values for every prefix ending at index i: Ai and Bi, which are the number of divisible divisions of this prefix whose last division is divisible by D (Type A) and the number of divisible divisions of this prefix whose last division is not divisible by D (Type B). The empty prefix has corresponding values A0=1 and B0=0. The solution to the problem is Alength(S)+Blength(S).

Let S[i..j] be the integer represented by the digits in S from index i to index j, inclusive. If i>j, then this is an empty substring and the corresponding value is 0 (this only happens with j=0 below). To compute Ak, we will iterate over all possible last divisions (S[1..k],S[2..k],…,S[(k−1)..k],S[k..k]). For each one of these substrings (say S[i..k]), we check if it is divisible by D, and if it is, we can append this division to any divisible division that ends at index i−1. Since the division we constructed is divisible by D, it does not matter if the divisible division we are appending to ends in a division that is divisible by D. The formulas below use modular arithmetic notation. Thus,
Ak= 
∑
1≤i≤kS[i..k]≡0(modD)
 (Ai−1+Bi−1).
For convenience below, we slightly rewrite this equation as:
Ak= 
∑
0≤i≤k−1S[(i+1)..k]≡0(modD)
 (Ai+Bi).

We can compute Bk similarly. We iterate over all possible last divisions. For each one of these substrings that is not divisible by D, we can append it to any divisible division that does end with a division that is divisible by D. Thus,
Bk= 
∑
0≤i≤k−1S[(i+1)..k]≢0(modD)
 Ai.

Since |S| is small, we can simply check all possible values. Some care is needed when determining whether S[i..k] is divisible by D. Using the fact that S[i..k]=10k−i⋅S[i]+S[(i+1)..k], we can avoid fully recomputing the value of S[i..k] by keeping this rolling value (as well as maintaining the current power of 10).

For each k, computing Ak and Bk requires a linear sweep through all smaller indices, so this solution takes O(|S|2) time.

Test Set 2
When D and 10 are relatively prime
In Test Set 2, S is too large to use the dynamic programming solution explained above. However, we will make use of the same foundation for our solution: for each index, compute Ak and Bk.

Rather than sweeping through all smaller indices to check which prefixes are divisible by D and which are not, we will instead use the following observation: S[(i+1)..k]=S[1..k]−S[1..i]⋅10k−i. To find all i such that S[(i+1)..k]≡0(modD) (which is needed in the formula above for both Ak and Bk), we instead search for all 0≤i≤k−1 such that S[1..i]⋅10k−i≡S[1..k](modD). This allows us to group all terms in the summation by the value of v≡S[1..i](modD):
A
(k)
v
= 
∑
0≤i≤k−1S[1..i]⋅10k−i≡v(modD)
 Ai,
and similar for B
(k)
v
.

With this framework, we can re-write our formula above for Ak and Bk:
Ak=(A
(k)
S[1..k]
)+(B
(k)
S[1..k]
)andBk= 
∑
v≠S[1..k]
 A
(k)
i
=( 
∑
0≤v<D
 A
(k)
v
)−A
(k)
S[1..k]

The only piece of the puzzle left is determining how to compute A
(k)
v
 and B
(k)
v
 quickly. Intuitively, to move from A
(k)
v
 to A
(k+1)
v
, we must do two things: (1) multiply every index v by 10 modulo D, then (2) apply our knowledge of Ak and Bk. In the equations below, we need to make use of the multiplicative inverse of 10 modulo D, 10−1 (this is why we need D and 10 to be relatively prime). Mathematically, we can write A
(k+1)
v
 as follows (proofs of these are at the very bottom): if S[1..k]≡v(modD), then
A
(k+1)
v
=A
(k)
10−1v
+Ak
and if S[1..k]≢v(modD), then
A
(k+1)
v
=A
(k)
10−1v
.

If we wish to store A
(k+1)
v
 as an array, we could naively loop through every index v of A
(k)
v
 (but that would be much too slow). Instead, we do the multiplication implicitly. If an index is v in A(k), then it is at index 10v(modD) in A(k+1). This means that after applying (1) above, to compute the value of A
(k+1)
v
, we can instead examine A
(k)
10−1⋅v
. We can recursively apply this logic and store just one array: A(1) and look at the appropriate index:
A
(k+1)
i
=A
(1)
10−ki
.
In the case where S[1..k]≡v(modD), we accomplish (2) similarly by increasing A
(1)
10−k⋅S[1..k]
 by Ak (and similar for B).

In total, we can keep track of all of these operations in O(|S|) time and O(D) memory. We do need one extra variable to store (∑0≤v<DA
(k)
v
) for the computation of Bk, but this is easy to maintain in O(1) time per k.

Chinese Remainder Theorem to the rescue!
The above algorithm works because D and 10 were assumed to be relatively prime. This was needed for 10−1 to exist in all cases. But what do we do when they are not? We write D=2ℓ5mn, where gcd(n,10)=1. Instead of checking that S[i..k]≡0(modD), we will instead break it up into three separate (simultaneous) checks: S[i..k]≡0(mod2ℓ), S[i..k]≡0(mod5m), and S[i..k]≡0(modn). By the Chinese Remainder Theorem, all three of these are true if and only if S[i..k]≡0(modD).

Recall that we are searching for all i such that S[1..i]⋅10k−i≡S[1..k](modD). The key observation needed is that if k−i≥ℓ, then S[1..i]⋅10k−i≡0(mod2ℓ). Similarly, if k−i≥m, then S[1..i]⋅10k−i≡0(mod5m). This means that a substring of S (say S[i..k]) that is longer than max(ℓ,m) can only contribute to A if S[1..k]≡0(mod2ℓ) and S[1..k]≡0(mod5m).

This leads us to our solution. For each k, we will compute Ak and Bk by breaking into two cases: the "small" substrings and "large" substrings. For "small" substrings (that is, substrings of length at most max(ℓ,m)), we use our algorithm from Test Set 1, looping through all small substrings naively.

For the "large" substrings, we know that if S[1..k]≢0(mod2ℓ) or S[1..k]≢0(mod5m), then no large substrings are divisible by D. So the large part of Ak=0 and the large part of Bk=∑0≤v<DA
(k)
v
. If, however, S[1..k]≡0(mod2ℓ) and S[1..k]≡0(mod5m), then we can use the technique described above (with n instead of D). One small modification is needed: do not add in our knowledge of Ak and Bk into A or B until they are out of range of the "small" substrings or else the "small" substrings will be counted multiple times.

Computing the "large" substrings takes linear time (as explained in the section above). To compute the "small" substrings, we must naively loop over max(ℓ,m) elements. Note that max(ℓ,m)≤log2D. This means that, in total, we do O(|S|logD) operations.

Proofs
If S[1..k]≡v(modD), then:
A
(k+1)
v
=	∑0≤i≤kS[1..i]⋅10k+1−i≡v(modD)Ai
 	=	(∑0≤i≤k−1S[1..i]⋅10k+1−i≡v(modD)Ai)+Ak
 	=	(∑0≤i≤k−1S[1..i]⋅10k−i≡10−1v(modD)Ai)+Ak
 	=	A
(k)
10−1v
+Ak
 
Otherwise, if S[1..k]≢v(modD), then:
A
(k+1)
v
=	∑0≤i≤kS[1..i]⋅10k+1−i≡v(modD)Ai
 	=	(∑0≤i≤k−1S[1..i]⋅10k+1−i≡v(modD)Ai)
 	=	(∑0≤i≤k−1S[1..i]⋅10k−i≡10−1v(modD)Ai)
 	=	A
(k)
10−1v
 

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2021 - Code Jam 2021

Infinitree (25pts, 40pts)

View problem and solution walkthrough video
The first thing to do to solve this problem is seeing it as a graph theoretical problem. We can consider the directed graph G
 where each node represents a color and there is an edge c1→c2
 if Lc1=c2
 or Rc1=c2
 (if Lc1=Rc1=c2
, then there are two edges c1→c2
). Nodes at level K
 in the tree correspond to the last node of paths in G
 of length K
 starting at the black node (the full paths correspond to the branches that go into those nodes). Therefore, if M
 is the adjacency matrix of G
, MKi,j
 is the number of descendants of color j
 that are K
 levels below a given node of color i
. In particular, ∑jMKblack,j
 is the number of nodes at level K
, and we can sum that over all K≤K′
 to get the number of nodes at levels up to K′
.

Matrix powers
We can calculate MK
 with only O(logK)
 matrix multiplications. Moreover, we need only values K≤B
, and since we will do this for the same M
 and lots of different values of K
, we can memoize M2p
 for each integer p≤log2B)
 by starting with M1=M20
 and then using M2p+1=(M2p)2
 to calculate each subsequent power. This requires O(logB)
 matrix multiplications to initialize, or O(N3logB)
 time. Since K
 can be expressed uniquely as a sum of powers of 2
, we can calculate MK
 as the product of those powers. While this still requires the same worst-case time, it is much faster in practice. More importantly, if we want to calculate VMK
 for a vector V
 we can do that by multiplying V
 by each matrix, requiring only O(logB)
 vector-matrix multiplications, which is O(N2logB)
 time.

Summation of powers of matrices
Let x
 be a tree node of color i
. We can calculate the number of descendants of x
 up to level K′
 as ∑K≤K′∑jMKi,j=∑j(∑K≤K′MK)i,j
. We can calculate ∑K≤K′MK
 efficiently with a divide and conquer approach, using M0+M1+M2+⋯+M2K−1=(I+MK)(M0+M1+⋯+MK−1),
 where I
 is the identity matrix. This would require O(logK′)
 matrix powers, or O(log2K′)
 matrix multiplications. By overlapping the divide and conquer needed to calculate the matrix powers and the summation, we can get that down to O(logK′)
 overall.

Notice that the values in the matrices can be really big. However, any value above B
 is equivalent for us, so we can do all math by capping the results at anything larger than B
. This allows to implement this with regular 64 bit integers and without adding a large-integer arithmetic factor to the overall time complexity.

Test Set 1
With the setup done in the previous paragraphs, solving Test Set 1 is straightforward. Since node A
 is always the root in this case, the answer is the level of node B
. We can get B
's level by using bisection on the function "number of nodes up to a certain level". We saw that we can implement that function efficiently above with O(logB′)
 matrix multiplications, making this algorithm run in O(N3log2B)
. Since the divide and conquer of the bisection and the matrix multiplication can be overlapped, this can be reduced to O(N3logB)
 overall, which allows for slower implementations/languages.

Test Set 2
The solution for Test Set 2 uses the same framing and graph theory as the solution for Test Set 1, but it requires a lot of additional work.

First, we will define a new naming convention for nodes in the tree. We will uniquely identify a node in the tree with a pair of numbers (h,x)
. The pair (h,x)
 represents the node at level h
 that has exactly X
 other nodes at level h
 to its left. This is similar to a coordinate system. We can convert from a node index to one of these pairs as follows. First, we can obtain the level of a node given its index as we did in the Test Set 1 solution. Then, if the node index is D
 and the level is h
, we can find X
 as the difference between D−1
 and the number of nodes up to level h−1
, which we saw how to calculate as well. Now, we turn our attention to solving the problem for two nodes identified as pairs.

A slow solution
We will work our way down the tree, while always maintaining both of the input nodes A
 and B
 inside the current subtree. As state we will keep the color of the root of the current subtree, and the pairs representing A
 and B
 relative to the current subtree. Initially, the color of the current root is black, and the pairs that represent A
 and B
 are calculated as mentioned in the previous paragraph.

Given the color of a current root C
 and a pair (h,x)
 representing a non-root node, we can check whether (h,x)
 is in the left or right subtree as follows: The number of nodes at level h
 in the left subtree is exactly the number of descendants of LC
 at level h−1
. Since this is simply eLCMh−1
, where ei
 is the vector that has a 1
 in position i
 and 0
 in all other positions, we can calculate it in O(N2logB)
 time, as we saw in the opening paragraphs. Therefore, we can simply compare that number with x
 to make our decision.

The observation above leads to an algorithm: Given the two target nodes as pairs (h1,x1)
 and (h2,x2)
 and current root color c
, if min(h1,h2)=0
, then the answer is h1+h2
. Otherwise, we check in which subtree each of the nodes is. If both are in different subtrees, the answer is also h1+h2
. If not, we move into the subtree. If we move into the left subtree, the root switches to Lc
 and the node represented by the pair (h,x)
 is now represented by the pair (h−1,x)
. If we move into the right subtree, the new root color is Rc
 and the node represented by the pair (h,x)
 is now represented by the pair (h−1,x−t)
 where t
 is the number of descendants at level h−1
 of Lc
 (the same amount we needed to calculate to decide on which subtree (h,x)
 belonged).

This algorithm would require min(h1,h2)
 steps, and can be too slow if that is a large amount, which can happen.

Speeding up the solution
Instead of speeding up all cases, we focus only on the ones that definitely need the speed up. If the graph implied by L
 and R
 has any reachable color belonging to more than one cycle, then the total number of nodes of the tree grows exponentially. In this case, the h
 values in the pair representation of the input nodes are necessarily small (logarithmic on the node indices) and the algorithm above just works. So, we need a faster algorithm only for the case in which every reachable color belongs to at most one cycle.

In the case in which the current root is a color that does not belong to a cycle, we proceed as in the slow algorithm and move one step down. After this, the color left behind will not be a root color again. If the root color c1
 belongs to a cycle c1,c2,…,ch
, then we want to do multiple steps at once. Consider the branch of the tree that is obtained by going through the cycle p
 times, with p×h<min(h1,h2)
. The branch splits the tree in 3
 parts: descendants of the last node of the branch, nodes that are to the left of that subtree, and nodes that are to the right of that subtree. Our strategy is to find out where in that partition are our target nodes. If they are both in the middle (right below the branch), we go into it. Otherwise, we cannot do that many passes without leaving one of the target nodes outside of our current subtree. We can try decreasing powers of 2
 as values for p
, which requires each value to be tried only once. After we try p=1
 (the smallest integer power of 2
), we fall back to moving one step at a time as in the slow solution until we reach a root color outside of the cycle (this requires at most h
 single move steps).

To do the fast moves, we calculate the vector representing the number of nodes of each color that lie exactly p×h
 levels below our current root on the left side. We can calculate it for p=1
 as ∑jec1Mh−j+1
 where the summation is only over the values j
 for which the step cj→cj+1
 (or ch→c1
 if j=h
) is to the right. For larger values of p
, we take the value for p=1
 and multiply it by M0+Mh+⋯+M(p−1)h
. This summation can be calculated similarly to the summation over consecutive powers, and it has to be done only once per value of h
, of which there are O(N−−√)
 different ones. We can calculate the right side analogously.

From the vectors at level p×h
 we can find out how many nodes at level hi
 are on each side by multiplying by Mhi−p×h
. With those values, we can decide similarly to how we did in the slow solution whether both nodes are in the middle subtree. If we were to move into the middle subtree, we simply subtract p×h
 from both hi
 values and the total number of nodes from the level that were left behind on the left side from the xi
 values, as we did in the slow solution.

With the details above, this leads to an algorithm that takes O(N3.5logN+N2logB)
 time to run. If less care is taken with how matrix powers and summations of matrix powers are calculated, larger complexities may arise. The time limit is purposefully not tight, so algorithms with larger complexities that are logarithmic in B
 can also get the problem correct.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2022 - Code Jam 2022

Punched Cards (11pts)

This problem required us to print out ASCII pictures of punched cards of different sizes. Although the problem and solution are relatively straightforward, we need to know how to read from stdin and write to stdout. Examples of this can be found in the "Coding" section of the FAQ.

Because the dimensions of the punched cards we need to print are so small, one option would be to store all the ASCII pictures in an array and just print the requested cards. However, we can make use of nested for-loops to generate the art for any sized punched card.

We can use a loop to print out the punched card one line at a time and use another loop to print the characters in each line. The odd-numbered (1-indexed) lines alternate between (+) and (-), and the other lines alternate between (|) and (.). We can check the line number and column number (mod2)
 to see which character we should print. The only exception is when both the line number and column number are ≤2
⁠. In this case, we always print a period (.).

As some of you may have already discovered, for this year's qualification round, you could submit solutions in Punched Card Python. The following is a solution to Punched Cards written using Punched Card Python:

Solution to Punched Cards in Punched Card Python (1/3).

Solution to Punched Cards in Punched Card Python (2/3).

Solution to Punched Cards in Punched Card Python (3/3).

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2022 - Code Jam 2022

3D Printing (13pts)

The first thing we can notice is that if a printer has u
 units of ink left of a given color, we cannot use more than u
 units of that color. Moreover, this is the only restriction imposed by that value. So, we can summarize the input by saying we cannot use more than C=min(C1,C2,C3)
 units of cyan ink, M=min(M1,M2,M3)
 units of magenta ink, Y=min(Y1,Y2,Y3)
 units of yellow ink, or K=min(K1,K2,K3)
 units of black ink.

If C+M+Y+K<106
 then the case is impossible and we are done. Otherwise, we may need to use lower amounts of each color. We can simply go one color at a time, lowering the amounts of ink until we make the sum exactly 106
. Doing it one unit at a time works, but it is very slow. We can do better: in the same way as before, we can consider all the colors one at at a time. Let S
 be the sum of the current amount of ink for the 3
 colors not currently under consideration. If S≥106
, we can simply set the amount of the current color to 0
 and continue with the next one. If S<106
 we can lower the current color to 106−S
 and finish immediately. This works because at all times we maintain the invariant that the total amount of ink we are considering is at least 106
 units.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2022 - Code Jam 2022

d1000000 (9pts, 11pts)

Test Set 1
There are multiple of ways to solve Test Set 1 of this problem. A particularly funny one is to throw the solution of an old finals problem at it (even the Test Set 1 solution of that problem works).

Test Set 2
Test Set 2 has very big numbers, so we need insights that are specific to this problem.

Insight 1. If a straight from A
 to B
 can be done, then one from 1
 to B−A+1
 can be done as well using the same dice in the same order, since a die showing a number X
 can always be used to show number X−A+1
.

Insight 2. If a straight is done with a di
 showing number X
 and a dj
 showing number X+1
 with i>j
, we can build the same straight but using dj
 for X
 and di
 for X+1
.

Insight 2b. Any straight that can be done, can also be done while using the dice in non-decreasing order of number of faces.

Combining insights 1 and 2b gives an algorithm: start by sorting the dice. Then, in that order, try to extend the current straight if possible. Or, in pseudo-code:

maximum_straight_length(S):
  sort(S)
  length = 0
  for si in S:
    if si > length: length += 1
  return length
This algorithm requires only linear time beyond sorting the input, which means O(NlogN)
 overall. This is fast enough to pass Test Set 2.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2022 - Code Jam 2022

Chain Reactions (10pts, 12pts, 5pts)

Test Set 1
In Test Set 1 there are very few modules. This means that we can try all possible orders for the manual initiators, simulate the rules as explained in the statement to get the overall fun yielded by each order, and keep the maximum result from among those. Notice that modules that are pointed at the abyss and not pointed at by any other module contribute their own fun to the total no matter the order. Therefore, we can assume they all trigger in any specific order at the beginning. This brings down the number of orders to try from N!
 to at most (N/2)!
, which is a lot less.

Test Set 2
We can start by modeling the problem. We can see the input as a rooted forest where the parenting relationship models the pointed at relationship. Root nodes are modules that are pointed at the abyss.

As it is often the case for problems on trees, we can solve this one efficiently with a divide and conquer approach, and the aid of memoization/dynamic programming to keep the running time small.

Notice we can solve each tree (connected component) in the forest independently. Then, we can notice that the root of the tree is triggered by the first manual initiator. So, we can try all possibilities for the first manual initiator and eliminate the path between them and the root. That leaves a lot of separated subtrees that we can solve recursively.

Formally, let F(i)
 be the fun of node i
 and Ftree(t)
 be the fun value of a given subtree rooted at t
. To compute Ftree(t)
, if t
 is a single node we simply return its fun F(t)
. Otherwise, we take the maximum (over all possible x) of fun(x,t)+∑sFtree(s)
 where fun(x,t)
 is the maximum fun of all nodes in the path from x
 to t
's root and x
 is a leaf of this subtree and the summation is over all subtrees s
 whose root parent in the original tree is in the mentioned path.

The domain of F
 is equal to the number of subtrees, which is equal to the number of nodes of the tree. If we memoize F
, the overall running time of computing it for all subtrees of a tree of size k
 is the domain size (k
) times the time it takes to compute it for a single item in the domain, disregarding the cost of recursive calls. This is O(k)
 because of the summation, which means an overall time complexity of O(k2)
. The worst case is when the entire input forest is a single tree, which means the time complexity of the algorithm overall is O(N2)
.

Test Set 3
To solve Test Set 3 we continue on our modelling from Test Set 2. We observe that the answer for Ftree(t)
 can be broken down to max(fun(x,a),F(t))+∑sFtree(s)
 where a
 is one of t
's children and x
 is a leaf node. We also observe that to maximize this, we need to choose a leaf x
 which minimizes fun(x,a)
. This way we guarantee that fun(x,t)
 benefits from having F(t)
 on the path, and that all other subtrees s
 have a maximum possible value. Otherwise if we pick a path other than the minimum, this means the minimum path will be considered as one of the other subtrees which reduces ∑sFtree(s)
 reducing the final answer we can get.

Identifiying x
 can be done using a DFS traversal solving each tree of size k
 in O(k)
. Leading to an overall complexity of O(N)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Qualification Round 2022 - Code Jam 2022

Twisty Little Passages (29pts)

In general, we can't just teleport to every room, because N can be much larger than K. We could teleport to a randomly-chosen subset of the rooms, calculate the average degree (number of adjoining passages) of those rooms we've visited, and assume that this is a good estimate of the average degree of all the rooms. The number of passages is half the sum of the room degrees (since each passage connects two rooms.)

When can this approach yield a poor estimate? If a small subset of the rooms have a degree much higher or much lower than the median degree, then we might not visit any of them, and our estimate would be too high or too low.

Cases where most rooms have high degree and a small number of rooms have low degree are not a problem, because we are judged on relative error, and the relative error in this case is low. If the average degree is 100 but we estimate that it is 103, we would only be 3% from the answer. But if the average degree is 5 and we estimate that it is 2, we would be 60% away from the answer!

So consider a troublesome case where most rooms have low degree, and a small set of rooms have high degree — few enough that we are unlikely to find one of them by teleporting randomly. If these rooms contribute a significant fraction of the total number of passages, then they must connect to a significant fraction of the total set of rooms. So we can find these with high probability by repeatedly teleporting to a random room and then walking through a random passage with the "W" command.

Consider a series of rounds where we alternate between teleporting to a random room with the "T" command and walking through a random passage with the "W" command. We cannot simply extrapolate the average degree for the whole cave from the average degree of all the rooms we've seen — the rooms we reach with "W" commands are not a uniform sample. We can, however, use the average degree of just the rooms we visited with the "T" command as our estimate for all the rooms we haven't visited, and then add the known degrees to that. This is sufficient to solve the problem.

Another solution is to use an alternating series of "T" and "W" commands as above, and then use a technique called importance sampling. Each visit to a room gives us a sample of the average degree, but each room does not have an equal chance of being visited in any given sample. By weighting each sample appropriately, we can compute a weighted average which serves as an unbiased estimate of the total. The weights are chosen to compensate for the non-uniform probabilities of visiting each room.

When we randomly choose a room and visit it via a "T" command, each room had an equal chance of being chosen; we give this sample a weight of 1. When we visit a room via a "W" command, each room did not have an equal chance of being visited. We need to calculate weights for these samples, such that the expected weight for each room (the probability of visiting that room via a "W" command, multiplied by the expected weight we assign for such visits) is 1/N
. This will give our final estimate the correct expected value.

Consider a sample where we were previously in a room R1
 with degree A
, and then we issued a "W" command and walked into a room R2
 with degree B
. The probability of us being in R1
 after the last "T" command was 1/N
. The probability of following the passage to R2
 was 1/A
, because R1
 had A
 passages connected to it. So the overall probability was 1/(AN)
. We assign this sample the weight A/B
, so that the contribution to the expected weight for room R2
 is 1/(BN)
. After we sum over all B
 ways we could arrive at R2
, we get a total expected weight of 1/N
 for R2
, as required.

As an example, consider the following interaction:

T 1
1 1
W
3 2
T 2
2 1
W
3 2
T 3
3 2
W
1 1
We get samples of degrees 1,2,1,2,2,1
 with weights 1,1/2,1,1/2,1,2
, and the weighted average degree is 8/6
.

Round 1A 2022 - Code Jam 2022

Double or One Thing (10pts, 15pts)

For any string S
, the number of new strings we can obtain from it is at most 2|S|
 because for each character in S
, there are 2
 choices: to highlight it or not.

Test Set 1
Since the length of S
 is at most 10
, the number of new strings we can obtain is at most 210
. We can enumerate all of them to find the lexicographically smallest one. The time complexity of this solution is O(2|S|×|S|)
 because there are 2|S|
 strings in total to compare, and the length of them is at most 2×|S|
.

Test Set 2
Now that the length of S
 can be up to 100
, 2100
 is too large to enumerate all of them.

Note that a string p
 appears before a different string q
 in alphabetical order if p
 is a prefix of q
 or if p
 has a letter lexicographically smaller at the leftmost position at which p
 and q
 differ.

Then, for any character Si
 in S
, we have the following rule to decide whether to highlight it or not:

If the next different character in S
 is lexicographically larger than the current character (in other words, there is an index j
 that Sj>Si
 and Sk=Si
 for all i<k<j
), then we must highlight Si
. That is because when we double Si
, we also push Sj
 right and replace that index with Si
 at the same time. Therefore doubling Si
 will make the new string lexicographically smaller.
If the next different character in S
 is lexicographically smaller than the current character (in other words, there is an index j
 that Sj<Si
 and Sk=Si
 for all i<k<j
), then we must NOT highlight Si
. That is because when we double Si
, we also push Sj
 right and replace that index with Si
 at the same time. Therefore doubling Si
 will make the new string lexicographically larger.
If there is no next characters different from Si
 (in other words, i
 is the last index in S
 or Sk=Si
 for all i<k<|S|
), then we must NOT highlight Si
. That is because doubling Si
 will make the original string be the prefix of the new string, which means the new string is lexicographically larger.
There are a lot of different ways to implement it and here is one: We can preprocess the given S
 into groups of same and continuous characters. For example, BOOKKEEPER is preprocessed as [(B,1), (O,2), (K,2), (E,2), (P,1), (E,1), (R,1)]. Then for each element in this list, we output the character with twice its original occurrence if the character of the next element is lexicographically larger than it. Otherwise, we output the character with its original occurrence.

The time complexity of this solution is O(|S|)
 because we need to iterate through S
 when preprocessing it, and iterate through the preprocessed list with length up to |S|
 when outputing the final answer.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1A 2022 - Code Jam 2022

Equal Sum (31pts)

In this problem we first pick half of the input, then the judge picks the other half, and then we need to solve an NP-complete problem on that input. This means we have to be really strategic on how we pick that input.

We only get to provide half of the numbers. So, it might be best to first see how good of a split we can do with using just the judge provided numbers and only afterwards come up with the numbers we provide.

A common heuristic to solve problems in which we do not know all of the input yet is to optimize locally. In this case, sort out the numbers into two sets, trying for their sums to be as close as possible. That means, we go through the input integers and assign them to the set that has the smallest current sum. After we process all of the judge's integers, this guarantees that the difference between the sets is bounded by the size of a single integer (109
), as opposed to being bounded by the total sum of the integers we have seen (1011
).

We need our integers to always be able to make up for that difference, so we need to guarantee at least some of the integers we pick are big enough. Moreover, we need to be able to make up for the difference exactly, so we want to pick integers that get progressively more precise. This smells like binary. However, when writing in binary we get to sum over a subset of the powers of 2
. In this case, the integers that do not end up in one subset go to the other one, so binary can just write any possible difference. That being said, powers of 2
 still work, as long as we do not write in binary.

After we finish sorting out the judge's input, we know that the difference between the two subsets is at most 109
. Therefore, if we decide where to put the largest power of 2
 (229
) using the same algorithm, that guarantees that the difference is now at most 229
. As long as the next number is always at least half of the previous one, we can maintain the difference bounded by the last processed number. If we process the powers of 2
 last, and in decreasing order, the difference between the two subsets at the end is no larger than 20=1
. Since the limits guarantee that the total sum is even, the difference is 0
.

The paragraph above shows that any set of integers up to X
 with even sum that contains every power of 2
 no greater than X
 can be partitioned into two subsets of equal sum. Moreover, the partition can be found efficiently. There are 30
 powers of 2
 between 1
 and 109
, which means we can choose the other 70
 integers we are entitled in any way.

Round 1A 2022 - Code Jam 2022

Weightlifting (13pts, 31pts)

Test Set 1
The limits for Test Set 1 are small enough that we can afford to use brute force. Since there is no reason to have more than maxjXi,j
 weights of type i
, the number of possible states of the stack to consider is very limited (in fact, it is ∑0≤a,b,c≤3(a+b+c)!a!b!c!=5248
). To solve for the minimum number of operations to finish all of the exercises, you can use a breadth-first search over all possible combinations of the stack states and the number of completed exercises. Specifically, the breadth-first search starts at the state of an empty stack with no completeted exercises, and the final answer is the distance from that to the state where the stack is empty but all exercises are completed.

Test Set 2
The first observation we can make to solve Test Set 2 of this problem is the following: if there are any weights that are used in every exercise, it is always optimal to put them at the bottom of the stack before the first exercise, and leave them there until after the last one. Moreover, if there are multiple such weights, putting them in any order is equivalent. Formally, if C
 is the multiset of weights that is common to all exercises, there is an optimal solution that has C
 at the bottom of the stack during all exercises.

Now, for a given full sequence of exercises, let us call A
 to the largest sequence of weights that appears at the bottom of the stack for all exercises (A
 could be empty). Since A
 is used for all exercises, A⊆C
. If we use the first observation, then also C⊆A
, so A=C
. Therefore, we can make a second observation: if there is more than one exercise, there is at least one time in between exercises in which the stack contains exactly C
. This points towards a divide and conquer solution: find that intermediate point and then recursively solve the problem of minimizing the operations before and after that point, disregarding C
.

As it is common with divide and conquer approaches, we can either find the place to split without recursion, or we can use memoization to be able to simply try every possible split point, without incurring significant additional computation time.

To formalize the approach, let C(ℓ,r)
 be the multiset of weights that is the intersection of the multiset of weights needed for each exercise between ℓ
 and r
, inclusive. Additionally, let M(ℓ,r)
 be the minimum number of operations to get from a stack containing C(ℓ,r)
, in any order, perform all operations between ℓ
 and r
, inclusive, and leave the stack with C(ℓ,r)
 on it, in the same order. To calculate M(ℓ,r)
 we follow the strategy above: for some mid-point x
 we do this twice: load the additional weights for one part of the split, optimize according to a recursive result, then unload those additional weights. The additional weights for the left part of the split are C(ℓ,x)∖C(ℓ,r)
 and for the right part of the split are C(x+1,r)∖C(ℓ,r)
 (notice that C(ℓ,r)
 is included in the other two multisets by definition). Therefore, the cost in addition to the recursion for the mid-point x
 is 2×(|C(ℓ,x)|+|C(x+1,r)|−2×|C(ℓ,r)|)
. Putting it all together:

M(ℓ,r)=0
, if ℓ=r
;
M(ℓ,r)=minℓ≤x<r(M(ℓ,x)+M(x+1,r)+2×(|C(ℓ,x)|+|C(x+1,r)|−2×|C(ℓ,r)|))
, otherwise.
Precomputing the O(E2)
 values of C
 takes O(E2×W)
 time. With memoization, we can compute the O(E2)
 values M
 in O(E3)
 overall time. This is fast enough to pass Test Set 2.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1B 2022 - Code Jam 2022

Pancake Deque (7pts, 8pts, 10pts)

Test Set 1
For the first test set, we could try a brute force solution for this problem. Given a remaining deque of pancakes D
, we could choose to serve the next customer from either the front or back of the deque. Meanwhile, we could update the maximum deliciousness of the pancakes served every time we decided to serve a pancake out. We could use a recursive brute force method to simulate the process.

Given that every time we have two choices for serving the pancake (either the first or the last one) , the overall time complexity of the approach will be O(2N)
, which is sufficient to solve the first test set.

Notice that the brute force method can be improved by using memoization to record the current optimal solution for a partial deque. There are (N2)+N=N⋅(N+1)2
 partial deques in total to take into consideration. Therefore, this will reduce the time complexity down to O(N2)
.

Test Set 2
The same approach cannot be applied to the second test set, since it will result in a time limit exceed. Therefore we need a more clever way other than trying to serve the pancakes brute-force. We could quickly make an observation that, if the deliciousness of the pancakes we could serve at some point are Dleft
 and Dright
, it will always be better to serve the one with less deliciousness. To prove this, we could do a quick analysis. For simplicity, let us assume that Dleft≤Dright
 , and denote Dmax
 as the greatest deliciousness of the pancakes served out so far. If Dleft<Dmax
, this means that the pancake will be served out free no matter when we serve it, so we could easily serve it out now and will not affect our final answer. Otherwise, since Dleft≤Dright
 it will always be better to serve Dleft
 first, or else Dmax
 will be updated to at least Dright
. In that case Dleft
 will be served out free. Therefore we could summarize into a criteria for serving pancakes: serve out min(Dleft,Dright)
, and update Dmax
 if needed. For each customer, this criteria takes O(1)
 time to perform, therefore the total time complexity of this approach is O(N)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1B 2022 - Code Jam 2022

Controlled Inflation (14pts, 21pts)

The key observation is that for each customer, either the increasing or decreasing order of target pressures will produce an optimal solution. That is, no other permutation of products will result in a strictly lower number of button presses. Let's prove this.

First, note that this is true for each individual customer i
. At one point in the process, the pump will be at the minimal pressure Mini=minj=1..PXi,j
. At another point, the pump will be at the maximal pressure Maxi=maxj=1..PXi,j
. This means that we have to reach both of them, so we will always have to press the buttons at least Maxi−Mini
 times, no matter the order. This can be achieved by arranging the products in either increasing or decreasing order of their target pressures.

Now, the pressure also has to be adjusted between customers, which requires pressing the buttons. Let's see why all other product orders do not improve the answer by consindering how many button presses between customers we can save. While processing the i
-th customer, the pump will be at Mini
 at one point, at Maxi
 at another point, and finally we will leave it at the pressure of the last product, Xi,last
. This requires at least (Maxi−Mini)+(Maxi−Xi,last)
 button presses, but the potential saving is at most (Maxi−Xi,last)
, which is the least amount of additional button presses we needed to do. If the pump reaches the maximum pressure before reaching the minimum pressure, a similar relationship holds, meaning this different order does not improve the answer.

Now that we know that it is enough to consider only the increasing and decreasing orders, we will only keep track of Mini
 and Maxi
 for each customer. For the test set with the visible verdict, it is enough to check all 2N≤1024
 possibilities of choosing the increasing or decreasing order for each customer and simulate the process.

To do this more efficiently, we can use dynamic programming. Let dpi,0
 be the answer after processing i
 customers where the products for the last customers are arranged in increasing order. Similarly, let dpi,1
 be the answer after the first i
 customers with the products for the last one arranged in decreasing order. We will also keep track of the pressure we left the pump at, l0
 and l1
 for the increasing and decreasing orders corresponsindly. Clearly, dp0,0=dp0,1=0
 and l0=l1=0
 (the starting pressure). Now, assume dp
 is calculated up to i
. The following equations give the values for the next customer:

dpi+1,0=min(dpi,0+|l0−Mini+1|+(Maxi+1−Mini+1),dpi,1+|l1−Mini+1|+(Maxi+1−Mini+1))

dpi+1,1=min(dpi,0+|l0−Maxi+1|+(Maxi+1−Mini+1),dpi,1+|l1−Maxi+1|+(Maxi+1−Mini+1))

And update the last pressures: l0=Maxi+1
, l1=Mini+1
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1B 2022 - Code Jam 2022

ASeDatAb (25pts, 15pts)

Test Set 1
Whatever we try to do, the randomly-rotating judge in this test set might scuttle our plans. But we can fight randomness with randomness!

First, we can observe that if the judge ever tells us that the record has eight 1
 bits, we have all but won (as long as we have at least one interaction left). This is because we can then submit 11111111
, and no matter how the judge rotates it, the final result of XORing will be 00000000
. Because of this, if we are ever told that the record has more than four 1
s, it should be to our advantage to aim for eight 1s rather than zero 1s!

For now let's suppose the judge tells us there are b
 bits in the record, for some b
 between 1
 and 4
. (We will handle the other cases by symmetry, as explained above.) Let's do the following: choose a string with b
 bits uniformly at random, then send that. Now it doesn't really matter how the judge rotates our string -- whatever the resulting rotated string is, we were just as likely to pick that in the first place.

What are the possible outcomes? Suppose that b=2
. Then we have a 1(82)=128
 chance of flipping both of the two 1
s (and therefore winning!), a (21)(61)(82)=1228
 chance of flipping one of the two 1
s and some innocent 0
 (and thus being back in the same b=2
 boat), and a (20)(62)(82)=1528
 chance of missing both 1
s and creating two new 1
s (taking us to b=4
). This may not seem too promising so far, but hang on...

Doing the same sort of analysis for the state b=4
, we find that we end up at one of b=0,2,4,6,8
, with probabilities 170,1670,3670,1670,170
, respectively. But, as we mentioned, being at b=6
 is just the same as being at b=2
. If we are at b=6
, we can try to use two 1
s to flip the two 0s, in the hopes of reaching 11111111
. Similarly, being at b=8
 is (essentially) as good as being at b=0
. So we will lump those two probabilities into b=2
 and b=0
, respectively, getting transition probabilities to b=0,2,4
 of 135,1635,
 and 1835
, respectively.

Notice that if b
 is even, we are trapped in the even-b
-verse, randomly walking (according to those transition probabilities) until we either reach b=0
 or reach b=8
 or we run out of guesses. And if we are not in the even-b
-verse, one round of sending exactly min(b,8−b)
 randomly placed 1
s will get us there; we leave this as an exercise. So our strategy can be to spend up to two rounds getting into the even-b
-verse, up to 297 rounds wandering, and up to one round possibly turning a 11111111
 into a 00000000
.

What are the chances that we will succeed in those 297 rounds? Observe that until we reach a winning state (00000000
 or 11111111
), we are always in one of two other states: b=2
 (lumped in with b=6
), or b=4
. In the former case, we have a 128
 chance of transitioning to a winning state, and in the latter case, that chance is 135
. Just for ease of argument, let's pessimistically guess that we get stuck hanging around in the less advantageous b=4
 state. But then to not win, we would still have to fail our 135
 lottery 297 times. The probability of that is (1−135)297≈0.0002
. So we have at least a 99.98%
 chance of succeeding with this strategy, and that is an overly conservative lower bound!

(If you're curious about the actual success probability, we can conservatively assume that we always start our journey at b=4
, and then find the upper left cell of ⎛⎝⎜⎜1001281228152813516351835⎞⎠⎟⎟297⎛⎝⎜001⎞⎠⎟
, which turns out to be about 0.99993≈99.993%
).

Of course, we still have to pass all 100 test cases in Test Set 1, and the probability of this is a bit smaller: ≈0.99993100≈0.993
. But 99.3%
 isn't so bad, and if we see an unlucky failure with this method, we can easily get another independent try by changing our code's random seed, since we have control over that source of randomness.

Test Set 2
In Test Set 2, the judge does not behave randomly and can and will choose rotation values that keep us away from reaching our goal. So, we need a strategy that is guaranteed to reset the record to all zeroes.

One way to do this is to consider the current state. Let's define a state as the set of all possible values the record could currently be set to. A key observation here is that two values that are cyclic rotations of each other are equivalent. Therefore, we can eliminate these duplicates from our sets. After the first exchange, we know how many bits are set to 1
 in the record. Thus, all potential states only have values that have the same bit count.

We can enumerate all possible values for each bit count (while removing duplicates that are cyclic rotations of another value). If we do this, we find the following sets of potential values:

0 bits: {00000000}
1 bit: {00000001}
2 bits: {00000011,00000101,00001001,00010001}
3 bits: {00000111,00001011,00001101,00010011,00010101,00011001,00100101}
4 bits: {00001111,00010111,00011011,00011101,00100111,00101011,00101101,00110011,00110101,01010101}
5 bits: {00011111,00101111,00110111,00111011,00111101,01010111,01011011}
6 bits: {00111111,01011111,01101111,01110111}
7 bits: {01111111}
8 bits: {11111111}
Notice that the largest of these sets (4 bits) has only 10 elements. Thus, there are at most 210=1024
 unique states for when we have 4 bits on. This is small enough to consider all possible states and so something similar to a BFS (breadth-first search) from the state with all zeroes.

Specifically, we can consider the set of all states that we know can be forced to reach all zeroes (initially just the solved state where the record is 00000000). Then, for a given state, A
, we can consider trying all possible values for V
 and simulate the results of the 8
 different rotation values the judge could choose. Grouping those by their bitcounts gives us the possible states that A
 could transition to for a specific value of V
. If all of those states are ones we have processed, then we know that when state A
, we can use this value of V
 to get us closer to setting the record to all zeroes.

It turns out that if we keep repeating the above process, we will eventually process all possible states. This gives us instructions on which numbers to provide for V
 given the current state. The alternative solution is proof provided below works to prove that this is always possible for 8
⁠-bit records.

Alternative Solution
It turns out that we can solve this problem without ever knowing the bit count after interactions (other than being told when we eventually reach 00000000).

Let's consider how we would solve this problem for a record that has only 1 bit. Since we know the value starts as not 0
, we can force the state to reach 0
 by sending 1
 to the judge. Let's call this sequence P[0]
:

1
Now, let's consider a record that is 2
 bits (and is not all zeroes). Let's start by assuming that the two bits are the same. If that's the case, we can force the record to be all zeroes by sending 11
. If We haven't reached all zeroes after that, that means our initial assumption that the left and right bit were the same was incorrect. So, if we send 10
, we can make the two bits the same. Then, if we are still not all zeroes, we can send another 11
. Let's call this sequence P[1]
:

11    // P[0] + P[0]
10    // P[0] + 0   
11    // P[0] + P[0]
Now, let's generalize this and assume that the record has 2k
 bits and is not all zeroes. Let's assume that the left 2k−1
 bits and the right 2k−1
 bits are the same. If that's the case, we can use P[k−1]
 (but each step is appended to itself) to force the record to reach all zeroes. If we did not reach all zeroes then our assumption that the left half and right were the same was not correct.

So, we can use the first instruction in P[k−1]
 and append 2k−1
 0's to it. Then we can repeat all of P[k−1]
 (each step doubled like before) again. As long as we keep not reaching all zeroes, we repeat this process with the next instruction in P[k−1]
.

The following Python code shows this process for how we can generate P[3]
 for 8
⁠-bit records:

def appendzero(s):
  return s + '0' * len(s)

def expand(s):
  return s + s

def P(k):
  if k == 0:
      return ['1']
  seq = P(k - 1)
  seq_with_zero = [appendzero(s) for s in seq]
  seq_with_copy = [expand(s) for s in seq]
  res = seq_with_copy[:]
  for ins in seq_with_zero:
      res += [ins]
      res += seq_with_copy
  return res

print(P(3))

Round 1C 2022 - Code Jam 2022

Letter Blocks (10pts, 15pts)

Let us first notice that this problem is equivalent to finding an order in which partial strings should be concatanted such that occurrences of the same letter appear together.

Test Set 1
Since N
 is at most 6
, the number of permutations of these strings is at most 6!=720
.

We can therefore generate all permutations and for each of them we need to verify the final string which results of concatenating the input strings in that particular order.

Let us take a look at the string CCCABDAEEF. To verify the string it is enough to:

Get a set of letters: {A, B, C, D, E, F}
Create a grouped represantation of a string: "CABDAEF"
The string is good if and only if the lengths coincide.
If for at least one permutation the size matches, we can print out this string. Otherwise, it's impossible.

The time complexity of this solution is O(N!×∑Ni=1|Si|)
, which means N!
 times the sum of the lengths of the input strings, because there are N!
 permutations and the verification of the final string takes linear time.

Test Set 2
In this test set N
 can be 100
, so N!
 is too large to enumerate all permutations.

First of all, we can verify that each of the strings Si
 meets the requirements from the task. This can be done by applying the verification method described in the section above for each Si
 individually. If the verification fails for one of the input strings, then it will certainly fail for any permutation of them and therefore we output IMPOSSIBLE.

Let us call middle letters all letters other than the first and last consecutive segment of letters. Next, let us notice that if the string Si
 has more than 2
 distinct letters, then:

If a letter is a middle letter in more than one input string, then those occurrences will not be together in the final string regardless of the order in which we concatenate them. Therefore, this case is impossible.
If the middle letters exist in a single input string, they don't influence the outcome as those occurrences will be together in the final string regardless of the order in which we concatenate. Therefore, in this case we can assume the input string is simply two letters long removing everything except the first and last letter of it.
Therefore, for each middle letter we can count in how many strings it appears and if the answer is more than 1
 for any of them, we can print out IMPOSSIBLE.
Since we also verified each string already, we know that each letter appears only in 1
 block inside each Si
.

After this step the problem is now simplified into strings of two forms:

X
: Represents the string consisting of only one block of letter X
.
XY
: Reperesents the string starting with a block of X
 letters and ending with a block of Y
 letters.
If there are two strings of the form X
 for the same letter, we can concatenate them as they can't be separated by other strings in the final solution. If there are two strings of the form X1Y1
 and X2Y2
, then the answer is IMPOSSIBLE if X1=X2
 (due to Y1
) or Y1=Y2
 (due to X2
), because it means that in any ordering, there would be at least one block of a different letter between letters X1
 and Y1
.
Therefore for each string Si
, we can create the following mappings using sets:

If Si
 is of the form X
, then insert Si
 into single[X]
.
If Si
 is of the form XY
, then insert Si
 into both starts[X]
 and ends[X]
.
In other words, single[X]
, starts[X]
 and ends[X]
 must all contain exactly 1
 element for each letter X
. If the element already exists, we return IMPOSSIBLE.
Starting string
Let us consider starting string as the input string which is not forced by any previous strings in the final answer. When can the given string Si
 be a starting string?

If Si
 is of the form X
, then there must be no other strings ending with letter X
, i.e. ends[X]=Si
.
If Si
 is of the form XY
, then starts[X]=Si
 and ends[X]=null
 and single[X]=null
.
With these two conditions, we consider a set of candidates C
 containing all such starting strings.
Extending the block
Let us consider we already built the partial answer A
 which ends with letter c
. If there exists a string at single[c]
, it is the last chance to append it, because otherwise it would be separated by at least one block of another letter. Similarly, if there exists a string at starts[c]
, we must extend it now for the same reason.

How to choose the starting string?
It turns out that for starting a new block, we can choose an arbitrary candidate from the candidates set.

Proof
: Let us assume that we picked string Si
 as the starting string but the optimal solution started the block with Sj
. Let us consider the swapped optimal solution in which we swap these blocks.

Let Si
 start with letter a
 and Sj
 with letter b
. Then:

Optimal = |b..X|a....Y|
Optimal (swapped) = |a....Y|b..X|
Let us consider what happens after swapping:

Middle letters of these blocks: Any letters between b
 and a
 can't be after a
 and any letters after a
 can't be before a
. Therefore after swap they also remain fine.
Letters b
: Optimal solution can't have any b
 letters after a
. Therefore this swap is okay.
Letters a
: Since Si
 belongs to the candidate set, therefore ends[a]=Si
 or ends[a]=null
. It means, that there are either no strings ending in a
 or Si
 is the only string ending with a
. Therefore X
 can't end with a
 and the swap remains correct.
Final solution
Repeat:

1. Pick an arbitrary string from the candidate's set and start the block with it.
2. Let e = last letter of the current solution. If starts[e] != null, add starts[e] to current solution and repeat step 2. Otherwise goto step 1.
3. If candidate's set is empty, print solution. Otherwise, print IMPOSSIBLE.
Time complexity: O(N×∑Ni=1|Si|)
, since we are touching each candidate only once.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1C 2022 - Code Jam 2022

Squary (9pts, 22pts)

The multinomial expansion for the power of 2 is the key to solving this problem. The expansion of the square of the sum of elements X1,X2,…,Xn
 in the list X
 looks like:

square of sum=(X1+X2+X3+…+XN)2=X21+X22+X23+…+X2N+2⋅X1⋅X2+2⋅X2⋅X3+2⋅X1⋅X3+…+2⋅XN−1⋅XN=sum of squares+2⋅sum of pairwise products
Let S(X)
 be the sum of elements, SQ(X)
 be the sum of squares of elements, and SP(X)
 be the sum of all pairwise products of elements of the list X
. We can now rewrite the above equation as:

S(X)2=SQ(X)+2⋅SP(X)
We can also observe the following about how the above values change when an additional element n
 is added to the list X
:
S(X+[n])SQ(X+[n])SP(X+[n])=S(X)+n=SQ(X)+n2=SP(X)+n⋅S(X)
Our task is to achieve S(E′)2=SQ(E′)
, where E′
 is the extended list that we get by adding extra elements to E
. In other words, we want to make SP(E′)=0
.

Test Set 1: K=1
If we are allowed only a single addition, we must choose an element n
 such that SP(E+[n])=0
.
SP(E+[n])=0⟹SP(E)+n⋅S(E)=0⟹n⋅S(E)=−SP(E)
If S(E)≠0
, we can get a squary list whenever −SP(E)/S(E)
 is an integer, which happens if and only if S(E)
 divides SP(E)
. In that case, −SP(E)/S(E)
 is our answer.

If S(E)=0
, then S(E+[n])=n
. Since we want S(E+[n])2=SQ(E+[n])
, we need SQ(E+[n])=n2
. This is possible only if SQ(E)=0
, that is, if all elements in E
 are zeros. In this case, we can choose any value as our answer. But if any element in E
 is not zero, it is impossible to get a squary list with only one addition.

Test Set 2: K>1
At first, the search space might seem hopelessly broad here. But we can observe (or surmise and then confirm) that it is always possible to get a squary list by adding only two elements:

n1=1−S(E)
n2=−SP(E+[n1])
After adding n1
, we have
S(E+[n1])=1
After adding n2
, we have
SP(E+[n1,n2])=SP(E+[n1])+n2⋅S(E+[n1])=SP(E+[n1])+(−SP(E+[n1]))⋅1=0
Thus, the two numbers satisfy the condition SP(E′)=0
. We can also see that since the numbers in the original list are each of magnitude no greater than 103
, |n1|≤106+1
, and |n2|≤2⋅1012
, both well within the limits.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 1C 2022 - Code Jam 2022

Intranets (17pts, 27pts)

We use terminology for graphs. The vertices are the M
 machines and the edges are the (M2)
 links.

Test Set 1
Let's consider the process of assigning priorities to the edges one by one, from the highest priority to the lowest. For simpicity, we number the i
-th highest priority as priority i
 so that the smaller the number, the higher the priority.

Suppose we have assigned the highest i
 priorities to i
 edges, and we need to assign the priority (i+1)
 to a new edge. How can we do?

We have three types of choices. Suppose the new edge with the priority (i+1)
 is (u,v)
. Let Si
 be the set of vertices that the edges with highest i
 priorities connect with.

If the vertices u
 and v
 don't occur in the previously assigned edges. I.e. u∉Si,v∉Si
, the number of such edges is (M−|Si|2)
. In this case, u
 and v
 form a new intranet. So, the number of intranets increase by 1
, and |Si+1|
 equals |Si|+2
.
If u∈Si,v∉Si
, the number of such edges is |Si|⋅(M−|Si|)
. In this case, v
 joins the intranet in which u
 is located. So, the number of intranets does not change, and |Si+1|
 equals |Si|+1
.
If u∈Si,v∈Si
, the number of such edges is (|Si|2)−i
. In this case, the edge is not activated. So, the number of intranets do not change, and |Si+1|
 equals |Si|
.
Then, we can use a dynamic programming approach to solve Test Set 1. Let dp(i,j,k)
 denote the probability that we assign the highest i
 priorities to the edges, the set of vertices introduced by these i
 edges has the size of j
, and k
 intranets have been formed. The transition function can be deduced from the discussion above. The time complexity is O(M4)
 and it is enough to pass Test Set 1.

We can speed this up to O(M2)
 by dropping i
 from the keys. Let dp(j,k)
 denote the probability that after assigning the highest i
 priorities for some i
, the set of vertices introduced by these i
 edges has the size of j
, and k
 intranets have been formed. The transition is to assign the highest priority among the edges of the types 1 and 2 above. The probability of there being a new intranet is (M−|Si|2)(M−|Si|2)+|Si|⋅(M−|Si|)
 as we are no longer interested in priorities of the edges of the type 3 above.

Observing the graph
From the solution for Test Set 1, we see that each intranet corresponds to a pair of vertices (u,v)
 such that both u
 and v
 activate the edge (u,v)
. This fact can also be proved directly, and we describe here.

Let's fix an assignment of priorities and consider the directed graph where the vertices are the machines and the edges are (u,v)
 such that machine u
 uses the links connecting machines u
 and v
. Since this graph is a functional graph (each vertex has outdegree 1
), each connected component contains exactly one cycle (and possibly some chains of nodes leading into the cycle).

A crucial observation is that the length of a cycle cannot be 3
 or more. Assume the contrary: that vertices u1,…,uc
 (c≥3
) form a cycle in this order, and let the priority of the link connecting ui
 and ui+1
 be pi
 (indices are modulo c
). Note that those links are distinct. Since machine ui
 uses the link with highest priority, we have pi<pi−1
. This gives us p1>p2>⋯>pc>p1
, a contradiction. As a self loop is also impossible, we conclude that every cycle in the graph has length 2
.

Test Set 2
Let's call the set of edges activated by both of their endpoints an active matching as they form a matching. Now the problem is to compute the probability that the size of the active matching is exactly K
.

For a matching X
, let f(X)
 be the probability that the active matching is X
. Since it is not easy to compute f(X)
 directly, we apply inclusion‐exclusion principle technique and so let g(X)
 be the probability that the active matching contains X
, that is, g(X)=∑Y⊇Xf(Y)
. By inverting this, we have f(X)=∑Y⊇X(−1)|Y|−|X|g(Y)
. Our answer is then calculated as follows:
∑|X|=Kf(X)=∑|X|=K∑Y⊇X(−1)|Y|−|X|g(Y)=∑|Y|≥K(|Y|K)(−1)|Y|−Kg(Y)=∑i≥K(iK)(−1)i−K∑|X|=ig(X)
All that remains is to compute g(X)
. Of course, this depends only on |X|
, and we want to compute it when |X|=i
, for each i=K,K+1,…,M
. There are i!
 possible orders of priorities assigned to X
. Let's fix one of them and let the edges in X
 be (u1,v1),(u2,v2),…,(ui,vi)
 from the lowest priority to the highest. Then the condition that the active matching contains X
 is equivalent to, for each j=1,2,…,i
, edge (uj,vj)
 has the highest priority among edges touching at least one of u1,v1,…,uj,vj
. Hence we have
g(X)=i!∏j=1i1(M2)−(M−2j2).
Here we note that the denominators can be factorized to see that they are not divisible by 109+7
. The number matchings of size i
 is 1i!2iM!(M−2i)!
, and this completes the O(M)
 time solution (the divisions can be done efficient by using the factorization, though this is not necessary).

In fact, we can find the answers for K=1,…,⌊M2⌋
 at the same time in O(MlogM)
 time: The transformation from g
 to f
 can be represented by a convolution and we can use FFT.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2022 - Code Jam 2022

Spiraling Into Control (3pts, 4pts, 13pts)

Test Sets 1 and 2
You may or may not have run across the old chestnut of a technical interview question that asks you to number all of the cells of a grid in a spiral pattern. In this problem, doing that is potentially helpful for the first two test sets. However, the last test set can be (and must be) solved without explicitly creating and numbering a spiral. (For a case with N=9999
, we would need to store and number almost 108
 cells!) We'll worry about that later in the analysis.

To make a numbered spiral, we can create a grid and loop through it, starting from the upper left cell, and making a 90 degree turn to the right each time we encounter a grid boundary or an already-numbered cell. One clean way to do this is to use an array of "directions": ((0,1),(1,0),(0,−1),(−1,0))
, where the starting direction (0,1)
 means stay in the same row and move one column right, and so on, with the other three referring to moves downward, leftward, and upward, respectively, in the grid. When our current direction is (Δr,Δc)
 and we are at cell (r,c)
 in the grid, we check cell (r+Δr,c+Δc)
 to see if it is outside the grid or has already been numbered. If so, we choose the next direction in the direction vector, looping back to the start if we go off the end, and then calculate (r+Δr,c+Δc)
 using the new (Δr,Δc)
. Then, whether or not we changed direction, we label the current cell, increment our label counter, and proceed to the cell (r+Δr,c+Δc)
. We stop when we label the N2
-th cell.

For Test Set 1, we can exhaustively enumerate all legal paths, using our spiral numbering to determine which moves are allowed. We can keep track of the shortcuts taken in each possible path, and see whether any path finishes in exactly K
 moves. Since there is a shortcut to take (or not take) in almost every cell, this solution is exponential.

For Test Set 2, we can refine this idea to avoid explicitly considering all possible paths. For each cell in the grid, we create an array that can hold up to one path for every possible number of moves "so far". Then we proceed along the spiral in consecutive numerical order. For every cell we visit, for every path in that cell's array, we try to extend it into all legal neighboring cells.

For example, in a 5×5
 grid, we start at cell 1 with only a way to get there in 0 moves. We tell each of the legal neighboring cells (2 and 16) that we have a way to get there in 1 move, starting from cell 1. Then, in cell 2, we tell each of cells 3 and 17 that we have a way to get there in 2 moves, with the starting prefix 1,2
, and so on. The key difference from the Test Set 1 solution is that when a cell is offered a path and it already has a path with exactly that number of moves, it does not store the new path. This cuts down on the proliferation of possible paths.

And so we have a dynamic programming / memoization solution. Since there is at most one shortcut to take in each cell, we do constant work per cell; since we are populating a table that is N2×N
, this solution takes O(N3)
 time.

Test Set 3
To be able to handle grids with N
 up to 9999, we need to do three things efficiently:

Given a value of K
, determine whether a path with exactly K
 moves exists.
Find such a path.
Given the coordinates of a grid cell, find its number in the spiral.
We can begin by observing that the following values of K
 are IMPOSSIBLE:

K<N−1
. In this case, even if we move as directly as possible toward the center (taking only shortcuts), there are not enough moves to get there.
Odd K
. We can see this via a "checkerboard argument". Imagine that the grid has a checkerboard pattern, with the upper left cell being black. Then the diagonal running from the upper left cell to the lower right cell is entirely black, and so the central cell is black. Now, each move is in an orthogonal (i.e., "compass") direction, so each move either takes us from a black cell to a red cell or vice versa. Therefore, to end on the central cell, which is black, we must make an even number of moves.
As it turns out, these are the only impossible cases! To see why, it helps to think of the spiral as consisting of concentric square rings, with "ring" 0 being just the central cell, ring 1 being the eight cells around that, ring 2 being the sixteen cells around ring 1, and so on. For example, the rings in the N=7
 grid look like this:

  3333333
  3222223
  3211123
  3210123
  3211123
  3222223
  3333333
In ring 1, there are three possible shortcuts: we can move into ring 0 from the top, right, or bottom, and this will save us 6, 4, or 2 moves, respectively, compared to just walking through all of ring 1 (and into ring 0) without taking a shortcut. But we can only take one of these shortcuts.

In ring 2 (and beyond), for simplicity, let's only consider taking shortcuts that are in the same row or column as the central cell. There are four such shortcuts, and they will save us 14, 12, 10, or 8 moves, respectively. Notice that if we take the 8-move shortcut from ring 2, for example, we cannot use any of the shortcuts in ring 1. However, if we take the 14-move shortcut from ring 2, we will still have access to all three shortcuts in ring 1.

Similarly, in ring 3, the shortcuts save us 22, 20, 18, or 16 moves, and if we take the first of these, we still have access to all four shortcuts in ring 2.

We can turn these observations into a constructive solution for any even K
 that has an answer:

If we want to save between 2 and 22 moves, we take the specific shortcut with that savings.
If we want to save between 24 and 36 moves, we can take the 22-move shortcut and then take the specific shortcut (from ring 1 or 2) that gets us the remaining savings.
If we want to save 38, 40, or 42 moves, we can take the 22-move and 14-move shortcuts and then the 2, 4, or 6-move shortcut from ring 1.
We already established that we cannot save 44 moves or more (since then K
 would be less than N−1
).
Generalizing this strategy: we can find the number of moves we need to save (which is N2−1−K
; call it s
), and then proceed from the outermost ring to the innermost. At each ring r
,

If s
 is greater than or equal to the number of moves saved by the largest shortcut (i.e., 8r−2
 moves), we take that shortcut, subtract its savings from s
, and proceed to the r−1
-th ring.
If s
 is equal to the number of moves saved by some other shortcut in ring r
 (i.e., 8r−4
, 8r−6
, or 8r−8
), we take that shortcut and stop. (We must be careful not to do this when 8r−8=0
, since that move — from cell N2−1
 to cell N2
 — saves us nothing and is not a shortcut!
Otherwise, we reject all shortcuts in ring r
, and proceed to ring r−1
.
All that remains is to find the room numbers for the shortcuts that we take. Instead of trying to generate the entire spiral, we can notice that, say, the upper left corners of the rings (starting in the central cell and going outward) have values N2,N2−8,N2−8−16,N2−8−16−24,
 etc. We can even turn this into a formula: the upper left cell of ring r
 has number N2−8∑ri=0i=N2−4(r)(r+1)
.

Once we have the number of the upper left cell of a ring, it is not too hard to find the values of the cells we are using for shortcuts in that ring (i.e., the ones in the central row or column). If the upper left cell's number is x
, the shortcuts have numbers x+r,x+3r,x+5r,x+7r
.

Because there are N+12
 rings in total, and we do O(1)
 work in each of them, this solution is O(N)
 and easily passes Test Set 3. There are values of K
 that require a shortcut to be used in every ring (except the central cell), so we can't do better than this in general.

It could have been worse...
We thought about presenting this problem in a different way, without mentioning spirals: starting from the upper left cell of an N×N
 grid, give a path that reaches the central cell in exactly K
 moves, or say it is impossible. One solution is to come up with the spiral path and strategy from this problem! But there were many other time-wasting roads to go down there, and the problem was already challenging for the first problem of a Round 2.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2022 - Code Jam 2022

Pixelated Circle (5pts, 16pts)

Let C and Cw be the set of pixels colored by draw_circle_filled(R) and draw_circle_filled_wrong(R), the number of pixels that have different colors in these pictures would be the cardinality (size) of the symmetric difference of C and Cw, that is: |CΔCw|=|(C∖Cw)∪(Cw∖C)|.

Test Set 1
For test set 1, R is small enough to build the sets of colored pixels from draw_circle_filled(R) and draw_circle_filled_wrong(R) with hash set of tuples. By implementing the pseudocode in the problem statement, the time complexity would be O(R2) to get all colored pixels and O(R2) to compute the symmetric difference of the two sets.

Test Set 2
The key observation for optimizing the solution is that for any R, every pixel colored by draw_circle_filled_wrong(R) is also colored by draw_circle_filled(R), that is Cw⊆C. Therefore, we can simplify the size of symmetric difference to:

|CΔCw|	=|(C∖Cw)∪(Cw∖C)|
=|(C∖Cw)∪∅|
=|C|−|C∩Cw|
=|C|−|Cw|
 
which means we can count the number of pixels colored by draw_circle_filled(R) and draw_circle_filled_wrong(R) separately, and the answer would be the difference between these two numbers. The proof of this observation is given at the end of this analysis.

Count pixels colored by draw_circle_filled(R)
To get the number of pixels colored by draw_circle_filled(R), we need to iterate through all possible values of x and find ymin and ymax for each x which satisfy round(
√
x2+y2
)≤R for all ymin≤y≤ymax. A solution for them is ymax=floor(
√
R+0.5
2−x2) and ymin=−ymax. Therefore, we can get the number of colored pixels with a for-loop for the following equation:

|C|=∑
R
x=−R
floor(
√
(R+0.5)2−x2
)×2+1
Time complexity: O(R)
Count pixels colored by draw_circle_filled_wrong(R)
draw_circle_filled_wrong(R) is composed of draw_circle_perimeter(r) calls with r from 0 to R. Notice that pixels colored by draw_circle_perimeter(r1) and draw_circle_perimeter(r2) never overlap if r1≠r2. Based on this observation, we can count the number of pixels colored by each draw_circle_perimeter(r) separately and sum them up to get the total number of colored pixels. The proof of this observation is given at the end of this analysis.

Looking into function draw_circle_perimeter(r), we can break the colored pixels into 4 quadrants and count them separately. Since the colored pattern is symmetric to both x-axis and y-axis, we just need to count the pixels in Quadrant 1 (Q1), and the total count excluding the origin pixel would be that number times 4, and plus 1 to include the origin pixel.

For r≥1, the colored pixels in Q1 are symmetric to the line x=y, and there are exactly xt colored pixels between y-axis and (xt,yt), the closest point to line x=y (above or on the line, xt≥yt). Since x=y is at 45∘ to the x-axis, the integer xt would be either ceil(r/cos(45∘)) or floor(r/cos(45∘)). We can compute the corresponding yt=round(
√
r2−x
2
t
) and choose the closer one above or on the line x=y. Afterwards, the number of colored pixels in Q1 including x-axis would be 2×xt+1, and minus 1 if (xt,yt) lies on the line x=y since it is not mirrored in this case.

Time complexity: O(1) for counting pixels colored by draw_circle_perimeter(r), and O(R) for all colored pixels.

Proof: Cw⊆C
For every positive R, r and x such that 0≤r≤R and −r≤x≤r, we want to prove that the following inequality always satisfies:

round(
√
x2+round(
√
r2−x2
)2
)≤r
⟺	
√
x2+round(
√
r2−x2
)2
−0.5≤r
⟺	
√
x2+round(
√
r2−x2
)2
≤r+0.5
⟺	x2+round(
√
r2−x2
)2≤r2+r+0.25
⟺	x2+(
√
r2−x2
+0.5)2≤r2+r+0.25
⟺	r2+
√
r2−x2
+0.25≤r2+r+0.25
 
which always holds since 
√
r2−x2
≤
√
r2
=r when |x|≤r and r≥0.

Secondly, y=round(
√
r2−x2
)≤round(
√
r2
)≤R. Therefore −R≤y≤R always holds.

The proof above shows that pixels colored by draw_circle_filled_wrong(r) with 0≤r≤R also satisfy the coloring condition in draw_circle_filled(R), which implies Cw⊆C.

Proof: draw_circle_perimeter(r1) and draw_circle_perimeter(r2) never overlap
For the first coloring statement in draw_circle_perimeter(r), we want to prove that given a fixed x, the inequality y1=round(
√
r
2
1
−x2
)≠y2=round(
√
r
2
2
−x2
) for any pair of integers r1 and r2 such that r1>r2≥0 and |x|≤r2 is always true:

|
√
r
2
1
−x2
−
√
r
2
2
−x2
|	
=	
√
r
2
1
−x2
−
√
r
2
2
−x2
r1>r2
≥	
√
r
2
1
−x2
−
√
(r1−1)2−x2
r1 and r2 are integers
≥	(
√
r
2
1
−
√
x2
)−(
√
(r1−1)2
−
√
x2
)	for any |x|≤r1−1
=	r1−x−(r1−1)+x
=	1
 
Based on the proof above, we can further get:

round(
√
r
2
1
−x2
)−round(
√
r
2
2
−x2
)
≥	round(
√
r
2
1
−x2
)−round(
√
(r1−1)2−x2
)
≥	round(
√
r
2
1
−x2
)−round(
√
r
2
1
−x2
−1)
≥	1
 
Therefore y1≠y2 always holds for all r1>r2≥0 given a fixed x such that |x|≤r2. For the cases that r2<|x|≤r1, the pixels will nevery satisfy the coloring condition in draw_circle_perimeter(r2).

We can further extend the proof above for the 2nd, 3rd, and 4th coloring statement in draw_circle_perimeter(r) and prove that pixels colored by draw_circle_perimeter(r1) and draw_circle_perimeter(r2) never overlap.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2022 - Code Jam 2022

Saving the Jelly (10pts, 18pts)

Test Set 1
With N≤10
, it is possible to use dynamic programming with bitmasking, with one bit for each candy and each student remaining. This effectively lets us try all possible orderings of students as well as all possible ways to break ties. The total complexity is O(N2×22×N)
. This runs in time because we only consider states where the same number of candies and students have been matched.

Test Set 2
Firstly, lets construct a bipartite graph with the N
 children and the N
 candies as vertices (not including Mr Jolly's blueberry jelly). Add an edge from child a
 to candy b
 if child a
 is equally close or closer to b
 than the blueberry jelly.

It's clear that a necessary (but perhaps not sufficient) condition for Mr Jolly to get his blueberry jelly is that the graph has a perfect matching.

It turns out, this is also a sufficient condition. Let's try to show this by turning a perfect matching into an order that Mr. Jolly can call the children's names.

Firstly, if there is a child who is matched to the candy that is closest to them, then we can call that child's name and remove them and the candy from the graph.

Otherwise, we are in the situation where every child is matched to a (ungrabbed) candy that is not the closest one to them. Then, we will find a cycle in the graph using the following procedure. Pick an arbitrary child a
 to start at.

Find the candy b
 that is closest to child a
. Go to b
. This edge is guaranteed not to be part of the current matching.
Find the child a
 that is currently matched to candy b
. Go to a
. This edge is in the current matching.
Eventually, this process will create a cycle of even length (not necessarily including the child we started with). Because we alternated between edges in the matching and edges not in the matching, we can swap the matched edges/unmatched edges to create a new perfect matching. Note that in doing so, all the children in the cycle are now matched to the closest candy to them, so at least one child's name can now be called.

We have shown that a perfect matching is a necessary condition, and that an order can be constructed from a perfect matching that satisfies Mr. Jolly's requirements, thus proving that a perfect matching exists if and only if Mr. Jolly's requirements can be satisfied.

A perfect matching can be found in O(N2N−−√)
 using Hopcroft-Karp, and constructing a solution can be done in O(N2)
 if implemented with some care.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 2 2022 - Code Jam 2022

I, O Bot (11pts, 20pts)

There is no value in carrying balls across the origin without depositing them into the warehouse, therefore, collecting the balls with positive coordinates Xi and those with negative coordinates are two similar but independent tasks. Hence, in what follows, we assume that Xi>0 for all i. Moreover, let us assume that the balls are sorted in ascending order by Xi.

A solution to the problem consists of a number of passes or round-trips from the origin and back with one or two balls collected in each pass. The time required to collect a single ball i in a pass is 2Xi. The time required to collect two balls i and j is 2×max(Xi,Xj) if the balls are of different shapes and 2×max(Xi,Xj)+C otherwise. We say that two balls i and j are matched (and write (i,j)) if they are collected in the same pass. Since the order of passes is not affecting the overall time for collecting all balls, we can equivalently think of the problem as one of finding an optimal matching of balls.

The following observation will be useful throughout the analysis.

Observation 1: Suppose we want to collect the first i balls (i≥2) and Si≠Si−1. In an optimal matching, the i-th ball is matched with the (i−1)-th ball.

Proof: Consider any matching of balls, where i-th ball is not matched with (i−1)-th ball, and assume that i-th ball is a 0.
1. If none of the two balls is matched, we can match the balls and save 2Xi−1 seconds.
2. If there is a matching (i−1,j), j<i−1, and i-th ball is not matched, then we can match (i−1)-th ball with i-th ball instead and save at least 2×(Xi−1−Xj) seconds (2×(Xi−1−Xj)+C, if j-th ball is 1⁠-shaped).
3. Similarly, if there is a matching (i,j), j<i−1, and (i−1)-th ball is not matched, we can match i-th ball with (i−1)-th ball instead and, again, save at least 2×(Xi−1−Xj) seconds.
4. Lastly, if there are matchings (i,j) and (i−1,k), j<i−1 and k<i−1, then we can rearrange the matchings as (i,i−1) and (j,k) saving at least 2×(Xi−1−max(Xj,Xk)) seconds.

Test Set 1
Observation 1 helps us match the balls if the last two balls have different shapes. But what if they have the same shape, say a 0?

Observation 2: Suppose we want to collect the first i balls (i≥2) and Si=Si−1=0. There is an optimal matching of balls such that one of the following conditions holds:
1. The last two 0⁠-shaped balls i and i−1 are matched.
2. There is a matching (i,j) with Sj=1 and, for all k∈[j+1,i]⁠, Sk=0. In other words, i-th ball is matched with the nearest 1⁠-shaped ball on its left.
3. There are no 1⁠-shaped balls and i-th ball remains unmatched.

Proof: The full proof is a lengthy case analysis, which we omit here. The idea is that matching i-th ball with the rightmost ball of a particular shape is generally at least as good as matching with another ball of that shape. For example, suppose that i-th ball is matched with a 1⁠-shaped ball l such that there is another 1⁠-shaped ball j with l<j<i. If the ball j is unmatched, we can match the ball i with j instead and save 2×(Xj−Xl) seconds. Otherwise, if the ball j is matched with some other ball k, we can swap the roles of balls l and j and create the matchings (i,j) and (k,l) obtaining the same overall time (if k>j) or better.

This means that we can try matching the last 0⁠-shaped ball with the 0⁠-shaped ball before or the rightmost 1⁠-shaped ball (if any), and at least one of these moves will be optimal.

The image shows the last five of i balls. The last three are 0-shaped, the remaining two are
            1-shaped. The i-th ball is connected to (i-1)-th and (i-3)-th balls with lines.

The two observations lead to a dynamic programming solution. Let dp[i][j] be the optimum time to collect the first i 0⁠-shaped balls and the first j 1⁠-shaped balls. The base case is dp[0][0]=0. For i+j>0, suppose again that the rightmost of these i+j balls is 0⁠-shaped and it has the coordinate x. The case when the rightmost ball is 1⁠-shaped is symmetric. To eliminate some other corner cases, dp[1][0]=2x, dp[i][0]=min(dp[i−1][0],dp[i−2][0]+C)+2x for i≥2, and dp[1][j]=dp[0][j−1]+2x for j≥1. For the general case with i≥2 and j≥1, if the penultimate ball is 1⁠-shaped, then dp[i][j]=dp[i−1][j−1]+2x (Observation 1). Otherwise, we can choose to match the last 0⁠-shaped ball with the previous 0⁠-shaped ball or the rightmost 1⁠-shaped ball (Observation 2), namely, dp[i][j]=min(dp[i−2][j]+C,dp[i−1][j−1])+2x.

The final answer is dp[N0][N1], where N0 and N1 denote the total number of 0⁠-shaped and 1⁠-shaped balls, respectively. The time complexity of this algorithm is O(N2).

Test Set 2
Using dynamic programming from a different angle, we can solve the problem in linear time, apart from the initial sorting. Let dp[i] be the optimum time to collect the first i balls. As the base cases, dp[0]=0 and dp[1]=2X1. To calculate dp[i] for i≥2, suppose once more that the i-th ball is 0⁠-shaped. If the (i−1)-th ball is 1⁠-shaped, we can match the last two balls and dp[i]=dp[i−2]+2Xi (Observation 1). Otherwise, using Observation 2, we have the options to match the last two 0⁠-shaped balls and collect all balls in dp[i−2]+C+2Xi seconds, or to match i-th ball with the rightmost 1⁠-shaped ball j. The dynamic programing recurrence is not obvious in the latter case, though, as we do not know the optimum matching for the first i−1 balls except for ball j. What happens to the 0⁠-shaped balls in-between j and i? We are missing another key observation here.

Observation 3: If there is an optimal matching of the first i balls such that the 0⁠-shaped ball i is matched with the rightmost 1⁠-shaped ball j and i−1≠j, then the 0⁠-shaped ball i−1 is not matched with another 0⁠-shaped ball.

Proof: Assume on the contrary that we have two pairs of matched balls (i,j) and (i−1,k), k<i−1, such that ball k is 0⁠-shaped. These two matched pairs contribute 2Xi+2Xi−1+C seconds to the overall matching cost. But then we can rearrange the matchings as (i,i−1) and (j,k) costing us only 2Xi+C+2×max(Xj,Xk) seconds, which is 2(Xi−1−max(Xj,Xk)) seconds less. This contradicts the optimality assumption of the given matching.

It follows from Observation 3 that the 0⁠-shaped ball i−1 must be matched with another 1⁠-shaped ball, specifically the rightmost unmatched 1⁠-shaped ball. And we can extend this argument and repeatedly match 0⁠-shaped balls with 1⁠-shaped balls sweeping leftward for as long as there is another 0⁠-shaped ball to the right of a matched 1⁠-shaped ball. This process is ilustrated in the drawing below.

The image shows the last 12 of the first i balls with the following shapes:
            ??1111001000. ? stands for Undefined. The last 0-shaped ball is labeled i. The
            last 1-shaped ball is labeled j. The second ball with unspecified shape is labeled k.
            Lines between balls indicate matchings (i,i-3),(i-1,i-6),(i-2,i-7),(i-4,i-8), and
            (i-5,i-9).

Let k be the rightmost unmatched ball after the above 0⁠-⁠1 matching process. There are no shape changes in the set of balls {k+1,k+2,…,i} and the cost of collecting those balls is twice the sum X0−shaped(k+1,i) of x-coordinates of 0⁠-shaped balls in {k+1,k+2,…,i}. Therefore, the cost of collecting all i balls in this way is dp[k]+2×X0−shaped(k+1,i).

X0−shaped(k+1,i) can be calculated in O(1) time using prefix sums. But how do we get the index k efficiently without actually carrying out the matching process? Note that k is the largest index such that k<i and the set {k+1,k+2,…,i} contains equal number of 0⁠-shaped and 1⁠-shaped balls. Consider the balance bi of 0/1 balls at each index i, namely, bi=zi−oi, where zi and oi is the number of 0⁠-shaped and 1⁠-shaped balls in the set {1,2,…,i}. The set {k+1,k+2,…,i} has equal number of 0⁠-shaped and 1⁠-shaped balls if and only if bk=bi. The index k can be looked up in O(1) time if we maintain a hash-table of indices, when each balance was last registered. If the current balance bi is seen for the first time, it means that there are not enough 1⁠-shaped balls to match all 0⁠-shaped balls with, and we can choose k=0.

We are performing a constant number of operations at each index in this approach, so the overall time complexity is dominated by the sorting, thus O(NlogN).

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2022 - Code Jam 2022

Revenge of GoroSort (8pts, 10pts, 3pts)

In the original GoroSort problem from 2011, Goro could hold as many elements in place as he wanted; using the terminology of this problem, he could create arbitrary many box color groups with 1
 ball (element) each. But unlike in this problem, he was only allowed to use at most one color group of size greater than 1
. That made the optimal strategy relatively straightforward: he could repeatedly permute all list elements that were not in the correct places.

That strategy puts one additional element in the right place each turn, in expectation, which is clearly too slow for this problem. (Our version of that solution takes almost 100000
 rounds!) What is more surprising is that the crux of the original problem — namely, thinking in terms of expected numbers of elements that become correctly placed after the bump — can even be misleading in this problem!

The perils of pairing
It is intutively clear that any elements that are already in the right place should be left alone (i.e., each should be put in its own color group). It also seems reasonable to not break up pairs of elements that are swapped (i.e., each is in the other's place). We can also reasonably guess that splitting such a pair would be useless, and that putting additional elements in that group would be worse than handling those other elements in their own group(s).

What about the other elements? One very tempting strategy is to try to pair them up into "trespasser pairs" such that one member of the group is in the other member's place. It may not be possible to do this with every element, but we can, e.g., find a way to tack leftovers onto existing trespasser pairs.

Consider a trespasser pair (x,y
), where x
 belongs where y
 is and y
 belongs somewhere else entirely. The judge's "bumping" (permutation) process will put x
 in the right place with probability 12
. So if we somehow manage to create around N2
 trespasser pairs, each bump will put around N4
 more elements in the right places. This seems quite good, and it is good enough to pass Test Set 1, but not the other two. (Our trespasser-pair-based solutions take between 15000
 and 16300
 rounds, depending on the care taken with implementation details.)

Thinking in terms of cycles
Any permutation — and therefore any state in this problem — can be described as a multiset of cycles of particular lengths. For example, each element that is already in the correct place is a 1
-cycle, and each pair of swapped elements is a 2
-cycle.

If we try to implement the pairing idea above in a greedy way, we may miss some opportunities to form trespasser pairs. We can get as many as possible by first finding all the cycles, then going through each cycle and chopping it into trespasser pairs, plus perhaps one "trespasser trio" at the end. (A trespasser trio is a set of elements x,y,z
, in some order, such that y
 is in x
's place, z
 is in y
's place, and z
 belongs outside of the group.) These improvements (especially creating trespasser trios where needed) help a lot, but not enough to pass Test Set 2; our trespasser pairs + one trio solution takes around 13100
 rounds.

But what if we leave the cycles as they are, and make each cycle its own permutation group? Now we do much better, and pass Test Set 2 as well (but not Test 3; we take about 11800
 rounds). Why is this cycle-based strategy so much better than the trespasser pair strategies?

When expectation doesn't satisfy our expectations
Here's the problem with chopping into trespasser pairs. Suppose that we have a 4
-cycle like 2341
. If we split it into two trespasser pairs 23
 and 41
, we will (in expectation) put one element in the correct place. But, as we will see later on, we get the same expectation of 1
 if we designate the entire cycle as one group.

Does this necessarily mean these strategies are equally good? Let's set our expectations appropriately! We really care about the expected total number of rounds to finish sorting, so let's work toward calculating that instead. First we observe that:

If we split the 4
-cycle into two trespasser pairs:
With probability 14
, both elements end up in their correct places in the group, and we get two 1
-cycles and one 2
-cycle.
With probability 14
, neither element ends up in its correct place in the group, and we end up stuck at a 4
-cycle.
Otherwise, with probability 12
, we end up with a 1
-cycle and a 3
-cycle.
If we don't split the cycle before permuting:
With probability 124
, we get four 1
-cycles.
With probability 14
, we get two 1
-cycles and a 2
-cycle.
With probability 13
, we get a 3
-cycle and a 1
-cycle.
With probability 18
, we get two 2
-cycles.
With probability 14
, we are stuck at a 4
-cycle.
Comparing these two probability distributions directly, and canceling out the parts that are the same, we need to know which is better:

a 16
 probability of getting a 3
-cycle and a 1
-cycle, or
a 18
 probability of getting two 2
-cycles and a 124
 probability of getting four 1
-cycles
Now we can assess each of those states in terms of the expected number of rounds to completion. A 3
-cycle turns out to take 3
 rounds, in expectation, to become all 1
-cycles. (This comes from solving E[3]=1+13⋅E[3]+12E[2]+16(0)
, and using the fact that E[2]=2
.) By a similar analysis, two 2
-cycles take 83
 rounds, in expectation, to become all 1
-cycles. (And if we are lucky enough to reach four 1
-cycles directly, 0
 additional rounds are needed.)

Therefore, chopping up a 4
-cycle into two trespasser pairs is strictly worse than leaving it as is! We wouldn't have known this if we had argued purely based on the expected number of correctly placed elements; it also matters what we leave behind. Intuitively, the trespassers can only be dealt with after their companions have been correctly placed, so the chopping strategy is less parallelizable. But we shouldn't assume this will always be true no matter how we chop...

Chopping is not always bad
It's tedious to perform the above analysis for cycles longer than 4
. But we shouldn't give up hope. The expectations for the two strategies were the same for 4
-cycles, but will that hold up in general?

Let's think about how many cycles we expect to see in a random permutation of length N
. You may have heard of the following problem: everyone loses their hats all at once, and each person puts on a random hat; in expectation, how many people get their own hats back? The probability that the each person gets their own hat is 1N
, and then by linearity of expectation, the total number of instances of someone getting their own hat is 1N⋅N=1
.

So this gives us the expected number of 1
-cycles. We can make a similar argument about 2
-cycles: there are (N2)
 distinct pairs of elements that could form a 2-cycle, and for each one, the probability that each has the other's element is 1N⋅1N−1
. This all boils down to 12
. Indeed, the answer for general k
-cycles is 1k
.

Then how many total cycles should we expect? It's 11+12+...+1N
. You may recognize this as the expression for the N
-th harmonic number. The harmonic numbers grow rather slowly; H100
, for example, is just over 5
.

Therefore, if there are only around 5
 cycles in our initial random permutation of length N=100
, we expect to place around 5
 elements. But if we chop the cycles into, say, 50
 trespasser pairs, we should expect to place around 25
 elements! How can it possibly be better to leave such large cycles alone? Does the argument about leaving more mess behind really still hold up?

One issue with chopping into pairs is that, intuitively, many roads to completion include forming one or more 4
-cycles, and we have now seen that the pair-chopping strategy mishandles them. (And now we have reason to suspect that it mishandles larger cycles as well).

What if we chop a cycle into chunks larger than pairs? For example, if we chop a 6
-cycle into two trespasser trios, each of them will produce 2⋅13=23
 correctly placed elements in expectation. That's 43
 overall, which is less than the 3⋅12=32
 we would have gotten from chopping into trespasser pairs. But these trespasser trios leave less of a mess; each one can only leave one element to clean up later, so there will be two such elements as compared to three.

It is hard to directly weigh the costs of cleaning up these leftover elements against the benefits of correctly placing more elements in expectation. The math for our 4
-cycle example was already a bit cumbersome to carry out in a timed round, and there is not a strong incentive to do even more of it when the local testing tool can quickly tell us how well a strategy does, and when all three test sets are Visible. Indeed, we can find that leaving all cycles of length less than 6
 intact, and breaking all cycles of length 6
 or more into trespasser trios (or trespasser sets of four or five, when there are extra elements) does much better than leaving all cycles intact, and lets us pass Test Set 3, taking about 10950
 rounds.

We can do even better by choosing different chunk lengths (e.g. break each cycle of length c
 into chunks of size roughly c√
, to get down to around 10500
 rounds), but that is not necessary for this problem.

Round 3 2022 - Code Jam 2022

Duck, Duck, Geese (12pts, 13pts)

Test Set 1
Given the lower limits for N
, we can check all possible contiguous subsets (as long as we check each one quickly). One way is to iterate through all possible starting indices for the contiguous subset. Then, for each starting index, we can loop through all possible sizes in order.

We can keep track of the counts of each color as well as the number of hat colors have counts that are invalid (not 0
 and not in the acceptable range for that color). Note that when we extend our subset, only the hat color that the newly added child has is affected. If this changed whether or not this color is valid, we update the count of valid and invalid colors as needed. Then, if this count is equal to C
, we can increment our total answer (as long as the subset is of length at least 2
 and at most N−1
).

Because checking each contiguous subset is done in O(1)
 time and there are O(N2)
 subsets to check, our time complexity for Test Set 1 is O(N2)
.

Test Set 2
For Test Set 2, N
 is too large to check each contiguous subset separately. We can speed this up by using a Segment Tree to check all possible subset lengths at once (for each starting index). We can remove the cyclic part of the problem by appending the array to itself.

Given a starting index, S
, each color has two (possibly empty) ranges of ending indices that are valid for this color (meaning the number of hats of this color is either 0 or in the acceptable range). If we use add 1
 to our segment tree for each value in these ranges and for all colors, we can count the number values (ending indices) in our segment tree for the range [S+1,S+N−2]
 that have a value of exactly N
. This count will tell us how many contiguous subsets are valid for our current start index.

Now, all we need to do is update our segment tree when moving the starting index to the right. Notice that moving our starting index to the right by one will only affect the valid end index ranges for the hat color of the child being removed (the one that previously was our starting index). If we precompute for each position, where is the next occurrence of that child's hat color, we can compute how the valid ranges for this color will move in O(1)
. The exact implementation of this is left as an exercise to the reader.

Given that our segment tree operations each take O(logN)
 time, we can count the number of contiguous subsets for each starting index and move the starting index to the right both in O(logN)
 time. This gives us a final time complexity of O(NlogN)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2022 - Code Jam 2022

Mascot Maze (12pts, 13pts)

This is a variation of the classic graph coloring problem, where the rooms, exits, and mascots of the problem form the nodes, edges, and colors of a graph.

Cases where there is a cycle of length two (two rooms x and y where there is an exit from x to y and an exit from y to x) are obviously impossible. We will show later that these are the only impossible cases, by giving an algorithm that always produces a coloring for graphs with no cycles of length two.

Test Set 1
Given the low limits, it is possible to implement a backtracking solution: recursively try coloring each vertex in each color. There are techniques which will help speed up the solution. For example, you can keep track of the colors that shouldn't be used for a vertex (since one of its neighbors was previously colored with that color).

However, plain backtracking algorithms are unlikely to work for Test Set 2, since early color assignments might make it impossible to color later nodes, and it can take a long time until those early assignments are revisited by the algorithm.

Test Set 2
Consider the graph G' which has an edge from one node to another if there is a path of length 1 or 2 linking them in G. That is, G' contains the edge (v,w) if (v,w) is in G, or there is a pair of edges (v,x) and (x,w) in G. It is sufficient to color G' so that no adjacent pair of nodes has the same color.

Since each node in G' has outdegree at most 6 (2 nodes that are one edge away in G and 4 nodes that are two edges away in G), the average indegree must be at most 6, and so at least some individual node V must have degree at most 6 + 6 = 12. No matter what colors were chosen for this node's neighbors, there is always at least one different color for this node because it has at most 12 neighbors.

The only case when the coloring is impossible for G' is when it contains a self-loop. In such a case it is also impossible for G.

Using these observations the following algorithm can be used to color G':

Find some V with the degree not greater than 12.
Temporarily remove V (and the edges connected to it) from the graph.
Recursively color the rest of the graph (which still maintains all the described properties).
Reinsert V and color it.
Here is one way to implement this solution efficiently:

Build an adjacency list and an array of degrees for G'. Make sure to track both incoming and outgoing edges.
Run a breadth-first search to remove the vertices of degree not greater than 12 one by one. Start by adding all such vertices to the queue and use adjacency lists for efficient removal. If the degree of any neighbor of the vertex currently being removed becomes 12, add this vertex to the queue. Note that you don't need to remove the vertex from the adjacency lists (as you will need those further), just update the degree. While running BFS keep track of the order in which the vertices are traversed.
Go through the vertices in the reverse order and color each one greedly: try each color and use it if none of the neighbours still have it.
Every step of the algorithm can be done in linear time, so the overall time complexity of the solution is linear.

Some heuristic approaches, like local search, can also be made to work if implemented efficiently.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

Round 3 2022 - Code Jam 2022

Win As Second (13pts, 16pts)

Introduction
This problem describes an impartial game which can be analyzed using the Sprague-Grundy theorem.

However, the number of possible states in the game is quite big. There can be 2N
 sets of blue vertices, and even after we use the Sprague-Grundy theorem to reduce the problem to consider only connected sets of vertices, the number of states is still big. For example, a star which has the center vertex connected to N−1
 leaves has 2N−1+N−1
 connected sets of vertices.

We could go a bit further and notice that isomorphic subtrees can only be considered once, which reduces the number of states much more. We do not know the exact number, but the worst trees we were able to find had between 7 and 8 million non-isomorphic connected subtrees for N=40
. This is still quite hard to process within the time limit given that the processing for each state becomes quite expensive, with tree isomorphism involved.

So how could we make things simpler for ourselves?

Test Set 1
One approach is to try to construct a concrete tree by hand that is winning for the second player. The game for this problem was intentionally chosen in such a way that this is not easy: for example, every chain (a tree where each vertex is connected to the previous one, 1−2−3−⋯−N
) is winning for the first player, as they can take either 1 or 2 middle vertices and then make symmetric moves.

However, it turns out that for even values of N
 there are several relatively simple constructions. For example, consider a starlike tree consisting of the center vertex 1
 plus N−3
 chains attached to it, one chain with 3 edges (1−2−3−4
) and all other chains with 1 edge (1−5,1−6,…,1−N
).

Illustration of the solution for Test Set 1.

The number of chains with 1 edge is even, so if the first player takes the endpoint of one of them, we can take another one and arrive at a smaller starlike tree with the same property, unless there is just one left, in which case we simply have a single chain with 4 edges and 5 vertices and can win by taking one or three middle vertices of it.

If the first player takes the center vertex together with some neighbors, then we are left with one chain with 2 or 3 vertices, and some number of isolated vertices. We can choose either to eliminate the chain, or to reduce it to an isolated vertex, in such a way that the number of resulting isolated vertices is even and we will win.

If the first player takes two vertices of the longer chain together with the center vertex, then we are left with an odd number of isolated vertices, so we can take one of them and win.

If the first player makes the longer chain shorter, then we can take the center vertex together with some neighbors in such a way that an even number of isolated vertices remains.

If the first player makes a move in the middle of the longer chain such that we have a star plus a separate isolated vertex, then we can again take the center together with some of its neighbors in such a way that an even number of isolated vertices remains.

Finally, if the first player makes a move in the middle of the longer chain such that we have a star plus a chain with 2 vertices, then we remove one of the leaves of the star. If we and the first player keep removing star leaves with each move, then after one of our moves only one leaf will be left, so we will have two chains with 2 vertices each and we can do symmetric moves to win. And if the first player tries taking the center of the star, or do something to the chain with 2 vertices, then we can always get an even number of isolated vertices after our move.

We were not able to come up with a similar explicit construction for odd values of N
. Also, even for even values of N
 coming up with this construction was not required to solve the problem. You can find an alternative approach in the next section.

Test Set 2
What can we do when N
 is odd, or when we cannot come up with the above construction for even N
?

The key idea is to consider some very restricted class of trees, so that:

The trees in the class are diverse enough so that some of them are winning for the second player — as we have seen above, chains are not diverse enough.
The trees in the class are uniform enough so that we can implement the nim-value computation based on the Sprague-Grundy theorem without having to deal with arbitrary tree isomorphism.
The number of subtrees of the trees in the class is small enough so that the number of states that the nim-value computation needs to process is small.
We have found that many classes of trees work, for example:

Starlike trees with at most 5 chains attached to the center vertex.
Chains with at most two additional chains attached at some points of the main chain.
Starlike trees with all chains having length 1 except at most 3 (generalizing the Test Set 1 construction).
Having chosen such a class, we can then quickly implement the nim-value computation, and then either do an exhaustive search over all trees in the class, or keep generating random trees in the class until we find at least one example for each value of N
 between 30 and 40.

It was also possible to choose a class of trees with enough diversity and fast enough nim-value computation, but that would still require to implement general tree isomorphism. This would of course complicate the implementation a bit more, but still allow to solve the problem. One such class is very narrow trees where vertex i
 is connected either to vertex i−1
 or to vertex i−2
.

Having found the winning tree for each value of N
, we can then hardcode them in the solution we submit. We still need the nim-value computation code as part of the solution in order to actually play the game after printing the tree. This was, in fact, one of the reasons for making this problem interactive: had we just required printing a tree for a given N
, the submitted solutions would likely just consist of 10 constants, and it might be possible to just find a solution in a few guesses, at least for Test Set 1.

Of course, it might happen that you choose a class of trees that does not give a solution to all possible cases. However, given the diversity of classes that work in this problem, it is most likely possible to adjust your class slightly in such a way that you do not need to complicate the nim-value code too much, but that allows to find a solution. Of course, doing this quickly required a certain intuition or "feeling" of the problem.

How does the judge work?
While the contestants had the luxury of choosing a class of trees where finding nim-values is quick, they did not have to do it, and therefore the judge had to deal with arbitrary trees.

The judge had a very well-optimized computation of nim-values for the general case, using tree isomorphism to reduce the number of states it has to process. In the worst case we found for N=40
, it could find the nim-values for all states in about 70 seconds (this was optimized about 20x from our first version). However, we were not sure that the case we found is truly the worst (and this was another reason for making this problem interactive), and we also did not want the contestants to have to wait for a long time for the verdict.

Therefore we have introduced additional logic in the judge when considering the very first move of the game. We iterated over possible first moves in the order of increasing size of the largest connected component remaining after the move, and stopped iteration in two cases:

If we find a move that leads to a losing position. Then we can make this move and win, and do not need to consider other options.
If we have visited 500000 different states during our search, and have already processed at least one first move fully. Then we stopped processing the current first move and made our solution only choose between the first moves already processed.
The order of checking the first moves ensured that only a few states would need to be visited to check the first option for the first move, because all connected components would have a size of at most 20 after it, therefore we would likely process quite a few first moves before we run out of the 500000 states budget. And for the values of N
 up to 33 or so, we would actually process all options for the first move. This allowed us to be relatively confident that we would catch most incorrect solutions, but still have the judge finish in 4 seconds per case.

After the first move, the judge always knew the nim-value exactly and played optimally.

The judge had to make another important decision: when it found itself in a losing position (which is the most important case to consider, as in the other case it could guarantee a win using the nim-values), which move should it do?

Initially the judge would make a random move in such situation. But then we noticed that even for the Test Set 1 solution described above, making random moves might not be enough to catch all bugs, since there are many bugs that would only be caught with a specific first move. Therefore, we have changed the behavior of the judge for choosing the first move: it tried to make such moves that would lead to different situations for the solution to handle after the first move in different games.

Two situations were considered different if they led to non-isomorphic configurations, with the additional step of collapsing 2K+1
 copies of the same subtree to 1 copy, and 2K
 copies of the same subtree to 2 copies, to avoid the judge wasting the games exploring different ways to remove leaves from a star. After the first move, if the judge was still in a losing position, it would make fully random moves.

Of course, this still left a possibility that some incorrect solutions would win all games against the judge, as it did not visit all possible states — there were at least 7 million states to visit potentially, but the judge had only 50 games, each visiting at most 40 states. However, we hope that the chances of this happening were quite low.

World Finals 2022 - Code Jam 2022

Wonderland Chase (8pts, 22pts)

Test Set 1
The labyrinths of Wonderland can be viewed as a undirected graph. If Alice can get to a cycle before the Queen can reach her, then Alice will be safe. This is because Alice can always pick a direction of travel away from the Queen around the cycle. Conversely, if Alice cannot get to a cycle before the Queen, then the Queen will catch her after at most 2×J
 moves. Every second move, Alice may move to a node, so after 2×J
 moves Alice must have entered a cycle because otherwise the Queen would have caught Alice.

Using these observations, we can formulate a dynamic programming solution. Define a recursive function solution(alice, queen, total_moves) that is true if the Queen can catch Alice in at most total_moves with both moving optimally and false otherwise. The solution to the problem is the minimum value for total_moves such that solution(A
, Q
, total_moves) is true. If it is never true, then Alice is safe.

We can compute solution(alice, queen, total_moves) using a recurrence relation in which we try all moves for the Queen and all moves for Alice in two nested loops calling solution recursively. The Queen is always trying to make the answer true, and Alice is always tryng to make it false.

An upper bound on the complexity of our solution is O(J5)
, since there are O(J3)
 states and computing a state requires at most O(J2)
 time. A better complexity analysis is possible, but we already know this will be fast enough to pass Test Set 1.

Test Set 2
A faster solution is needed for Test Set 2. We will need some more observations. Call a node good when, if Alice reaches it before being caught, Alice will always be safe. That is, if Alice reaches a good node, then regardless of where the Queen is, Alice can always move in a way that is safe. Let's consider an algorithm for finding all good nodes.

Assuming a connected graph, leaves (nodes with degree 1) are never good because Alice can become cornered in them. We can start by deleting them. In fact, if we iteratively remove leaves until there are none left, then all the remaining nodes must be good, since Alice can never be cornered. This algorithm can be implemented in linear time using a queue of leaves and keeping track of the degree of each node throughout deletions.

Define DAu
 as the shortest path from Alice's starting node to a node u
. Likewise, define DQu
 as the shortest path from the Queen. For a node j
, if DAj<DQj
, then Alice can safely reach that node before being caught. We can prove this using a contradiction: if the Queen was able to intercept Alice anywhere on her path, then the Queen must have reached some node on Alice's shortest path before Alice, which would imply that the Queen can reach u
 first. Note that DA
 and DQ
 can be computed by running one Breadth-First Search (BFS) each and storing the resulting table of distances.

We can use our observations to solve the problem by considering a few cases. Alice will be safe if:

The graph is disconnected, meaning Alice and the queen start in two disconnected components. This can be checked by looking at either DA
 or DQ
.

Alice can enter a good node j
 such that DAj<DQj
.

Otherwise, Alice will get caught. Since Alice's strategy is to maximize the number of moves until she is caught, she will pick the junction that has the maximum distance from the Queen with the condition that she can get to it first (DAj<DQj
).

Since the solution requires only three linear passes over the graph (one to find good nodes, and two BFS runs), the total complexity is O(J+C)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2022 - Code Jam 2022

Goose, Goose, Ducks? (11pts, 24pts)

Test Set 1
Consistency in this problem has a property which is not true for most typical logic systems, in that a set of statements is consistent if and only if each subset of size 2
 of it is consistent. The left to right implication is trivial, but the converse is not, so let us prove it.

Assume you have a set of statements S
 such that any two of them are consistent. Then, for each bird b
, consider all statements that state that b
 is at a specific point in time and space. If we sort all those points by time and linearly interpolate between consecutive points, we obtain a path for b
 that is consistent with all statements in S
. Notice that, because any pair of statements is consistent — in particular, pairs of consecutive statements — the linear interpolation part does not exceed the maximum speed. In this way, we can obtain a path for each mentioned bird, all of which are consistent with all statements. Then, by definition, S
 is consistent.

Meetings also state "this bird was at this time-space point" for birds that are ducks. Thus, an analogous reasoning shows that a set of statements and known ducks is consistent with the meetings if and only if they are pairwise consistent. Since the meetings are pairwise consistent according to the limits, this leaves only pairs of two statements and pairs of a statement a meeting to check.

At this point, with the limits in Test Set 1 being so small, we can simply try everything. We know there is at least one duck, so start by trying every bird as "the first duck". Within each option, go through statements and check them against meetings (if they involve a known duck) and against previous statements. If a statement contradicts a meeting, the issuer of that statement must be a duck. If a statement contradicts a previous statement, then the issuer of such previous statement must be a duck (since ducks cannot contradict geese). Each time we find a new duck, we start checking everything again. When we go through all statements without finding any contradictions with our current set of ducks, we are done and we have a candidate set of ducks. Notice that all birds being ducks is always a valid answer. Finally, we keep the smallest set of ducks and output its size.

This solution requires N
 iterations of the outer loop, to try every possible "first duck". Checking a pair of statements or a statement and a meeting for consistency takes constant time, as it is only checking whether birds that are mentioned in both can get from one point in space-time to another, which is a simple bit of math. Thus, checking every statement against every other statement and every meeting takes O(S×(S+M))
. On every iteration through statements except for one (the last one) we find at least one additional duck, so there are at most N−1
 iterations (remember we start with an identified duck). Therefore, the overall running time is O(N2×S×(S+M))
. With all those variables being bounded by 50
, that should fit in time.

Test Set 2
In Test Set 2, we need to speed up things significantly. We can start by making use of a more refined version of our consistency observations. As you can see in the proof, we do not need to require every pair of statements or a statement and a meeting to be consistent: only those that are "consecutive".

Formally, let us call two statements s1
 and s2
 consecutive in S
 if they refer to times t1
 and t2
, respectively, and to bird b
, and there is no other statement in S
 that refers to a time t3
 such that t1<t3<t2
 and to bird b
. Similarly, let us call a statement s
 in S
 that refers to time t1
 and a meeting m
 at time t2
 consecutive if there is no other meeting at time t3
 such that t1<t3<t2
. Notice that consecutive is not a total order, because of statements referring to the same time. However, making it a total order by breaking ties arbitrarily maintains the validity of the theorem.

Then, we can say that a set S
 of statements and/or meetings with known ducks is consistent if and only if any consecutive pair of them is consistent. The proof is the same as the one given above.

With the observation above, if we can maintain a sorted list of meetings and goose-made statements about each bird, we can check the consistency for each statement s
 in logarithmic time by only checking its consecutive neighbors (at most 2
 meetings and 2
 statements per bird in s
). This would already reduce the O(S×(S+M))
 inner-most check in the Test Set 1 solution to logarithmic time, a significant improvement.

To maintain that, we can keep a tree structure that allows insertion and lookup in logarithmic time for each bird (like set in C++ or TreeSet in Java). In this way, every time we process a statement, we simply add the new information to the appropriate birds.

We can further improve by not resetting every time we find a duck. If our structure also allows removal in logarithmic time (like the examples above), we can do the following: When we discover a duck, delete all information coming from their statements. For that, we keep a list for each bird of all information they contributed. Since each piece of information is removed at most once, the overall number of removals is at most the overall number of insertions and does not affect the time complexity.

At this point we have reduced the complexity to O(N×(S+M)×F)
 where F
 is only logarithmic factors. The N
 comes from the external "first duck" iteration. To improve upon that, we can first notice that, if we start from the empty set of ducks and still find some, those birds must always be ducks, and would be ducks when starting from any set. Therefore, in that case, that set of ducks is the answer. If there are no forced ducks, we need to do something different, but we know there is no inconsistency between statements.

At this point, we only need to check consistency between statements and meetings. A statement being inconsistent with a meeting is equivalent to a bird b1
 saying that bird b2
 is not a duck. Therefore, if b2
 were a duck, so would b1
. We can represent the situation with a directed graph in which the edges represent these implications. If there is a path in that graph from b1
 to b2
, then there is an implication (possibly not coming directly from a single statement) that if b1
 is a duck, then so is b2
. So, if a bird is a duck, then so is every other bird in its strongly connected component (SCC). That means we want to choose an SCC that is as small as possible. Moreover, we want to choose a final component, that is, one that is not pointed to by other components. Non-final components imply other components also being made of ducks, and ultimately bringing in at least one final component. In summary, in this final case the answer is the size of the smallest final SCC, which we can find in linear time.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2022 - Code Jam 2022

Slide Parade (11pts, 24pts)

In this problem, we are given a simple graph G. We are required to find a sufficiently-small circuit that passes through each edge at least once, and passes through all vertices the same number of times.

Let the input graph be G=(V,E).
V={1,2,...,B}, E={(X1,Y1),(X2,Y2),...,(XS,YS)}.

If the circuit exists, let k be the number of times that each vertex is passed through. Collecting the edges in the circuit forms a multigraph G′=(V,E′), where E′ is a multiset. G′ has the following properties:

The underlying set of E′ is E. Which means
Each edge in E appears at least once in E′.
Each edge in E′ appears in E.
For each node v, indegree(v)=outdegree(v)=k.
On the other hand, if we could find a multigraph G′ satisfying the above properties for some k, then we could compute an answer from G′ with an Eulerian circuit construction algorithm. An Eulerian circuit is a circuit that visits each edge in a graph exactly once. In the following solutions, we will focus on finding a G′ via different approaches.

Let L be the output limit 106. Note that L is also a significant factor while analyzing the complexity.

O(LBS) approach
In this approach, we will use a max-flow algorithm to find a set of valid multiplicities di for each (Xi,Yi) in E. i.e. (Xi,Yi) appears di times in E′.

Note that the total number of edges in the cycle would be kB, so the upper bound of k is 
L
B
 
. Enumerating all possible di is not feasible, but we can try all possible values of k, and see if a set of valid di can be generated under the fixed k. Once we find a set of valid di, then G′ is also determined.

This can be reduced to a maximum flow problem. We will construct a flow network with source s and sink t to help us find a valid set of di.

The vertices of the flow network are:

The source s.
The sink t.
A node inv for each v in V.
A node outv for each v in V.
There are (2B+2) vertices in the network in total.
The edges of the flow network are:

(s,outv) for each v in V, with capacity k.
(outu,inv) for each (u,v) in E. No capacity limit, but the lower bound of flow is 1.
(inv,t) for each v in V, with capacity k.
There are (S+2B) edges in the network in total.
The amount of flow through inv (outv, resp.) indicates the indegree (outdegree, resp.) of vertex v in G′. The amount of flow on (outu,inv) indicates the multiplicity of edge (u,v).

Now we search for the maximum flow on the graph. If the total flow is kB (the maximum possible), then the amount of flow through inv and outv is k for each v, and we have found a valid set of multiplicities di. We can then construct G′=(V,E′) with those multiplicities, find an Eulerian circuit in G′, and output that circuit.

On the other hand, if we don't find such a flow for any possible k, then we output IMPOSSIBLE.

Now let's analyze the time complexity of this solution. In this solution, we run a maximum flow algorithm for each k. There are 
L
B
 
 possibilities of k, and the graph has at most O(B) vertices and O(S) edges. Thus the total time complexity is O(LBS), assuming Dinic's O(B2S) algorithm is chosen for solving maximum-flow. This can be made faster by computing a maximum flow for each k by starting with the flow found for the previous value of k.

O(S2) approach
The previous algorithm could be somewhat slow. It would be good if we could generate a graph G′ more directly. Consider this simple iterative algorithm that produces a G′:

Choose an edge (u, v) that is not yet in G′.
Add edges on a path from building 1 to building u to G′.
Add the edge (u, v) to G′.
Add edges on a path from building v to building 1 to G′.
Repeat until each edge occurs at least once in G′.
The G′ produced by this algorithm is able to produce a circuit, since we can simply follow the edges in the same order they were added. But the buildings would not necessarily be visited an equal number of times. So, it would be good if we could do the following instead:

Choose an edge (u,v) that is not yet in G′.
Find a set A of edges, such that the indegree and outdegree of each node is equal in A, and A includes (u,v).
Add A to G′.
Repeat until each edge occurs at least once in G′.
If this succeeds, it would yield a G′ with all the required properties we listed earlier. Also, if every set A found in the algorithm was as small as possible, that is, if the indegree and outdegree of each node in each A was 1, then the total number of edges in G′ would be at most SB, which is at most 106, which conveniently is the limit in the problem!

Now, the question arises — if the problem is possible, can we always find such a set A for any edge (u,v)? We can show that we can.

The problem of finding a set A of edges can be reduced to finding a perfect bipartite matching, on a graph similar to the flow graph in the previous solution.

The bipartite graph consists of vertices (s1,s2,…sB) and (t1,t2,…tB). There is an edge (su,tv) if and only if (u,v) is in G. If we use an edge in the perfect matching, then we add the corresponding edge in G to A.

We need to show that a perfect matching exists in this graph if the problem instance is possible. To do that, we apply Hall's marriage theorem, which is a common technique for proving whether a graph has a perfect matching.

What we need to prove to use the theorem is the following: if there is a solution to the problem, then for any subset S of the nodes(s1,s2,…sB), let T be the set of all nodes adjacent to a node in S. Then |S|≤|T|. (We must also show that a similar result holds if we start with a subset of the nodes (t1,t2,…tB), but the proof for that is the same.)

Assume there is a solution to the problem, which has a corresponding multigraph G′, in which the indegree and outdegree of each node is k.

For each i, let g(si) be the number of edges in G′ whose tail is node i.

For each i, let g(ti) be the number of edges in G′ whose head is node i.

Now for any pair of S and T above, ∑s∈Sg(s)≤∑t∈Tg(t) since the edges in G′ whose head is in T must include all the edges in G′ whose tail is in S, plus possibly some more. But because G′ corresponds to a solution, the values of g must all be equal to k. So we can conclude that |S|≤|T| as required.

But this is not exactly the bipartite matching problem we need to solve - we need to include some fixed edge (su,tv) in the matching each time. So remove all other edges adjacent to su or tv from the bipartite graph, and now define g as follows:

For each i, let g(si) be the number of edges in G′ whose tail is node i, excluding those edges deleted from the bipartite graph.

For each i, let g(ti) be the number of edges in G′ whose head is node i, excluding those edges deleted from the bipartite graph.

Consider a set S which does not contain su.

k|S|−k<∑s∈Sg(s), since less than k edges were removed from the graph. Also, ∑t∈Tg(t)≤k|T| since k is still the maximum value of any g(t). We still have ∑s∈Sg(s)≤∑t∈Tg(t) as before, so

k|S|−k<∑s∈Sg(s)≤∑t∈Tg(t)≤k|T|

∴k|S|−k<k|T|

∴|S|−1<|T|

∴|S|≤|T| as required.

The same result holds for sets S which do contain su, since we simply match su with tv and are left with a set S without su and we can proceed as above.

If we fail to find a perfect matching for any edge (su,tv), then we can deduce that it's impossible to find a solution to the problem.

This application of Hall's marriage theorem is also known as Birkhoff's theorem on doubly stochastic matrices.

In the current approach, we find a perfect matching on S different bipartite graphs. Each of them has O(B) vertices and O(S) edges. The total time complexity is O(BS2) if we use a flow-based algorithm.

However, this approach can still be optimized further. We can make use of a previous result to avoid re-calculating the whole matching every time.

We first find an arbitrary perfect matching as the base matching. Then, for each edge (su,tv), if it's not in the base matching, we remove the edges from the matching that were adjacent to su and tv, and add the edge (su,tv). This would make exactly two vertices unmatched (those who originally matched with su and tv in the base matching), and we could find the new matching by searching for an augmenting path between the two unmatched vertices.

Finding the base matching takes O(BS) time, and for each edge (su,tv), it takes O(S) time to find an augmenting path. There are S edges in the bipartite graph, so the total time complexity is O(S2).

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2022 - Code Jam 2022

Schrödinger and Pavlov (8pts, 42pts)

The difficulty of this problem resides on the fact that the state of a box can affect outcomes long after the dog passed them by blocking or not blocking a tunnel. We deal with that difficulty by remembering the state of boxes. Of course, there are way too many boxes to remember the entire state, so we compress it to a smaller amount of information that is manageable. How we do that is different for each test set.

Test Set 1
In Test Set 1, all tunnels go to nearby boxes. That means once the dog is at box number i
, the state of boxes with numbers lower than i−5
 cannot affect the outcome anymore. So, we can do a dynamic programming solution that computes the probability that there is a cat at box i
 given the state of the k
 closest boxes. That is only O(N×2k)
 states, and because k
 is bounded by only 10
 for Test Set 1, that is small enough to solve the problem.

Test Set 2
For Test Set 2, the number of close boxes that we may need to remember is not bounded by a small number, so we cannot use a solution exponential in it. Let us consider a directed graph with boxes as the nodes and tunnels as directed edges. Because it has the same number of nodes and edges, it is a functional graph. Functional graphs look like forests except they have cycles as the "roots". Notice that only the connected component of the underlying undirected graph that contains the last box matters for the final answer (boxes in other components do not affect whether or not there is a cat in the last box). So, we can discard all other components and assume moving forward the graph is connected, so it's actually a functional graph with a single cycle.

As a thought exercise, let us assume the tunnels graph is actually a directed tree instead (one tunnel is removed). In this case, we can solve the problem by simulating the dog run and remembering things in a smart way. We maintain a forest of the nodes corresponding to every box and the tunnels that may have been used so far, that is, tunnels coming out of boxes that the dog already passed. For each node, we compute the probability that a cat is there. The probabilities of a cat being in any two boxes are independent if they are on different trees of this forest, but otherwise they might not be.

We start with the forest with all nodes and no edges. Probabilities for each node start at 0
, 1/2
c or 1
 depending on whether the box is empty, unknown, or contains a cat, respectively. Then, when we simulate the dog passing through box i
, that adds one edge to the forest, which merges two trees. Because the probability of the roots of those trees are independent up to this step, we can calculate the probability of the tunnel being used by multiplying the probability that there is a cat in box i
 and no cat in the destination box. Then, we update all probabilities as the weigthed average of the outcome of both cases (a cat running through the new tunnel or not).

To make the approach above work for an actual functional graph instead of a tree, we need to deal with the sole cycle. We can do this by branching out when we add the first edge of the cycle to the forest (that is, when the dog runs through the box with the smallest number from among those in the cycle). Instead of merging those two components, we consider all 4
 cases for the state of the two boxes at the endpoints of the tunnel. For each one, we can compute its probability with a multiplication as before, because at this points the two endpoint probabilities are independent. Then, instead of merging the two components, we start 4
 computations, one for each case, but in all of them the components are kept separated. At the end, we have 4
 results for the last box. The final result is the weighted average of that box where the weights are the probabilities of each case that we calculated when forking the computation.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

World Finals 2022 - Code Jam 2022

Triangles (8pts, 42pts)

Test Set 1
In Test Set 1 there are so few points that we can just try every possible way to assign them to triangles. With a maximum of N=12
 points, there are 12!/(3!4⋅4!)=15400
 ways of getting 3
 or 4
 triangles, even less to get 1
 or 2
, and a single way to get 0
, which means there are less than 4⋅15400+1=61601
 things to try. For each one, we need to check that every triangle is valid (i.e., made up of non-collinear points) and that each pair of triangles meets the definition given in the statement. Since there are very few triangles, this requires quite a bit of code but not a lot of computation time.

Test Set 2
We solve this problem by constructively proving the following theorem: a set S
 of 3t
 points can be split to form t
 triangles that fulfill the conditions of the statement if and only if it does not contain a subset of 2t+1
 collinear points.

The case where t=1
 is trivial. If t>1
 we consider 3
 different cases. Let C
 be the largest subset of S
 made up of collinear points.

If |C|<2t−1
, we find a triangle that is separated from all other points by a line and then recursively solve an instance with less points. Since a triangle can use at most 2
 points from C
, this does not make the remaining set exceed the maximum number of collinear points allowed. One way of finding such a triangle is to get the 2
 maximal points in the lexicographical order (i.e., ordering by X-coordinate and breaking ties by Y-coordinate) v
 and w
, and then find the third points x1
 and x2
 that minimize and maximize, respectively, the angle vwxi
, breaking ties by the distance |wxi|
. Then, pick x1
 if the angle is less than π
 or x2
 otherwise (in that case, vwx2
 is guaranteed to be greater than π
). Notice that this makes The line wxi
 separate the three points from all others (there could be more points over that line, but w
 and xi
 are the extreme points over it, so any triangles we can make from the remaining points will not interfere with this triangle).
If |C|≥2t−1
 and |C|≠3
, let B,D⊆S
 be the subsets of points on each side of the line that goes through C
. If neither is empty, assuming without loss of generality that |B|≤|D|
, we can match the lexicographically greatest 2|B|
 points of C
, let us call that C′
 with B
 to recursively solve B∪C′
 and D∪(C∖C1)
. Notice that there is a separation line between those two sets, so the recursive solutions do not interfere with each other. If one of B
 or C
 is empty we can take the two lexicographically greatest points in C
 and match it with one of the remaining points, as in the previous step, solving the rest recursively. If |C|=2
, this makes a single triangle. Otherwise, |C|>3
, so after removing two points from C
 and one point from S∖C
, the requirements of the theorem applies to the remaining points.
Otherwise, |C|=3
 and t=2
. If the convex hull of S
 has 3
 vertices, we can use those for one triangle, and the other 3
 for the other. If the convex hull of S
 has 5
 or 6
 vertices, it contains at least two from C
, so we can use those 2
 plus a single intermediate or adjacent point to form one triangle, and the rest for the other. If the convex hull of C
 has 4
 points, there are a few cases to consider depending on where the other 2
 points are located, but all are solvable. Implementation-wise, we can simply try all possible ways to match it, since there is a small amount anyway.
Because of the above, we can do the following: find a largest set of collinear points in the input C
. If it is larger than ⌈2N/3⌉
, ignore points from it to make it equal to that amount. After that, ignore points not in C
 to make the set of non-ignored points have size multiple of 3
. We will match all those points into triangles using the above idea. If we keep the points sorted lexicographically, we can implement each step finding a new triangle in linear time, except for the fact that after each step of the first type we may need to recalculate C
.

To speed up the calculation of largest subset of collinear points, we can use three techniques. The first two speed it up in complexity, but can be really slow in practice due to a combination of large constants and an accumulation of logarithmic factors depending on the exact implementation.

Calculate all subsets of collinear points and keep them updated and on a priority queue to quickly identify the largest. There are N
 point removals and each one updates up to N−1
 sets, so there are O(N2)
 updates overall. If the sets are linked lists and the priority queue is over an array (the sizes are limited to the range 1
 through N
), this can be theoretically be done in O(N2)
 overall. However, since an initial calculation of C
 for the entire input necessitates O(N2logN)
 operations, it might be tempting to use less efficient but quicker to code structures.
If |C|
 is a lot smaller than the threshold needed to not be in the first case, we can simply take multiple steps of the first case without recalculating. It can be shown that doing this leads to only a logarithmic number of recalculations. Notice that this still makes the overall complexity O(N2log2N)
 and it has large constants because the input size before each recalculation is not halved, but reduced by about 1/5
 in the worst case.
A simpler and a lot faster way is to not calculate C
 explicitly at all. Just keep doing the first case until you detect all remaining points are collinear (this happens when both vwx1
 and vwx2
 are planar angles). At that point, undo just enough of the latest made triangles to make that identified set of collinear points big enough to not be in the first case, and continue with the second and third cases. This makes the overall time complexity O(N2)
 and, not having complicated structures or logarithms with small bases, it does not hide any large constants.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.

arrow_back
Round A - Coding Competitions Farewell Rounds

Practice mode

question_answer
There is a recent issue with our system in which a solution that prints some non-ASCII characters (intentionally or non-intentionally) may get SERVER ERROR as a result when it should get WRONG ANSWER (because non-ASCII output is never correct). Sorry for the inconvenience.

Colliding Encoding (4pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:26

PROBLEM
ANALYSIS
Analysis
Test set 1
For the small test set, we compare each word encoding with the other words encodings. If any of the two encodings are equal then the answer is YES. Otherwise, the answer is NO. The time complexity for encoding all the words is O(L)
 where L
 is the sum of the lengths of the given N
 words. Since the length of a word is at most 10
, O(L)
 can be written as O(N)
. Therefore overall time complexity for this solution is O(N2)
 which is sufficient for the small test set.

Test set 2
The above solution is not suitable for the large test set. Instead of comparing each pair of encoded words, we can calculate encoding for each word and then use hashing or sorting algorithms to check if any two encodings are equal.

Method 1
Hashing: We use a hash table to store the encodings. Before inserting an encoding of a word, we check if it already exists there. If it already exists then the answer is YES. If we had to insert all the words encodings then all are unique and the answer is NO. The time complexity for encoding all the words is O(N)
. The time complexity for checking all the words in the hash table is also O(N)
. The overall time complexity of this solution is O(N)
 which is sufficient for the large test set.


Sample Code(C++)

string anyCollisions(vector<string> words, vector<int> encoding) {
   unordered_set<string> encoded_words;

   for(int i = 0; i < words.size(); i++) {
     string encoded_word;
     // Calculate the encoding for the word and store in encoded_word.
     if (encoded_words.contains(encoded_word)) {
       return "YES";
     }
     encoded_words.insert(encoded_word);
   }

   // If we reach here then there are no collisions.
   return "NO";
}
Method 2
Sorting: We store the encoding for each word in an array and sort them. If any two adjacent encodings are equal then the answer is YES. Otherwise, the answer is NO. The time complexity for encoding all the words is O(N)
 and for sorting it is O(NlogN)
. The overall time complexity of this solution is O(NlogN)
 which is sufficient for the large test set.


Sample Code(C++)

string anyCollisions(vector<string> words, vector<int> encoding) {
   vector<string> encoded_words;

   for(int i = 0; i < words.size(); i++) {
     string encoded_word;
     // Calculate the encoding for the word and store in encoded_word.
     encoded_words.add(encoded_word);
   }

   encoded_words.sort();
   for(int i = 0; i < encoded_words.size() - 1; i++) {
     if (encoded_words[i] == encoded_words[i+1]) {
       return "YES";
     }
   }

   // If we reach here then there are no collisions.
   return "NO";
}
Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round A - Coding Competitions Farewell Rounds

Practice mode

question_answer
Illumination Optimization (4pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:26

PROBLEM
ANALYSIS
Analysis
Test Set 1
For Test Set 1, we can use brute force and check all possible combinations of street light locations. For each combination, we need to check if it covers the entire freeway, and if it does, we add it to the list of candidates. We then return the combination which uses the least number of street lights. This will take O(N×2N)
.

Test Set 2
For the Test Set 2, there are two observations we can make:

The street light locations are given in sorted order
The radius of each street light is the same
Since the radius is the same, this ensures that if the left point of a street light starts before the left point of another street light, it must also end before that street light. Since the light locations are given in sorted order, we will be able to iterate through them from left to right without skipping any street lights. Our ultimate goal is to find the minimum number of street lights where the freeway is lit up. Thus we can iterate through the given street light locations from left to right, and when deciding which street light to use, we take the rightmost light that also is able to cover all area (on the left) that has not been covered yet. This greedy strategy is always an optimal one, because it always leaves the rest of the uncovered areas of the street to be as small as possible.

To do this, we can keep a counter of how many street lights we used, as well as a variable to keep track of what area has been covered so far. We can do this by saving a variable curr_rightmost_covered
 for what is covered so far. Specifically, the range [0,curr_rightmost_covered]
 denotes what is already covered. Everytime we see a new street light at Xi
, we check that it does not leave a gap in the uncovered area by checking if Xi−R≤curr_rightmost_covered
. If it does leave a gap, then it means that no street light can cover that gap, so the answer is impossible. Otherwise, we check the next street light after it, Xi+1
, to see whether that one also leaves no gap. If so, then we would prefer to use Xi+1
 instead of Xi
 and so forth. We will need to keep checking the next street light until we see one that leaves a gap Xk+1
. If the next street light Xk+1
 does leave a gap, that means we must take Xk
. In that case, we update curr_rightmost_covered
 to Xk+R
 where Xk
 is the rightmost street light that leaves no gap.

When the entire freeway is covered, we can stop and return the count of how many streets lights we used. The time complexity of this solution is O(N)
 for each test case.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round A - Coding Competitions Farewell Rounds

Practice mode

question_answer
Rainbow Sort (4pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:26

PROBLEM
ANALYSIS
Analysis
For simplicity, let us say the positive integers to be written on the cards are represented with an array A.
Test Set 1
In this test set, since N
 is very small, we can try all possible orderings of unique values of colours and use their indices in these orderings to build candidate A arrays and find one non-decreasing A from them.
Find all the unique values of S
 using a hash set in O(N)
 time. Let M
 be the array holding the unique values.
Find all the permutations of M
. For each permutation P
 of M
, follow the below logic to create A.

Initialize a hash map X
. For each Pi
, assign X[Pi]=i
. Iterate through S
 and for each Si
, assign A[i]=X[Si]
. Once A is constructed, we can easily check whether A is non-decreasing by iterating through the array.
Return IMPOSSIBLE if none of the A arrays is non-decreasing, else return one of them as the answer. For each permutation, we take O(N)
 time to check whether the array A is non- decreasing. Since there are O(N!)
 such permutations, the overall complexity of this solution is O(N!×N)
.

Test Set 2
The previous algorithm would be too slow for Test Set 2, so we must find a more efficient solution.

For cards to have a possible valid A array, we can conjecture that cards with the same value should be consecutive to each other, i.e., not separated by another card of a different value.
By way of contradiction, let us suppose our above conjecture is incorrect i.e., at least one card is present with a different number between two cards with same value.
Let Si
, Sj
, Sk
 be three values in S such that i<j<k
 and Si=Sk≠Sj
. Hence A[i]=
A[k]≠
A[j]
.
Since Si≠Sj
, A[i]<
A[j]
.
Similarly, since Sj≠Sk
, A[j]<
A[k]
.
Hence, A[i]<
A[k]
 which is contradictory.
So cards with the same values should be consecutive to each other.

It follows from the above observation that we only need to consider unique values of S
. For simplicity, let K
 be the array after removing consecutive duplicates while keeping one instance of one unique value in S
 and also keeping the order of unique elements same. Let L
 be the array such that Li
 is the index position of Si
 in K
. We can easily see that L
 is a non-decreasing array and the mapping between S
 and L
 is one to one.
Initialize an A array and an empty hash set to store elements of S
 seen while iterating. Iterate through S
 from left to right and for each Si
, find if the element appeared before in the list i.e., if the element is present in the hash set.
If the element is not present, then insert the element into the hash set and A array and continue iterating.
If the element is present in the hash set and it is equal to the previous element in S
 then continue iterating else return the answer as IMPOSSIBLE.
If we are able to successfully iterate all the way through S
, then return the A array built while iterating. Since we are iterating through S
 only once and using the hash set, the overall time complexity of this solution is O(N)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round A - Coding Competitions Farewell Rounds

Practice mode

question_answer
ASCII Art (9pts, 20pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:26

PROBLEM
ANALYSIS
Analysis
Test Set 1
For relatively small values of N
, it suffices to simulate the program in the problem statement to generate characters accordingly and output whichever character gets generated at the N
-th position. Since each character is generated in order and no additional calculation needs to be done, the time complexity of this solution is O(N)
, which suffices for Test Set 1.

Test Set 2
The solution described for Test Set 1 would fail for Test Set 2 as N
 is significantly larger here. We would need to utilize the pattern of the characters to improve on it.

One observation we can make is that if we know which iteration of Cody-Jamal's program printed out the N
-th character (let us say it is the i
-th iteration) and how many characters were already printed before that (let us say M
 characters were already printed before the i
-th iteration), we can easily figure out which character is the N
-th character.

Since we know M
, we can say that our answer is the (N−M)
-th character printed in the i
-th iteration. Let's define k=N−M
. Clearly, if k≤i
, the character is A, if i<k≤2i
, the character is B and so on. Simply put, the N
-th character printed is the ⌈ki⌉
-th letter of the alphabet, i.e. the ⌈N−Mi⌉
-th letter of the alphabet. Also note that if we know i
 alone, we can compute M
 as 26×∑i−1j=1j
. All that remains is to determine i
, which can be done in the following ways:

Linear Search
We essentially need to find the largest i
 such that M=26×∑i−1j=1j<N
. We can simply compute this sum in order until the point where 26×∑i−1j=1j=13×i×(i−1)≥N
. The time complexity for this approach is O(N−−√)
 since 13×i×(i−1)
 increases quadratically.

Binary Search
The previous approach can be sped up by utilizing the fact that 13×i×(i−1)
 monotonically increases, which allows us to binary search for i
 instead. The bounds for i
 to binary search on can be roughly set as 1 and N
. The time complexity in this case is O(logN)
.

Directly Calculating i
As mentioned earlier, we want to find the largest i
 such that 13×i×(i−1)<N
. Let z
 be the positive root of the quadratic equation 13×i×(i−1)−N=0
. For 0≤i<z
, the value of 13×i×(i−1)−N
 is negative, after which it becomes positive. Thus, our problem reduces to finding the largest integer i<z
, which is simply ⌈z−1⌉
. z
 can be calculated using the quadratic formula as 13+169+52N√26=1+1+4N13√2
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round A - Coding Competitions Farewell Rounds

Practice mode

question_answer
Untie (9pts, 20pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:26

PROBLEM
ANALYSIS
Analysis
Test Set 1
In Test Set 1, the limits are small enough that we can try every possible string made entirely of Rs, Ps, and Ss as the destination (there are only 310=59049
 of them). For each one, we can check pairs of adjacent letters to make sure it is valid. For the valid destinations, we can see how many changes they require, and keep a running minimum of it.

This solution could be sped up by either generating the strings in a smarter way that does not generate lots of invalid cases only to filter them out later, or by precalculating all valid strings for each length only once, and then using that list directly for each test case. However, none of this is required to pass Test Set 1.

Test Set 2
For Test Set 2 we need a completely different approach, as there are way too many strings to try. We can observe that if we choose any one person, we can always make a choice for them that is different than their neighbors (because there are 2
 neighbors and 3
 possible choices). Therefore, any time we have two adjacent equal letters with their other neighbors different, we can always fix it with a single change, and we need at least one. Generalizing that, if we have exactly k
 consecutive letters that are equal, surrounded by different ones on both sides, we need at least ⌊k/2⌋
 changes, and we can do it with exactly that many by changing alternating letters. Therefore, we can add that quantity for each run of consecutive letters (remembering to consider a prefix and a suffix that are made of the same letter as a single run) to the total.

There is one case that we need to solve differently, though. Notice that our precondition above is that a run of consecutive letters is surrounded by other letters. However, if all letters in the input are the same, there is no such run! This case is simple, though, as the answer is, as hinted by Sample Case #2, ⌈
the length of C/2⌉
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round B - Coding Competitions Farewell Rounds

Practice mode

question_answer
Collecting Pancakes (4pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:27

PROBLEM
ANALYSIS
Analysis
Let Xa
 and Xb
 be the indices of the stacks chosen by Alice and Bob respectively. As per the definition in the question, La≤Xa≤Ra
 and Lb≤Xb≤Rb
.

The key observation to note here is that, while collecting pancakes, Alice and Bob cannot cross over any stack claimed by the other person at any point. Specifically, if Xa>Xb
, then Bob cannot collect pancakes from any stacks with indices >Xa
 and vice versa. This leads to the observation that only the pancake stacks in between Alice and Bob's initial choices are being contested for. All pancake stacks are guaranteed to be claimed by the person that started closer to them. Thus, the optimal strategy for either player would be to first start claiming stacks between Xa
 and Xb
, then start claiming the other stacks, leaving each player with a contiguous range of stacks which is either a prefix or suffix of the overall line of stacks.

Test Set 1
The constraint N≤100
 allows a naive solution based on the observations above, which is to try out all valid combinations of Xa
 and Xb
, and simulate the optimal strategy described.

The optimal strategy would be for Alice and Bob to first start moving towards each other's initial positions, claiming stacks along the way. Once all stacks between Xa
 and Xb
 have been claimed, each player would claim all the stacks closer to them. The final answer would thus be the maximum number of pancakes collected by Alice in any such simulation.

Since there are O(N2)
 combinations of Xa
 and Xb
, and simulating the strategy for any such Xa
 and Xb
 would take at most N
 steps, the overall time complexity of this approach is O(N3)
.

Test Set 2
Another observation to be made is that once Xa
 is chosen, Bob will always try to set Xb
 as close to Xa
 as possible, to minimize the number of additional pancakes Alice can get by choosing stacks between Xa
 and Xb
. This leads to three scenarios:

Xa≤Lb
: In this case, Bob can only choose Xb
 to the right of Xa
. This will result in Bob choosing Xb=Lb
 (or Xb=Lb+1
 if Xa=Lb
). Then, once Alice and Bob start claiming stacks between Xa
 and Xb
, Alice will end up claiming all the stacks from Xa
 to ⌊Xa+Xb2⌋
. Finally, Alice would claim the stacks from 1
 to Xa−1
, leaving her with all the stacks from 1
 to ⌊Xa+Xb2⌋
.
Xa≥Rb
: Mirror image of Case 1.
Lb<Xa<Rb
: Here, Bob can choose a position immediately adjacent to Xa
 (either Xa−1
 or Xa+1
). This leaves no stacks between Alice and Bob's initial positions, so both players would just claim all the stacks closer to them without crossing the other player. Bob would choose between Xa−1
 or Xa+1
 with the goal of minimizing the number of panackes Alice collects, so the final number of pancakes collected by Alice would be min(
 sum of pancakes in stacks from 1
 to Xa
, sum of pancakes in stacks from Xa
 to N)
.
Using these cases, for a given Xa
, we can exactly predict the contiguous range of stacks that Alice will claim, assuming optimal play, in O(1)
. Given a range, calculating the sum of pancakes in that range can be done in constant time if we precompute prefix sums. The precomputation can be done in O(N)
 time. Finally, if we do the constant time computation for each possible Xa
 and take the maximum, we find the answer in O(N)
 time overall.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round B - Coding Competitions Farewell Rounds

Practice mode

question_answer
Intruder Outsmarting (4pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:27

PROBLEM
ANALYSIS
Analysis
Reachable Numbers on a Wheel
Let us first define the concept of "reachable numbers" on a wheel. These are the set of numbers that can be displayed on a wheel by rotating it in increments of D
, starting from the initial number on the wheel as given in the input.

Example: Let us take N=6
 and D=4
, and we start from the number 1
 on the wheel. Then the reachable numbers are:

1+4=5
5+4=9⇒9−6=3
3+4=7⇒7−6=1
Which brings us back to the beginning. Thus, our set of reachable numbers is {1,3,5
}. Note here that we subtract 6
 from numbers >6
 because of the circular sequence of numbers on the wheel.

Test Set 1
We can try all combinations of reachable values on the wheels, check for each combination if it forms a palindrome and if it does, calculate the minimum number of operations required to form that combination.

Each wheel can have a maximum of N
 reachable values. For W
 wheels, the number of possible combinations that can be shown on the security key will be of the order of NW
. A combination can be generated by generating the possible values for each wheel. For a given index i
, we can start with Xi
, then incrementing the values in steps of D
. If the value overflows beyond N
, we circle back around to 1
 by subtracting N
, which then brings us to a value lying in [1⋯N]
. When we get to a value that we have previously encountered, we then have the complete set of values reachable for that index. In this way, we can generate all combinations by iterating through all possible reachable values for each index.

For each index, the set of reachable positions can be generated in O(N)
, and for W
 wheels, we can do this in O(N×W)
. Then, we can generate all combinations in O(NW)
.

For a particular combination A
, we can check if it is a palindrome by simply iterating over the array and verifying that Ai=AW−i+1
 for all i=1
 to W2
 in O(W)
.

If A
 is verified to be a palindrome, we can find the minimum cost of achieving the combination by summing over the minimum cost of converting each Xi
 to Ai
. For each i
, we can perform a process similar to the combination generation process by starting with Xi
 and simulating successive forward operations (that is keep adding D
) or by simulating successive backward operations (that is keep subtracting D
) till we reach Ai
. We take the minimum of the number of forward or backward operations as the minimum number of operations it will take to achieve the conversion. For each index i
, we can hence find out the cost in O(N)
. Therefore, the cost for all W
 wheels can be calculated in O(N×W)
 time.

Thus, the overall time complexity: O(NW×W×N)

Test Set 2
The above brute force of generating all possible combinations approach is not feasible to pass test set 2 constraints. Instead for each pair (Xi,XW−i+1)
 in the array X
 (for all i=1
 to W2
) that need to be equalized to make the array a palindrome, we calculate the minimum number of operations that can do so.

Before we move forward, we transpose the set of possible values for each wheel from [1⋯N]
 to [0⋯N−1]
. This will allow us to visualize the circular nature of operations as modulo N
. We do this by simply subtracting 1
 from all elements in X
.

With that, let us try to find the minimum operations to change Xi
 to XW−i+1
. Say we rotate the i
-th wheel in the positive direction and try to make it equal to Xj
. Let x
 be the number of operations it takes to do so. With operations being circular in nature, we get:

Xi+Dx≡Xj(modN)
⇒Xi+Dx+Ny=Xj
⇒Dx+Ny=Xj−Xi
The above equation is Linear Diophantine Equation. The greatest common divisor (GCD) of two numbers i
 and j
 can be written as GCDi,j
.

From the Extended Euclidean Algorithm, an LDE (Linear Diophantine Equation) ax+by=c
 has a solution only if c
 divides GCDa,b
. Thus, the above equation Dx+Ny=Xj−Xi
 will have a solution for x
 and y
 only if Xj−Xi
 is divisible by GCDD,N
. Thus, for any pair (Xi,XW−i+1)
, if XW−i+1−Xi
 is not divisible by GCDD,N
 the answer is IMPOSSIBLE.

Otherwise, we can use the Extended Euclidean Algorithm to solve for one possible solution for x
 and y
 in O(logmin(D,N))
. We need the minimal operations xmin
, which is (N+x)(modN)
. This is because a general solution for (x,y)
 will be of the form (x+kNGCDD,N,y−kDGCDD,N)
, where k
 is an arbitrary integer. From the above general form, we can see that the minimum number of operations xmin
 would be in the range [0,NGCDD,N−1]
 (if it is larger than that, we can simply subtract NGCDD,N
 from it to get a smaller value).

Hence, xmin=x(modNGCDD,N)
. Since x
 returned from the Extended Euclidean Algorithm can be negative, we add NGCDD,N
 and take modulo NGCDD,N
 again to get xmin=(NGCDD,N+x)(modNGCDD,N)
. Let us call this "forward direction xmin
" as xforward
.

Similarly, we can find the minimum number of operations if we rotate i
-th wheel in the backward direction. We get a similar equation in this case as well:

Xi−Dx≡Xj(modN)
⇒Xi−Dx+Ny=Xj
⇒Dx−Ny=Xi−Xj
We solve for xmin
 for this equation in a similar fashion using the Extended Euclidean Algorithm as the forward case. Let us call the xmin
 we get this time as xbackward
.

Thus, the overall minimum operations it takes to change Xi
 to Xj
 is min(xforward,xbackward)
. We add together the minimum costs for all such pairs (that is for all i=1
 to W/2
) to get the minimum cost to make the array a palindrome.

The overall time complexity: O(W×loglogmin(N,D))
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round B - Coding Competitions Farewell Rounds

Practice mode

question_answer
Spacious Sets (4pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:27

PROBLEM
ANALYSIS
Analysis
For simplicty let us define a function minDiff(X)
, which takes a list of numbers X
 as input and returns the minimum absolute difference between any two elements in the list.

Test Set 1
Since N
 is very small, for each i
 we can find all subsets of A
 containing Ai
 in O(2N)
 and then we can filter the subsets that have minDiff(subset)≥K
 in O(N2)
. We can find the size of such subsets in O(N)
 and then we can find the maximum size of all such subsets while iterating through the subsets. So the total time complexity will be O(N⋅2N⋅N2)=O(2N⋅N3)
 which should work for Test Set 1.

Test Set 2
The previous algorithm would be too slow for Test Set 2, so we have to find another efficient solution.

Since the order of elements in the subset does not matter, let us sort the list A
 in increasing order and call it B
. The output list L
 for B
 can be rearranged to find the output list of A
.

For simplicty, let us use the below definition on some key lists:
Ei
 is the maximum length of some subsequence s
 of B
 such that s
 ends with Bi
 and minDiff(s)≥K
.
Si
 is the maximum length of some subsequence s
 of B
 such that s
 starts with Bi
 and minDiff(s)≥K
.
Now, Li
 can be written as Li=Ei+Si−1
 (since Bi
 would be repeated). So we can derive L
 by computing E
 and S
.

We can use dynamic programming to compute E
 and S
:
Ei=Elefti+1
, where lefti
 is the largest index less than i
 such that Bi−Blefti≥K
.
Si=Srighti+1
, where righti
 is the smallest index greater than i
 such that Brighti−Bi≥K
.
Binary Search
Now the problem reduces to compute the left
 and right
 lists efficiently. Since B
 is sorted, by using binary search we can find lefti
 and righti
 for each i
 in O(logN)
. Once we know left
 and right
 lists then we can easily compute the E
 and S
 lists in O(N)
, and then we can compute the L
 list in O(N)
. The total complexity will be O(NlogN+NlogN+N+N)=O(NlogN)
.

Sweeping Technique
Instead of binary search, there is a more efficient algorithm if we observe the pattern of lefti
 and righti
 values. The observation is: as i
 increases, lefti
 increases, and as i
 decreases, righti
 decreases.

Now initialize a variable x=0
 and try to find lefti
 while iterating through i
. At the i
-th iteration, keep incrementing x
 by 1
 till you find Bi−Bx+1<K
 or x=i−1
. Now lefti=x
. At the end of the i
-th step x=lefti
.
As x
 always increases, total increment step of x
 can be at max length of list which is N
. So the amortized time complexity of finding the left
 list is O(N)
.
Similarly, right
 list can found by iterating from right to left (from N
 to 1
), and initializing the x
 with N
 and keep decrementing in O(N)
.
Once we compute the left
 and right
 lists, we can compute L
 list in O(N)
. Total time complexity is O(NlogN+N)=O(NlogN)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round B - Coding Competitions Farewell Rounds

Practice mode

question_answer
Railroad Maintenance (9pts, 20pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:27

PROBLEM
ANALYSIS
Analysis
Test Set 1
We can represent the railroad network as an undirected graph by creating a node for each station, and then creating an edge for each pair of adjacent stations in each train line, that is, for every 1≤i≤L
, and for every 1≤j≤Ki−1
, add an edge between nodes Si,j
 and Si,j+1
.

Let's define an undirected graph to be connected if, for every pair of vertices x
 and y
, there is a path between them. From that definition, we can say that a train line is essential if, by removing it, a connected graph stops being connected.

Given an undirected graph, to check if it's connected, it is enough to check if a single arbitrary vertex can reach all others. To prove it, let's assume that a vertex v
 can reach all other vertices. Then, because the graph is undirected, all vertices can also reach vertex v
. Therefore, for any pair of vertices x
 and y
, we can see that x
 can reach y
 because x
 can reach v
 and v
 can reach y
.

Now we can check if a train line is essential by building the graph without the edges that belong to that train line, and checking if the graph is not connected. If we do that for each train line, we will be able to solve Test Set 1.

Let's define E
 as the number of edges of the entire graph, that is, E=K1+K2+⋯+KL
. The time complexity to build the graph is O(N+E)
. The time complexity to check if the graph is connected is O(N+E)
. These two operations have to be performed for every train line, therefore the overall time complexity of the algorithm will be O(L×(N+E))
.

Test Set 2
The algorithm described for Test Set 1 is too slow for the limits of Test Set 2.

Now let's introduce the concept of articulation points (or cut vertices), which are vertices that, when removed (along with its adjacent edges), make a connected undirected graph not be connected anymore.

Let's explore a different way to represent the railroad network. We will first create L
 special nodes to represent each train line, and then add edges between each train line's special node and the stations that such train line goes through. In other words, for every 1≤i≤L
, let's create a special node Pi
, and for every 1≤j≤Ki
, add an edge between nodes Si,j
 and Pi
.

For this graph representation, the action of shutting down a train line can be seen as removing its special node and all its adjacent edges. Now notice that if a train line's special node is an articulation point, that means that such train line is essential.

To find all articulation points of an undirected graph we can use this modified Depth First Search (DFS) algorithm.

The time complexity to build the graph is O(N+E)
. The time complexity to run the modified DFS algorithm is O(N+E)
. Therefore, the overall time complexity of the algorithm will be O(N+E)
.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round B - Coding Competitions Farewell Rounds

Practice mode

question_answer
Railroad Management (9pts, 20pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:27

PROBLEM
ANALYSIS
Analysis
For this problem, let's define Ri
 as the number of railroad cars that the station i
 has already received from other stations. At the beginning, Ri=0
 for all i
. Then, each time we make a shipment from station i
 to station Di
, RDi
 increases by Ci
.

Test Set 1
The shipment order can be defined as a permutation of the network stations. In Test Set 1, the limits are small enough that we can try every possible order and output the smallest answer among all of them.

For a given order, we process the shipments one by one. For the station i
, let cost(i)
 be the number of additional railroad cars required to complete the shipment. Since we can reuse the received cars to reduce the answer, cost(i)=max(Ci−Ri,0)
. We can then add cost(i)
 to the answer, and also increase RDi
 by Ci
.

The answer for a given shipment order can be calculated in O(N)
 and there are N!
 different orders. Therefore, a test case can be solved in O(N!×N)
.

Test Set 2
Let's create a weighted directed graph with N
 vertices (one for each station) and N
 edges (one for each shipment). In other words, for each vertex i
 there will be exactly one outgoing edge, from i
 to Di
, with weight Ci
.

Note that for each edge e
 from u
 to v
, if e
 is not part of a cycle, then it is optimal to complete the shipment of u
 before the shipment of v
. The reasoning behind this is that v
 can reuse railroad cars sent by u
, possibly reducing the answer, while it is impossible to do the opposite: no car can ever travel from v
 to u
. Therefore, we can apply the following process:

Pick any vertex u
 with no incoming edges
Increase the answer by cost(u)
 and RDu
 by Cu
Erase the vertex u
 and its outgoing edge from the graph
Repeat while there are vertices with no incoming edges.
Now, since each remaining vertex has at least one incoming edge and exactly one outgoing edge, the graph is now a collection of disjoint simple cycles. Thus, we can add the cost of each cycle separately.

One way to calculate the cost of a cycle is to traverse it for each possible starting point, completing the shipments in order, and then use the smallest answer among all traversals. However, the time complexity of this is O(N2)
, which is not enough to pass Test Set 2.

We can optimize this to a linear solution with the following observation: suppose that the starting vertex is s
. The cost of s
 is cost(s)=max(Cs−Rs,0)
. For any other vertex u
 which is not the starting one, its cost is cost_in_cycle(u)=max(Cu−Ru−Cpred(u),0)
, where pred(u)
 is the predecessor of u
 in the cycle (i.e. Dpred(u)=u
). Note that this is independent of s
, hence we can precalculate sum_of_costs
 as the sum of cost_in_cycle(u)
 for all vertices u
 in the cycle and compute the cost for each starting vertex s
 as sum_of_costs−cost_in_cycle(s)+cost(s)
, in constant time.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round C - Coding Competitions Farewell Rounds

Practice mode

question_answer
Game Sort: Part 1 (4pts, 9pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:28

PROBLEM
ANALYSIS
Analysis
Test Set 1
In Test Set 1, the initial idea is to try all possible permutations of each part, but the limits are not small enough to try everything. We observe that it is always better to sort the first part to be the (lexicographically) smallest possible permutation, and to sort the last part to be the (lexicographically) largest possible permutation. Then we can try all permutations of the middle part (there are at most 8!
) and check if any of them makes all parts sorted in non-decreasing lexicographical order.

Test Set 2
For Test Set 2 we need a different approach, as there are too many permutations per part to try. We observe that, for each part, it is best to make it the smallest possible permutation that is not smaller than the previous part (or the smallest permutation in case of the first part). By doing so, we give the next part more opportunities (if any) to be a permutation that is not smaller than the current part.

We see that the best permutation of the current part is the one that shares the longest common prefix with the previous part, such that one of the following conditions holds:

There is a character we can append to that prefix in the current part that is larger than the next character in the previous part; or
The whole previous part is a prefix of the current part.
Then we can append the rest of the characters to the current part in sorted order.
One way to do this is to maintain a count of each character per part. When we are at a part, we need to find the longest common prefix with the previous part, subject to the condition above.

The solution above only requires a constant number of scans for each part. This makes the algorithm O(∑i|Si|)
, which works for the problem constraints. However, less efficient implementations can also pass.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round C - Coding Competitions Farewell Rounds

Practice mode

question_answer
Immunization Operation (4pts, 9pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:28

PROBLEM
ANALYSIS
Analysis
Test Set 1
We can simulate each query. First, we can store all the vaccines and the patients in an ordered data structure. Then, for each query, we can go through the range from start to end positions, maintain the list of picked up vaccines, and each time we encounter a patient whose vaccine is picked up and has not been administered yet, we mark them as vaccinated. This can give us the count of vaccinated patients for every move. For each query, there will be at most O(V)
 patients and vaccines we will encounter. Saving the picked up vaccine and looking up if we can vaccinate someone with the available vaccines can be done in O(logV)
 using a set-like data structure. So the total time complexity of this approach is O(M×VlogV)
, which is sufficient for Test Set 1.

Test Set 2
For Test Set 2, we need to optimize the simulation. One important observation here is that for each patient who is located to the east side of their vaccine, they would be vaccinated when the robot is moving in the east direction. Let us consider two cases where that can happen. The vaccine is at x
, the patient at y
, and y
 > x
 (the patient is on the east of the vaccine). Let us assume the move range from r1
 to r2
.

The move range covers both the vaccine and the patient. The vaccine will be picked up, and then the patient will be vaccinated in the same move.
image 1
The move covers only the vaccine, not the patient. The vaccine will be picked in this move and the robot will finish at position r2
. To complete the vaccination, the robot needs to go to y
, so it must move in the east direction to come to position y
.
image 2
The same can be said about the patients who are on the west side of their vaccines, that they will be vaccinated when the robot is moving in the west direction. With this insight, we can process these two types of cases separately. Let us call them east case (where patients will be vaccinated in an east move) and west case (where patients will be vaccinated in a west move).

For the east cases, we can keep track of the farthest right point MaxR
 that the robot visited so far. When the robot is making an east move with the range (r1
,r2
] and r2
 is on the west of the MaxR
, then this range has been already visited by the robot, and nothing new happens. Otherwise, we can count the east case patients in that range(max(r1
, MaxR
), r2
], and that count will be the answer of the query. Since each patient can be vaccinated only once, this does not take longer than O(V)
 time in total.

For the west cases, this simple strategy does not work, because we need to know not just the farthest right point, but also all the west case vaccines that have been picked in previous moves, and not used yet. Since the robot starts at position 0, and the vaccines and patients are always on the positive coordinates, for any west move that visits a patient, their vaccine is already picked up in one of the past east moves. So we can maintain an ordered list of west case patients whose vaccines have been picked during the previous east moves, and in each west move, find the number of patients in that move range from the list. We should later delete those patients. Since each patient will be counted and deleted only once, and we can complete each find, insert, delete operations in O(logV)
 time, the total time complexity here is O(VlogV)
. The overall time complexity of the whole solution is O(M+V+VlogV)
, which is sufficient for Test Set 2. O(M)
 is included since we need to go through each query once at least.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round C - Coding Competitions Farewell Rounds

Practice mode

question_answer
Evolutionary Algorithms (7pts, 16pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:28

PROBLEM
ANALYSIS
Analysis
From the definition of an interesting triplet (a,b,c)
, it is clear that the node b
 has a central role, so let us find the number of interesting triplets for a fixed node b
. If Ab
 is the number of nodes a
 in the subtree of b
 such that Sb>K×Sa
 and Cb
 is the number of nodes c
 outside the subtree of b
 such that Sb>K×Sc
, then the number of interesting triplets with the given middle node b
 is Ab×Cb
. The answer is the sum ∑Nb=1Ab×Cb
 of contributions over all nodes b
.

Test Set 1
The constraint N≤1000
 suggests that we are a looking for a quadratic algorithm. If we manage to compute Ab
 and Cb
 in linear time for a fixed node b
, we would obtain a solution with the overall time complexity O(N2)
.

For a fixed node b
, Ab
 can be computed by performing a Depth-first search (DFS) rooted at the node b
 and verifying the inequality for each and every node. As for Cb
, we can do a linear search and count the total number X
 of nodes x
 such that Sb>K×Sx
 holds. Then Cb=X−Ab
.

Test Set 2
For the large test set, the general idea remains the same. However, we cannot afford to spend linear time to compute Ab
 and Cb
. It seems like we need a data structure D
 that supports the following queries efficiently:

What is the number of nodes a
 in the subtree rooted at b
 such that Sb>K×Sa
?
What is the total number of nodes x
 such that Sb>K×Sx
?
That sounds complicated though. Fortunately, we can simplify these questions a great deal if we process the nodes b
 in a non-decreasing order by Sb
. Then, by using a two pointer technique, we can make sure that the data structure D
 contains precisely the nodes x
 such that Sb>K×Sx
 and nothing else. Now the above questions can be translated as follows:

How many nodes of D
 are in the subtree rooted at b
?
What is the size of D
?
A very efficient technique for testing if a node a
 is in the subtree of the node b
 is called tree flattening. Essentially, we perform a post-order tree traversal to compute the labels end(v)
 for each node v
, which are shown at the right side of each node in the following illustration. Moreover, using the same DFS traversal, we compute the label start(v)
 for each node v
 (shown at the left side of nodes in the example), which is the minimum end(u)
 over all nodes u
 in the subtree of v
. Then a node a
 is in the subtree rooted at b
 if and only if start(b)≤end(a)≤end(b)
.

Illustrates the labels start(v) and end(v) for an example tree.

For example, the set of labels end(v)
 of all nodes in the subtree of node 3
 in the above drawing form a consecutive interval [2,6]
, which is conveniently stored as [start(3),end(3)]
, hence, a node v
 belongs to the subtree of node 3
 if and only if 2≤end(v)≤6
.

Answering the first question now seems a lot like a range query that a segment tree or a Fenwick tree is a perfect fit for. For example, we can use a segment tree D
 on the range [1,N]
, which is initially empty, namely, D[i]=0
 for all i
. Whenever we add a node v
 to D
, we modify the segment tree to set D[end(v)]=1
. It is important to note that we are marking the presence of a node at the position end(v)
 of the segment tree rather than v
 itself. In this way, a range query D[start(b),end(b)]
 returns the number of nodes a
 that are in the subtree of b
 and present in D
.

The time complexity of this solution is O(NlogN)
 because of the sorting and 2N
 segment tree operations.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round C - Coding Competitions Farewell Rounds

Practice mode

question_answer
The Decades of Coding Competitions (7pts, 6pts, 10pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:28

PROBLEM
ANALYSIS
Analysis
Let's start by treating the city as a graph, where nodes represent bus stops, edges represent bus routes, and a label on each edge represents the club cheered on by the bus route driver.

There is a key observation for this problem: if two nodes Pj
 and Cj
 are connected, we would be able to traverse all the edges in the connected component in the path from Pj
 to Cj
. The proof for this observation is provided at the end of this analysis. With this observation, we can find that if the total number of distinct edge clubs in this connected component is odd, there is always a path from Pj
 to Cj
 that can walk on an odd number of clubs by traversing all the edges in the connected component at least once.

Therefore, for each query, we can check if there exists a subgraph that satisfies:

Pj
 and Cj
 are connected.
There is an odd number of distinct edge clubs in the connected component that contains Pj
 and Cj
.
Test Set 1
Since the number of distinct clubs K
 is small in Test Set 1, we can enumerate all combinations of an odd number of distinct clubs. For each combination, build a new graph G′
 by removing edges with clubs not in the combination. Afterwards, for each query we can check if Pj
and Cj
 are connected in any G′
, and if there is an odd number of edge clubs in the corresponding connected component by iterating through all the edges in the connected component.

We can use Disjoint Set Union (DSU) to find the connected componnets in G′
 and all the nodes and edges in them. A connected component has edge club c
 if and only if u
 and v
 are in this component (in the same set in DSU) and the edge (u,v)
 has club c
. We can prebuild the DSU for all G′
s and find the number of distinct clubs for each of the connected component in them. For each query, we can iterate through all prebuilt DSUs and check if Pj
 and Cj
 are connected in a component with odd number of edge clubs in it.

Space complexity:
There are at most 2K
 G′
s, and each graph has at most N
 connected components:

Disjoint Set Unions(DSUs): O(2K×N)
Number of distinct clubs in connected components in graphs: O(2K×N)
Time complexity:
Build DSUs and component clubs set (number of distinct clubs): O(2K×(N+M))
Per query: O(2K)
 for checking all DSUs and number of distinct clubs in the corresponding connected component.
Overall: O(2K×(N+M)+Q×2K)=O(2K×(N+M+Q))
Test Set 2 and Test Set 3
We can improve our solution by reducing the club combinations to check: we just need to check the original graph G
 and all G′
 built by removing edges with one club. We can prove this statement by considering all possible cases for query Pj
 and Cj
:

Let Clubs(G,Pj,Cj)
 be the set of distinct clubs in the connected component containing Pj
 and Cj
 in graph G
, and |Clubs(G,Pj,Cj)|
 be the number of distinct clubs:

If Pj
 and Cj
 are not connected in G
, there is no valid path for this query.
If Clubs(G,Pj,Cj)
 has an odd number of distinct clubs (|Clubs(G,Pj,Cj)|=2n+1
), there is always a valid path walking on an odd number of distinct clubs.
Otherwise, if there is a path from Pj
 to Cj
 composed of edges with odd number (2n+1
) of distinct clubs, Pj
 and Cj
 will always be connected after removing edges with those K−(2n+1)
 clubs not in that path from graph G
. Then, let Gminimal
 be graph after removing those edges, we can find another path with an odd number of clubs by traversing additional connected edges with two extra clubs c1
 and c2
 where c1,c2∉Clubs(Gminimal,Pj,Cj)
 and c1,c2∈Clubs(G,Pj,Cj)
. Recursively, we can find a valid path in G′
 by removing only one club from Clubs(G,Pj,Cj)
. It means if there is a valid path, there is always a G′
 where |Clubs(G′,Pj,Cj)|
 is odd and built by removing edges of one club.
Therefore, we just need to build the DSUs and connected components clubs sets for the K+1
 graphs that result from removing 0
 or 1
 club from the original graph.

Space complexity:
There are K+1
 G′
s, and each graph has at most N
 connected components:

Disjoint Set Unions(DSUs): O((K+1)×N)=O(K×N)
Number of distinct clubs in connected components in graphs: O(K×N)
Time complexity:
Build DSUs and component clubs set (number of distinct clubs): O((K+1)×(N+M))=O(K×(N+M))
Per query: O(K)
 for checking all DSUs and number of distinct clubs in the corresponding connected component.
Overall: O(K×(N+M+Q))
Test Set 2 does not require prebuilding and sharing the DSU and clubs sets across queries. Solutions with time complexity O(Q×(K×(N+M)))
 that build DSUs and clubs sets for each of the query separately are acceptable.

Proof: Path traversing all edges in the connect component always exists
Suppose there is a path from v1
 to vn
 where p=(v1,v2,…,vn−1,vn)
, and let Nodes(p)={v1,v2,…,vn}
 to be the set of nodes visited at least once in path p
:

Node Reachability: For any node vi∈Nodes(p)
, if there is a node um
 and a path from vi
 to um
 where p′=(vi,u1,u2,…,um)
 in the graph, there is always a path pmerged
 from v1
 to vn
 where Nodes(pmerged)=Nodes(p)∪Nodes(p′)⊇Nodes(p)∪{um}
. A valid path would be pmerged=(v1,…,vi,u1,…,um−1,um,um−1,…,u1,vi,vi+1,…,vn)
. Notice that we can visit same node or edge multiple times.
Edge Reachability: For any node vi∈Nodes(p)
, and for edges (vi,u1),(vi,u2),…,(vi,um)
 directly connect to vi
, there always exists a path p′merged
 from v1
 to vn
 which traverses all these edges at least once. A valid path would be p′merged=(v1,…,vi,u1,vi,u2,…,vi,um,vi,vi+1,…,vn)
.
From (1), we can derive that if nodes P
 and C
 are connected, there exist a path from P
 to C
 which visits all reachable nodes from P
 at least once, which are all nodes in the connected component.
From (2) and (3), we can derive that if nodes P
 and C
 are connected, there exists a path which traverses all the edges in the connected component at least once.
node reachability
Node Reachability: pmerged
 visits additional nodes u1
 and u2
, as there exists a path p′=(v2,u1,u2)
.
node reachability
Edge Reachability: pmerged
 visits all edges connected to v2
 — (v2,u1)
 and (v2,u2)
.
Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round C - Coding Competitions Farewell Rounds

Practice mode

question_answer
Game Sort: Part 2 (8pts, 20pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:28

PROBLEM
ANALYSIS
Analysis
Let N
 be the length of S
.

Test Set 1
In Test Set 1, we have either P=2
 or P=3
. We can solve each of those cases separately:

If P=2
, there are exactly N−1
 possible places to split S
 into two parts. We can try them all, and for each one, check whether Badari can win by using a solution to problem Game Sort: Part 1. Then we know that Amir wins if any of those N−1
 places results in a situation where Badari cannot win.
If P=3
, there are exactly (N−1)(N−2)2
 possible ways to separate: as before, there are N−1
 possible places to split, and we must now choose two of those in order to separate S
 into three parts. Just like before, one option is to try all such pairs, and for each one, check whether Badari can win.
Such an algorithm has a time complexity O(N3lgN)
 or perhaps O(N3)
 depending on implementation details, so it is enough to pass Test Set 1.

Test Set 2
First of all, we should notice that, whenever P=N
, Amir has no choice and S
 has to be separated into its N
 individual letters. Furthermore, for each single-letter part Badari has no choice of rearrangement. Thus, Badari wins in this case if and only if the letters of S
 are already sorted in non-decreasing order.

This is a special case to check separately, as many of the properties that we will use later to solve Test Set 2 are false if P=N
. From now on, we assume P<N
.

We can solve the case P=2
 as in Test Set 1, trying all N−1
 possible ways to split S
. To avoid doing so in quadratic time, we can keep two arrays with a count of each character per part. Then, when moving from position i
 to i+1
, we only need to increment one count and decrement the other, and use the check from the solution for Game Sort: Part 1. We thus get an O(N)
 solution for P=2
.

Finally, the only remaining case is 3≤P<N
. Having at least three parts allows Amir to win lots of strings easily. For instance, for any string which does not start with its alphabetically smallest character (let us call it c
), Amir can win by simply splitting the first occurrence of c
 as a single-letter part.

Similarly, if the alphabetically smallest character c
 appears anywhere on the string that is not the first or second character, it is possible to win by splitting that occurrence of c
 as a single-letter part, while leaving the first and second characters of the string unsplit (it is possible because P<N
).

In the remaining case for P≥3
, S
 has the following property:

c
 appears in S
 either exactly once as the first character of S
, or exactly twice as the first two characters of S
.
Let us call a string having that property a bad string, and any string without the property good. The only remaining case to solve is for bad strings and P≥3
.

We can assume that Badari plays optimally, and that she follows the optimal strategy explained in the solution of Game Sort: Part 1. That strategy implies that, for a bad string, no matter how we separate S
 into parts, after Badari's rearrangement, the first part is lexicographically lower than or equal to all the other parts. This is because the first part will either contain a single c
, or every occurrence of c
.

Thus, to split a bad string into P
 parts, we must choose the length l
 of the first part, and then separate the remaining N−l
 characters into P−1
 parts. Note that the first part will be in sorted order, and by the property of bad strings, it will be the lexicographically smallest part and will not impose restrictions on the rearrangement of the second part. So Amir wins such a split if and only if he wins the split of the last N−l
 characters into P−1
 parts.

Using all these ideas, we can recursively compute, for each suffix of the initial string S
 and each k
 with 2≤k≤P
, who wins when the game is played for that suffix as initial string and k
 as the number of parts.

Note that, with this approach, computing who wins for each suffix when splitting into k=2
 parts must be done independently as a base case. It is possible to compute all such values efficiently, but it involves even more case analysis and careful implementation, so we will leave it as an exercise for the reader. Such an approach would lead to an overall O(N×P)
 time solution, which should pass if implemented carefully.

Although the main idea of this solution is relatively direct, the implementation has many special cases and details, and is tricky to code quickly and correctly. This approach can be implemented more efficiently and in a simpler way, by not storing all of the N×P
 subproblems. Instead, we can store, for each k
 between 2
 and P
, the index of the shortest suffix that Amir wins when splitting that suffix into k
 parts. In a sense, the shortest suffix that works is the only relevant working suffix, because when choosing the length l
 of a first part to cut, if any winning suffix is available, then the shortest one is. This key observation also avoids having to code the general problem of identifying all winning suffixes for k=2
, since we only need to find the shortest such suffix now, which is quite a simpler problem that we will also leave as an exercise. (Hint: focus only on which characters are lower, equal to, or higher than the very last character of the input string.) With this, we get an O(N)
 solution.

Another approach
There is another approach which is quite simpler to code, but maybe harder to find and prove correct. We will sketch the algorithm and leave the proof of correctness as an exercise for the reader.

We solve cases P=N
 and P=2
 exactly as before. Then we observe that, for P≥4
, Amir has even more favorable strings:

If S
 is not sorted, then Amir wins by taking the first consecutive pair of letters in S
 that are out of order, and separating them both into single-letter parts.
If S
 contains three consecutive identical letters, then Amir wins by splitting so that the first two occurrences form a part of length 2
, and the third consecutive occurrence forms a single-letter part.
If neither (1) nor (2) hold, then it is easy to see that Amir cannot win.
Only the hardest case where P=3
 remains. Suppose that Amir splits S
 into three parts, say A,B,C
 (so S=ABC
). It is clear that, if partitioning string AB
 into A,B
 is a win for Amir with P=2
 and input string AB
, then A,B,C
 works for P=3
 and input string S=ABC
. Similarly, if partitioning BC
 into B,C
 is a win for Amir for P=2
 and input string BC
, then also A,B,C
 for P=3
 and input S=ABC
.

The surprising result is that, for any string S
, Amir wins if and only if one of these two cases occur. This is not obvious: There are examples of partitions A,B,C
 that work, but neither A,B
 nor B,C
 work; e.g. XY,AZ,XY for input string XYAZXY. However, in any such case there will be some other partition that works and has this property, e.g. XY,A,ZXY for input string XYAZXY, as XY,A works.

Finally, we only need to check if Amir wins any prefix or suffix of S
 with P=2
. Note that this is actually simpler than computing for every suffix and prefix whether Amir wins with P=2
, and can be done by a slight modification of the full solution to the P=2
 case, which we leave as an exercise.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round D - Coding Competitions Farewell Rounds

Practice mode

question_answer
Indispensable Overpass (5pts, 7pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:29

PROBLEM
ANALYSIS
Analysis
Test Set 1
A brute-force solution would be to calculate and sum the distances between every pair of stations, then divide by the total number of pairs. The total number of stations is W+E
, which means that there are N=(W+E)×(W+E−1)2
 total pairs of stations. Calculating the distance between any given pair takes O(W+E)
 time, so this solution has a time complexity of O((W+E)3)
, which is too slow.

First notice that the western stations make a tree, because there is exactly one path between any two stations. This also applies to the eastern stations. Let us define the western tree as TW
, the eastern tree as TE
, and the tree created by connecting the two trees as TW+E
.

Instead of calculating the distances between every pair of stations, we can instead compute how many paths go through each edge, which we call the "contribution" of each edge, in TW+E
. The total sum of all distances between pairs of nodes is equal to the total sum of edge contributions. We just have to sum up these contributions and divide by N
 to get the average distance for the given case.

To compute an edge's contribution, we need two depth-first-search (DFS) traversals. First, choose an arbitrary root r
 in TW+E
. For the first traversal, for each node n
 starting from r
, compute and store the size of the subtree Sn
 under n
. This can be calculated recursively by Sn=1+∑iSi
, where the sum is over all children i
 of n
. Fig.1 has an example shown below.

Illustration of Sn=1+sum(Si)
Fig.1 - The size of a subtree under a node is the sum of subtrees under each child of the node, plus the node itself.
Then, traverse the tree a second time to compute each edge's contribution. The key insight here is that for an edge between nodes n
 and p
, where n
 is the child of p
, the edge's contribution is equal to Sn×(W+E−Sn)
. Thus, the second traversal goes through all the edges in TW+E
 and calculates their contributions using the stored subtree sizes from the first traversal. This is demonstrated below in Fig.2.

Illustration of edge contribution Sn*(W+E-Sn)
Fig.2 - The edge contribution, which is the number of node pairs that cross that edge, can be calculated as the number of nodes on one side of the edge times the number of nodes on the other side of the edge.
Afterwards, summing up the edge contributions and dividing by N
 yields the answer. Since the two traversals take O(W+E)
 each, and we need to redo the traversals for each of the C
 cases, the total time complexity is O((W+E)×C)
. This is sufficient for Test Set 1, but not for Test Set 2.

Test Set 2
The key insight for Test Set 2 is that the total path sum of the N
 pairs can be computed from the formula E×PWAk+W×PEBk+W×E+QW+QE
, where PWn
 is the path sum from node n
 to all other nodes in TW
, QW
 is the total path sum within TW
, and likewise for PEn
 and QE
. If these values are precomputed already, then whenever we are given a connection (Ak,Bk)
, we can return the average distance in a constant time operation using the formula above. Before deriving this formula, let us first find out how to precompute QW,QE
, and PWn,PEn
 for each node n
 efficiently first. This can be accomplished with two DFS traversals for both TW
 and TE
 individually.

Let us look at TW
 for convenience. For the first DFS, starting from an arbitrary root r
, calculate at each node n
:

The size of the subtree rooted by n
, defined as SWn
. This can be calculated recursively by SWn=1+∑iSWi
, where the sum is over all children i
 of n
.
The path sum from n
 to all its descendants, defined as DWn
. This can be computed recursively by DWn=∑iDWi+(Sn−1)
, where the sum is over all children i
 of n
.
Illustration of Dn=sum(Di+Si)
Fig.3 - The path sum from a node to its descendants can be calculated as the sum of the path sums of the node's children to their descendants, plus the contribution of the edges between the node and its children, which is the total number of descendants the node has.
This first traversal calculates the number of subtrees and subtree path sums below n
. However, DWn
 is not the full sum of all paths from n
 to other nodes in TW
 because we only counted paths travelling downwards from n
. For every node, we also need to compute path sums that go above and through its parent. This is done via a second DFS. In this traversal, given a node n
 and its parent p
, the actual path sum PWn
 is computed as follows:

PWn=DWn+(W−SWn)+(PWp−(DWn+SWn))

Essentially, we add onto DWn
 two values:

(W−SWn)
, the contribution of the edge connecting n
 to p
PWp−(DWn+SWn)
, the path sums starting from p
 that do not go through n
.
For the root r
 of TW
, PWr=DWr
, as shown below in Fig.4.

Illustration of Pn=Dn+(N-Sn)+(Pp-Dn-Sn)
Fig.4 - The path sum from a node to all other nodes can be calculated as the path sum from the node to all its descendants (e.g. path sum from 8 to all green nodes), plus the edge contribution between the node and its parent (e.g. the number of blue nodes), plus the path sum from the node's parent to all other nodes (e.g. path sum from 10 to all blue nodes).
With PWn
 computed for all nodes n
 in TW
 and again PEn
 computed for all nodes in TE
, we can calculate the average distance for each case in constant time. Now let us derive the formula mentioned earlier. Suppose we are given a connection between Ak
 from the west, and Bk
from the east. We first note that the total path sum can be computed as:

∑i∈TW∑j∈TEdi,j+QW+QE

where di,j
 is the distance between nodes i
 and j
. Here, QW
 is the sum of all paths in TW
, and QE
 is the sum of all paths in TE
. We can already calculate them just by summing the Pn
s on each side.

QW=∑n∈TWPWn2,QE=∑n∈TEPEn2

The first sum about di,j
 is trickier, but we can separate it into three terms that incorporate Ak
 and Bk
:

∑i∈TW∑j∈TE(di,Ak+dj,Bk+1)
.

All the terms now allow us to simplify the double summation to single summations. The above equation is equivalent to:

E×∑i∈TWdi,Ak+W×∑j∈TEdj,Bk+W×E
.

Because we precomputed all the path sums, ∑i∈TWdi,Ak
 is equal to PWAk
, and ∑j∈TEdj,Bk
 is equal to PEBk
. Thus, the total path distances between the two trees is

E×PWAk+W×PEBk+W×E+QW+QE
.

Divide that by N
 to get the average distance for each query. Each traversal for TW
 and TE
 is O(W)
 and O(E)
 respectively, which makes the overall complexity O(W+E+C)
 for C
 cases.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round D - Coding Competitions Farewell Rounds

Practice mode

question_answer
Genetic Sequences (5pts, 17pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:29

PROBLEM
ANALYSIS
Analysis
Test Set 1
The problem asks us to answer queries on two strings A
 and B
. In each query, we are given an A
-prefix and a B
-suffix. We are asked to find the longest prefix of the B
-suffix that is a substring of the A
-prefix.

A naive algorithm might try every single prefix of the B
-suffix and do a substring search in the A
-prefix. Assuming a fast string-searching algorithm, this gives us an O(QM(N+M))
 algorithm where Q
 is the number of queries, N
 is the length of A
, and M
 is the length of B
. This is too slow to pass the first test set.

This approach can be sped up. One approach is to use the Z-function. The i
-th entry of the Z-function is denoted by Zi
 is the longest match between a string and the suffix of that string starting at index i
. We can precompute the Z-function for the concatenation of A
 onto the end of each B
-suffix. This requires O(NM)
 time. When answering a query, we find the Z-function table corresponding to the B
-suffix in that query. Then, we can iterate through each index corresponding to the A
-prefix to find the longest match. Overall, this gives an O(QN+NM)
 algorithm, which is fast enough for Test Set 1. Other techniques including Knuth-Morris-Pratt or suffix trees/arrays can be used too.

Test Set 2
A faster approach is needed for the larger second test set. We can start with some observations. Consider a single query. The longest match in the A
-prefix must be fully contained in the prefix. Imagine we had an algorithm that could quickly find the longest match but that match is allowed to go outside the A
-prefix. Specifically, the match only needs to start in the A
-prefix, but might go outside of the prefix later on. Call this a relaxed query. This may seem arbitrary, but we will see later that relaxed queries can be answered efficiently using well-known techniques.

Assume we have a fast algorithm for answering relaxed queries. Can we now solve the problem efficiently? Assume we want to check if the longest match is at least k
 characters long. We know that any match must start in the A
-prefix with the last k−1
 characters removed, so we can solve it using a relaxed query. Also, observe that if there is a match that is at least k
 characters, it implies there is a match of at least k−1
 characters. This means we can binary search on k
. This allows us to answer a query in O(RlogM)
 time, where R
 is the time required to answer a relaxed query.

Now we need to find an efficient algorithm for anwering relaxed queries. One way to do this involves two data structures. A suffix array with its longest common prefix table, and a persistent binary search tree.

Consider the suffix array of A
 concated with B
. We will need some properties of the suffix array and the longest common prefixes between pairs of suffixes. We can find the suffix array position of any B
-suffix by keeping a lookup table into the suffix array. Let us say that that position is b
. The longest common prefix between suffix b
 and suffixes b+1,b+2,…
 will decrease monotonically. The same is true for b−1,b−2,…
. To answer a relaxed query, we need to find the longest common prefix between b
 and any suffix beginning in the A
-prefix. Using the previously mentioned property, we will therefore need to find the first index less than b
 that corresponds to a suffix starting in the A
-prefix, as well as the first index greater than b
 also corresponding to a suffix starting in the A
-prefix.

We can find these two entries by sweeping over the suffixes of A
 and constructing a persistent binary search tree T
. We can sweep over A
 in increasing order of indexes. The corresponding suffix array location can be found and added to T
. The end result will be N different versions of T
, each corresponding to the set of suffixes for each A
-prefix. When answering a relaxed query, we can look up the version of T
 corresponding to the A
-prefix, and do a tree traversal to find the first elements immediately before and after the B
-suffix. Note that the query value will be the suffix array location of the B
-suffix.

This leads to an algorithm whose complexity is O((N+M)log(N+M))
 to build the suffix array and persistent binary search trees. Then, we require O(Qlog(M)log(N))
 to answer all of the queries. This is sufficient to pass Test Set 2.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round D - Coding Competitions Farewell Rounds

Practice mode

question_answer
Hey Google, Drive! (5pts, 17pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:29

PROBLEM
ANALYSIS
Analysis
Since there are at most 26 interesting finishes, we can find a list of interesting starts that are drivable to each of the interesting finishes separately. We can solve this in a more generic way: For a given interesting finish cell location, determine whether each of the other cells in the grid is drivable to the interesting finish cell or not.

Our goal is to build a graph from the given grid, where each cell is seen as a node, and for each pair of adjacent cells (ca,cb)
, add a directed edge from ca
 to cb
 if there exist a strategy to force the car move from ca
 to cb
 safely. Note that the edges between two cells are not necessary to be bidirectional.

Illustration of Sample Case #1b.

Take Sample Case #1's interesting finish B
 as example, the blue arrow edges are added for each pair of cells if there exist a strategy to force the car go along the direction safely. From observation, we can see that a,b,c
 are all drivable to the interesting finish B
. Also note that there is not an edge from B
 to its south side neighbor. This is because there is a hazard in its north side so it is not safe to make the "north/south" command when the car is on the cell B
.

Determining an edge between two cells is not as simple as avoiding hazards, because in addition to the hazards, there will also be some empty cells that are not drivable to the interesting finish, which we will need to avoid. Below we show how to build a graph for a fixed interesting finish, and once the graph is built, how to determine which nodes (cell) are drivable to the interesting finish.

We can first temporarily put edges between each pair of adjacent cells, as the car has possibilities to move between them. Then whenever we identify a move is unsafe, i.e., some edges are found as we are unable to force the car to go along their directions safely, then we remove those edges.

Specifically the steps are, we first build an edge from each of the empty cells to its four neighboring cells if it is not a wall. Then after running a BFS/DFS from the interesting finish with edges reversed, we can discover the nodes that are never drivable to the interesting finish (i.e., there does not exist a path from that node to the interesting finish). These nodes are now seen as "non-winning" nodes.

Illustration of edge initialization.

Take Sample Case #4 for example. The red cells are hazards and the gray cells are walls. Let us build an edge from each of the empty cells to its four neighboring cells if it is not a wall.
Illustration of non-winning node finding.

For simplicity, let us use (r,c)
 to refer the cell at the r
-th row and the c
-column in 0 index. Running a BFS or a DFS from the interesting finish F
, we can see that cells (0,1)
, (0,5)
, (0,6)
, (1,3)
, (1,6)
 cannot reach the interesting finish F
, thus they are non-winning cells. Non-winning cells are as bad as hazards, thus we mark them red.
Having these non-winning nodes identified, we must remove some edges from the graph. Precisely, if a node has a non-winning node at its west side, then we must remove the edges to its west neighbor and also the edge to its east neighbor. Likewise for the north and south edges.

Illustration of edge removing.

Remove all the edges according to the non-winning nodes we found in the last step.
Since some edges are removed, it will result in getting more non-winning nodes. We can run a BFS/DFS again on the current graph to find out those non-winning nodes. We iteratively repeat these two steps to identify non-winning nodes and remove edges, until there is no more non-winning nodes to be identified. Then this final graph is what we wanted to build, and all the cells on the final winning positions are the cells drivable to the interesting finish.

Illustration of finding more non-winning nodes.

After removing the edges, (0,0)
, (0,2)
, (1,0)
, (1,1)
 and (1,2)
 become non-winning nodes.
Illustration of removing more edges.

Remove all the edges that are possible to lead to non-winning nodes
Illustration of the final state.

Finally, cell (0,3)
 is found as non-winning too. After this step, there is no more edge to be removed, and no more node to be marked as non-winning. Thus this is the final graph we are looking for, and (0,4)
 is the cell drivable to the interesting finish F
.
Since in each iteration, there are at least one new non-winning node identified, and there are at most O(R×C)
 nodes in the graph, thus we have at most O(R×C)
 iterations. And in each iteration, a O(R×C)
 BFS/DFS is run. Thus the time complexity of this solution is O((R×C)2)
. This must be done separately for each interesting finish, hence the total time complexity for finding all the drivable pairs is O((R×C)2×num_of_finishes)

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round D - Coding Competitions Farewell Rounds

Practice mode

question_answer
Old Gold (5pts, 17pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:29

PROBLEM
ANALYSIS
Analysis
Test Set 1
For a given gold placement, we can consider what the markings would look like if they were all known. For example, if two consecutive gold nuggets are 3 kilometers apart, the expected pattern of markings would be o<>o. If they are 6 kilometers apart, the expected pattern would be o<<=>>o, and if they are 1 kilometer apart, it would be simply oo.

Because each of the expected patterns only depends on the positions of the two gold nuggets to the left and right, our solution can involve repeatedly adding a new gold nugget to the east and checking that all of the markings between the new gold nugget and the previous gold nugget match the expected pattern. We can make this algorithm efficient with dynamic programming by defining dp(i)
 as the number of gold placements for the first i
 kilometers that have a gold nugget at the i
-th kilometer.

dp(i)
 is calculated as the sum of dp(j)
 for all (j<i)
 where the markings between kilometer j
 and i
 are valid, plus 1
 if i
 can be the first gold nugget on the road. We can check if the markings are valid by iterating over all positions between j
 and i
 and checking if each one is either (.) or the expected marking of (<), (=), or (>). Also, the positions j
 and i
 themselves must have a (.) or (o).

We also need to specially handle the parts of the road to the west of the first gold nugget and to the east of the last gold nugget. All markings to the west of the first gold nugget must be (.) or (>). Similarly, our final answer will be the sum of dp(i)
 for all i
 where all markings to the east are (.) or (<).

In the solution above, we calculate the value of each dp(i)
 by iterating over all positions for the previous nugget and checking if the markings in between are valid. This is O(n2)
, where n
 is the length of S
. Applying this to each element of the dynamic programming table leads to a final time complexity of O(n3)
.

Test Set 2
We will solve Test Set 2 by speeding up the calculation of each dp(i)
. Let's use Sx
 to denote the marking at position x
, with S0
 being the first marking. Also, let's use p
 to denote the position of the last known (<, =, >, or o) marking before i
, or −1
 if there is no known marking before i
. This means that Sj
 is (.) for all (p<j<i
). We need to find all positions j
 that are valid and part of our summation for dp(i)
. We can make the following observations:

All positions (p<j<i
) are valid since there are only (.)s in this range.
If Sp
 is (o), then its position j=p
 is valid.
If Sp
 is (<), then it can be part of the left half of the pattern between two gold nuggets. The range of valid positions extends to the left of p
 as long as the markings are (.) or (<) and the center of the pattern has not passed p
. More precisely, let q
 be the largest position less than p
 where Sq
 is (=), (>), or (o), or −1
 if there is no such position, and let r
 be the leftmost position for the gold nugget that keeps the center of the pattern to the right of p
. In this case, r=p−(i−(p+1))=2p−i+1
. Then all positions (max(q,r−1)<j<p
) where Sj
 is (.) are valid. If Sq
 is (o), then j=q
 can be valid as well as long as q≥r
.
If Sp
 is (=), then it can be the center of the pattern between two gold nuggets, as long as the markings that will end up on the left side of the pattern are (.) or (<). More precisely, let q
 again be the largest position less than p
 where Sq
 is (=), (>), or (o), or −1
 if there is no such position. Let r
 be the position of the gold nugget that will result in the center of the pattern being at p
. In this case, r=p−(i−p)=2p−i
. Then j=r
 is valid as long as r>q
, or if Sq
 is (o) and r=q
.
If Sp
 is (>), then it can be part of the right half of the pattern between two gold nuggets. As such, the gold nugget cannot be anywhere to the left of p
 that causes the center of the pattern to be to the right of p
. More precisely, let r
 be the rightmost position for the gold nugget with the center of the pattern to the left of p
, so r=p−1−(i−p)=2p−i−1
. Then (r<j<p
) is not valid.
Observations 2, 3, and 4 all impose a limit on the lowest possible values for j
, but observation 5 does not. This means that we can traverse backwards through S
 applying observation 5 for each (>) until we reach a (o), (<), (=), at which point the limits from observations 2, 3, and 4 take effect, or until we reach the start of the road.

We need to make one more observation about the number of contiguous ranges of positions for j
 that are valid. Observation 5 causes the next range of valid positions to be double the distance from the current (>) to i
. Let's suppose that the position of the last (>) is l
 kilometers to the west of i
. If we were trying to maximize the number of contiguous ranges of valid positions, we would place the next (>) at a position 2l+2
 kilometers west of i
 so that there is a valid range of length 1 at 2l+1
 kilometers west. Similarly, we would place the following (>) at a position 2(2l+2)+2=4l+6
 kilometers west of i
. Continuing this logic and ignoring the constant term in the equation, we get that having k
 ranges of valid positions requires n>2kl
 kilometers. Therefore, k<logn−logl
, which is O(logn)
 even if l=1
!

Our resulting algorithm will iterate over all ranges of valid positions for j
 and calculate the sum of dp(j)
 in that range. We can calculate this sum for a single range in O(1)
 time complexity using a prefix sum array. This prefix sum array should be updated as we calculate each dp(i)
. Our target overall time complexity will be O(nlogn)
.

Implementation
To aid in the implementation of the observations above, we can precalculate the positions of the last marking for each kilometer on the road, for various combinations of marking types. In particular, we can precalculate the largest j<i
 such that Sj
 is (>) for each possible i
. This would be needed to calculate observation 5. We can also precalculate similar arrays for the last instance of (o), (<), or (=) to choose between observations 2, 3, and 4, and the last instance of (o), (=), or (>) for the calculation of observation 3 or 4.

Unfortunately, simply traversing through each (>) until we reach observation 2, 3, or 4 could be O(n)
 if there are many (>)s. However, if p
 is the position of the current (>), we know that there are no valid positions between r=2p−i−1
 and p
, so we can simply skip to the leftmost (>) in that range, as long as we don't skip past any (o), (<), or (=). We can precalculate an array with the next (>) for each position i
 for this purpose. Even though we have not travelled our expected 2k
 kilometers, we will reach that point in the next iteration anyway because there were no more (>)s before the one we chose.

Also note that, when calculating observation 3, we can use the sum of all values for j
 in (max(q,r−1)<j<p
), even if some of the positions have a <, because those positions cannot have a gold nugget and the corresponding dp
 value will be 0
.

Finally, note that the range of invalid positions in observation 5 between r=2p−i−1
 and p
 can cause an upper bound on the valid positions when applying observations 2, 3, or 4. An example of this is o.>.i, where the (o) cannot be a valid j
 when calculating dp(i)
.

Since none of these implementation details worsens the time complexity, our final algorithm is O(nlogn)

Alternative Implementation
TODO: Add explanation of alternative implementation idea if desired.

Test Data
save_alt
info We recommend that you practice debugging solutions without looking at the test data.



arrow_back
Round D - Coding Competitions Farewell Rounds

Practice mode

question_answer
Ring-Preserving Networks (5pts, 17pts)

Practice Submissions
You have not attempted this problem.
Last updated: Apr 17 2023, 09:29

PROBLEM
ANALYSIS
Analysis
For each testcase in this interactive problem, we are first given two integers C
 and L
. First we send the judge a design, G
, that contains exactly C
 computers and L
 links, then the judge sends back another design H
 which is almost identical to the previous design G
, except that the IDs are randomly permuted. At the end, we are required to find out a ring on H
.

However, it's difficult to efficiently find out a ring on an arbitrary design. It's actually equivalent to the Hamiltonian cycle problem , and currently there's no efficient solution for it yet.

Thankfully, we don't need to do that in this problem. We can construct G
 in such a way that finding a ring of H
 is easier. There are several heuristics to do so, such as:

Make some computers distinguishable enough to find part of the bijection between G
 and H
Make some computers sort of equivalent, so permuting their IDs doesn't matter
When C
, L
 meets some condition, construct some kinds of special designs whose ring could be efficiently found. For example, making use of Ore's theorem when L
 is sufficiently large.
We might need to combine these heuristics to build a complete strategy that applies to all possible C
, L
.

The IDs in H
 are randomly permuted, which become meaningless to us. Afterward in this analysis, the ID of a computer refers to the one in G
, that is, the ID before shuffled.

In the following approaches, we first add the links (1,2),(2,3),...,(C−1,C),(C,1)
 to form a ring R
. Then, we add the remaining links apart from R
 to fulfill the link number limit L
.

Let's define the degree of computer i
, or deg(i)
, to be the number of neighbors of computer i
. It is an important property of a computer in this problem, because it remains invariant after the ID permuted.

Test Set 1
In this test set, the number of links is at most (C+10)
. Apart from the ring R
, there are at most 10
 links remaining.

When C
 is large enough (>12
), we can put the extra links in such a way that allow us to efficiently find a ring in H
. Otherwise, C
 is suffiently small (≤12
) that we could find a ring in H
 by brute-force. These two different approaches are explained in detail in the following sections.

C>12
We could add the links in the order (1,3),(1,4),(1,5),...
. There are at most 10
 remaining links, so the last link we add is at most (1,12)
. In this case, C>12
, so all of these links (1,3),(1,4),...,(1,12)
 are not in R
.

All extra links are added on computer 1
, so computer 1
 has the greatest degree among all computers. This helps us to identify the computer 1
 in H
.

Then, we ignore the computer 1
 and all the links on it. The rest part in H
 are the links (2,3),(3,4),...,(C−1,C)
. Combine these links with computer 1
, then the ring R
 is reconstructed.

C≤12
In this case, C
 is small enough for most of the brute-force approaches.

We can add the remaining 10
 links arbitrarily, and find a ring via plain backtracking, or the O(C2×2C)
-time algorithm for Hamiltonian cycle.

Test Set 2
We add the remaining links in the order
(1,2),

(1,3),(2,3),

(1,4),(2,4),(3,4),

...,
(1,C),(2,C),(3,C),...,(C−1,C)

if it's not in the ring R
 yet, until we put L
 links in total.

Let the row (1,k),(2,k),...,(k−1,k)
 be the row k
. Note that the last link in each rows is aready in the ring R
, and so do the first link in row C
. These links are only listed to clarify the order.

Let row K
 be the last row that are completely added to G
, then each pair of computers among 1..K
 is directly connected with a link.

A clique is a set of computers such that each pair of computers among them is directly connected with a link. In this case, computers 1..K
 form a clique of size K
.

Let's handle the edge cases K≤3
 and K≥C−1
 separately first.

When K≤3
, H
 is a ring with at most 2
 extra links. We can solve this case by the TS1 approach.
When K=C−1
, we can identify computer C
 as the computer with minimum degree. We first pick two neighbors of computer C
, say, computer A
 and B
, and there's a ring A→C→B→
 other computers →A
.
When K=C
, H
 itself is a clique, so each pair of computers is directly connected with a link. Any order of the C
 computers forms a ring.
Afterward in this analysis,, 3<K<C−1
.

The simpler case
If no extra links from the next row (K+1)
 were added to G
, that is, the last link is (K−2,K)
, then things are simpler.

Illustration of the simpler case.

Let's call computers 1..K
 the clique part, and computers (K+1)..C
 the ring part.

In H
, we can classify the computers into the clique part and the ring part by their degrees.

In the ring part, the only links on the computers are the links in the ring R
, so the degrees of them are all 2
. On the other hand, if the degree of a computer is greater than 2
, then it's in the clique part.

(Actually that's why we handled the case K=3
 beforehand. When K=3
, the degree of computer 2
 is also 2
, but it belongs to the clique part. This kind of confusion won't happen when K>3
.)

Then, we can identify computer 1
 and computer K
 from the clique part, since they are the only computers that connected with the ring part.

Though we can't tell which one is computer 1
 and which one is computer K
, we can start from either of them, pass along the ring part to the other, pass through the rest of the clique part in any order, and go back to the starting computer.

The resulting ring is one of the following.

1
 →
 ring part →
 K
 →
 clique part →
 1
K
 →
 ring part →
 1
 →
 clique part →
 K
The more complicated case
Now let's handle the complicated case. Part but not all computers in the next row (K+1)
 are added to G
. Since the link (K,K+1)
 is already in the ring R
, the last added link cannot be (K−1,K+1)
, otherwise the row (K+1)
 is completely added.

Illustration of the more complicated case.

In this case, we have the clique part 1..K
, the ring part (K+2)..C
, and a special computer (K+1)
.

The degrees of the computers in the ring part are still all 2
, so we can identify the ring part in H
.

Among the computers not in the ring part, computers 1
 and (K+1)
 are the only computers that connect with the ring part.

Though we can't tell which one is computer 1
 and which one is (K+1)
, the following difference helps us to distinguish the two.
The non-ring part excluding computer (K+1)
, which are the computers 1..K
, form a clique.
The non-ring part excluding computer 1
, which are the computers 2..(K+1)
, cannot form a clique, because (K−1,K+1)
 must not be added.
Now we are able to identify the clique part, the ring part, and the special computer (K+1)
, so we can obtain a ring using the same approach as the previous case.



