[Keep some private to vie for 阿里巴巴 Global Mathematics Competition $10000]

----------

	2025-08 Task Titled: Factorise The Input 1000ms Time Limit

I was thinking about the ECM Elliptic Curve Factorisation Method and a different task... say one is trying to factorise a number like 8856480460906759504036060944892171332717165162660899 very fast in a competitive programming setting... or way bigger up to 500 bits say.

	Competitive Programming Round Interlude Meta Hacker Cup Style 4 Hours

A


B
Contrived MathForces task where using long long passes the validation set but long double is needed for the test set. Maybe instead do one where competitors need to have or dig up an arbitrary precision C++ library class or something. Would be cooler here to ensure some off the dome Scientific Computation style computation to estimate where which precision is needed uh to hit which precision upstream could demand 10 figures of precision in some combinatorial setting where the precise precision needs to be micro managed in order to not TLE Exceed Time Limit. Another way to modify that IBM Research Ponder This 2024-11 task may be to contrive a large polynomial with degree say 20 and a solution long long of size around 10^10 or something and then somehow manipulate long doubles to get an approximation for the solution followed up with a relatively simple uh somehow try and do something with Chinese Remainder Theorem.

C
Thinking further about that classic technique where one goes from the source and target to the midway point to generate the number of paths by summing the product. Maybe there is a nice combinatorics setting to hit a MathForces where instead of the halfway point the competitors need to actually select something in some kind of cleverish way uh.

D
Maybe steal from AtCoder.

E
Harder data structure.

F
Harder data structure. Maybe steal from AtCoder.

----------

	Meta Hacker Cup 2024 Round 2 Task C

	Common n-Grams

Updated from the Master Of Computer Science task I posted in Algorithms.txt files. Given some sentences, compute common 2-grams, 3-grams, ... perhaps bounded by 5-grams.

	Naive Question

Say we draw some values from a distribution and approximate it by sorting the drawn values and then just having the pdf function on each interval be inverse the length of the interval. What desiderata does this approximation satisfy? How to best smooth that approximation in to a continuous unimodal distribution if we know the underlying is continuous and unimodal e.g. or want to approximate it as such for some good reason computationally?

	Average Of Distributions

Plot common or plausible hyperprior combinations in R2 and R3.

	Another Search

A task where 2^30 Exceeds Time Limit but binary searching for threshold via [30 choose 15] + [30 choose 7] + [30 choose 11] + ... does not.

	Optimised Searching

Task where some statistical properties lead to a special search function being optimal like testing m = [3l + r] / 4 in loop rather than the simple binary search function here m = [l + r] / 2. Note that in some streaming settings on an uncertain underlying distribution with comparison queries one might simply store the Ordered Set of values thus far, or the Ordered Set of the previous X values which appeared in the stream and functions as an acceptable approximation, and essentially execute a binary search for the rough value on the underlying approximation for the distribution thusly. It would sort of depend on the streaming setting but in another setting one might take a fast Ordered Set up to ~100000 or ~10000 values even and then store those values in order in an array for faster lookups if one expects a similar distribution over the next 10^9 values or so. Andrew Critch style exponential logarithmic posterior might work here.

	Optiver Task: read in 300000 horses, the market betting odds, our credence function upon their Winning probabilities, and compute the betting strategy portfolio maximising the expected value of logarithm of bankroll.

	Sum Of Distinct Powers

Compute the 10000000th positive integer which can be represented as a sum of distinct powers of 11, 13, 17, and 19 e.g.

	Queries

Say there exist n different normal distributions. Compose a program with m online queries to estimate the underlying mean of the means.

	Cracks Maze Puzzle

A blind bull shitter encounters Christopher The Pharmacist and walks into a dark stone labyrinth of spiritual meaningness next to a chapel with COVID-19 chimes and traces with his Left hand along the wall of the path, mapping out the entire territory with occasional jumps as needed. But he wants to cheat on the return route now that he is enlightened into the internal structure of the maze and use at most C shortcut cheats through cracking the walls of the maze. Compute the shortest minimal return path.

	Printing Out Sorted Factors Of 2^99 * 3^99 * 5^99

Consider if one is tasked with printing out the numbers or the underlying power representation. Essentially Dijkstra's on the underlying logarithm sum of relevant logs in a 3-Dimensional cubic space works in faster than 1000000 * log [1000000] time I think but it would depend and native or performant sorting algorithms on the complete string representations can also work fast in C++ or Python.

	Searching For Threshold

Task with a relatively large number of test cases and for each test case there is a hidden A in [0,1]. Querying a value B costs ~B^2 and completes the test case if and only if A <= B. Optimal querying structure? Transformation means A^2 in [0,1] isomorphism with A^2 <= B^2 at which point a strategy of simply checking power-of-2 costs upwards until clearing the A^2 threshold will cost ~4A^2 in the worst case and ~2A^2 in the best case. Still depends a lot on the a priori distribution for A. What if each query costs ~B^2+C? Could be interesting to somehow task candidates with an interesting structure a la the Google Code Jam which forces them to get within 95% and 99.9% of an optimal constant factor. In this simple linear to exponential blowup structure the minimum of [1/[1-x]+1/[x*[1-x]]]*[1/2] occurs for x=sqrt[2]-1 at 3/2+sqrt[2]~2.91421356 times the minimum cost necessary if we had oracular access to the hidden variable A. However that is not the relevant function to be minimising here I think. Literature on minimising expected cost in certain live streaming settings. Modifying live constructed cost functions over time. [Joking technical addendum: this is like the eggs dropping floor deduction task except now one is earning $1000|hour and thus the cost of the eggs is negligible compared to the counterfactual value of one's own time and labour energy units walking up and down the stairs if needed to verify the eggy outcome... perhaps one ought to hire a reliable trusty undergraduate or graduate student to schedule up such trivial tasks. The eggs would need to shatter at lower floors and not higher and we would also care about precision on approximations for shattering floors or whatever but I digress sometimes. In that particular variant one can consider walking up and dropping a variable numbers of eggs from variable heights and coming back down the stairs to count the precise number of eggs states.]

In any case this digression naturally leads to the formal discrete integral and telescoping sum hmm...

https://www.wolframalpha.com/input?i=minimise+a%2Bb%2Bc%2Bd%2Be%2Bf%2Bg%2Bh-%28a*b%2Bb*c%2Bc*d%2Bd*e%2Be*f%2Bf*g%2Bg*h%2Bh*i%29+subject+to+0%3Ca%2C+a%3Cb%2C+b%3Cc%2C+c%3Cd%2C+d%3Ce%2C+e%3Cf%2C+f%3Cg%2C+g%3Ch%2C+h%3Ci%2C+i%3C1

https://www.wolframalpha.com/input?i=minimise+a%2Bb%2Bc-%28a*b%2Bb*c%2Bc*d%29+subject+to+0%3Ca%2C+a%3Cb%2C+b%3Cc%2C+c%3Cd%2C+d%3C1

Partitioned Into By:
0 ... x1 x2 x3 ... 1
x1*(x1)+(x2-x1)(x2+x1)+(x3-x2)(x3+x2+x1)... +(1-xn)(1+...+x1) =
x1+x2(1-x1)+x3(1-x2)+...+1(1-xn)=
x1+x2+x3+...+xn+1-(x1x2+x2x3+...+xn-1xn+xn)

Minimise

x1+x2+x3+...+x[n-1]-(x1x2+x2x3+...+x[n-1]xn)

And voila of course it turns out that minimising this integral if it was like uniform on [0,1] could cost 1 on average for either the [0,1] case or the [0,a,1] partition case means roughly 2x as much as the A captured in the long run is clearly superior in the 1-Dimensional finite bounded case here so we need to construct more interesting such tasks.

	Does Nonrandom Walk Hit Target Point?

Task where somehow it is not a random walk... I want competitors to realise that it is advantageous to check partway through and exit a test case early but in a reasonable fashion, not every single iteration. For example, for a 1-Dimensional Random Walk of length 1000000 one can easily check a threshold around iteration 999202 and potentially easily cut down runtime by handwaved ~0.001 is non trivial.

	CodeMajmunce

At some point we need a CodeMajmunce catching the Yellow Gold bananas and Saving The World.

	2-Dimensional Snake

There exists a big beautiful hairsy tigress cow at [c,d] in a grid from [0,0] to [a,b]. Count the number of Hamiltonian paths terminating at the tigress cow [c,d] which cover the entire grid e.g. ways a massive Python could snake from the tigress cow and cover each grid space.

	String

Some task about querying perhaps some pseudorandom queries up front to discover some information and deduce some property of an underlying mystery string and the task has to do with producing an algorithm with very low or near optimal linear expected runtime to halt with sufficiently low error.

	"Little Monkey Man" "Autoboto" Counting Grains Of White Rice

Perhaps task is related to estimating the number of grains in a huge batch of White rice where somehow we discover or comment on size and weight distribution or discover evidence and also perhaps have an auxiliary mechanism for bucketing grains of White rice by size.

	Prove That P ≠ NP

This will demonstrate production of the 200 IQ poo poo poo poo. "Does anybody make real shit any more?" 'bout to go smart, go ape shit. Numb on God's thumb with Zer0 to $10 millie on Mark Zuckerberg's Gold Medal like a SuperUberHero.

	Integral Approximation

Approximate x = integral blah. Answers in [x/10,10x] will be Accepted.

	Accumulation Summation

Bessie is pooing again. She has decided to walk in a rows of poo poo and each row has b poos. The weight of the poos in each row are given by the formula for the cth poo in the row being (d+c)^e*(f)^[c+g]. Compute the sum of the weights of the poo poo in each row.

	Holy Crap

Bessie the cow wombat hybrid has taken a poo poo. It is a subset of an n x n x n cubes such that beneath every poo cube is another poo cube or the ground. Also, every poo column height is at most m different from each of its neighbour columns' heights. And the edges must have height <=m. How many such poo poos can Bessie take? Variant: what if each poo cube must have a trenbolone acetate metabolites level between 1 and n and this must be decreasing up each column?

	Loopy

I wanted to compose a task where contestants had to choose a good conditional prior to executing some other code so that only efficient implementations would be accepted, and others would be given the time limit exceeded verdict. But I do not know if this is possible without also forcing accepted programs to be heavily optimised, -O3 tuning, etc. Another idea would be to present a task where for inputs under some threshold value, one algorithm is faster, and for inputs over that threshold value another algorithm is faster. It would humour me to compose an ICPC task which returns a ton of time limit exceeded verdicts and trolls the competitors participants.

	Determine The Determinants

Given a matrix A and a vector B your task is to compute the determinants of the matrices which result from replacing each column of A with B.

	Apples And Buckets

There are n buckets of indistinguishable apples. Your task is to count the number of ways you can select some number of apples in a range.

Input

The first input line has the integer n,a,b: the number of buckets, the minimum allowed, and maximum allowed number of apples.

The next line contains n integers c1,c2,...,cn: the number of apples in each bucket.

Output

Print one integer: modulo 1000000007, the number of ways to select between a and b apples inclusive e.g. the number of n-tuples of nonnegative integers (d1,d2,...,dn) such that 0<=di<=ci and a<=sum di<=b.

Constraints

0<=n<=100000
0<=a,b<=100000
1<=ci<=100000
1<=sum ci<=500000

Example

Input:
4 3 5
2 3 4 5

Output:
90