2022-01-26

Interesting lecture today. We observed a variety of simple R functions to output simple statistics about datasets and then a variety of graphics and visualisations. Scatter plots, lines, clusters, point sets, box plots, all sorts of stuff really it was pretty lit. A lot of tasks and questions today on some critical things. There was a task about mpg and gear and so I observed what one may expect which is that higher gear means lower mpg because gasoline usage going up hills e.g. There was a task about observing a linear drift trend. And it really becomes very clear that the human brain really is adept at the task of comprehending visualisations whereas I chimed in if we had seen some raw numbers it would not have been so clear, especially if there was an underlying cyclic or seasonal effect. And later there was one such a graph. There were also 2 relatively flat ones which may have had some degree of anti correlation in fact and taking a deltas differences vector could reveal if the derivatives were often like negatives of eachother. A really funny task on a trend from 2005 to 2008 where the question was what caused this trend in personal saving. And a ton of kids had no clue that they had no clue what that metric even means because they said 2008 financial crisis caused the trend from 2005 to 2008! Come on kids your epistemics are wack and data science is often about at least half understanding the data! I quoted an NBC article on the matter of culture and housing spending. They got the causality all wrong! He showed a histogram with area normed to 1 and I chimed in that this could be seen as an approximation for the probability density/mass function when the integral/area discrete is 1. Et voila! And I look forward to him posting an R file to execute and manipulate a little bit to produce some more such visualisations.

2022-01-31

So we find ourselves back in class of course again on mute for a little of the old stimulation. The reason I go to class is because the instructor has a very mild points based incentive to do so and I execute his R code on Kaggle. We see a unimodal what appears to be a low degree polynomial curve on top of a histogram. I point out that in the degenerate cases of a histogram if the bin width is very large we just see 1 rectangle which captures all the points and if it is very narrow then we just see point mass same height rectangles, 1 for each point. We saw a line on a histogram and what is precisely algorithmically happening under the hood here is critical. Because I want to know what it is that I am executing and choosing to expose myself to in executing a visualisation. In any case it gets me wondering just what precisely was the prior over all such functions which got updated and how to produce that. So I look up documentation for the ggplot geometric distribution function which was called and learn that it is some Kernel density estimation.

"
In statistics, kernel density estimation (KDE) is a non-parametric way to estimate the probability density function of a random variable. Kernel density estimation is a fundamental data smoothing problem where inferences about the population are made, based on a finite data sample. In some fields such as signal processing and econometrics it is also termed the Parzenâ€“Rosenblatt window method, after Emanuel Parzen and Murray Rosenblatt, who are usually credited with independently creating it in its current form. One of the famous applications of kernel density estimation is in estimating the class-conditional marginal densities of data when using a naive Bayes classifier, which can improve its prediction accuracy.
"

So this is some really interesting topic to be touching upon here but he seems to like these violin plots vertically whereas I might like to see them horizontally in my final project. He has yet to produce a homework file for us but he manages to produce massive massive massive pedagogical R files so we seeing all sorts of pretty colours and things and frankly the R language is really OK for some purposes and use cases. In any case we go through multiple example R executions in Kaggle and confirm via textual message that we are in fact engaged in such executions and produce light not insightful commentary on the tables and visualisations therein. He then produces a network visualisation and another one with pretty colours and I am firstly a little sad in myself for never really explicating in writing clearly and precisely or noting things about the visualisations I have seen in the wild. In any case it evokes that Google Games puzzley task like the old Facebook game with words where you type in a word to unlock that vertex and then keep on typing in related words until you unlock the whole graph. Man what a fun game for me back when I was a kid with the old 8 letters in search of a word game as well prior to memorising words lists. This dude is a visual flexer man multiple world maps wow it really is whoop dee whoop man visualisations are pretty litty lit lit but we finna get back to that Kernel density estimation thought a little later.

2022-02-02

"So it can be useful, it can tell us about effect sizes, medical drugs, mainstream science. It can help us forecast and predict future data. It can help us build causal models." Predicting real estate price canonical task on inputs such as house age, distance to the nearest station, number of convenience stores, etc. Well he casually discusses prediction and inference. Subset selection, which predictor is most "important", effect sizes, etc.? Parametric methods so we can optimise the parameters to minimise the RSS for a model. He asks whether we should prefer a quadratic to a linear model and I forget if there is a standard native metric to use to decide how to prefer simplicity versus simplicity of compute versus performance here I think there is I think one exists I think that in fact just a basis expansion linear regression will account for this in the adjusted R^2 score. Clarify that low degree is usually good simple a lot of real world processes really can be well modeled and approximated by low degree polynomials, square roots, a few monomial terms, logs, etc. arbitrary basis expansion is wacky wonky keep it simple for the most part unless you have strong evidence understanding of some other relationship causal structure. OK and K-nearest neighbours and kernel regression come up again. Recall K-nearest neighbours is a simple based upon the k-nearest neighbours. And kernel regression is a sort of continuous like that the weight to each dude is related to the distance from each other dude by some kernel function so for example maybe the distance^[-2] inverse square type of metric or one of those continuous functions which nearly or does in fact smoothly continuously differentiably vanish to 0 rather rapidly. So this is like end of week 3 and we are in fact touching upon some of the key ideas as one might really expect because by the end of this course we are running the k-fold cross validation examples I already ran through lot of great content multiple flavours and variants we now like it is a little jokey feel like do 1000 implementations then level up be a real data scientist wannabe whizz know a real thing or 2 about eyeballing and manipulations and the craft and nuances involved. Recall that k=n gives the simple linear regression and eyeball some decision boundaries it is a little unclear task and again maybe recall there exist algorithms to decide upon k. So there is a bias-variance tradeoff yeah yeah and so the small k is a complex model with small bias and high variance. And the vice versa. And this has to do with stability, deltas under slightly different input values. So we see a train test set split for nonparametric. I mean Dr. Ho is kind of baiting and trolling the class here they all said prefer quadratic over linear because the RMSE is lower I clocked in and said it is literally guaranteed to always be lower so that is not a valid justification and we need to think about a variety of things, simplicity of model, ease of computation, etc. here in particular I said without the red lines we would see the blue point set and it probably would strike us as a genuine quadratic relationship but we got trolled a bit. Because he did not actually choose to simply show us the blue points which is what we might encounter in the real world practice of data science first here.

2022-02-07

Pretty solid lecture with some more maths on bias-variance tradeoff and how to select the k value in k nearest neighbours algorithm.

2022-02-09

So we discuss interaction between X variables predictors and he runs an example R linear regression like Y ~ X1 + X2 + X1*X2 which produces a very high P value for that X1*X2 term which suggests not much interaction.

The thing about the actual coefficient value that must be noted is that the actual units of the variables matter so what may appear to be a small coefficient may actually be large hence the small P value is really what matters more to check.

And a bunch of other linear regressions examples, polynomial best fitting curves, etc. with red line on blue point set plots too. Residuals. Residual plots you know kind of more flagrantly visually revealing non linearity. So another litmus test with which to justify the usage of say a quadratic model. It is a lot of the usual content from this section in my Interviews.pdf file.

2022-02-14



2022-02-16

Classification. So here today we expect to see some things on regression, binary, multiclass classification. We are interested in P[Y=k|X] where k is in [0,1,2,...,K] so K+1 classes want to output not a dummy variable which perhaps could be a maximum amongst but we want a posterior probability mass function basically. Parametric methods gee I will re read the relevant textbook chapter and textbook seems good to know brush up on all of this again. Simple linear probability models like how are we going to fit here when the actual observations data points are these discrete values. Want to produce the function which minimises error still on the continuous pre thresholded probability function. Some simple extension of the model he currently is presenting would be like a simple linear model which takes each input vector cross with each output so like a coefficient for every pair of input and dummy one hot there are K+1 like [0,1,0,0,...,0] and then some error loss function can be whatever sum of distances of whatever predicted points in K+1 dimensional space with respect to the actual outcome vertex on this K dimensional simplex hyperplane unit coordinates sum. There is a linear model on a heart disease dataset and one supposes that it is very imbalanced so the model performs well because it can just always output no heart disease and that performs well. I suggest random subset selection of the no heart disease group to produce a balanced dataset and some sharper feature detection on the heart disease positive set. Simple logistic regression I have never seen it written out quite so simply so it is a generalised linear model with I guess a sigmoid or 1- a the function can be g(z)=1/(1+e^z) or g(z)=e^z/(1+e^z) in any case "Fantastic this will always take on a value in [0,1] so that corresponds with a real probability!" Multiclass classification using softmax. And link function g(z) in terms of z which is the simple linear combination of input vectors. In this setting the behaviour outside the range of actual input values may be OK because actually this blood pressure input is biologically determined. But in some settings this could perform quite badly under likelihood estimation if the test set X values were quite different because we might have very extremal fit behaviour at those ends where real phase transitions occur or other models are needed, perhaps with less sharp asymptotic approaches to 0 and 1 if we anticipate such things.

2022-02-21



2022-02-23

Leave One Out Cross Validation is less prone to being biased and overestimating the test error rate. However it is computationally expensive. K fold cross validation so Leave One Out Cross Validation is simply the K=n case of this. Tradeoffs on computation. Recall that K=10 is a standard numeric for quite a few machine learning applications. Might depend on computations and size of dataset. If error rate does not drop much for higher degree polynomial approximations after degree d then one might simply use that degree approximation.

2022-02-28



2022-03-02

L2 Ridge Regression versus simple Least Squares. Finding the best tuning parameter lambda. Root Mean Squared Error, k-cross validation. As lambda increases, coefficients go to 0. L1 Lasso Regression example. "Kid's stuff".

2022-03-07

L1 Lasso.