\Large

\textbf{Mathematical Statistics Solutions}

1.1 \\
1. $\left( \frac{1}{2} \right)^5=\frac{1}{32}$ \\
2. $1-\frac{1}{32}=\frac{31}{32}$ \\
3. $\frac{5}{6} \cdot \frac{4}{6} \cdot \frac{3}{6} \cdot \frac{2}{6} = \frac{5}{54}$

1.2 \\
1. $[1,2]$ \\
2. $[0,1,4,\dots,n^2]$ \\
3. $[-5,-4,\dots]$ \\
4. $[0,2,4,\dots]$

1.3 \\
(a) - definition

1.4 \\
$\sum_{n=16}^{23} \binom{23}{n} (0.6)^n (0.4)^{23-n}$

1.5 \\
(e) - (a) if independent

1.6 \\
1. Sum of even indexed $\text{P}$s is $e^{-l} \cosh(l)$ \\
2. Bob as $>\frac{1}{2}$

1.7 \\
$\text{Var}[Y]=\text{E}[Y^2]-\text{E}[Y]^2=\text{E}[Y^2]=\text{E}[Y_1^2]^2=0 \cdot \frac{1}{4} + 1 \cdot \frac{1}{2} + 4 \cdot \frac{1}{4} = \frac{3}{2}$

1.8 \\
1. $[1,2,3,4,5,6,8,10,12]$ with $[1,2,1,2,1,2,1,1,1]/12$ \\
2. $\text{E}[Y] = \frac{7}{2} \cdot \frac{1}{2} + 7 \cdot \frac{1}{2} = \frac{21}{4}$ and $\text{Var}[Y]=\frac{1}{12} \cdot 1^2 + \frac{2}{12} \cdot 2^2 + \frac{1}{12} \cdot 3^2 + \frac{2}{12} \cdot 4^2 + \frac{1}{12} \cdot 5^2 + \frac{2}{12} \cdot 6^2 + \frac{1}{12} \cdot 8^2 + \frac{1}{12} \cdot 10^2 + \frac{1}{12} \cdot 12^2 - \left( \frac{21}{4} \right)^2 = \frac{497}{48}$

1.9 \\
(d) - definition

1.10 \\
$\text{E}=\frac{n}{2},\text{Var}=\frac{(n+1)^2-1}{12}$

1.11 \\
Interesting with respect to Rolling A Die Until The First $1$, I mean for example if we have this as a staircase over the unit interval $[0,1]$ then the integral area under the curve is by definition the left hand side summing over columns and is the right hand side summing over rows gives a nice visual.

1.12 \\
$\left[ 1,\frac{1}{2},\left( \frac{1}{2} \right)^2,\dots \right]$ with $\left[p,p(1-p),p(1-p)^2,\dots \right]$ so $\text{E}[Y]=\frac{p}{1-\frac{1-p}{2}}=\frac{2p}{1+p}$

1.13 \\
For $X,Y$ uncorrelated $\text{Var}[aX+bY]=\text{Var}[aX]+\text{Var}[bY]=a^2 \text{Var}[X]+b^2 \text{Var}[Y]$ so in this case we have the system of linear equations $4 \text{Var}[Y_1]+ \text{Var}[Y_2]=17$ and $\text{Var}[Y_1]+4 \text{Var}[Y_2]=5$ implies $\text{Var}[Y_1]=\frac{21}{5},\text{Var}[Y_2]=\frac{1}{5}$, and $\text{Var}[Y_1-Y_2]=\frac{22}{5}$

1.14 \\
(d) - definition

1.15 \\
$n\left( 1-\left( \frac{n-1}{n} \right)^l \right)$

\newpage

2.1 \\
1. $f(y)=cy^2 1_{[-1,1]}$ \\
2. $c=\frac{3}{2}$ \\
3. $\text{E}[Y]=\int_{-1}^1 y\frac{3}{2}y^2dy=0$ symmetry and $\text{Var}[Y]=\int_{-1}^1 y^2\frac{3}{2}y^2dy=\frac{3}{5}$

2.2 \\
$Y^2\le\frac{1}{4}$ if and only if $-\frac{1}{2}\le Y \le \frac{1}{2}$ i.e. $P[Y^2\le \frac{1}{4}]=\int_{-\frac{1}{2}}^{\frac{1}{2}}f(y)dy=\frac{17}{64}$

2.3 \\
i.e. $1-\int_{-\frac{1}{\sqrt{2}}}^{\frac{1}{\sqrt{2}}}f(y)dy=1-\frac{1}{2\sqrt{2}}$

2.4 \\
i.e. by symmetry $2\int_{\frac{1}{2}}^{2} f(y)dy=\frac{\tan^{-1}(\frac{24}{7})}{\pi}$

2.5 \\
1. $0$ definition\\
2. $0$ definition\\
3. $\int_0^y f(y)dy=1-e^{-\frac{y}{t}}$ for $y>0$ and $0$ for $y\le 0$\\
4. $e^{-\frac{1}{t}}$\\
5. $1-e^{-\frac{3}{t}}(e^{\frac{2}{t}}-1)$\\
6. $t$\\
7. $2t^2$\\
8. $t^2$\\
9. $0$\\
10. $t\ln{2}$\\
11. even dominating i.e. 0 to 1 greater than 1 to 2 etc. integral rectangle trapezoidal sums style argumentation

2.6 \\
1. $\frac{4}{(r-l)^2}$\\
2. $\text{E}[Y]=\frac{r+l}{2}$ symmetry and $\text{Var}[Y]=\frac{(r-l)^2}{24}$\\
3. $\frac{7-2\sqrt{6}}{6}$

2.7 \\
$u_n=\frac{b^{n+1}-a^{n+1}}{(n+1)(b-a)}$ and setting $a,b$ to $-\frac{b-a}{2},\frac{b-a}{2}$ one obtains $u_n^c=\frac{(b-a)^n}{(n+1)2^n}$ for even $n$ and $0$ for odd $n$

\newpage

3.1 \\
$[0,1,2]$ stair steps up from $0$ to $\frac{1}{4}$ then $\frac{3}{4}$ then $1$.

3.2 \\
(c) - definition

3.3 \\
Inverse, reflect over $x=y$

3.4 \\
1. $c=12$\\
2. $F(y)=(4-3y)y^3, S(y)=1-(4-3y)y^3$ on $[0,1]$\\
3. $[0,1], h(y)=\frac{f(y)}{S(y)}=\frac{12y^2}{-3y^3+y^2+y+1}$\\
4. $\frac{2}{3}$\\
5. $\frac{1}{2}$

3.5 \\
$F(y)=y^2, S(y)=1-y^2, h(y)=\frac{f(y)}{S(y)}=\frac{2y}{1-y^2}$

3.6 \\
(b) - definition

3.7 \\
1. $\frac{1}{e}$ integrating past the mean\\
2. $t\ln{2}$

3.8 \\
$7$

\newpage

4.1 \\
$g(y)=y^3$, $g^{-1}(w)=w^{\frac{1}{3}}$, $(g^{-1})'(w)=\frac{1}{3}w^{-\frac{2}{3}}$, $F_W(w)=1-e^{-\frac{w^{\frac{1}{3}}}{t}}$, $f_W(w)=\frac{1}{t}e^{-\frac{w^{\frac{1}{3}}}{t}}\frac{1}{3}w^{-\frac{2}{3}}$

4.2 \\
$g(y)=e^y$, $g^{-1}(w)=\ln{w}$, $(g^{-1})'(w)=\frac{1}{w}$, $\text{E}[W]=\int_0^1 e^ydy=e-1$, $F_W(w)=\ln{w}$, $f_W(w)=\frac{1}{w}$

4.3 \\
1. $g(y)=y^3$, $g^{-1}(w)=w^{\frac{1}{3}}$, $(g^{-1})'(w)=\frac{1}{3}w^{-\frac{2}{3}}$, $f_W(w)=w^{\frac{1}{3}}\frac{1}{\sqrt{2\pi 0.1^2}}e^{-\frac{(w^{\frac{1}{3}}-1)^2}{2\cdot 0.1^2}} \frac{1}{3}w^{-\frac{2}{3}}$ \\
2. $1.1^{\frac{1}{3}}\approx 1.03228$, z-score $\approx 0.3228$, P $\approx 0.3734$

4.4 \\
(b) - $g(y)=y^2$, $g^{-1}(w)=w^{\frac{1}{2}}$, $(g^{-1})'(w)=\frac{1}{2} w^{-\frac{1}{2}}$, $f_W(w)=\frac{2+3w^{\frac{1}{2}}}{16}\frac{1}{2} w^{-frac{1}{2}}$

4.5 \\
$g(y)=my^2$, $g^{-1}(w)=(\frac{w}{m})^{\frac{1}{2}}$, $(g^{-1})'(w)=\frac{1}{2}(\frac{w}{m})^{-\frac{1}{2}} \frac{1}{m}$, $f_W(w)=\text{blahblahblah}$

4.6 \\
(b) - $g(y)=-\frac{1}{2}\ln{y}$, $g^{-1}(w)=e^{-2w}$, $(g^{-1})'(w)=-2e^{-2w}$, $f_W(w)=2e^{-2w}$

4.7 \\
(b) - integral of $pdf\cdot f$

4.8 \\
(b) - $g(y)=\frac{1}{y^2}$, $g^{-1}(w)=(\frac{1}{w})^{\frac{1}{2}}$, $(g^{-1})'(w)=-\frac{1}{2} w^{-frac{3}{2}}$, $f_W(w)=$(b)

4.9 \\
(d) - $g(y)=\frac{1}{y^2}$, $g^{-1}(w)=(\frac{1}{w})^{\frac{1}{2}}$, $(g^{-1})'(w)=-\frac{1}{2} w^{-\frac{3}{2}}$, $f_W(w)=$(d)

4.10 \\
(c) - same as 11 by symmetry

4.11 \\
(c) - $g(y)=y^2$, $g^{-1}(w)=w^{\frac{1}{2}}$, $(g^{-1})'(w)=\frac{1}{2} w^{-frac{1}{2}}$, $f_W(w)=$(c)

4.12 \\
$g(y)=\frac{235.24}{y}$, $g^{-1}(w)=\frac{235.24}{w}$, $(g^{-1})'(w)=-\frac{235.24}{w^2}$, $f_W(w)=\frac{1}{20} \frac{235.24}{w^2}$, $E=\int_{10}^{30} \frac{1}{20} \frac{235.24}{y}dy \approx 12.922$

4.13 \\
$g(y)=-\ln{y}$, $g^{-1}(w)=e^{-w}$, $(g^{-1})'(w)=-e^{-w}$, $f_W(w)=e^{-w}$

\newpage

5.1 \\
1. $[0,1,2,3]$ with $[\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}]$ \\
2.\\
$
\begin{bmatrix}
\text{ } & 0 & 1 & 2 & 3 \\
0 & \frac{1}{8} & \frac{1}{8} & 0 & 0 \\
1 & \frac{1}{8} & \frac{1}{8} & 0 & 0 \\
2 & 0 & 0 & \frac{1}{8} & \frac{1}{8} \\
3 & 0 & 0 & \frac{1}{8} & \frac{1}{8}
\end{bmatrix}
$
\\
3. No - definition

5.2 \\
(b) - definition

5.3 \\
(c) - definition in particular the mean of $W$ is $1\cdot 1+\frac{1}{2}\cdot 2+\frac{1}{3}\cdot 3=1$ and the variance is $1\cdot 1+\frac{1}{4}\cdot 4+\frac{1}{9}\cdot 9=3$

5.4 \\
Geometric probability the distribution is uniform on the unit square and the success region gives $\frac{3}{4}$

5.5 \\
(b) - definition

5.6 \\
1. $f(y_1,y_2)=\frac{1}{t_1}e^{-\frac{y_1}{t_1}} \frac{1}{t_2}e^{-\frac{y_2}{t_2}}$ \\
2. $\frac{t_1}{t_1+t_2}$ by integrating or simply considering the ratio of annihilation at each point in time

5.7 \\
1. No, if we know that $y_1\approx 1$ for example our posterior on $y_2$ is very precise around $0$ i.e. this is clearly not a product of $2$ distributions \\
2. No, $f(y_1)=\frac{1}{\pi} \cdot 2(1-y_1^2)^{\frac{1}{2}}$ and similar by definition \\
3. Geometric probability the success region is symmetry on $1$ side of the line $x=y$ so $\frac{1}{2}$ \\
4. $\frac{1}{2}$

5.8 \\
(d) - definition $\frac{2}{3}\cdot \frac{2}{3}+\frac{1}{3}\cdot \frac{1}{3}=\frac{5}{9}$

5.9 \\
1. $c=\frac{8}{3}$ \\
2. $\frac{4}{5}$ and $\frac{56}{45}$ \\
3. $\frac{28}{27}$ \\
4. $\frac{28}{27}-\frac{4}{5}\cdot\frac{56}{45}=\frac{28}{675}\neq 0$, No

5.10 \\
1. $f(y)=ye^{-y}$ - definition integral\\
2. one could certainly log transform and then find the pdf of $\ln{y_2}-\ln{y_1}$ and then exponentially transform back

\newpage

6.1 \\
(e) - definition the underlying distribution of $Y$ is $[1,2,3,4]$ with $[0.1,0.2,0.3,0.4]$

6.2 \\
1. $\frac{1}{1-\frac{1}{2} t}$ exponential $\tau=\frac{1}{2}$ \\
2. Poisson $\lambda=2$ \\
3. $N(-2,2^{\frac{1}{2}})$ \\
4. $\frac{\frac{1}{3}}{1-\frac{2}{3} e^t}$ Geometric $p=\frac{1}{3}$ \\
5. Binomial $(2,\frac{2}{3})$ \\
6. Uniform $[-1,0]$ \\
7. $[-1,4]$ with $[\frac{3}{4},\frac{1}{4}]$

6.3 \\
(b) - definition

6.4 \\
Sum of $3$ independent geometric with $p=\frac{1}{2}$ so $\text{Var}[Y]=3 \frac{1-p}{p^2}=6$

6.5 \\
(c) - definition the $n$ variances each scaled $\frac{1}{n^2}$

6.6 \\
1. Binomial $(k,p)$ \\
2. Binomial $(n,p)$ \\
3. Poisson $P(\lambda)$

6.7 \\
1. definition \\
2. $(1-2t)^{-\frac{1}{2}}$ \\
3. $(1-2t)^{-1}$ Exponential $\tau=2$

6.8 \\
1. $\alpha=(\frac{1-p}{p})^{\frac{1}{2}}$ \\
2. Normal \\
3. Normal approximation to Binomial

\newpage

7.1 \\
1. direct sum or Normal approximation \\
2. direct $P$ of $0$ or Poisson approximation

7.2 \\
(d) - definition

7.3 \\
(c) - definition

7.4 \\
(d) - definition

7.5 \\
(d) - definition

7.6 \\
(d) - definition

7.7 \\
(a) - definition

7.9 \\
(e) - definition

7.10 \\
1. factor in the $\frac{1}{2} m$ and $\sigma^2$ so $\Gamma (\frac{3}{2}, \sigma^2 m)$\\
2. $\Gamma (\frac{3n}{2}, \sigma^2 m)$

7.11 \\
1. $(n-1)!$\\
2. $\frac{1}{\Gamma (k) \tau^k}$\\
3. Yes

\newpage

9.1 \\
(e) - definition

9.2 \\
(e) - definition

9.3 \\
(c) - definition

9.4 \\
1. Yes. $\text{Var}[\hat{\mu}]=\frac{\sigma^2}{2}$

9.5 \\
1. $3$ and No \\
2. $\text{Var}[\hat{\theta}]=(\frac{3}{n})^2\cdot n\cdot \frac{\theta^2}{12}=\frac{3\theta^2}{4n}$

9.6 \\
(c) - $\text{bias}(\hat{\theta})=\text{E}[\hat{\theta}]-\theta=c\frac{\theta}{2}-\theta=\theta(\frac{c}{2}-1)$ and $\text{Var}[\hat{\theta}]=\frac{c^2}{n}\frac{\theta^2}{12}$ and (c) from minimizing the relevant sum

9.7 \\
(b) - definition the Variance is constant and minimizes to unbiased

9.8 \\
(b) - definition all unbiased Variance order

9.9 \\
(c) - This is the unbiased estimator for Variance in the known mean setting whence it suffices to compute $\text{Var}[\hat{\theta}]$ sum or $\Gamma$ i.e.

9.10 \\
1. $2$ and $\frac{3}{2}$\\
2. $\frac{\theta^2}{3}$ and $\frac{\theta^2}{8}$\\
3. $\hat{\theta_3}$ of course strictly dominating

9.11 \\
1. $0$ and $\tau$ definition symmetry \\
2. Yes \\
3. sum or $\Gamma$ i.e. $\frac{\tau^2}{n}$

\newpage

10.1 \\
1. $\Gamma (n,2)$ Yes

10.2 \\
just do it

10.3 \\
just do it

10.4 \\
just do it

10.5 \\
(c) - definition

10.6 \\
(a) most likely

10.7 \\
just do it

10.8 \\
just do it

10.9 \\
1. by definition the cdf is $(\frac{y}{\theta})^n$ whence pdf is $\frac{ny^{n-1}}{\theta^n}$\\
2. multiply by $\theta^n$ so $y^n$ and $ny^{n-1}$ on the interval $[0,1]$ of course \\
3. just do it

10.10 \\
1. just do it \\
2. just do it \\
3. just do it

\newpage

11.1 \\
1. i.e. maximize $e^{-n\theta}\theta^{\sum y}$ at $\theta=\frac{\sum y}{n}$\\
2. maximize $\sum (\ln{y}-\theta)^2=\sum \theta^2-2\ln{y}\theta=n\theta^2-2\theta(\sum \ln{y})$ at $\theta=\frac{\sum \ln{y}}{n}$\\
3. just do it \\
4. just do it probably biased the MLE being some value below the sample whence its expected value is less than the true mean

11.2 \\
(e) - 1-(d)

11.3 \\
(d) - definition

11.4 \\
(e) - mean

11.5 \\
1. indeed can be converted into sum into product of $f$ terms exponent \\
2. indeed the overall $P$ is based only on total number of $0$s and $1$s which this counts implicitly \\
3. well the product of these $f$ terms decomposes precisely into these dudes by log sum product and the other dudes can go in $h$ for example without reference to $\beta$

11.6 \\
(c) - can't be mapped into the requisite sum in the exponent in the product

11.7 \\
(d) - definition if cleared then uniform random point in that space $\frac{1}{\theta^n}$

11.8 \\
2. 11.3 (d) \\
3. Yes, definition

11.9 \\
1. Mean and as in 11.3 (d) for Variance\\
2. Mean is unbiased, Variance is biased\\
3. immediate via hint i.e.